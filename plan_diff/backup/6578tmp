		176687 	[virtual networks]Cloned domain wfith macvtap NIC - bug 786648 	yupzhang 	yupzhang 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    Regression

bug:

    786648 - From Run 54159

Actions:

1. Create and install a guest.
 #virsh define test
 #virsh start test
2. Shutdown the guest after installation.
#virsh destroy test.

3. Change the guest's NIC.
#virsh edit test
... 
 <interface type='direct'>
      <mac address='52:54:00:df:f6:0d'/>
      <source dev='eth0' mode='vepa'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>
...
OR
Use virt-manager,update NIC,
        Source device: Host device eth0 : macvtap
        Device model: Hypervisor default
Click Apply.

4. In virt-manager ,click "Clone".

5. After cloning finished, try to start cloned guest.
#virsh start test-clone

	
Expected Results:

5. The guest can be started successfully.
Notes:
Comments:

		176692 	[miscellanea] Clear prompt should be seen when specify a non-existent image file in domain config file - Bug 591363 	jialiu 	jialiu 	Manual 		Regression 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh cmd

bug:

    No bug found

Actions:

1. Specify a non-existent image file in domain config file
e.g.
   <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/demo1.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
The image file - '/var/lib/libvirt/images/demo1.img' is not existent in host.
2. Try to start domain
# virsh start demo

	
Expected Results:

2. Clear prompt should be seen. E.g:

# virsh start demo
error: Failed to start domain demo
error: cannot resolve symlink /var/lib/libvirt/images/demo1.img: No such file
or directory    

Notes:
Comments:

		176743 	[virtual networks] can't add new netdevs after most recently added netdev is detached  BZ#827544 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

you must use RHEL5 guesft to reproduce and test this case
	fa
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. install a RHEL5 guest named "r5"

 

2. # virsh attach-interface r5 network default --model virtio --mac 52:54:00:12:34:56Interface attached successfully

 

# virsh detach-interface r5 network --mac 52:54:00:12:34:56
Interface detached successfully



# virsh attach-interface r5 network default --model virtio --mac 52:54:00:12:34:56
error: Failed to attach interface
error: internal error unable to execute QEMU command 'device_add': Duplicate ID 'net1' for device


	
Expected Results:

Expected :

 

no error like:

# virsh attach-interface r5 network default --model virtio --mac 52:54:00:12:34:56
error: Failed to attach interface
error: internal error unable to execute QEMU command 'device_add': Duplicate ID 'net1' for device

Notes:
bug is closed NOTABUG
Comments:

		176745 	Expose 'virDomain{Get,Set}InterfaceParameters' APIs in python binding BZ#770971 	whuang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

Run the following codes in interactive python:

import libvirt
con = libvirt.open(None)
dom = con.lookupByName('foo')
dom.interfaceParameters('vnet0', None, None, 0)
dom.setInterfaceParameters('vnet0', None, None, 0)

	
Expected Results:

NO  results like :

Traceback (most recent call last):
  File "test.py", line 4, in <module>
    print dom.interfaceParameters('vnet0', None, None, 0)
  File "/usr/lib64/python2.6/site-packages/libvirt.py", line 635, in
interfaceParameters
    ret = libvirtmod.virDomainGetInterfaceParameters(self._o, device, params,
nparams, flags)
AttributeError: 'module' object has no attribute
'virDomainGetInterfaceParameters'


Traceback (most recent call last):
  File "test.py", line 5, in <module>
    print dom.setInterfaceParameters('vnet0', None, None, 0)
  File "/usr/lib64/python2.6/site-packages/libvirt.py", line 1140, in
setInterfaceParameters
    ret = libvirtmod.virDomainSetInterfaceParameters(self._o, device, params,
nparams, flags)
AttributeError: 'module' object has no attribute
'virDomainSetInterfaceParameters'

Notes:
Covered in test-API.
Comments:

		176746 	[libvirtd] memory leak when performing persistent network device update (e.g. virsh domif-setlink --persistent)BZ#802854 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

download valgrind  or use yum

https://brewweb.devel.redhat.com/packageinfo?packageID=1193
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1 use valgrind to start libvirtd:
# service libvirtd stop
Stopping libvirtd daemon:                                  [  OK  ]

valgrind --leak-check=full libvirtd >& /root/valgrind.log 2>&1

2 excute domif-setlink command and check valgrind log:
# virsh domif-setlink vr-rhel6-x86_64-kvm 52:54:00:2f:e1:f7 down --persistent
Device updated successfully

# service libvirtd stop
Stopping libvirtd daemon:                                  [  OK  ]

3) # cat /root/valgrind.log
...

	
Expected Results:

3)
No memory leak 

==7475==    definitely lost: 0 bytes in 0 blocks
==7475==    indirectly lost: 0 bytes in 0 blocks
==7475==      possibly lost: 4,048 bytes in 11 blocks
...

Notes:
Comments:

		176747 	memory leaks/dangling pointers caused by virDomainDetachDeviceConfig (virsh detach-*) BZ#802851 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Install   valgrind
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

# valgrind --check-leak=full virsh detach-disk vr-rhel6-x86_64-kvm vdb --persistent
...
==10592== LEAK SUMMARY:
==10592==    definitely lost: 0 bytes in 0 blocks
==10592==    indirectly lost: 0 bytes in 0 blocks
==10592==      possibly lost: 0 bytes in 0 blocks
==10592==    still reachable: 127,911 bytes in 1,362 blocks
==10592==         suppressed: 0 bytes in 0 blocks
...

# valgrind --leak-check=full virsh detach-interface vr-rhel6-x86_64-kvm network 52:54:00:2f:e1:f7 --persistent
...
==10592== LEAK SUMMARY:
==10592==    definitely lost: 0 bytes in 0 blocks
==10592==    indirectly lost: 0 bytes in 0 blocks
==10592==      possibly lost: 0 bytes in 0 blocks
==10592==    still reachable: 127,957 bytes in 1,362 blocks
==10592==         suppressed: 0 bytes in 0 blocks
...

# valgrind --check-leak=full virsh detach-device vr-rhel6-x86_64-kvm /root/disk.xml --persistent ...
==10592== LEAK SUMMARY:
==10592==    definitely lost: 0 bytes in 0 blocks
==10592==    indirectly lost: 0 bytes in 0 blocks
==10592==      possibly lost: 0 bytes in 0 blocks
==10592==    still reachable: 126,996 bytes in 1,362 blocks
==10592==         suppressed: 0 bytes in 0 blocks
...

	
Expected Results:

No memory leak  comt out
Notes:
Comments:

    #1 dyuan@redhat.com 2012-07-02 16:48:18
    moved to virsh command plan 6577.

		176748 	Syscall param rt_sigaction(act->sa_flags) points to uninitialised byte(s) BZ#814080 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

install  valgrind
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

$ qemu-img create /var/lib/libvirt/images/test 1M

$ cat > /tmp/test.xml <<EOF
<domain type='qemu'>
  <name>test</name>
  <memory>219200</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
    <boot dev='hd'/>
  </os>
  <devices>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/test'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes' listen='0.0.0.0'/>
  </devices>
</domain>
EOF

$ virsh define /tmp/test.xml
$ valgrind -v virsh blockpull test /var/lib/libvirt/images/test --wait

	
Expected Results:

Do not like this: 

==10906== 1 errors in context 1 of 1:
==10906== Syscall param rt_sigaction(act->sa_flags) points to uninitialised
byte(s)
==10906==    at 0x39CF80F5BE: __libc_sigaction (sigaction.c:67)
==10906==    by 0x43016C: cmdBlockPull (virsh.c:7638)
==10906==    by 0x4150D4: vshCommandRun (virsh.c:18574)
==10906==    by 0x425E73: main (virsh.c:20178)
==10906==  Address 0x7fefffae8 is on thread 1's stack

Notes:
Comments:

    #1 dyuan@redhat.com 2012-07-02 16:48:28
    moved to virsh command plan 6577.

		176749 	[virtual networks] Unable to determine device index for network device" when attaching new network device to a guest that already has a netdev of type='hostdev' BZ#827519 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

use SRIOV host

enable hotplug pci device

#modprobe -r kvm_intel; modprobe -r kvm; modprobe kvm allow_unsafe_assigned_interrupts=1; modprobe kvm_intel

#modprobe -r igb

#modprobe igb max_vfs=7
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    889249 - From Run 53753

Actions:

 



add this into the guest xml 


  <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:3b:3e:02'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x03' slot='0x10' function='0x1'/>
      </source>
      <alias name='hostdev0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>


[root@test ~]# virsh start whuang
Domain whuang started

[root@test ~]# virsh attach-interface whuang network default --model virtio
error: Failed to attach interface
error: internal error Unable to determine device index for network device
	
Expected Results:

No error like ,virsh cmd should succeed:


error: Failed to attach interface
error: internal error Unable to determine device index for network device
Notes:
https://bugzilla.redhat.com/show_bug.cgi?id=827519 is verified
Comments:

		176750 	[remote access] virsh connect should not disconnect current connection until a new connection succeeds BZ#829160 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

 

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access
    upstream

bug:

    No bug found

Actions:

# virsh
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit



virsh # connect test
error: Failed to connect to the hypervisor
error: no connection driver available for No connection for URI test
error: Failed to reconnect to the hypervisor

NOTE: the connect will break up after connect failed 

virsh # list
error: Failed to reconnect to the hypervisor
error: no valid connection
error: no connection driver available for No connection for URI test
error: Failed to reconnect to the hypervisor

virsh # connect

virsh # list
 Id    Name                           State
----------------------------------------------------





	
Expected Results:



Actual results(if bug is not fixed):
libvirt connection is broken after connect a bad hyperv




Expected results:
 virsh connect should not disconnect current connection until a new connection succeeds

Notes:
Comments:

		176752 	[Domain async job handling] loop to cancel a stuck migration job do not cause libvirtd hang - 821468 	bili 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Prepare tls and p2p migration environment .

Refer to case : [Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4

and case: [Migration] P2P migration using a TLS enabled URI - Bug 721411

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    async job handling
    Regression

bug:

    No bug found

Actions:

Do migrate with
# virsh migrate-setspeed mig 1; virsh migrate --live --p2p mig qemu+tls://{target ip}/system --verbose --unsafe

at the same time on another console, do
# iptables -A OUTPUT -d {target ip} -j REJECT
then do domjobabort loop
# while true; do virsh domjobabort mig; sleep 1; done

	
Expected Results:

In the migrate console will get error:

# virsh migrate-setspeed mig 1; virsh migrate --live --p2p mig qemu+tls://10.66.5.143/system --verbose --unsafe

Migration: [  1 %]error: operation aborted: migration job: canceled by client

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 2     mig                            running

 libvirtd do not hang.
Notes:
Comments:

		176773 	[console and serial devices] set escape character for console - bug 754800 845460 	gsun 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Done with case:

123994 [Console and serial devices] Connect to a guest console

Please update this case after Bug 845460 - exit console will crash libvirtd is fixed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices
    regression

bug:

    No bug found

Actions:

1. use -e with virsh to set escape character
# virsh -e ^[

2. enter console of a guest
virsh # console rhel6u2

3. escape from console using the set character.
press ctrl+[

For bug 845460:
4. check libvirtd status:
virsh # list --all

5.# service libvirtd status


	
Expected Results:

1.

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # 

2. 
Connected to domain rhel6u2
Escape character is ^[

3. 

Success exited.

 4. Works well, and should not get error:
error: Failed to list domains
error: internal error client socket is closed

5.Should running, not like:
libvirtd dead but pid file exists

 
Notes:
Comments:

		176774 	[console and serial devices] use non-root user to connect guest console - bug 700443 	gsun 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

 Bug 700443     is still assign, we need update this case after the bug is fixed .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL7
    console and serial devices

bug:

    No bug found

Actions:

TBD
	
Expected Results:
Notes:
Comments:

		176777 	[console and serial devices]forced console connection--Bug:811497 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

# cat console_deadlock.py

#!/usr/bin/python -u
import sys, os, logging, libvirt, tty, termios, atexit

def reset_term():
    termios.tcsetattr(0, termios.TCSADRAIN, attrs)

def error_handler(unused, error):
    # The console stream errors on VM shutdown; we don't care, right?
    # if (error[0] == libvirt.VIR_ERR_RPC and
    #    error[1] == libvirt.VIR_FROM_STREAMS):
    #    return
    logging.warn(error)

class Console(object):
    def __init__(self, uri, uuid):
        self.uri = uri
        self.uuid = uuid
        self.connection = libvirt.open(uri)
        self.domain = self.connection.lookupByUUIDString(uuid)
        self.state = self.domain.state(0)
        self.connection.domainEventRegister(lifecycle_callback, self)
        self.stream = None
        self.run_console = True
        logging.info("%s initial state %d, reason %d",
                     self.uuid, self.state[0], self.state[1])

def check_console(console):
    if (console.state[0] == libvirt.VIR_DOMAIN_RUNNING or
        console.state[0] == libvirt.VIR_DOMAIN_PAUSED):
        if console.stream == None:
            console.stream = console.connection.newStream(libvirt.VIR_STREAM_NONBLOCK)
            console.domain.openConsole(None, console.stream, 0)
            console.stream.eventAddCallback(libvirt.VIR_STREAM_EVENT_READABLE, stream_callback, console)
    else:
        if console.stream:
            console.stream.eventRemoveCallback()
            console.stream = None

    return console.run_console

def stdin_callback(watch, fd, events, console):
    readbuf = os.read(fd, 1024)
    if readbuf.startswith(""):
        console.run_console = False
        return
    if console.stream:
        console.stream.send(readbuf)

def stream_callback(stream, events, console):
    try:
        received_data = console.stream.recv(1024)
    except:
        return
    os.write(0, received_data)

def lifecycle_callback (connection, domain, event, detail, console):
    console.state = console.domain.state(0)
    logging.info("%s transitioned to state %d, reason %d",
                 console.uuid, console.state[0], console.state[1])

# main
uri = sys.argv[1]
uuid = sys.argv[2]

logging.basicConfig(filename='msg.log', level=logging.DEBUG)
logging.info("URI: %s", uri)
logging.info("UUID: %s", uuid)

libvirt.virEventRegisterDefaultImpl()
libvirt.registerErrorHandler(error_handler, None)

atexit.register(reset_term)
attrs = termios.tcgetattr(0)
tty.setraw(0)

console = Console(uri, uuid)
console.stdin_watch = libvirt.virEventAddHandle(0, libvirt.VIR_EVENT_HANDLE_READABLE, stdin_callback, console)

while check_console(console):
    libvirt.virEventRunDefaultImpl()

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices

bug:

    No bug found

Actions:

1. Start a guest with a serial console (the guest OS doesn't need to support
it)
2. save the script code in Setup into a file "console_deadlock.py"
3. #python console_deadlock.py qemu:///system $guset_uuid
4. #virsh console <guestname> --force

	
Expected Results:

step 3) get a console

 

step 4) if  virsh console without --force should error

# virsh console sss
Connected to domain sss
Escape character is ^]
error: operation failed: Active console session exists for this domain

if add --force virsh will get the console

verify:

open another terminal do some virsh operation

1# virsh
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list
 Id    Name                           State
----------------------------------------------------
 100   guest                         running

virsh # dumpxml guest
virsh # shutdown guest

2 check the libvirtd service was not lock

# service libvirtd status
libvirtd (pid  2511) is running...
# ps aux|grep libvirtd
root      2511  5.1  0.2 1031476 17308 ?       Sl   Oct30 197:53 libvirtd --daemon

virsh not locks up,the daemon not deadlocks.

 
Notes:
Comments:

		176787 	[CPU Management] Check KVM host/guest cpu capping isolation-bug822090 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    822090

Actions:

# virsh edit rhel6.2

<domain type='kvm'>
......
  <vcpu placement='auto'>2</vcpu>
  <cputune>
    <shares>2048</shares>
    <period>1000000</period>
    <quota>1000</quota>
    <emulator_period>1000000</emulator_period>
    <emulator_quota>1000</emulator_quota>
    <vcpupin vcpu='0' cpuset='0'/>
    <vcpupin vcpu='1' cpuset='1'/>
    <emulatorpin cpuset='2'/>
  </cputune>
.....
</domain>
Domain rhel6.2 XML configuration edited.


# virsh start rhel6.2
Domain rhel6.2 started


# virsh schedinfo rhel6.2
Scheduler      : posix
cpu_shares     : 2048
vcpu_period    : 1000000
vcpu_quota     : 1000
emulator_period: 1000000
emulator_quota : 1000

# virsh schedinfo rhel6.2 --set emulator_period=900000
Scheduler      : posix
cpu_shares     : 2048
vcpu_period    : 1000000
vcpu_quota     : 1000
emulator_period: 900000
emulator_quota : 1000


# virsh schedinfo rhel6.2 --set emulator_quota=2000
Scheduler      : posix
cpu_shares     : 2048
vcpu_period    : 1000000
vcpu_quota     : 1000
emulator_period: 900000
emulator_quota : 2000

# virsh schedinfo rhel6.2 --set emulator_period=800000 --config
Scheduler      : posix
cpu_shares     : 2048
vcpu_period    : 1000000
vcpu_quota     : 1000
emulator_period: 800000
emulator_quota : 1000

# virsh schedinfo rhel6.2 --set emulator_quota=3000 --config
Scheduler      : posix
cpu_shares     : 2048
vcpu_period    : 1000000
vcpu_quota     : 1000
emulator_period: 800000
emulator_quota : 3000

# virsh destroy rhel6.2
Domain rhel6.2 destroyed

# virsh start rhel6.2
Domain rhel6.2 started


# virsh schedinfo rhel6.2
Scheduler      : posix
cpu_shares     : 2048
vcpu_period    : 1000000
vcpu_quota     : 1000
emulator_period: 800000
emulator_quota : 3000

# cat /cgroup/cpu/libvirt/qemu/rhel6.2/emulator/cpu.cfs_quota_us
3000

# cat /cgroup/cpu/libvirt/qemu/rhel6.2/emulator/cpu.cfs_period_us
800000

 

 
	
Expected Results:

step 6 and step 9 display 1GHz  frenquency.
Notes:
Comments:

		176788 	[CPU Management] Check KVM host/guest cpu pinning isolation-bug822064.bug871312,bug870099 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    822064
    871312
    870099

Actions:

1.
# virsh start rhel6.2
Domain rhel6.2 started

2. Pin vcpu
# virsh vcpupin rhel6.2 0 0 

3. Pin emulator 
# virsh emulatorpin rhel6.2 1-3,^2

4. Check emulator pinning
# virsh emulatorpin rhel6.2
emulator: CPU Affinity
----------------------------------
       *: 1,3

5. 
# pidof qemu-kvm
18180

6. Check Cpus_allowed_list of emulator
# cat /proc/18180/task/18180/status |grep Cpus_allowed_list
Cpus_allowed_list:	1,3

7.
# virsh destroy rhel6.2 
Domain rhel6.2 destroyed

8.
# virsh emulatorpin rhel6.2 1-3 --current 

9.
# virsh emulatorpin rhel6.2
emulator: CPU Affinity
----------------------------------
       *: 1-3

10.
# virsh emulatorpin rhel6.2 1,3 --config

11.
# virsh start rhel6.2 
Domain rhel6.2 started

12.
# virsh emulatorpin rhel6.2
emulator: CPU Affinity
----------------------------------
       *: 1,3

13.
# pidof qemu-kvm
19020

14.
# cat /proc/19020/task/19020/status |grep Cpus_allowed_list
Cpus_allowed_list:	1,3

# virsh dumpxml rhel6.2
......
 <cputune>
    ....
    <emulatorpin cpuset='1,3'/>
  </cputune>
-----------------------------------------------------------------------------------------------------------------------------------------------
15.

# virsh destroy rhel6.2 
Domain rhel6.2 destroyed

16.

# virsh edit rhel6.3
......

<vcpu placement='static' cpuset='2-3' current='1'>7</vcpu>
......

17.

# virsh start rhel6.2 
Domain rhel6.2 started


18.

# virsh emulatorpin rhel6.3

emulator: CPU Affinity

----------------------------------

            *: 2-3

19. 
# pidof qemu-kvm 30119

30119 


20.
# grep Cpus_allowed_list /proc/30119/task/*/status

/proc/30119/task/30119/status:Cpus_allowed_list:	2-3

/proc/30119/task/30141/status:Cpus_allowed_list:	2-3

/proc/30119/task/30142/status:Cpus_allowed_list:	2-3

 ----------------------------------------------------------------------------------------------------------------------------------------------

21.

# virsh destroy rhel6.2 
Domain rhel6.2 destroyed

22.

# virsh edit rhel6.3
......

<vcpu placement='auto' cpuset='2-3' current='1'>7</vcpu>
......

 

23.

# virsh start rhel6.2 
Domain rhel6.2 started

24.

# virsh emulatorpin rhel6.3 1
error: Requested operation is not valid: Changing affinity for emulator thread dynamically is not allowed 
when CPU placement is 'auto'

 



 

 


 

 
	
Expected Results:

The expected result is the same as the result of command.
Notes:
1. From confirmed -> proposed. Add new test steps21-24 based on bug870099
Comments:

		176826 	[Disk hotplug] Attach virtual disks with multifunction = on 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Prepare a shutdown guest and an image

# qemu-img create /var/lib/libvirt/images/test.img 1G

2. Attach disk to guests with multifunction

# virsh attach-disk guest  /var/lib/libvirt/images/test.img vda --multifunction --address pci:0x0000.0x00.0x07.0x0 --persistent

3. Start the guest

# virsh start guest

4. Check the domain xml

# virsh dumpxml guest

5. Check the qemu-kvm command
	
Expected Results:

Step 4

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/var/lib/libvirt/images/test.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0' multifunction='on'/>
    </disk>

Step 5

-drive file=/var/lib/libvirt/images/test.img,if=none,id=drive-virtio-disk19,format=raw -device virtio-blk-pci,scsi=off,bus=pci.0,multifunction=on,addr=0x7.0x0,drive=drive-virtio-disk19,id=virtio-disk19
Notes:
Comments:

		176835 	[Disk hotplug] Attach/Detach a FC disk with readonly mode 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

do case " 124554 [NPIV] Discover SAN Storageï»¿" first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    virtual disks

bug:

    No bug found

Actions:

1. Start a guest
# virsh start demo

2. Attach the fc disk to guest
# virsh attach-disk demo /dev/disk/by-id/wwn-0x600a0b80005ad1d7000013bf4ecab373 vdb --mode readonly

3. Check if the new disk file has been added into domain
# virsh dumpxml demo

4. Log in guest and check if the disk is writable
# mount /dev/vdb1 /mnt
# cd /mnt
# touch test

5. Detach the iscsi disk from guest
# virsh detach-disk demo vdb

6. Check the device on guest is removed   

7. Destroy guest
# virsh destroy demo

8. Attach the following xml to guest

<disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/dev/disk/by-id/wwn-0x600a0b80005ad1d7000013bf4ecab373-part1'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
</disk>

9. Start guest
# virsh start demo

10. Log in guest and check if the disk is writable as step 4

 
	
Expected Results:

Step 2:
Disk attached successfully

Step 3:
 ...
<disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/disk/by-id/wwn-0x600a0b80005ad1d7000013bf4ecab373'/>
      <target dev='vdb' bus='virtio'/>
      <readonly/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
</disk>
...

Step 4:

# mount /dev/vdb1 /mnt
   mount: block device /dev/vdb1 is write-protected, mounting read-only

# touch test
   touch: cannot touch `test': Read-only file system

Step 5:

   Disk detached successfully

Step 6:

   Device is removed from guest, can not find in # virsh dumpxml demo

Step 9:

    Guest can be started successfully

Step 10:

# mount /dev/cdrom /mnt
   mount: block device /dev/sr0 is write-protected, mounting read-only

# cd /mnt

# touch test
   touch: cannot touch `test': Read-only file system

 
Notes:
Comments:

		176836 	[Disk hotplug] Attach/Detach a local disk with readonly mode 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    virtual disks

bug:

    No bug found

Actions:

1. Start a guest
# virsh start demo
2. Create a image on local dir and make ext3 fs on it

# qemu-img create /var/lib/libvirt/images/test.img 1G
# mkfs.ext3 /var/lib/libvirt/images/test.img

3. Attach the fc disk to guest
# virsh attach-disk demo /var/lib/libvirt/images/test.img vdb --mode readonly --sourcetype file

4. Check if the new disk file has been added into domain
# virsh dumpxml demo

5. Log in guest and check if the disk is writable
# mount /dev/vdb /mnt
# cd /mnt
# touch /mnt/test

6. Detach the iscsi disk from guest
# virsh detach-disk demo vdb

7. Check the device on guest is removed   

8. Destroy guest
# virsh destroy demo

9. Attach the following xml to guest

  <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/test.img'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
 </disk> 

10.Start guest
# virsh start demo

11. Log in guest and check if the disk is writable

12. Create a new image with 1.44M
# qemu-img create /var/lib/libvirt/images/floppy.img 1.44M

# mkfs.ext3 /var/lib/libvirt/images/floppy.img

13. Attach the following xml to guest

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/floppy.img'/>
      <target dev='fda' bus='fdc'/>
      <readonly/>
    </disk>

14. Destroy and start guest again

15. Log in guest and check if the floppy disk is writable

 
	
Expected Results:

Step 3:
Disk attached successfully

Step 4:
...
  <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/test.img'>
        <seclabel relabel='no'/>
      </source>
      <target dev='vdb' bus='virtio'/>
      <readonly/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0d' function='0x0'/>
 </disk>
...

Step 5:

# mount /dev/vdb /mnt
   mount: block device /dev/vdb is write-protected, mounting read-only

# touch /mnt/test
   touch: cannot touch `test': Read-only file system

Step 6:

   Disk detached successfully 

Step 7:

   Device is removed from guest, can not find in # virsh dumpxml demo

Step 10:

   Guest can be started successfully

Step 11:

# mount /dev/cdrom /mnt
   mount: block device /dev/sr0 is write-protected, mounting read-only

# cd /mnt

# touch test
   touch: cannot touch `test': Read-only file system

Step 15:

# modprobe floppy

# mount /dev/fd0 /mnt
   mount: block device /dev/fd0 is write-protected, mounting read-only

# cd mnt

# touch test
   touch: cannot touch `test': Read-only file system

 
Notes:
Comments:

		176838 	[Disk hotplug] Attach/Detach a nfs disk with readonly mode 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

Prepare a nfs server and mount to it.

Here is an exist nfs server

10.66.90.121:/vol/S3/libvirtmanual

set virt_use_nfs sebool

# setsebool virt_use_nfs on
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    virtual disks

bug:

    No bug found

Actions:

1. Start a guest
# virsh start demo

2. Mount nfs on host

# mount 10.66.90.121:/vol/S3/libvirtmanual /mnt -o vers=3

3. Create a image on nfs dir and make ext3 fs on it
# qemu-img create /mnt/test.img 1G
# mkfs.ext3 /mnt/test.img

4. Attach the nfs disk to guest
# virsh attach-disk demo /mnt/test.img vdb --mode readonly --sourcetype file

5. Check if the new disk file has been added into domain
# virsh dumpxml demo

6. Log in guest and check if the disk is writable
# mount /dev/vdb /mnt
# cd /mnt
# touch test

7. Detach the nfs disk from guest
# virsh detach-disk demo vdb

8. Check the device on guest is removed   

9. Destroy guest
# virsh destroy demo

10. Attach the following xml to guest

  <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/mnt/test.img'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
 </disk> 

11.Start guest
# virsh start demo

12. Log in guest and check if the cdrom is writable

13. Create a new image with 1.44M on nfs
# qemu-img create /mnt/floppy.img 1.44M

# mkfs.ext3 /mnt/floppy.img

14. Attach the following xml to guest

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/mnt/floppy.img'/>
      <target dev='fda' bus='fdc'/>
      <readonly/>
    </disk>

15. Destroy and start guest again

16. Log in guest and check if the floppy disk is writable
	
Expected Results:

Step 4:
Disk attached successfully

Step 5:
...
  <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/mnt/test.img'>
        <seclabel relabel='no'/>
      </source>
      <target dev='vdb' bus='virtio'/>
      <readonly/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0d' function='0x0'/>
 </disk>
...

Step 6:

# mount /dev/vdb /mnt
   mount: block device /dev/vdb is write-protected, mounting read-only

# touch test
   touch: cannot touch `test': Read-only file system

Step 7:

   Disk detached successfully 

Step 8:

   Device is removed from guest, can not find in # virsh dumpxml demo

Step 11:

   Guest can be started successfully

Step 12:

# mount /dev/cdrom /mnt
   mount: block device /dev/sr0 is write-protected, mounting read-only

# cd mnt

# touch test
   touch: cannot touch `test': Read-only file system

Step 16:

# modprobe floppy

# mount /dev/fd0 /mnt
   mount: block device /dev/fd0 is write-protected, mounting read-only

# cd mnt

# touch test
   touch: cannot touch `test': Read-only file system
Notes:
Comments:

		176844 	[Disk hotplug] Attach/Detach an iscsi disk with readonly mode 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

do case " 124815 [Storage] iSCSI based storage pool" first

you can get the iscsi storage by setting /etc/iscsi/initiatorname.iscsi

InitiatorName=iqn.1994-05.com.redhat:libvirt

Then run

# iscsiadm --mode discovery --type sendtargets --portal 10.66.90.100

to discover the iscsi disk
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    virtual disks

bug:

    No bug found

Actions:

1. start a guest
# virsh start demo

2. Attach the iscsi disk to guest with readonly mode
# virsh attach-disk rhel5u5 /dev/disk/by-path/ip-10.66.90.100:3260-iscsi-iqn.2001-05.com.equallogic:0-8a0906-12a1f7d03-0daf49b25a84ee02-s3-kyla-131842-lun-0-part1 vdb --mode readonly

3. Check if the new disk file has been added into domain
# virsh dumpxml demo

4. Log in guest and check if the disk is writable
# mount /dev/vdb1 /mnt
# cd /mnt
# touch test

5. Detach the iscsi disk from guest
# virsh detach-disk demo vdb

6. Check the device on guest is removed   

7. Destroy guest
# virsh destroy demo

8. Attach the following xml to guest

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/dev/disk/by-path/ip-10.66.90.100:3260-iscsi-iqn.2001-05.com.equallogic:0-8a0906-12a1f7d03-0daf49b25a84ee02-s3-kyla-131842-lun-0-part1'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>

9. Start guest
# virsh start demo

10. Log in guest and check if the disk is writable as step 4
	
Expected Results:

Step 2:
Disk attached successfully

Step 3:
...

  <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/disk/by-path/ip-10.66.90.100:3260-iscsi-iqn.2001-05.com.equallogic:0-8a0906-12a1f7d03-0daf49b25a84ee02-s3-kyla-131842-lun-0-part1'/>
      <target dev='vdb' bus='virtio'/>
      <readonly/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0a' function='0x0'/>
    </disk>
...

Step 4:

# mount /dev/vdb1 /mnt
   mount: block device /dev/vdb1 is write-protected, mounting read-only

# touch test
   touch: cannot touch `test': Read-only file system

Step 5:

   Disk detached successfully

Step 6:

   Device is removed from guest, can not find in # virsh dumpxml demo

Step 9:

    Guest can be started successfully

Step 10:

# mount /dev/cdrom /mnt
   mount: block device /dev/sr0 is write-protected, mounting read-only

# cd /mnt

# touch test
   touch: cannot touch `test': Read-only file system

 
Notes:
Comments:

		176859 	[Disk hotplug]Attach cdrom disk with startupPolicy='mandatory' 	weizhan 	weizhan 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Prepare a cdrom disk

# qemu-img create /tmp/cdrom.img 1G

2. Start a guest with this cdrom disk with

# cat guest.xml

...

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/cdrom.img' startupPolicy='mandatory'/>
      <target dev='hdc' bus='ide'/>    
     <readonly/>
  <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
....

3. Check the floppy disk on guest

# ls /dev/sr0
# dmesg | grep CD-ROM   

4. Destroy guest

5. Remove image /tmp/cdrom.img then start again

6. Recover image /tmp/cdrom.img then start again

7. Save the image

# virsh save guest /tmp/cdrom.save

8. Remove image /tmp/cdrom.img then restore the guest

# virsh restore /tmp/guest.save

9. Recover image /tmp/cdrom.img then start again

10. Create snapshot

# virsh snapshot-create guest

# virsh destroy guest

11. Remove image /tmp/cdrom.img then revert to running snapshot

# virsh snapshot-revert [snapshot name]
	
Expected Results:

Step 2

Guest can be started successfully

Step 3

scsi 1:0:0:0: CD-ROM            QEMU     QEMU DVD-ROM     0.12 PQ: 0 ANSI: 5
Uniform CD-ROM driver Revision: 3.20
sr 1:0:0:0: Attached scsi CD-ROM sr0

Step 5

Should report error

error: cannot access file '/var/lib/libvirt/images/cdrom.img': No such file or directory

Step 8

Should report error

error: cannot access file '/var/lib/libvirt/images/cdrom.img': No such file or directory

Step 11

Should report error

error: cannot access file '/var/lib/libvirt/images/cdrom.img': No such file or directory



Notes:
Comments:

		176860 	[Disk hotplug]Attach cdrom disk with startupPolicy='optional' 	weizhan 	weizhan 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Prepare a cdrom disk

# qemu-img create /tmp/cdrom.img 1G

2. Start a guest with this cdrom disk with

# cat guest.xml

...

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/floppy.img' startupPolicy='optional'/>
      <target dev='hdc' bus='ide'/>    
     <readonly/>
  <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
....

3. Check the floppy disk on guest

# ls /dev/sr0
# dmesg | grep CD-ROM   

4. Destroy guest

5. Remove image /tmp/cdrom.img then start again

6. Recover image /tmp/cdrom.img then start again

7. Save the image

# virsh save guest /tmp/cdrom.save

8. Remove image /tmp/cdrom.img then restore the guest

# virsh restore /tmp/guest.save

9. Recover image /tmp/cdrom.img then start again

10. Create snapshot and destroy the guest

# virsh snapshot-create guest

# virsh destroy guest

11. Remove image /tmp/cdrom.img then revert to running snapshot

# virsh snapshot-revert [snapshot name]
	
Expected Results:

Step 2

Guest can be started successfully

Step 3

scsi 1:0:0:0: CD-ROM            QEMU     QEMU DVD-ROM     0.12 PQ: 0 ANSI: 5
Uniform CD-ROM driver Revision: 3.20
sr 1:0:0:0: Attached scsi CD-ROM sr0

Step 5

Should succeed without error, and the source file part wil be removed

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='optional'/>
      <target dev='hdc' bus='ide'/>    
     <readonly/>
  <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Step 8

Should succeed without error, and the source file part wil be removed   

<disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='optional'/>
      <target dev='hdc' bus='ide'/>    
     <readonly/>
  <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Step 11

Should succeed without error, and the source file part wil be removed

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='optional'/>
      <target dev='hdc' bus='ide'/>    
     <readonly/>
  <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
Notes:
Comments:

		176861 	[Disk hotplug]Attach cdrom disk with startupPolicy='requisite' 	weizhan 	weizhan 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Prepare a cdrom disk

# qemu-img create /tmp/cdrom.img 1G

2. Start a guest with this cdrom disk with

# cat guest.xml

...

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/cdrom.img' startupPolicy='requisite'/>
      <target dev='hdc' bus='ide'/>    
     <readonly/>
  <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
....

3. Check the floppy disk on guest

# ls /dev/sr0
# dmesg | grep CD-ROM   

4. Destroy guest

5. Remove image /tmp/cdrom.img then start again

6. Recover image /tmp/cdrom.img then start again

7. Save the image

# virsh save guest /tmp/cdrom.save

8. Remove image /tmp/cdrom.img then restore the guest

# virsh restore /tmp/guest.save

9. Recover image /tmp/cdrom.img then start again

10. Create snapshot

# virsh snapshot-create guest

# virsh destroy guest

11. Remove image /tmp/cdrom.img then revert to running snapshot

# virsh snapshot-revert [snapshot name]
	
Expected Results:

Step 2

Guest can be started successfully

Step 3

scsi 1:0:0:0: CD-ROM            QEMU     QEMU DVD-ROM     0.12 PQ: 0 ANSI: 5
Uniform CD-ROM driver Revision: 3.20
sr 1:0:0:0: Attached scsi CD-ROM sr0

Step 5

Should report error

error: Failed to start domain xxx
error: cannot access file '/var/lib/libvirt/images/cdrom.img': No such file or directory

Step 8

Should succeed without error, and the source file part wil be removed

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='requisite'/>
      <target dev='hdc' bus='ide'/>    
     <readonly/>
  <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Step 11

Should succeed without error, and the source file part wil be removed 

   <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='requisite'/>
      <target dev='hdc' bus='ide'/>    
     <readonly/>
  <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Have a bug here:https://bugzilla.redhat.com/show_bug.cgi?id=798938
Notes:
Comments:

		176862 	[Disk hotplug]Attach floppy disk with startupPolicy='mandatory' 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Prepare a floppy disk

# qemu-img create /tmp/floppy.img 1.44M

2. Start a guest with this floppy disk

# cat guest.xml

...

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/floppy.img' startupPolicy='mandatory'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
....

3. Check the floppy disk on guest

# ls /dev/fd*
# dmesg | grep -i floppy     

4. Destroy guest

5. Remove image /tmp/floppy.img then start again

6. Recover image /tmp/floppy.img then start again

7. Save the image

# virsh save guest /tmp/guest.save

8. Remove image /tmp/floppy.img then restore the guest

# virsh restore /tmp/guest.save

9. Recover image /tmp/floppy.img then start again

10. Create snapshot

# virsh snapshot-create guest

# virsh destroy guest

11. Remove image /tmp/floppy.img then revert to running snapshot

# virsh snapshot-revert [snapshot name]
	
Expected Results:

Step 2

Guest can be started successfully

Step 3

ide floppy driver 0.99.newhide

Floppy drive(s-): fd0 is 1.44M

Step 5

Should report error

error: cannot access file '/var/lib/libvirt/images/cdrom.img': No such file or directory

Step 8

Should report error

error: cannot access file '/var/lib/libvirt/images/cdrom.img': No such file or directory

Step 11

error: cannot access file '/var/lib/libvirt/images/cdrom.img': No such file or directory


Notes:
Comments:

		176863 	[Disk hotplug]Attach floppy disk with startupPolicy='optional' 	weizhan 	weizhan 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Prepare a floppy disk

# qemu-img create /tmp/floppy.img 1.44M

2. Start a guest with this floppy disk

# cat guest.xml

...

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/floppy.img' startupPolicy='optional'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
....

3. Check the floppy disk on guest

# ls /dev/fd*
# dmesg | grep -i floppy     

4. Destroy guest

5. Remove image /tmp/floppy.img then start again

6. Recover image /tmp/floppy.img then start again

7. Save the image

# virsh save guest /tmp/guest.save

8. Remove image /tmp/floppy.img then restore the guest

# virsh restore /tmp/guest.save

9. Recover image /tmp/floppy.img then start again

10. Create snapshot

# virsh snapshot-create guest

# virsh destroy guest

11. Remove image /tmp/floppy.img then revert to running snapshot

# virsh snapshot-revert [snapshot name]
	
Expected Results:

Step 2

Guest can be started successfully

Step 3

ide floppy driver 0.99.newhide

Floppy drive(s-): fd0 is 1.44M

Step 5

Should succeed without error, and the source file part wil be removed

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='optional'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Step 8

Should succeed without error, and the source file part wil be removed

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='optional'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Step 11

Should succeed without error, and the source file part wil be removed

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='optional'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

 

NOTE: If live snapshot still not supported, may be revert have problem, such as just start from scratch but the check point is that with this policy revert may not be prohibited
Notes:
Comments:

		176864 	[Disk hotplug]Attach floppy disk with startupPolicy='requisite' 	weizhan 	weizhan 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Prepare a floppy disk

# qemu-img create /tmp/floppy.img 1.44M

2. Start a guest with this floppy disk

# cat guest.xml

...

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/floppy.img' startupPolicy='requisite'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
....

3. Check the floppy disk on guest

# modprobe floppy

# ls /dev/fd*
# dmesg | grep -i floppy     

4. Destroy guest

5. Remove image /tmp/floppy.img then start again

6. Recover image /tmp/floppy.img then start again

7. Save the image

# virsh save guest /tmp/guest.save

8. Remove image /tmp/floppy.img then restore the guest

# virsh restore /tmp/guest.save

9. Recover image /tmp/floppy.img then start again

10. Create snapshot

# virsh snapshot-create guest

# virsh destroy guest

11. Remove image /tmp/floppy.img then revert to running snapshot

# virsh snapshot-revert guest [snapshot name]
	
Expected Results:

Step 2

Guest can be started successfully

Step 3

ide floppy driver 0.99.newhide

Floppy drive(s-): fd0 is 1.44M

Step 5

Should report error

error: cannot access file '/var/lib/libvirt/images/floppy.img': Unknown error 512

Step 8

Should succeed without error, and the source file part wil be removed

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='requisite'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Step 11

Should succeed without error, and the source file part wil be removed

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='requisite'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Have a bug here:https://bugzilla.redhat.com/show_bug.cgi?id=798938


NOTE: If live snapshot still not supported, may be revert have problem, such as just start from scratch but the check point is that with this policy revert may not be prohibited
Notes:
Comments:

		176867 	[Docs] Check libvirtd and virt-xml-validate manual page - Bug 819364 	bili 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Bug 819364 - Libvirtd and virt-xml-validate manual page need to update

case STATUS should be confirmed after bug is fixed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Docs
    Regression

bug:

    No bug found

Actions:

1. #man libvirtd

2. #man virt-xml-validate

	
Expected Results:

1. Check the COPYRIGHT should be 2012; 

     check the libvirt version should be libvirt-0.9.10;

Should not be:

....
COPYRIGHT
       Copyright (C) 2006-2010 Red Hat, Inc., and the authors listed in the
libvirt AUTHORS file.
....

 and 

....

SEE ALSO
       virsh(1), virt-install(1), virt-xml-validate(1), virt-top(1),
virt-df(1), <http://www.libvirt.org/>

libvirt-0.9.6                    2011-10-07                LIBVIRTD.POD.IN(8)
....

2. Also check the copyright and libvirt version as step 1.

Should not be: 

COPYRIGHT
       Copyright (C) 2009-2010 by Red Hat, Inc.  Copyright (C) 2009 by Daniel P. Berrange

LICENSE
       virt-xml-validate is distributed under the terms of the GNU GPL v2+.  This is free software; see
       the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS
       FOR A PARTICULAR PURPOSE

SEE ALSO
       virsh(1), online XML format descriptions "http://libvirt.org/format.html"

libvirt-0.9.3                 2011-06-10           VIRT-XML-VALIDATE.IN(1)

 
Notes:
Remove the feature for "Document", will track the doc issue individually.
Comments:

		176870 	[Docs]Check the doc of virsh help command - bug 738411/736297/837761 	yupzhang 	None 	Manual 		Regression 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    docs
    Regression

bug:

    No bug found

Actions:

1. # virsh help snapshot-create-as
2. # virsh help send-key
3. # virsh help change-media
4.Check all other commands help information
#virsh help <command>

	
Expected Results:

1.Check output is right, and no typo.

# virsh help snapshot-create-as
  NAME
    snapshot-create-as - Create a snapshot from a set of args

  SYNOPSIS
    snapshot-create-as <domain> [<name>] [<description>] [--print-xml] [--no-metadata] [--halt] [--disk-only] [[--diskspec] <string>]...

  DESCRIPTION
    Create a snapshot (disk and RAM) from arguments

  OPTIONS
    [--domain] <string>  domain name, id or uuid
    [--name] <string>  name of snapshot
    [--description] <string>  description of snapshot
    --print-xml      print XML document rather than create
    --no-metadata    take snapshot but create no metadata
    --halt           halt domain after snapshot is created
    --disk-only      capture disk state but not vm state
    [--diskspec] <string>  disk attributes: disk[,snapshot=type][,driver=type][,file=name]

2.# virsh help send-key
NAME
send-key - Send keycodes to the guest

SYNOPSIS
send-key <domain> [--codeset <string>] [--holdtime <number>] <keycode>...

DESCRIPTION
Send keycodes (integers or symbolic names) to the guest

OPTIONS
[--domain] <string> domain name, id or uuid
--codeset <string> the codeset of keycodes, default:linux
--holdtime <number> the time (in milliseconds) how long the keys will be
held
<keycode> the key code

3. Check the output, should include like:
    --force force operation (BZ 837761 is not fixed ,pls update this result)

4.Check all output of virsh help <command> is right and no typo.
Notes:
Remove the feature for "Document", will track the doc issue individually.
Comments:

		176871 	[Docs]Check the manual of virsh - bug 736297/738933/769506/770458/839557 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    docs
    Regression

bug:

    No bug found

Actions:

1.Check virsh manual about command send-key

#man virsh

2.Check virsh manual of command memtune

3.Check virsh manual of command dump

4.Check virsh manual of domxml-from/to-native.

5. Check 'send-key' and 'echo' descriptions should be in the generic commands sectionn in virsh man page

6.Check all content of #man vish
	
Expected Results:

1.The content is right.

1) All options are supported

2)The format is right

3)No typo

4)All content is verifiable

send-key domain-id [--codeset codeset] [--holdtime holdtime]
           keycode...
               Parse the keycode sequence as keystrokes to send to domain-id.
               Each keycode can either be a numeric value or a symbolic name
               from the corresponding codeset.  If --holdtime is given, each
               keystroke will be held for that many milliseconds.  The default
               codeset is linux, but use of the --codeset option allows other
               codesets to be chosen.

               linux
                   The numeric values are those defined by the Linux generic
                   input event subsystem. The symbolic names match the
                   corresponding Linux key constant macro names.

               xt  The numeric values are those defined by the original XT
                   keyboard controller. No symbolic names are provided

               atset1
                   The numeric values are those defined by the AT keyboard
                   controller, set 1 (aka XT compatible set). Extended keycoes
                   from atset1 may differ from extended keycodes in the xt
                   codeset. No symbolic names are provided
    .....

2.(1)For --min-guarantee option,it need to document it like this :

    This option is supported only by the ESX hypervisor. This patch adds a
    statement about this fact, to prevent user confusion.
 (2)Should include like: <memtune> will be rounded by the hypervisor (BZ 839557 Not FIXED yet pls update)

3. It should contain a note like this:

"virsh dump" continues to dump a core dump even if it is interrupted. (It will be fixed in rhel6.3)

4.....  

domxml-from-native format config
           Convert the file config in the native guest configuration format
           named by format to a domain XML format. For QEMU/KVM hypervisor,
           the format argument must be qemu-argv. For Xen hypervisor, the
           format argument may be xen-xm or xen-sxpr.

       domxml-to-native format xml
           Convert the file xml in domain XML format to the native guest
           configuration format named by format. For QEMU/KVM hypervisor, the
           format argument must be qemu-argv. For Xen hypervisor, the format
           argument may be xen-xm or xen-sxpr.

......

5. Virsh's echo command looks not having any relations with domains and its

description should go into the generic commands section instead of the domain commands section.[bug 770458]

Virsh's send-key command manipulates domains and its description should

go into the domain commands section instead of generic commands section

 

6. All content of virsh manual is right:

      1) All options are supported

      2)The format is right

      3)No typo

      4)All content is verifiable

     ...

 

 
Notes:
Remove the feature for "Document", will track the doc issue individually.
Comments:

		176872 	[Docs]domjobinfo and domjobabort command does not work for domain restore job - 816721 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    docs
    regression

bug:

    No bug found

Actions:

virsh commands domjobinfo and domjobabort  does not work for domain restore job for RHEL6,it will be fixed in RHEL7.

Bug in rhel6,it need to documentation,in "man " or ...

=Bug is not fixed.=

https://bugzilla.redhat.com/show_bug.cgi?id=816721
	
Expected Results:
Notes:
Remove the feature for "Document", will track the doc issue individually.
Comments:

		176891 	[Graphical framebuffers] libvirt: missing spice channel 'default' - bug 819499 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

prepare windows guest like below:

<domain type='kvm'>
  <name>win7_64</name>
  <uuid>d551e24f-1368-6ffb-02fd-01eb28301ab5</uuid>
  <memory unit='KiB'>2097152</memory>
  <currentMemory unit='KiB'>2097152</currentMemory>
  <vcpu placement='static'>4</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/win7-64.qcow2'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </controller>
    <controller type='usb' index='0' model='ich9-ehci1'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x7'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci1'>
      <master startport='0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0' multifunction='on'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci2'>
      <master startport='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x1'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci3'>
      <master startport='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x2'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:38:7e:d1'/>
      <source network='default'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <input type='mouse' bus='ps2'/>

<graphics type='spice' port='5900' tlsPort='5901' autoport='no' listen='0.0.0.0' keymap='en-us' defaultMode='secure'>
      <listen type='address' address='0.0.0.0'/>
      <channel name='main' mode='secure'/>
      <channel name='inputs' mode='insecure'/>
      <channel name='usbredir' mode='secure'/>
    </graphics>

    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' vram='65536' heads='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <video>
      <model type='qxl' vram='65536' heads='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </video>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='3'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='4'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='5'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='6'/>
    </redirdev>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' relabel='yes'/>
</domain>

 

config tls : refer tcms case: 176918 [Graphical framebuffer] spice ssl connection
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1, prepare windows guest support usb redirector and config tls

2, edit guest xml, add the following content in graphics elemet

 <graphics type='spice' port='5900' tlsPort='5901' autoport='no' listen='0.0.0.0' keymap='en-us' defaultMode='secure'>

3, start the guest

4, check qemu command line

#ps -ef | grep $guestname
	
Expected Results:

4. Can find 

"tls-channel=default,tls-channel=main,plaintext-channel=inputs,tls-channel=usbredir"

in qemu command line
Notes:
Comments:

		176894 	[Graphical framebuffers] libvirt: missing spice channel 'usbredir' - bug 819498 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

prepare windows guest like below:

<domain type='kvm'>
  <name>win7_64</name>
  <uuid>d551e24f-1368-6ffb-02fd-01eb28301ab5</uuid>
  <memory unit='KiB'>2097152</memory>
  <currentMemory unit='KiB'>2097152</currentMemory>
  <vcpu placement='static'>4</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/win7-64.qcow2'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </controller>
    <controller type='usb' index='0' model='ich9-ehci1'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x7'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci1'>
      <master startport='0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0' multifunction='on'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci2'>
      <master startport='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x1'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci3'>
      <master startport='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x2'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:38:7e:d1'/>
      <source network='default'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' tlsPort='5901' autoport='no' listen='0.0.0.0' keymap='en-us'>
      <listen type='address' address='0.0.0.0'/>
      <channel name='main' mode='secure'/>
      <channel name='inputs' mode='insecure'/>
    </graphics>
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' vram='65536' heads='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <video>
      <model type='qxl' vram='65536' heads='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </video>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='3'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='4'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='5'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='6'/>
    </redirdev>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' relabel='yes'/>
</domain>

 

config tls : refer tcms case: 176918 [Graphical framebuffer] spice ssl connection
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1, prepare windows guest support usb redirector and config tls

2, add <channel name='usbredir' mode='secure'/> in graphics elemet

3, start the guest

4, check qemu command line

#ps -ef | grep $guestname

5, use remote-viewer to open the guest

#remote-viewer spice://$hostip/?tls-port=5901
--spice-host-subject="C=IL,L=Raanana,O=Red Hat,CN=my server"
--spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem'

 

 
	
Expected Results:

4. Can find 

 "tls-channel=main,plaintext-channel=inputs,tls-channel=usbredir"

  in qemu command line.

5. The guest can connected, and usb redirector can worked well.

 

Notes:
Comments:

		176909 	[Graphical framebuffers] Configuring Spice compression options - bug 682237 	vbian 	vbian 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1. Delete the vnc graphic info in guest xml file and add the following spice
graphic info in guest
# cat rhel61.xml

...
<graphics type='spice' port='5901' autoport='no' listen='0.0.0.0'>
<image compression='auto_glz'/>
<jpeg compression='auto'/>
<zlib compression='auto'/>
<playback compression='on'/>
</graphics>

...


2. # virsh define rhel61.xml


3.# virsh start rhel61
Domain rhel61 started


4. Check the xml info is the same as set
# virsh dumpxml rhel61
...
<graphics type='spice' port='5901' autoport='no' listen='0.0.0.0'>
<image compression='auto_glz'/>
<jpeg compression='auto'/>
<zlib compression='auto'/>
<playback compression='on'/>
</graphics>

...
5. Connect to the guest via virt-viewer (local)

# virt-viewer $guestname

#remote-viewer spice://$ip:5901
	
Expected Results:

step 3 :  domain can start successfully.

if you don't specify -vga qxl, there is a Bug 880276.

step 5 :

Connect successfully and work well.

verify :

#ps -ef | grep kvm

qemu command have these like below:

.......image-compression=auto_glz,jpeg-wan-compression=auto,zlib-glz-wan-compression=auto,playback-compression=on.........
Notes:
Comments:

		176911 	[Graphical framebuffer] Filter out certain devices from redirection 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

follow doc:https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/USB_Redirection to config usb_redirection
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

step :
   1.prepare a win7 guest.
   2.add this part in guest xml
    <redirdev bus='usb' type='spicevmc'>
      <alias name='redir0'/>
      <address type='usb' bus='0' port='3'/>
    </redirdev>
    <redirfilter>
      <usbdev class='0x08' vendor='0x1234' product='0xBEEF' version='2.0' allow='yes'/>
      <usbdev class='-1' vendor='-1' product='-1' version='-1' allow='no'/>
    </redirfilter>
   3.start guest and check qemu cmd,
  #ps -ef | grep $guest_name
   .....
  -device usb-redir,chardev=charredir0,id=redir0,filter=0x08:0x1234:0xBEEF:0x0200:1|-1:-1:-1:-1:0,bus=usb.0,port=3
   .....
   4.plug a usb device to host,and use virt-viewer to connect guest, 
     click "USB device selection" in menu
   5.check my usb device vendor&product , then edit xml to allow my usb redir.
   <redirfilter>
      <usbdev class='0x08' vendor='0x0951' product='0x1625' version='2.0' allow='yes'/>
      <usbdev allow='no'/>
    </redirfilter>
   6. restart guest, then use virt-viewer to connect guest
  

 
	
Expected Results:

after step 4 verify:

will get msg: "Some USB devices are blocked by host policy", the filter take effect.

 after step 6 verify:

  usb device can redirect to the guest successful
Notes:
Comments:

		176921 	[Graphical framebuffer] Support for network usb redirection 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

steps:
1. prepare the guest xml, only modify os type part in 
http://libvirt.org/git/?p=libvirt.git;a=blob;f=tests/qemuxml2argvdata/qemuxml2argv-usb-redir.xml 

prepare a windows guest with guest tools installed (using rhev-guest-tools-iso on brew). The guest xml is attached. 
steps:
A: Without usb-redir source mode 
  1. remove source mode xml 
# virsh edit win7_64 Remove following xml part in guest xml 
.. 
<redirdev bus='usb' type='tcp'>
    <source mode='connect' host='localhost' service='4000'/>
   <protocol type='raw'/>
 </redirdev> 
..
 2. restart guest # virsh start win7_64
 3. connect to guest and check 
# virt-viewer win7_64 
  

B: With usb-redir source mode
 1. add source mode xml back
 # virsh edit win7_64 
 Add following xml part in guest xml 
.. 
<redirdev bus='usb' type='tcp'>
 <source mode='connect' host='localhost' service='4000'/>
 <protocol type='raw'/> 
</redirdev>
 .. 
2. start usbredirserver with a usb device on host 
2.1 check host usb device
# lsusb
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 002: ID 17ef:600e Lenovo 
Bus 001 Device 024: ID 0951:1646 Kingston Technology 
Bus 003 Device 004: ID 04e6:5116 SCM Microsystems, Inc. SCR331-LC1 / SCR3310 SmartCard Reader

2.2 Using smartcard to start usbredirserver in a separate console
# usbredirserver -p 4000 04e6:5116

3. start guest
# virsh start win7_64


	
Expected Results:

after A step 3 verify

check with File -> usbredir and choose a usb device on host, success. 

 

after B step 3 verify

Connect to the guest, the smartcard reader directly connected in guest.

 
Notes:
Comments:

		176923 	[Graphical framebuffer] Try to connect one guest with several console -- VNC 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Assume there are running guests:

virsh # list --all
 Id Name                              State
------------------------------------------------
 0 Domain-0                         running
 2 guest                                 running
 - guest2                                shut off
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1, open a running guest by "virt-viewer guest-name" in a terminal window

2, open a new terminal window and run "virt-viewer guest-name" to try to open the same guest again.
3 open the third new terminal window and run "virt-viewer guest-name" to try to open the same guest again.

4. Input some characters in one of the console
	
Expected Results:

step 1, A virt-viewer winow will be opened and it will connected to the guest.
step 2, A second virt-viewer winow will be opened and it will connected to the guest.

step 3, A third virt-viewer winow will be opened and it will connected to the guest.

step4, Operation works in all three console samely

===========TBD======

https://bugzilla.redhat.com/show_bug.cgi?id=803602  the bug not fixed now.
Notes:
Comments:

		176925 	[Graphical framebuffers]Specify spice mouse mode 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

mode attribute is server or client
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

step:

 1: greate a guest with spice graphic

2: edit guest graphics element:

<graphics type='spice' port='-1' autoport='yes'>
    <streaming mode='filter'/>
    <mouse mode='client'/>
  </graphics>

 3: restart the guest

4: use virt-viewer to connect the guest

#virt-viewer $guestname

#remote-viewer spice://$ip:$port

5: shutdown the guest ,edit guest graphics element:

<graphics type='spice' port='-1' autoport='yes'>
    <streaming mode='filter'/>
    <mouse mode='server'/>
  </graphics>

 6: repeat step 4.
	
Expected Results:

verify:

after step 4: the guest can start w/o error

 the guest worked well

check qemu command:

#ps -ef | grep kvm

.......spice port=5902,tls-port=5903,addr=0.0.0.0,agent-mouse=on

after step 6: the guest can start w/o error

 the guest worked well

check qemu command:

........spice port=5902,tls-port=5903,addr=0.0.0.0,agent-mouse=off
Notes:
Comments:

		176926 	[Graphical framebuffers]support USB redirection 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

make sure client install spice-server spice-gtk usbredir kmod-kspiceusb-rhel60 usbredir-server

prepare a windows guest with guest tools installed (using rhev-guest-tools-iso on brew).

 a windows guest  , defined like below

<domain type='kvm'>
  <name>win</name>
  <uuid>f5abcf4d-a2d0-8d02-1e21-fede2070974e</uuid>
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/win.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
<source file='/root/rhev.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='usb' index='0' model='ich9-ehci1'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x7'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci1'>
      <master startport='0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0' multifunction='on'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci2'>
      <master startport='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x1'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci3'>
      <master startport='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x2'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:01:ae:46'/>
      <source network='default'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes' listen='0.0.0.0' keymap='en-us'>
      <listen type='address' address='0.0.0.0'/>
    </graphics>
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' vram='65536' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='3'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
 <address type='usb' bus='0' port='4'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='5'/>
    </redirdev>
    <redirdev bus='usb' type='tcp'>
      <source mode='connect' host='localhost' service='4000'/>
      <protocol type='raw'/>
    </redirdev>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
</domain>



	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

step:

1. prepare the guest xml,Make sure have the contents below

Usb controller:
    </controller>
    <controller type='usb' index='0' model='ich9-ehci1'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x7'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci1'>
      <master startport='0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0' multifunction='on'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci2'>
      <master startport='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x1'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci3'>
      <master startport='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x2'/>
    </controller>

a Spice graphic:
    <graphics type='spice' autoport='yes' listen='0.0.0.0' keymap='en-us'>
      <listen type='address' address='0.0.0.0'/>
    </graphics>
a qxl video:
       <video>
      <model type='qxl' vram='65536' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
redirdev:
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='3'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='4'/>
    </redirdev>
    <redirdev bus='usb' type='spicevmc'>
      <address type='usb' bus='0' port='5'/>
    </redirdev>
    <redirdev bus='usb' type='tcp'>
      <source mode='connect' host='localhost' service='4000'/>
      <protocol type='raw'/>
    </redirdev>

2.releated package setup
2.1 rhev-guest-tools-iso
      You can download the iso.zip from brew , and make a .iso with command mkisofs , fininally attach the iso into the guest via guest's cdrom
     setup RHEV-toolsSETUP in guest

2.2 kmod-kspiceusb-rhel60
      The package named kspiceusb-rhel60 on brew

2.3 others you can install through yum or find the rpm on brew easily.


A: Without usb-redir source mode 
1. remove source mode xml
# virsh edit win7_64
Remove following xml part in guest xml
..
     <redirdev bus='usb' type='tcp'>
       <source mode='connect' host='localhost' service='4000'/>
       <protocol type='raw'/>
     </redirdev>
..

2. start guest
# virsh start win7_64

3. connect to guest and check
# virt-viewer win7_64
Pay attention to the menu bar of virt-viewer window, and click File->USB device selection.
If USB device selection is grey, make sure rhev-guest-tools-iso is properly installed on guest.

Select a usb device, for example keyboard.
Then you will find the keyboard is only available in the guest.

B: With usb-redir source mode

1. add source mode xml back 
# virsh edit win7_64
Add following xml part in guest xml
..
     <redirdev bus='usb' type='tcp'>
       <source mode='connect' host='localhost' service='4000'/>
       <protocol type='raw'/>
     </redirdev>
..
2. start usbredirserver with a usb device on host
2.1 check host usb device
# lsusb
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 002: ID 17ef:600e Lenovo 
Bus 001 Device 024: ID 0951:1646 Kingston Technology 
Bus 003 Device 004: ID 04e6:5116 SCM Microsystems, Inc. SCR331-LC1 / SCR3310
SmartCard Reader

2.2 Using smartcard to start usbredirserver in a separate console
# usbredirserver -p 4000 04e6:5116

The package named  usbredir on brew


3. start guest
# virsh start win7_64

Connect to the guest, the smartcard reader directly connected in guest.

 

 
	
Expected Results:

after A step 3 verify

check with File -> usbredir and choose a usb device on host, success. 

 

after B step 3 verify

Connect to the guest, the smartcard reader directly connected in guest.

 
Notes:
Comments:

		176940 	[Guest kernel debugging] Send Alt+SysRq+B to trigger guest reboot - bug 708756 	yoyzhang 	None 	Manual 		Feature 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

The RFE bug is defered to be fixed on 6.2

1. Get a running guestï¼enable SysRq key

#echo 1 > /proc/sys/kernel/sysrq
2ãUse virsh command to send Alt+SysRq+B to trigger guest reboot
#virsh send-key $GUEST KEY_LEFTALT KEY_SYSRQ KEY_B
	
Expected Results:

Guest reboot successfully
Notes:
Comments:

		176943 	[Guest kernel debugging]Check libvirtd with poorly formatted human monitor command - 688723 	yupzhang 	None 	Manual 		Negative test 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1.Run virsh qemu-monitor-command with wrong command

eg .#virsh qemu-monitor-command rhel6 {system_reset}

2. Repeat several times of step 1.

3.Check the libvirtd status

# service libvirtd status

 4. conduct some operations via virsh

# virsh list --all
	
Expected Results:

1.Output:

# virsh qemu-monitor-command rhel6 {system_reset}
error: internal error cannot parse json {system_reset}: lexical error: invalid char in json text.
                                            {system_reset}
                     (right here) ------^

3.

# service libvirtd status
libvirtd (pid  24069) is running...

 4. # virsh list --all
 Id Name                 State
----------------------------------
  4 dom                  running

Notes:
Comments:

		176947 	[Guest kernel debugging]Run virsh dump with both --live and --crash options - 698490 826315 	yupzhang 	None 	Manual 		Negative test 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    Regression
    RHEL6.4.0

bug:

    No bug found

Actions:

1. # virsh start rhel61_x86_64

2. # virsh dump rhel61_x86_64 --live --crash /tmp/rhel61_x86_64.dump

3. 
   # virsh start guest
   # virsh dump core-file-path [--bypass-cache] { [--live] | [--crash] | [--reset] } guest 

4. "virsh dump" support for automatic capturing and automatic actions after capturing when:

(1) Guest OS kernel panic or no response and so on but qemu-kvm process is still fine.
(2) qemu-kvm process crashed. 

actions may be  - Reboot - Halt - Shutdown  in the future.

	
Expected Results:

1.# virsh start rhel61_x86_64

Domain rhel61_x86_64 started

2:Output:

# virsh dump rhel61_x86_64 --live --crash /tmp/rhel61_x86_64.dump
error: Failed to core dump domain rhel61_x86_64 to /tmp/rhel61_x86_64.dump
error: invalid argument in crash and live flags are mutually exclusive

3. all args works fine.

 4. It's not support yet. BZ 826315 is not fixed. KEEP it NEED_UPDATE
Notes:
Comments:

		176948 	[guest kernel debugging]Run virsh dump with option --bypass-cache to avoid file system cache when saving 	yupzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1.Start a guest.

2.Before virsh dump guest,flush and free caches

#sync

# echo 3 > /proc/sys/vm/drop_caches

# head -n4 /proc/meminfo

3.Run virsh dump without --bypass-cache option

#virsh dump rhel6 rhel6.dump

#head -n4 /proc/meminfo

4.Clean cache again

# echo 3 > /proc/sys/vm/drop_caches

# head -n4 /proc/meminfo

5. Run virsh dump with --bypass-cache option

# virsh dump rhel6 rhel6.dump --bypass-cache

# head -n4 /proc/meminfo

	
Expected Results:

2.# head -n4 /proc/meminfo
MemTotal:        8060940 kB
MemFree:         6922512 kB
Buffers:            2764 kB
Cached:            72316 kB

3.# virsh dump rhel6 rhel6.dump
Domain rhel6 dumped to rhel6.dump

# head -n4 /proc/meminfo
MemTotal:        8060940 kB
MemFree:         6654240 kB
Buffers:            6164 kB
Cached:           315592 kB

4. # head -n4 /proc/meminfo
MemTotal:        8060940 kB
MemFree:         6903656 kB
Buffers:            2636 kB
Cached:            71584 kB

5.# virsh dump rhel6 rhel6.dump --bypass-cache
Domain rhel6 dumped to rhel6.dump

# head -n4 /proc/meminfo
MemTotal:        8060940 kB
MemFree:         6935368 kB
Buffers:            5588 kB
Cached:            81852 kB
Notes:
Comments:

		176949 	[Guest kernel debugging]Run virsh save after performing some domain operations - 688850 	yupzhang 	None 	Manual 		Negative test 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    880919 - From Run 51770

Actions:

1. Do a sequence of operations on a guest, for example, domain start,
shutdown, start, destroy, suspend, resume, the guest can be saved to a
file. 

2. #virsh save rhel6 rhel6.save

 

	
Expected Results:

1.All operations pass

2.# virsh save rhel6 rhel6.save
Domain rhel6 saved to rhel6.save
Notes:
Comments:

		176951 	[Guest kernel debugging]Send NMI to QEMU guests - 632495 	yupzhang 	None 	Manual 		Feature 	P1 	None 	Edit
Setup:


https://bugzilla.redhat.com/show_bug.cgi?id=644919#c14


1. Prepare one guest with 2 cpus, and config guest kernel to preserve small memory

title Red Hat Enterprise Linux Server (2.6.32-166.el6.x86_64)
    root (hd0,0)
    kernel /vmlinuz-2.6.32-166.el6.x86_64 ro root=/dev/mapper/vg_dhcp9372-lv_root rd_LVM_LV=vg_dhcp9372/lv_root rd_LVM_LV=vg_dhcp9372/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM 
LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYBOARDTYPE=pc KEYTABLE=us crashkernel=128M@64M rhgb quiet
    initrd /initramfs-2.6.32-166.el6.x86_64.img

2. Reboot the guest 
3. Check the memory is preserved successfully in guest
#dmesg | less
.......
[    0.000000] Reserving 128MB of memory at 64MB for crashkernel (System RAM:
9088MB)
.......
4. In guest start kdump service

   # chkconfig kdump on
   # service kdump start
   # service kdump status
     Kdump is operational

5. In guest, config the target type
   # vim /etc/kdump.conf
   .......
   path /var/crash # specify path to save core dump file
   default shell # Action to default when system crashes
   .......

 

Note: only CPU0 in guest can get the NMI now, since there is some updates in qemu-kvm package.

Following is the referece info:

https://bugzilla.redhat.com/show_bug.cgi?id=738565#c79

https://bugzilla.redhat.com/show_bug.cgi?id=738565#c82

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

For Linux guest:

1.On Guest :
#cat /proc/interrupts

2. On host, send NMI to guest
# virsh inject-nmi rhel6

3.On Guest:
# cat /proc/interrupts


4.Run with:
# virsh qemu-monitor-command rhel6 '{"execute":"inject-nmi"}'

5.On Guest
# cat /proc/interrupts


For windows guest:
1.virsh inject-nmi win2k8r2-x86_64 

	
Expected Results:

For Linux guest:

1.Output:

.....

NMI:         0          0   Non-maskable interrupts

....

2.On the Guest,the output like this:

Message from syslogd@dhcp-93-72 at Jul 11 16:59:40 ...
 kernel:Uhhuh. NMI received for unknown reason 20 on CPU 0.

Message from syslogd@dhcp-93-72 at Jul 11 16:59:40 ...
 kernel:Do you have a strange power saving mode enabled?

Message from syslogd@dhcp-93-72 at Jul 11 16:59:40 ...
 kernel:Dazed and confused, but trying to continue


3.Output:

.....

NMI:         1         0   Non-maskable interrupts

....

 4.

# virsh qemu-monitor-command rhel6 '{"execute":"inject-nmi"}'
{"return":{},"id":"libvirt-796006"}

5.

.....

NMI:         2          0   Non-maskable interrupts

....

 

For windows guest:

1.Windows will BSOD with display:

*** Hardware Malfunction

Call your hardware vendor for support

*** The system has halted ***
Notes:
Comments:

		176953 	[Guest kernel debugging]Sending a bad keycode to guest or send key to a shutdown guest - bug 733597/746268 	yupzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:



	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    Regression

bug:

    No bug found

Actions:

1.Start a guest

#virsh start rhel6

2.Send a bad keycode to guest

# virsh send-key rhel6 --codeset win32 12

 3.Check the libvirtd

# /etc/init.d/libvirtd status

#virsh list --all

4.#virsh shutdown rhel6

5.# virsh send-key rhel6 22

6.# virsh start rhel6
	
Expected Results:

2.# virsh send-key 11 --codeset win32 12
error: internal error cannot translate keycode 12 of win32 codeset to rfb keycode

3.# /etc/init.d/libvirtd status
libvirtd (pid  530) is running...
# virsh list --all
 Id Name                 State
----------------------------------
  - demo                 shut off
  - test                 shut off
  - rhel6                running

 

5.# virsh send-key rhel6 22
error: Requested operation is not valid: domain is not running

6. # virsh start rhel6-test
Domain rhel6-test started

Notes:
Comments:

		176960 	[Guest network driver]macvtap (negtive testing) --- Bug 649964 	vbian 	None 	Manual 		Regression 	P3 	None 	Edit
Setup:

1. you have a guest with following section in the guest xml
   <interface type='direct'>
     <mac address='54:52:00:54:9e:f4'/>
     <source dev='eth0' mode='bridge'/>
     <target dev='macvtap0'/>
     <model type='virtio'/>
   </interface>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. start the guest with macvtap interface
   # virsh start guest
2. check the guest xml for macvtap device
   # virsh dumpxml guest |grep macvtap
3. restart libvirtd
   # service libvirtd restart
4. stop guest
   # virsh destroy guest
5. start guest
   # virsh start guest
6. recheck the guest xml for macvtap device
   # virsh dumpxml guest |grep macvtap

	
Expected Results:

1. you could start the guest defined with macvtap device
2. in the dump xml for guest , you could see macvtap device
6. in the dump xml for restarted guest ,you could see macvtap device, and the device is still the macvtap0 you defined

 
Notes:
Comments:

		176992 	[Guest resource control]Support policy base guest management BZ 826576 	zhpeng 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    guest resource control
    RHEL6.5.0

bug:

    No bug found

Actions:
	
Expected Results:

This is a new feature just like a set of temple, keep it to NEED_UPDATE, the more info here:

https://www.redhat.com/archives/libvirt-users/2012-June/msg00035.html
Notes:
Comments:

		177000 	[hooks] kvm-guest failed to start; avoid double-close bug in libvirt - bug 827050 	dyuan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    hooks

bug:

    No bug found

Actions:

1. Put the following bash script as /etc/libvirt/hooks/qemu:

#!/bin/sh

echo "Hi" > /tmp/yo
sleep 3

2. Restart libvirtd.

3. Start guest repeatedly(running the following bash script) and run "virsh list" also repeatedly in parallel.

#! /bin/bash

for i in {1..100}
do
    echo "---start guest ---"
    virsh start $1
    sleep 3
    echo "---destroy guest ---"
    virsh destroy $1
    sleep 3
done

At the same time, run "while true; do virsh list; sleep 1; done" in another terminal.

	
Expected Results:

guest can start, list and destroy normally

Notes:
Comments:

		177001 	[Hooks] qemu - pre-migration hook on desination - bug 795127 822916 	weizhan 	weizhan 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare a nfs server

write /etc/exports with

/var/lib/libvirt/images    *(rw,no_root_squash,async)

Then start nfs service

# service nfs start

Stop iptables

# iptables -F

2. Prepare 2 hosts and mount nfs dir on both hosts in different dir, and setting the virt_use_nfs boolean on both sides

on host1

  # mount  $NFS_IP:/var/lib/libvirt/images /mnt  -o vers=3

on host2

 # mount  $NFS_IP:/var/lib/libvirt/images /nfs  -o vers=3

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

  # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    hooks
    upstream

bug:

    No bug found

Actions:

1. Start a guest on host1 with images in /mnt

# virsh start guest

# virsh dumpxml mig > /mnt/mig.xml

<source file='/mnt/guest.img'>

2. Create /etc/libvirt/hooks/qemu  on host2(target) with following content
   # cat /etc/libvirt/hooks/qemu
   #!/bin/bash
   echo "$0" "$@" >> /tmp/qemu.log
   cat /nfs/mig.xml
   exit 0

  Create /etc/libvirt/hooks/qemu on host1 with following content  
  # cat /etc/libvirt/hooks/qemu
   #!/bin/bash
   echo "$0" "$@" >> /tmp/qemu.log
   exit 0

  And add execute permission for this file on both sides

  # chmod +x /etc/libvirt/hooks/qemu

3. on source, edit /mnt/mig.xml and change the disk source file with /nfs/guest.img

<source file='/nfs/guest.img'>

4. Restart libvirtd service on both sides

 # service libvirtd restart

5. Do migration from host1 to host2

 

  
	
Expected Results:

Step 5

Migration will succeed without error

on host2, the guest xml change to

<source file='/nfs/guest.img'>

and can see the log in /tmp/qemu.log

/etc/libvirt/hooks/qemu mig migrate begin -
/etc/libvirt/hooks/qemu mig prepare begin -
/etc/libvirt/hooks/qemu mig start begin -

on host1, you can see the hooks log in /tmp/qemu.log

/etc/libvirt/hooks/qemu mig stopped end -
/etc/libvirt/hooks/qemu mig release end -
BUG 822916 is not fixed, so change status to NEED_UPDATE

 
Notes:
Comments:

		177004 	[Hooks] Start guest with hooks script - bug 823716 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. Put the following bash script as /etc/libvirt/hooks/qemu:

#!/bin/sh

echo "yo" > /tmp/yo
sleep 3

2. Restart libvirtd.

3. Run the attached burst_test.py to start guest repeatedly


#!/bin/bash

for i in {1..100}
do
    echo "---start guest---"
    virsh start $1
    sleep 3
    echo "---destroy guest---"
    virsh destroy $1
    sleep 3
done

 

	
Expected Results:

Guest can be started successfully
Notes:
Comments:

		177006 	[Host network interface management] 'XML error: bond interface misses the bond element' when try to stop a bond through command - bug 786696 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management
    upstream

bug:

    No bug found

Actions:

1. Stop NetworkManager service.

2. Start virt-manager.

3. Click Edit->Connection Details->Network Interfaces

4. Click '+' button and choose bond then click 'Forward'

5. Choose an eth device(mine is eth1), Set 'Start mode'=none, and check 'Activate now'.

6. Click 'Configure' button besides 'Bond mode'

7. Choose Bond mode as 'active-backup', Bond monitor mode as 'arpmon',
interval=100, Validate mode=active, Target address: 192.168.50.1, then press OK and finish the wizard

8. Destroy the bond in terminal. 

#virsh iface-destroy bond0

9. Check Connection Details->Network Interfaces via virtManager

10. Check the libvirtd log.

	
Expected Results:

9. The following error message should NOT appeared:

XML error: bond interface misses the bond element

10. The following error message should NOT appeared:

error : virInterfaceDefParseXML:793 : XML error: bond interface misses the bond element

 
Notes:
Comments:

		177008 	[Host network interface management] Define an interface with netmask > 24 -- bug 848722 	ydu 	ydu 	Manual 		Function 	P1 	None 	Edit
Setup:

1. save the config file of eth0

  # cp /etc/sysconfig/network-scritps/ifcfg-eth0 /tmp

2. Before destroy eth0, should stop NetworkManager

 # service NetworkManger stop

3. After the following test step is finished, please copy the ifcfg-eth0 back, and restart network to restore netowrk.

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    848722 - From Run 48625
    848722 - From Run 49805
    848722 - From Run 51207
    848722 - From Run 53754

Actions:

1. Prepare the following xml, just make sure the prefix value greate than 24.

<interface type='ethernet' name='eth0'>
  <start mode='onboot'/>
  <mac address='xx:xx:xx:xx:xx:xx'/>
  <protocol family='ipv4'>
    <ip address='xx.xx.xx.xx' prefix='25'/>
  </protocol>
</interface>

NOTE;

In order to avoid conflict with other host in the network, please use the orignal IP address from dhcp in xml.

2. Destroy and undefine the orignal interface.

# virsh iface-destroy eth0

# virsh iface-undefine eth0

3. Define new eth0 with the XML file prepared in step 1.

# virsh iface-define new-eth0.xml

 


	
Expected Results:

3. For now, bug 848722 not verify, so actual result is:

# virsh iface-define new-eth0.xml
error: Failed to define interface from net.xml
error: internal error could not get interface XML description: XSLT transformation failed - runtime error: file /usr/share/netcf/xml/initscripts-get.xsl line 196 element node

 Expected Results:

3. new eth0 can be defined successfully.
Notes:
Comments:

		177036 	[input/output device]Change the order of video devices - bug 823535 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    input/output device

bug:

    823535 - From Run 46474

Actions:

1.Add two vedio devices to guest.

#virsh dumpxml rhel6.2

    <video>
      <model type='qxl' vram='65536' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <video>
      <model type='qxl' vram='65536' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </video>

2.Edit the guest,change order of two vedio devices,then save.

#virsh edit rhel6.2

...

    <video>
      <model type='qxl' vram='65536' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </video>
    <video>
      <model type='qxl' vram='65536' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>

...

3.Start the guest.
	
Expected Results:

2.The XML can be saved.

3.The guest started successfully.
Notes:
Comments:

		177037 	[input/output devices] Configure sound pass-through to appear as MIC as opposed to line-in- bug816503 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Complete the case [Sound devices] Intel HDA support.

 

2. Prepare one win 7 guest wich spice model .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    input/output device
    RHEL6.4.0

bug:

    No bug found

Actions:

1. Edit the guest,add micro type in it.
#virsh edit win7-32
...
    <sound model='ich6'>
+      <codec type='micro'/>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
...

2.#virsh start win7-32

3.#ps -ef | grep win7-32

4.In guest,check "Start -> Control Panel -> Hardware and Sound -> Manage audio devices -> Recording tab".

5. Record sound wave into a file using microphone.

6. Play the file, listen from spice console.
	
Expected Results:

3.In qemu-kvm command line,it has hda-micro

...
-device hda-micro,id=sound0-codec0,bus=sound0.0,cad=0 
...

 

4. See microphone instead of line-in there.

 

5. Record sound successful.

6. Smoothly played.

 
Notes:
Comments:

		177043 	[installation]Can't install an OS via PXE on a VM with NIC defined to work in NAT mode BZ#809723 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    installation
    upstream

bug:

    No bug found

Actions:

1. Create a VM and define its network interface to operate in NAT mode.

ï»¿ï»¿........

  <interface type='network'>
      <mac address='52:54:00:c5:75:20'/>
      <source network='default'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>

........

 

2. Boot the VM, Click on Ctrl+ B, type config

dns: 10.66.191.13

domain: englab.nay.redhat.com

next-server  10.66.94.104

filename pxelinux.0

 

3. In the menu, set the next server and the bootfile to something that matches
your setup. 
4. Click on Ctrl+x. Type autoboot 

	
Expected Results:

can not install guest 
Notes:
Comments:

		177049 	[interface hotplug] Attach interface with same mac or target as already attached interface 	gsun 	None 	Manual (Autoproposed) 		--default-- 	P1 	None 	Edit
Setup:

Bug 638570 & 638619

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp


Prepare a guest with an interface:

1 - # virsh dumpxml test
....
....
    <interface type='network'>
      <mac address='52:54:00:69:43:ea'/>
      <source network='default'/>
      <target dev='vnet1'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
.... 

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1 . attach a new interface to the guest with same mac address as already attached interface

# virsh attach-interface test network default  --mac 52:54:00:69:43:ea
Interface attached successfully

 2. attach a new interface to the guest with same target as already attached interface

# virsh attach-interface test network default  --target vnet1
Interface attached successfully

 

 
	
Expected Results:

1. # virsh dumpxml test

....

    <interface type='network'>
      <mac address='52:54:00:69:43:ea'/>
      <source network='default'/>
      <target dev='vnet1'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:69:43:ea'/>
      <source network='default'/>
      <target dev='vnet3'/>
      <alias name='net1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>

....

2.

# virsh dumpxml test

....

    <interface type='network'>
      <mac address='52:54:00:69:43:ea'/>
      <source network='default'/>
      <target dev='vnet1'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:69:43:ea'/>
      <source network='default'/>
      <target dev='vnet3'/>
      <alias name='net1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:4c:73:b1'/>
      <source network='default'/>
      <target dev='vnet4'/>
      <alias name='net2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </interface>

...

 
Notes:
Comments:

		177055 	[interface hotplug] [RFE] virsh attach-interface can't attach a new device to an inactive domain - bug 677229 	gsun 	None 	Manual (Autoproposed) 		--default-- 	P1 	None 	Edit
Setup:

make sure domain is not running

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. # virsh attach-interface fedora14 network default vnet1

2. add --persistent option 
# virsh attach-interface fedora14 network default  vnet1 --persistent


3. Check the xml of the guest, and make sure the network hotplug successully.

# virsh dumpxml fedora14

4. start the guest and check the interface can get ip and ping outside

	
Expected Results:

1.

error: Failed to attach interface
error: Requested operation is not valid: cannot do live update a device on
inactive domain

 2.

Interface attached successfully

 3.

...
 <interface type='network'>
      <mac address='52:54:00:e4:72:50'/>
      <source network='default'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </interface>
...

Interface show up in dumpxml.

 
Notes:
Comments:

		177057 	[interface hotplug]hotplug supported type of interfaces 	vbian 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:

1.Make sure you have a running guest

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1.start the guest
  # virsh start guest
2.hotplug interfaces to guest
  # for i in {e1000,ne2k_pci,pcnet,rtl8139,virtio}; do virsh attach-interface cdrom_test network default vnet0 --model $i; done
3.login to guest , and perform following command to check the interface is attached
  [in guest] # ifconfig -a

	
Expected Results:

1.you could start the guest successfully
2.you could perform the for loop successfully without error output
3.you could see 5 more interfaces attached in your guest , and if you are using a RHEL guest , you could get IP for each interface

Notes:
Comments:

		177063 	[Libvirt domain event handler] libvirt domain event handler can not receive messages after several times virKeepAliveTimer warning BZ#821991 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

When cpu is busy and domain operation can not reply for more than 30s(default), 
libvirt will report error like error: End of file while reading data: Input/output error, 
and libvirt event-python will exit, for several times loop, 
then libvirt event-python can not receive messages any more.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt domain event handler

bug:

    No bug found

Actions:

1. change libvirtd.conf for easily catching error and restart libvirtd
keepalive_interval = 2
keepalive_count = 1

2. on another console do cp to occupy the cpu
# while true; do cp rhel6u3-1 test; rm -rf test; done

3. Do start->save->restore->destroy loop for a guest
# while true; do virsh start kvm-rhel6u3-x86_64; sleep 10; virsh save kvm-rhel6u3-x86_64 /var/lib/libvirt/images/kvm-rhel6u3-x86_64.save; virsh restore /var/lib/libvirt/images/kvm-rhel6u3-x86_64.save; virsh destroy kvm-rhel6u3-x86_64; done

4. On the third console do
# while true; do python /usr/share/doc/libvirt-python-0.9.10/events-python/event-test.py; done
to get domain operation messages

	
Expected Results:



libvirt event python can always receive messages

if ibvirt event python can not receive messages any more 
case will failed 

 
Notes:

Bug 821991 - libvirt domain event handler can not receive messages after several times virKeepAliveTimer warning NOTABUG
Comments:

		177071 	[libvirt-snmp] Control domain lifecycle 	xhu 	None 	Manual 		Feature 	P1 	None 	Edit
Setup:

1 Firstly run "102233 [libvirt-snmp] Request information about domain status"

2 Have a healthy running domain
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt snmp

bug:

    No bug found

Actions:

1 set the state of a domain (running->paused, paused->running, running->shutdown, shutdown->running, running->shutdown):
# snmpset -m ALL -v 2c -c public localhost libvirtGuestState.\'7ad4bc2a-16db-d8c0-1f5a-6cb777e17cd8\' = paused
# snmpset -m ALL -v 2c -c public localhost libvirtGuestState.\'7ad4bc2a-16db-d8c0-1f5a-6cb777e17cd8\' = running
# snmpset -m ALL -v 2c -c public localhost libvirtGuestState.\'7ad4bc2a-16db-d8c0-1f5a-6cb777e17cd8\' = shutdown

# snmpset -m ALL -v 2c -c public localhost libvirtGuestRowStatus.\'7ad4bc2a-16db-d8c0-1f5a-6cb777e17cd8\' = createAndGo

# snmpset -m ALL -v 2c -c public localhost libvirtGuestRowStatus.\'7ad4bc2a-16db-d8c0-1f5a-6cb777e17cd8\' = destroy
	
Expected Results:

1. use "virsh list --all" to check after every single set operation and the domain status change cycle is as follows:

running->paused, paused->running, running->shutdown, shutdown->running, running->shutdown
Notes:
Comments:

		177072 	[libvirt-snmp] Request information about domain status 	xhu 	None 	Manual 		Feature 	P3 	None 	Edit
Setup:

1. install related packages required by compiling libvirt-snmp
# sudo yum install autoconf automake gcc rpm-build
# sudo yum install net-snmp-perl net-snmp net-snmp-utils net-snmp-devel libvirt-devel

2. install libvirt-snmp
# sudo yum install libvirt-snmp

3. make sure there is at least one running healthy guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirt snmp

bug:

    No bug found

Actions:

1. prepare /etc/snmp/snmpd.conf as follows:
# cat /etc/snmp/snmpd.conf
rwcommunity public
master agentx
trapcommunity public
trap2sink  localhost

2. append the following lines to /etc/snmp/snmptrapd.conf
# Example configuration file for snmptrapd
authCommunity log,execute,net public
logOption f /var/log/snmptraps.log

3. add the following lines to /etc/sysconfig/snmptrapd:
OPTIONS="-m ALL -p /var/run/snmptrapd.pid"

4. restart snmpd and snmptrapd service

# service snmpd restart

# service snmptrapd restart


5. run libvirt snmp agent daemon
# LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L

6. running snmpwalk to query domain status
# snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB

stop step 5 agent

7. edit /etc/rsyslog.conf
add one line:
*.*                            /var/log/alllog
restart rsyslog
8. run step 5 without -L

9. /var/log/alllog will get log like:

Jun 14 13:51:11 zhpeng snmpd[9913]: Connection from UDP: [127.0.0.1]:38716->[127.0.0.1]
Jun 14 13:51:11 zhpeng snmpd[9913]: Connection from UDP: [127.0.0.1]:38716->[127.0.0.1]
Jun 14 13:51:11 zhpeng snmpd[9913]: Connection from UDP: [127.0.0.1]:38716->[127.0.0.1]
Jun 14 13:51:11 zhpeng snmpd[9913]: Connection from UDP: [127.0.0.1]:38716->[127.0.0.1]
Jun 14 13:51:11 zhpeng snmpd[9913]: Connection from UDP: [127.0.0.1]:38716->[127.0.0.1]
Jun 14 13:51:11 zhpeng snmpd[9913]: Connection from UDP: [127.0.0.1]:38716->[127.0.0.1]
Jun 14 13:51:11 zhpeng snmpd[9913]: Connection from UDP: [127.0.0.1]:38716->[127.0.0.1]
Jun 14 13:51:11 zhpeng snmpd[9913]: Connection from UDP: [127.0.0.1]:38716->[127.0.0.1]

10.stop libvirt-snmp agent, run step5 without -f -L

11. [root@zhpeng ~]# ps xua|grep libvirtMib
root      9949  0.5  0.0 352964  5004 ?        Sl   13:52   0:00 libvirtMib_subagent
root      9952  0.0  0.0 103240   852 pts/2    S+   13:52   0:00 grep libvirtMib

12.snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB

result is the same as step 6

13. stop libvirt-snmp agent

14. print debug info

LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L -D ALL
LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L -D trace,9

15. print config info

LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L -H

16. run libvirt-snmp agent as a normal SNMP Agent instead of an AgentX sub-agent.

[root@zhpeng snmp]# /etc/init.d/snmptrapd stop
Stopping snmptrapd:                                        [  OK  ]
[root@zhpeng snmp]# /etc/init.d/snmpd stop
Stopping snmpd:                                            [  OK  ]

[root@zhpeng snmp]# cp /etc/snmp/snmpd.conf /etc/snmp/libvirtGuestTable.conf

[root@zhpeng snmp]# LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L -M
Turning on AgentX master support.   
Connection from UDP: [127.0.0.1]:55200->[127.0.0.1] 
Connection from UDP: [127.0.0.1]:55200->[127.0.0.1]
Connection from UDP: [127.0.0.1]:55200->[127.0.0.1]
...
[root@zhpeng ~]# snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB
LIBVIRT-MIB::libvirtGuestName[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = STRING: "aaa"
LIBVIRT-MIB::libvirtGuestState[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = INTEGER: running(1)
LIBVIRT-MIB::libvirtGuestCpuCount[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = Gauge32: 1
LIBVIRT-MIB::libvirtGuestMemoryCurrent[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = Gauge32: 976
LIBVIRT-MIB::libvirtGuestMemoryLimit[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = Gauge32: 977
LIBVIRT-MIB::libvirtGuestCpuTime[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = Counter64: 18350000000
LIBVIRT-MIB::libvirtGuestRowStatus[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = INTEGER: active(1)
LIBVIRT-MIB::libvirtGuestRowStatus[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = No more variables left in this MIB View (It is past the end of the MIB tree)

Now try agent with changed port

# LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L -M 165
# snmpwalk -m ALL -v 2c -c public -OX localhost:165 libvirtMIB
17. connect agent to new socket

stoplibvirt-snmp agent and snmpd snmptrapd

New unix socket:
[root@zhpeng ~]# snmpd -x /tmp/snmp.agent      
[root@zhpeng ~]# LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L -x /tmp/snmp.agent
NET-SNMP version 5.5 AgentX subagent connected
[root@zhpeng ~]# snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB

kill snmpd process
New interface socket:
[root@zhpeng ~]# snmpd -x TCP:1161
[root@zhpeng ~]# LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L -x 0.0.0.0:1161
NET-SNMP version 5.5 AgentX subagent connected
[root@zhpeng ~]# snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB
LIBVIRT-MIB::libvirtGuestName[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = STRING: "aaa"
LIBVIRT-MIB::libvirtGuestState[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = INTEGER: running(1)
LIBVIRT-MIB::libvirtGuestCpuCount[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = Gauge32: 1
LIBVIRT-MIB::libvirtGuestMemoryCurrent[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = Gauge32: 976
LIBVIRT-MIB::libvirtGuestMemoryLimit[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = Gauge32: 977
LIBVIRT-MIB::libvirtGuestCpuTime[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = Counter64: 20210000000
LIBVIRT-MIB::libvirtGuestRowStatus[STRING: 15cdea6c-468f-f12c-9e8f-6ed9bb319ef8] = INTEGER: active(1)

	
Expected Results:

6. it should print something like this:
# snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB
LIBVIRT-MIB::libvirtGuestName[STRING: 176eeb3-c392-3e78-4bd0-b4a4d162bc42] = STRING: "win2003"
LIBVIRT-MIB::libvirtGuestState[STRING: 176eeb3-c392-3e78-4bd0-b4a4d162bc42] = INTEGER: running(1)
LIBVIRT-MIB::libvirtGuestCpuCount[STRING: 176eeb3-c392-3e78-4bd0-b4a4d162bc42] = Gauge32: 1
LIBVIRT-MIB::libvirtGuestMemoryCurrent[STRING: 176eeb3-c392-3e78-4bd0-b4a4d162bc42] = Gauge32: 1024
LIBVIRT-MIB::libvirtGuestMemoryLimit[STRING: 176eeb3-c392-3e78-4bd0-b4a4d162bc42] = Gauge32: 1024
LIBVIRT-MIB::libvirtGuestCpuTime[STRING: 176eeb3-c392-3e78-4bd0-b4a4d162bc42] = Counter64: 7610000000
LIBVIRT-MIB::libvirtGuestRowStatus[STRING: 176eeb3-c392-3e78-4bd0-b4a4d162bc42] = INTEGER: active(1)

14.

No log handling enabled - turning on stderr logging
trace: netsnmp_ds_set_boolean(): default_store.c, 205:
netsnmp_ds_set_boolean: Setting APP:1 = 1/True
trace: netsnmp_ds_set_boolean(): default_store.c, 205:
netsnmp_ds_set_boolean: Setting LIB:11 = 1/True
trace: netsnmp_ds_set_boolean(): default_store.c, 205:
netsnmp_ds_set_boolean: Setting APP:1 = 0/False
trace: netsnmp_register_handler(): agent_handler.c, 219:
handler::register: Registering  (::null) at .0
trace: netsnmp_register_mib(): agent_registry.c, 741:
register_mib: registering "" at .0 with context "(null)"
trace: netsnmp_subtree_find_first(): agent_registry.c, 288:
subtree: looking for subtree for context: ""
trace: netsnmp_subtree_find_first(): agent_registry.c, 297:
subtree: didn't find a subtree for context: ""
trace: netsnmp_register_handler(): agent_handler.c, 219:
handler::register: Registering  (::null) at .0
....

15.
   .....
   clientRecvBuf            integerValue
    noPersistentLoad         (1|yes|true|0|no|false)
    noPersistentSave         (1|yes|true|0|no|false)
    noContextEngineIDDiscovery (1|yes|true|0|no|false)
    defDomain                application domain
    defTarget                application domain target
    defSecurityModel         string
    defSecurityName          string
    defContext               string
    defPassphrase            string
    defAuthPassphrase        string
    defPrivPassphrase        string
    defAuthMasterKey         string
    defPrivMasterKey         string
    defAuthLocalizedKey      string
    defPrivLocalizedKey      string
    defVersion               1|2c|3
    defAuthType              MD5|SHA
    defPrivType              DES|AES
    defSecurityLevel         noAuthNoPriv|authNoPriv|authPriv
  In agentx.conf and agentx.local.conf:
    agentxsocket             AgentX bind address
    agentxperms              AgentX socket permissions: socket_perms [directory_perms [username|userid [groupname|groupid]]]
    agentxRetries            AgentX Retries
    agentxTimeout            AgentX Timeout (seconds)

Notes:
Comments:

		177073 	[libvirt-snmp] Snmptrap of domain lifecycle event 	xhu 	None 	Manual 		Feature 	P1 	None 	Edit
Setup:

1 Firstly run "102234 [libvirt-snmp] Control domain lifecycle"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt snmp

bug:

    No bug found

Actions:

1. Check /var/log/messages
	
Expected Results:

paused:

localhost [UDP: [127.0.0.1]:45652->[127.0.0.1]]: Trap , DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (41790) 0:06:57.90, SNMPv2-MIB::snmpTrapOID.0 = OID: LIBVIRT-MIB::libvirtGuestNotif, LIBVIRT-MIB::libvirtGuestName.0 = STRING: "aaa", LIBVIRT-MIB::libvirtGuestUUID.1 = STRING: ec87d904-9dfb-3d8c-8db0-37b271b71efb, LIBVIRT-MIB::libvirtGuestState.2 = INTEGER: paused(3), LIBVIRT-MIB::libvirtGuestRowStatus.3 = INTEGER: active(1)

running:


localhost [UDP: [127.0.0.1]:45652->[127.0.0.1]]: Trap , DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (45052) 0:07:30.52, SNMPv2-MIB::snmpTrapOID.0 = OID: LIBVIRT-MIB::libvirtGuestNotif, LIBVIRT-MIB::libvirtGuestName.0 = STRING: "aaa", LIBVIRT-MIB::libvirtGuestUUID.1 = STRING: ec87d904-9dfb-3d8c-8db0-37b271b71efb, LIBVIRT-MIB::libvirtGuestState.2 = INTEGER: running(1), LIBVIRT-MIB::libvirtGuestRowStatus.3 = INTEGER: active(1)

shutdown:

localhost [UDP: [127.0.0.1]:45652->[127.0.0.1]]: Trap , DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (5895) 0:00:58.95, SNMPv2-MIB::snmpTrapOID.0 = OID: LIBVIRT-MIB::libvirtGuestNotif, LIBVIRT-MIB::libvirtGuestName.0 = STRING: "aaa", LIBVIRT-MIB::libvirtGuestUUID.1 = STRING: ec87d904-9dfb-3d8c-8db0-37b271b71efb, LIBVIRT-MIB::libvirtGuestState.2 = INTEGER: paused(3), LIBVIRT-MIB::libvirtGuestRowStatus.3 = INTEGER: active(1)
localhost [UDP: [127.0.0.1]:45652->[127.0.0.1]]: Trap , DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (5895) 0:00:58.95, SNMPv2-MIB::snmpTrapOID.0 = OID: LIBVIRT-MIB::libvirtGuestNotif, LIBVIRT-MIB::libvirtGuestName.0 = STRING: "aaa", LIBVIRT-MIB::libvirtGuestUUID.1 = STRING: ec87d904-9dfb-3d8c-8db0-37b271b71efb, LIBVIRT-MIB::libvirtGuestState.2 = INTEGER: shutoff(5), LIBVIRT-MIB::libvirtGuestRowStatus.3 = INTEGER: notInService(2)

createAndGO:

localhost [UDP: [127.0.0.1]:45652->[127.0.0.1]]: Trap , DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (32207) 0:05:22.07, SNMPv2-MIB::snmpTrapOID.0 = OID: LIBVIRT-MIB::libvirtGuestNotif, LIBVIRT-MIB::libvirtGuestName.0 = STRING: "aaa", LIBVIRT-MIB::libvirtGuestUUID.1 = STRING: ec87d904-9dfb-3d8c-8db0-37b271b71efb, LIBVIRT-MIB::libvirtGuestState.2 = INTEGER: running(1), LIBVIRT-MIB::libvirtGuestRowStatus.3 = INTEGER: active(1)

destroy:

localhost [UDP: [127.0.0.1]:45652->[127.0.0.1]]: Trap , DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (17808) 0:02:58.08, SNMPv2-MIB::snmpTrapOID.0 = OID: LIBVIRT-MIB::libvirtGuestNotif, LIBVIRT-MIB::libvirtGuestName.0 = STRING: "aaa", LIBVIRT-MIB::libvirtGuestUUID.1 = STRING: ec87d904-9dfb-3d8c-8db0-37b271b71efb, LIBVIRT-MIB::libvirtGuestState.2 = INTEGER: running(1), LIBVIRT-MIB::libvirtGuestRowStatus.3 = INTEGER: active(1)
localhost [UDP: [127.0.0.1]:45652->[127.0.0.1]]: Trap , DISMAN-EVENT-MIB::sysUpTimeInstance = Timeticks: (24298) 0:04:02.98, SNMPv2-MIB::snmpTrapOID.0 = OID: LIBVIRT-MIB::libvirtGuestNotif, LIBVIRT-MIB::libvirtGuestName.0 = STRING: "aaa", LIBVIRT-MIB::libvirtGuestUUID.1 = STRING: ec87d904-9dfb-3d8c-8db0-37b271b71efb, LIBVIRT-MIB::libvirtGuestState.2 = INTEGER: shutoff(5), LIBVIRT-MIB::libvirtGuestRowStatus.3 = INTEGER: notInService(2)

Notes:
Comments:

		177074 	[libvirt-snmp]libvirtMib_subagent will stop when request information after libvirtd restarted - bug 736596 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt snmp
    Regression
    upstream

bug:

    No bug found

Actions:

1, yum install net-snmp net-snmp-libs net-snmp-utils libvirt-snmp libvirt-snmp-debuginfo

2,make sure a guest is running

# virsh list

 Id Name                 State
----------------------------------
  9 redhat               running

3. prepare /etc/snmp/snmpd.conf as follows:

# cat /etc/snmp/snmpd.conf
rwcommunity public
master agentx
trapcommunity public
trap2sink  localhost

4. append the following lines to /etc/snmp/snmptrapd.conf

# Example configuration file for snmptrapd
authCommunity log,execute,net public
logOption f /var/log/snmptraps.log

5. add the following lines to /etc/sysconfig/snmptrapd:

OPTIONS="-m ALL -p /var/run/snmptrapd.pid"

6. restart snmpd and snmptrapd service

# service snmpd restart
# service snmptrapd restart

7. open a new terminal and run libvirt snmp agent daemon. DO NOT close it.

# LIBVIRT_DEFAULT_URI="qemu:///system" libvirtMib_subagent -f -L

8. running snmpwalk to query domain status

# snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB

LIBVIRT-MIB::libvirtGuestName[STRING: 5a00dd1c-2953-246c-e0a6-1144d315ea57] = STRING: "redhat"
LIBVIRT-MIB::libvirtGuestState[STRING: 5a00dd1c-2953-246c-e0a6-1144d315ea57] = INTEGER: running(1)
LIBVIRT-MIB::libvirtGuestCpuCount[STRING: 5a00dd1c-2953-246c-e0a6-1144d315ea57] = Gauge32: 1
LIBVIRT-MIB::libvirtGuestMemoryCurrent[STRING: 5a00dd1c-2953-246c-e0a6-1144d315ea57] = Gauge32: 1024
LIBVIRT-MIB::libvirtGuestMemoryLimit[STRING: 5a00dd1c-2953-246c-e0a6-1144d315ea57] = Gauge32: 1024
LIBVIRT-MIB::libvirtGuestCpuTime[STRING: 5a00dd1c-2953-246c-e0a6-1144d315ea57] = Counter64: 15440000000
LIBVIRT-MIB::libvirtGuestRowStatus[STRING: 5a00dd1c-2953-246c-e0a6-1144d315ea57] = INTEGER: active(1)

9. restart libvirtd

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

10. running snmpwalk to query domain status

# snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB

11. check the status of libvirt snmp agent of Step 7

 
	
Expected Results:

Step 10: can get MIB information the same as Step 8

Step 11: libvirt snmp agent is still running

 

NOT LIKE THIS:

Step 10:

# snmpwalk -m ALL -v 2c -c public -OX localhost libvirtMIB
Error in packet.
Reason: (genError) A general failure occured
Failed object: LIBVIRT-MIB::libvirtMIB

LIBVIRT-MIB::libvirtMIB = No Such Object available on this agent at this OID

 Step 11:

the libvirt snmp agent is stop
Notes:
Comments:

		177075 	[libvirt-snmp]Use "ctrl+c" to close libvirtMib_subagent will not lead to memory leak - Bug 736258 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt snmp
    Regression
    rhel6.5

bug:

    No bug found

Actions:

1, yum install net-snmp net-snmp-libs net-snmp-utils libvirt-snmp libvirt-snmp-debuginfo valgrind

2,make sure a guest is running

# virsh list

 Id Name                 State
----------------------------------
  9 redhat               running

3,prepare /etc/snmp/snmpd.conf as follows:

# cat /etc/snmp/snmpd.conf
rwcommunity public
master agentx
trapcommunity public
trap2sink  localhost

4,append the following lines to /etc/snmp/snmptrapd.conf

# Example configuration file for snmptrapd
authCommunity log,execute,net public
logOption f /var/log/snmptraps.log

5. add the following lines to /etc/sysconfig/snmptrapd:

OPTIONS="-m ALL -p /var/run/snmptrapd.pid"

6. restart snmpd and snmptrapd service

# service snmpd restart
# service snmptrapd restart

7. open a new terminal, run libvirt snmp agent daemon with valgrind leakcheck

# LIBVIRT_DEFAULT_URI="qemu:///system" valgrind --leak-check=full libvirtMib_subagent -f -L >& tmp.log 2>&1

8. use "ctrl+c" twice to close libvirtMib_subagen
	
Expected Results:

1, cat ./tmp.log make sure there is no memleak information like this:

==8147== Memcheck, a memory error detector
==8147== Copyright (C) 2002-2010, and GNU GPL'd, by Julian Seward et al.
==8147== Using Valgrind-3.6.0 and LibVEX; rerun with -h for copyright info
==8147== Command: libvirtMib_subagent -f -L
==8147== 
NET-SNMP version 5.5 AgentX subagent connected
==8147== 
==8147== HEAP SUMMARY:
==8147==     in use at exit: 396,810 bytes in 1,469 blocks
==8147==   total heap usage: 32,074 allocs, 30,605 frees, 3,266,745 bytes allocated
==8147== 
==8147== 80 bytes in 1 blocks are definitely lost in loss record 123 of 156
==8147==    at 0x4A04A28: calloc (vg_replace_malloc.c:467)
==8147==    by 0x3000449A7D: virAlloc (in /usr/lib64/libvirt.so.0.9.4)
==8147==    by 0x300045E81A: ??? (in /usr/lib64/libvirt.so.0.9.4)
==8147==    by 0x300045F728: virResetLastError (in /usr/lib64/libvirt.so.0.9.4)
==8147==    by 0x300043F641: virEventRegisterDefaultImpl (in /usr/lib64/libvirt.so.0.9.4)
==8147==    by 0x405F94: libvirtSnmpInit (libvirtSnmp.c:325)
==8147==    by 0x405413: libvirtGuestTable_container_init (libvirtGuestTable_data_access.c:138)
==8147==    by 0x409AAA: _libvirtGuestTable_initialize_interface (libvirtGuestTable_interface.c:1644)
==8147==    by 0x406D1E: main (libvirtMib_subagent.c:156)
==8147== 
==8147== 368 bytes in 1 blocks are possibly lost in loss record 138 of 156
==8147==    at 0x4A04A28: calloc (vg_replace_malloc.c:467)
==8147==    by 0x3FE9211732: _dl_allocate_tls (in /lib64/ld-2.12.so)
==8147==    by 0x3FEA20700F: pthread_create@@GLIBC_2.2.5 (in /lib64/libpthread-2.12.so)
==8147==    by 0x405EF5: libvirtRegisterEvents (libvirtSnmp.c:284)
==8147==    by 0x405F6F: libvirtSnmpInit (libvirtSnmp.c:339)
==8147==    by 0x405413: libvirtGuestTable_container_init (libvirtGuestTable_data_access.c:138)
==8147==    by 0x409AAA: _libvirtGuestTable_initialize_interface (libvirtGuestTable_interface.c:1644)
==8147==    by 0x406D1E: main (libvirtMib_subagent.c:156)
==8147== 
==8147== LEAK SUMMARY:
==8147==    definitely lost: 80 bytes in 1 blocks
==8147==    indirectly lost: 0 bytes in 0 blocks
==8147==      possibly lost: 368 bytes in 1 blocks
==8147==    still reachable: 396,362 bytes in 1,467 blocks
==8147==         suppressed: 0 bytes in 0 blocks
==8147== Reachable blocks (those to which a pointer was found) are not shown.
==8147== To see them, rerun with: --leak-check=full --show-reachable=yes
==8147== 
==8147== For counts of detected and suppressed errors, rerun with: -v
==8147== ERROR SUMMARY: 2 errors from 2 contexts (suppressed: 8 from 6)

Notes:
Comments:

		177080 	[libvirtd] libvirtd does not close all fds opened by virt-install - bug 766308 	ydu 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Check the fds opened by libvirted before starting installation:
# lsof | grep libvirt | wc
    90     754   10107
2. Install a guest via virt-install command, such as:

#virt-install -r 1000 -n test --disk path=/home/images/test.img,format=raw,bus=virtio,size=10 --network
bridge=br1,model=virtio --os-type=linux --os-variant=rhel6 --vcpus=2,maxvcpus=4 --cdrom=/tmp/rhel6.1-x86_64-dvd.iso

 

3. Check the fds opened by libvirted during installation:

# lsof | grep libvirt | wc 
    105     957   11177

4. Check the fds opened by libvirted after installation:

# lsof | grep libvirt | wc
    90     754   10107

 5. Restart libvirtd and check again.

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

# lsof | grep libvirt | wc
    90     754   10107

 

 

	
Expected Results:

As test steps output.
Notes:
Comments:

		177082 	[libvirtd] polkit authorization broken in libvirt 0.9.10 - bug 795978 	ydu 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

1. enable dbus.service
# /etc/rc.d/init.d/messagebus start
Starting system message bus:                               [  OK  ]
2. enable avahi-daemon.service
# /etc/init.d/avahi-daemon start
Starting Avahi daemon...                                   [  OK  ]
3. start libvirtd.service
# /etc/init.d/libvirtd start
Starting libvirtd daemon:                                  [  OK  ]
4. Switch to regular user and connect to libvirtd
$ virsh --connect qemu:///system
error: authentication failed: Authorization requires authentication but no
agent is available.

error: failed to connect to the hypervisor

	
Expected Results:

4. Connect to qemu should succeed. For now, the bug still has problem, need update the case after get confirm form developer.
Notes:
Comments:

		177083 	[libvirtd] /etc/init.d/libvirtd restart provides wrong status - bug 730510 	ydu 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1. log on a user with non-root Permissions.
2. $ /etc/init.d/libvirtd restart
rm: cannot remove `/var/run/libvirtd.pid': Permission deniedFAILED]
3. $ echo $?
   1

	
Expected Results:
Notes:
Comments:

		177084 	[libvirtd] /var/run/libvirtd.pid will be lost after restarting libvirtd with vdsm installed - bug 726649 	ydu 	None 	Manual 		Feature 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1.

# /etc/init.d/libvirtd restart
Stopping libvirtd daemon: libvirtd: libvirtd is managed by upstart and started, use initctl instead

# /etc/init.d/vdsmd restart
Shutting down vdsm daemon:
vdsm watchdog stop                                         [  OK  ]
vdsm stop                                                  [  OK  ]
Stopping libvirtd daemon: libvirtd: libvirtd is managed by upstart and started, use initctl instead
vdsm: libvirt already configured for vdsm                  [  OK  ]
Starting iscsid:
Starting up vdsm daemon:
vdsm start                                                 [  OK  ]

2.

# service vdsmd status
VDS daemon server is running
# service libvirtd status
libvirtd (pid  2521) is running...

3.
# cat /var/run/libvirtd.pid
2521
	
Expected Results:

Step 1 2 3 sucess
Notes:
Comments:

		177086 	[libvirtd] Check for domain being active on successful job acquire 	ydu 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. run the following shell script and pass it a domain name.

# cat test.sh

#!/bin/bash

if [ "x$1" = "x" ]; then
	echo "Pass me a name"
	exit 1
fi

for ((i=0; i<100; i++)); do
	echo $i
	virsh start $1 && sleep 10
	virsh qemu-monitor-command --hmp $1 'info block'

	virsh save $1 /tmp/tmp.save 
        sleep 10
done

ï»¿# sh test.sh $domain

 

2. check the status of libvirtd .

# service libvirtd status

	
Expected Results:

2. # service libvirtd status

libvirtd (pid  10381) is running...

 
Notes:
Comments:

		177090 	[libvirtd] fail to start guest with bad file descriptor error - bug 823716 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Prepare a guest.

2. In one terminal, run the following script, and pass a guest name for it:

ï»¿#!/bin/bash

for i in {1..100}

do

    echo "---start guest---"

    virsh start $1

    sleep 3

    echo "---destroy guest---"

    virsh destroy $1

    sleep 3
done

3. In another terminal, run :

 for i in {1..100}; do virsh list; sleep 1; done

ï»¿
	
Expected Results:

Actual results:

For now, there's bug of this case

2. During the script running, when start the guest, sometimes it will fail with

error: Failed to start domain kvm-rhel6u2-x86_64-new
error: Unable to wait for child process: Bad file descriptor

 

Expected results:

No fail.

 

 

Notes:
Comments:

		177094 	[libvirtd] Libvirt client may crash when it closes connection to libvirtd - bug 800185 	ydu 	None 	Manual 		Function 	P2 	None 	Edit
Setup:

Pls install package "libvirt-devel-*.rpm".
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1 . Complie the C file, rpc-crash.c

#include <stdlib.h>
#include <stdbool.h>
#include <signal.h>
#include <pthread.h>
#include <libvirt/virterror.h>

static bool run = true;

static void *eventLoop(void *opaque)
{
    virConnectPtr conn = opaque;

    while (run) {
        if (virEventRunDefaultImpl() < 0)
            break;
    }

    return NULL;
}

static void dummyTimer(int timer, void *opaque)
{
    /* nothing to be done here */
}

int main(int argc, char **argv)
{
    virConnectPtr conn;
    pthread_t eventThread;
    int timer;

    virInitialize();
    virEventRegisterDefaultImpl();
    signal(SIGPIPE, SIG_IGN);

    conn = virConnectOpenAuth(argc > 1 ? argv[1] : NULL,
                              virConnectAuthPtrDefault, 0);
    if (!conn)
        return 1;

    if (pthread_create(&eventThread, NULL, eventLoop, conn)) {
        perror("pthread_create");
        return 1;
    }

    sleep(5);

    virConnectClose(conn);

    run = false;

    timer = virEventAddTimeout(0, dummyTimer, NULL, NULL);

    pthread_join(eventThread, NULL);

    if (timer != -1)
        virEventRemoveTimeout(timer);

    return 0;
}

2. gcc -o rpc-crash rpc-crash.c -lvirt -g

3. run rpc-crash in a loop

4. see if it crashes

 
	
Expected Results:

4. No 'Segmentation fault (core dumped)',

    but  maybe there're some warning output, like:

2012-03-07 11:05:19.225+0000: 8505: warning : virEventPollUpdateTimeout:262 :
Ignoring invalid update timer -1

 
Notes:
Comments:

		177095 	[libvirtd] libvirt lost SIGHUP reloading behavior - bug 730428 	ydu 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1. check the libvirtd status

# service libvirtd status

libvirtd (pid  5717) is running...

 

2. # kill -SIGHUP `pidof libvirtd`

3. check the libvirtd status again

# service libvirtd status

libvirtd (pid  5717) is running...

 

	
Expected Results:

This should NOT appear:

# service libvirtd status
libvirtd dead but pid file exists

Notes:
Comments:

		177096 	[libvirtd] libvirt unresolved dependency: dmidecode Bug: 782444 	zhpeng 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    libvirtd

bug:

    No bug found

Actions:

Step 1. # rpm -qRp libvirt-0.9.9-2.el6.s390x.rpm |grep -i dmi 
	
Expected Results:

Step 1 get nothing
Notes:
Comments:

		177097 	[libvirtd] libvirtd crash/hang at virFDStreamEvent when qemu guest shuts down - BZ#716393 	ydu 	None 	Manual 		Feature 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. virsh # start domain1 --console and leave it open.
2. On qemu guest vnc console, type halt as root.
3. Loop Step1 & Step2 more than 3 times.

	
Expected Results:

No error.
Notes:
Comments:

		177101 	[libvirtd] Make avahi failure on startup non-fatal - bug 785269 	ydu 	None 	Manual 		Function 	P2 	None 	Edit
Setup:

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

Verify the dependency on avahi removed.

1. Prepare a new host with avahi and avahi-libs and libvirt releated NOT installed. 

  #rpm -qa|grep avahi

  #rpm -qa|grep libvirt

If already installed, you can remove them

  #rpm -e avahi avahi-libs --nodeps

  #rpm -e libvirt --nodeps

2. Down load the latest libvirt rpm package and install them

  #rpm -ivh libvirt-*

There should a dependency error like:

error: Failed dependencies:
	avahi-libs is needed by libvirt-0.9.10-0rc2.el6.x86_64
	libavahi-client.so.3()(64bit) is needed by libvirt-0.9.10-0rc2.el6.x86_64
	libavahi-common.so.3()(64bit) is needed by libvirt-0.9.10-0rc2.el6.x86_64

 

It will requireï»¿ avahi-libs installed, but NOT require avahi installed. 

3. After install abahi-libs, all libvirt packages can install and libvirtd can ï»¿start successfully.

# service libvirtd restart
Stopping libvirtd daemon:                                  [FAILED]
Starting libvirtd daemon:                                  [  OK  ]

 

 
	
Expected Results:

As step2 & step3.
Notes:
Comments:

		177109 	[libvirtd] per-device boot elements cannot be used together with os/boot elements - bug 751287 	ydu 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    866288 - From Run 47627

Actions:

1. Create a new domain XML with both <boot dev='...'/> and <boot order='...'/>   elements. Such as:

# cat test.xml |grep  -w3  boot 
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
--
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
       <boot order='1'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>

2. Define the guest. 

#virsh define test.xml 

	
Expected Results:

2. #virsh define test.xml

error: Failed to define domain from test.xml
error: unsupported configuration: per-device boot elements cannot be used
together with os/boot elements

Notes:
Comments:

		177110 	[libvirtd] QEMU attach a nonexistent PID 	ydu 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. get  a random number and make sure no such PID exists

 e.g. 12345678

2. virsh qemu-attach $PID

3. observe libvirtd SIGSEGV 

#service libvirtd status

	
Expected Results:

2.

# virsh qemu-attach 12345678
error: Failed to open file '/proc/12345678/cmdline': No such file or directory

3.

# service libvirtd status
libvirtd (pid  10381) is running...

 
Notes:
Comments:

		177118 	[libvirtd] Restart libvirtd without killing guest - bug 816465 	ydu 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Prepare a running domain
# virsh list
 Id    Name                           State
----------------------------------------------------
 100   vr-rhel6u2-x86_64-kvm          running

2. Run the script(During restart the libvirtd service, do some actions, like
save/restore a domain)
# cat repro.sh
#! /bin/bash

for i in {1..100}; do
    service libvirtd restart
    virsh save vr-rhel6u2-x86_64-kvm /tmp/test.save
    virsh restore /tmp/test.save
    service libvirtd status
done 

	
Expected Results:

2. libvirtd will not die during the script running.
Notes:
Comments:

		177120 	[libvirtd] run libvirtd foreground BZ#831044 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Stop libvirtd service 
# service libvirtd stop
Stopping libvirtd daemon:                                  [  OK  ]
2. Run libvirtd foreground
# libvirtd

	
Expected Results:

No error or get errors with clear message " Permission denied"

it looks like:

2012-09-11 05:47:44.173+0000: 2957: error : virCommandWait:2332 : internal error Child process (/sbin/iptables --table nat --delete POSTROUTING --source 192.168.122.0/24 -p tcp ! --destination 192.168.122.0/24 --jump MASQUERADE --to-ports 1024-65535) unexpected exit status 1: libvir:  error : cannot execute binary /sbin/iptables: Permission denied

Notes:
Comments:

		177123 	[libvirtd] start libvirtd on s390x machine - BZ#698208 & BZ#678027 	ydu 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:

Make sure run this test on s390x machine.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1. #service libvirtd start

2.#ps ax | grep -e udev -e libvirt

3. #service libvirtd restart

4. #service libvirtd start

5. #virsh list --all
	
Expected Results:
Notes:
Comments:

		177127 	[libvirtd] virsh crashes at remoteStreamEventTimer when libvirtd restarts - BZ#716781 	ydu 	None 	Manual 		Feature 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Open a terminal, execute the cmd and leave it open.

    virsh # start domain1 --console

2. kill -TERM libvirtd and restart.

3. Type any key in virsh.

4. Retry connecting to the console.

  virsh # console domain1

5. Retry connecting to the console.

virsh # console domain1
	
Expected Results:

1

2

3 virsh # list
error: Failed to list active domains
error: Cannot write data: Broken pipe
error: Reconnected to the hypervisor

4. 5 all can connect the console

 
Notes:
Comments:

		177128 	[libvirtd] virt-manager causes iowait, due to rewriting XML files repeatable - bug 831149 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Run command, observe IO-wait via 'vmstat 1' column "wa"

#vmstat 1

2. Run the following command in other terminal

#strace -f -p `pgrep libvirtd` -s 512 -e trace=rename

3. Start virt-manager, and observe all the syscall rename calls from the strace

3.1 Destroy  a domain via virt-manager, 

3.2 Start a domain via virt-manager,

	
Expected Results:

3.1 what's show up in strace, like:
[pid 25648] rename("/var/run/libvirt/qemu/aaa.xml.new", "/var/run/libvirt/qemu/aaa.xml") = 0

At same time the wa value stay low.

3.2 what's show up in strace, like:
[pid 25645] --- SIGCHLD (Child exited) @ 0 (0) ---
[pid 25642] rename("/var/run/libvirt/qemu/aaa.xml.new", "/var/run/libvirt/qemu/aaa.xml") = 0
Process 26127 attached
Process 26128 attached
[pid 26124] --- SIGALRM (Alarm clock) @ 0 (0) ---
[pid 25642] rename("/var/run/libvirt/qemu/aaa.xml.new", "/var/run/libvirt/qemu/aaa.xml") = 0

Also, the wa value stay low.

 
Notes:
Comments:

		177132 	[libvirtd]libvirtd crash when start a net with special MAC address Bug#817234 , 823765 	whuang 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:

libvirtd Do not crash when start a net with special MAC address

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    823765 - From Run 43379
    866364 - From Run 47627

Actions:

1)

virsh # net-list
Name                 State      Autostart
-----------------------------------------
default              active     yes      

2)
virsh # net-destroy default
Network default destroyed

3)
virsh # net-edit default
Network default XML configuration edited.
<network>
  <name>default</name>
  <uuid>0adda191-5e1a-43ef-9039-240076c9a6ca</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='00:00:00:00:00:00'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.122.2' end='192.168.122.254' />
    </dhcp>
  </ip>
</network>

4)
virsh # net-start default
error: Failed to start network default
error: Cannot set interface MAC on 'virbr0-nic': Cannot assign requested address

5)
[root@zhpeng ~]# service libvirtd status
libvirtd dead but pid file exists

	
Expected Results:

After step4 

libvirtd do not crash 
Notes:
Comments:

		177144 	[Log and debugging] Improve libvirt debug capability - bug 818467 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

This case aim to test the libvirt debug capability. That means if libvirt get error, such as qemu-kvm error, libvirt can give user more usful information in libvirtd.log.


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging

bug:

    No bug found

Actions:

1. rpm install qemu-kvm and libvirt packages 
Notes, make sure 'usbredir' hasn't been installed on the host.

2. service libvirtd start or run virt-manger

3. see libvirtd.log or virt-manager error message 

	
Expected Results:

 

3 Actual results:

 

 For libvirtd.log:

2012-08-23 09:45:53.495+0000: 11237: error : virCommandWait:2332 : internal error Child process (LC_ALL=C PATH=/sbin:/usr/sbin:/bin:/usr/bin /usr/libexec/qemu-kvm -help) unexpected exit status 127: /usr/libexec/qemu-kvm: error while loading shared libraries: libusbredirparser.so.0: cannot open shared object file: No such file or directory

2012-08-23 09:45:53.497+0000: 11237: error : virCommandWait:2332 : internal error Child process (LC_ALL=C PATH=/sbin:/usr/sbin:/bin:/usr/bin /usr/libexec/qemu-kvm -help) unexpected exit status 127: /usr/libexec/qemu-kvm: error while loading shared libraries: libusbredirparser.so.0: cannot open shared object file: No such file or directory

For virt-manager error msg:

Error starting domain: internal error Child process (LC_ALL=C PATH=/sbin:/usr/sbin:/bin:/usr/bin /usr/libexec/qemu-kvm -help) unexpected exit status 127: /usr/libexec/qemu-kvm: error while loading shared libraries: libusbredirparser.so.0: cannot open shared object file: No such file or directory
using virt-manager to to connect libvirtd.
Notes:
Comments:

		177145 	[Log and debugging] Check the install.log for all libvirt related pkgs 	dyuan 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging

bug:

    No bug found

Actions:

1. install an os tree with all virtualization pkgs, such as:

http://download.englab.nay.redhat.com/pub/rhel/released/RHEL-6/6.2/Server/x86_64/os/

Tips: please use the correct os tree during the corresponding test phase, if you are not sure about it, make sure to confirm with the project leader.

2. check /root/install.log after the installation completes
	
Expected Results:

step 2:

Expected results:
...
Installing libvirt-0.9.9-2.el6.x86_64
Installing libvirt-client-0.9.9-2.el6.x86_64
Installing libvirt-python-0.9.9-2.el6.x86_64
Installing libvirt-java-0.4.7-1.el6.noarch
Installing libvirt-java-devel-0.4.7-1.el6.noarch
Installing perl-Sys-Virt-0.9.9-1.el6.x86_64
Installing net-snmp-5.5-37.el6_2.1.x86_64
Installing libvirt-devel-0.9.9-2.el6.x86_64
Installing libvirt-cim-0.6.0-1.el6.x86_64
Installing libcmpiutil-0.5.6-1.el6.x86_64
Installing libvirt-qmf-0.3.0-4.el6.x86_64
Installing libvirt-snmp-0.0.2-3.el6.x86_64
Installing netcf-libs-0.1.9-2.el6.x86_64
Installing virt-manager-0.9.0-8.el6.x86_64
Installing python-virtinst-0.600.0-6.el6.noarch
Installing virt-viewer-0.4.1-9.el6.x86_64
Installing virt-top-1.0.4-3.11.el6.x86_64
Installing virt-v2v-0.8.3-5.el6.x86_64
...

 no error or warning msgs in the log.
Notes:
Comments:

		177146 	[Log and debugging] Check the vm-pid in VIRT_CONTROL 	dyuan 	None 	Manual 		Function 	P2 	None 	Edit
Setup:

Bug 786534 - Add vm-pid to VIRT_CONTROL audit events
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging

bug:

    No bug found

Actions:

1. startup a vm

# virsh start rhel6

2. run ausearch -m VIRT_CONTROL

# ausearch -m VIRT_CONTROL

3. run ps -ef

# ps -ef|grep rhel6
	
Expected Results:

step 2:

Get the vm-pid field for current guest rhel6.

...snip...
time->Wed Feb 15 14:36:26 2012
type=VIRT_CONTROL msg=audit(1329287786.381:110288): user pid=2557 uid=0
auid=4294967295 ses=4294967295 subj=system_u:system_r:virtd_t:s0-s0:c0.c1023
msg='virt=kvm op=start reason=restored vm="rhel6"
uuid=4f2e1779-7040-702c-efd0-380e87f73a5d vm-pid=29067:
exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=?
terminal=? res=success'

 

 step 3:

Get the pid of the qemu process for the guest rhel6, it should be the same as vm-pid.

qemu     29067  0.4  4.3 1323320 323504 ?      Sl   14:36   0:04
/usr/libexec/qemu-kvm -S -M rhel6.2.0 -enable-kvm -m 1024 -smp
1,sockets=1,cores=1,threads=1 -name rhel6 -uuid
4f2e1779-7040-702c-efd0-380e87f73a5d -nodefconfig -nodefaults -chardev
socket,id=charmonitor,path=/var/lib/libvirt/qemu/rhel6.monitor,server,nowait
-mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown
-device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive
file=/var/lib/libvirt/images/rhel62.img,if=none,id=drive-virtio-disk0,format=raw,cache=none
-device
virtio-blk-pci,scsi=off,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1
-netdev tap,fd=21,id=hostnet0,vhost=on,vhostfd=23 -device
virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:32:3e:2f,bus=pci.0,addr=0x3
-chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0
-device usb-tablet,id=input0 -vnc 127.0.0.1:1 -vga cirrus -device
intel-hda,id=sound0,bus=pci.0,addr=0x4 -device
hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -incoming fd:19 -device
virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6


Notes:
Comments:

		177150 	[Log and debugging] null dereference while preparing libvirt logs - bug 728546 	ydu 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    log and debugging

bug:

    No bug found

Actions:

1. Edit /etc/libvirt/libvirtd.conf.
   set log_level = 1
   set log_outputs = "1:file:/tmp/libvirtd.log"
2. kill libvirtd process
    # kill `pidof libvirtd`
3. start libvirtd in foreground:
   # libvirtd
4. #virsh destroy test
    #virsh start test

    # virsh domuuid test 
   92dd267d-3ac5-4338-9d18-328cf3526a88

    #grep 92dd267d-3ac5-4338-9d18-328cf3526a88 -i /tmp/libvirtd.log 

	
Expected Results:

4. can get the VM name/uuid/etc in domain messages

------
20:31:24.331: 3152: debug : virLockManagerLogParams:98 :   key=uuid type=uuid
value=92dd267d-3ac5-4338-9d18-328cf3526a88
20:31:24.334: 3152: debug : virDomainFree:2144 : dom=0x7f936c110590, (VM:
name=test, uuid=92dd267d-3ac5-4338-9d18-328cf3526a88), 
20:31:24.334: 3152: debug : virReleaseDomain:238 : release domain
0x7f936c110590 test 92dd267d-3ac5-4338-9d18-328cf3526a88
20:31:24.335: 3154: debug : virDomainLookupByUUID:1922 : conn=0x7f935c000a60,
uuid=92dd267d-3ac5-4338-9d18-328cf3526a88
20:31:24.335: 3154: debug : virDomainFree:2144 : dom=0x7f9364007700, (VM:
name=test, uuid=92dd267d-3ac5-4338-9d18-328cf3526a88), 
20:31:24.335: 3154: debug : virReleaseDomain:238 : release domain
0x7f9364007700 test 92dd267d-3ac5-4338-9d18-328cf3526a88
20:39:45.498: 3155: debug : virDomainFree:2144 : dom=0x7f93600008e0, (VM:
name=test, uuid=92dd267d-3ac5-4338-9d18-328cf3526a88), 
------

 
Notes:
Comments:

		177151 	[Log and debugging] null dereference while preparing libvirt logs - BZ#711206 	ydu 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:

1. set value of "log_level" in "/etc/libvirt/libvirtd.conf" as 1

2. set value of "log_outputs" in "/etc/libvirt/libvirtd.conf" as "1:file:libvirtd.log"

3. if libvirtd process is running, kill it

    # kill `pidof libvirtd`

4. start it in foreground:

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    log and debugging
    Regression

bug:

    No bug found

Actions:

1. virsh destroy guest 

2. virsh start guest

3. grep domain -i libvirtd.log

4.# cat > demo.c <<EOF
#include <libvirt/libvirt.h>
#include <stdlib.h>

int main(void) {
  virInitialize();
  virDomainDestroy(NULL);
}

EOF

# gcc -o demo -lvirt demo.c



# LIBVIRT_DEBUG=1 ./demo 2>&1 | grep Destroy
15:57:42.131: 10388: debug : virDomainDestroy:2056 : dom=(nil), (VM: name=(null), uuid=), 
libvir: Domain error : invalid domain pointer in virDomainDestroy

5.check the libvirtd is stil alive.

	
Expected Results:

3. #grep 92dd267d-3ac5-4338-9d18-328cf3526a88 -i /tmp/libvirtd.log 
------
20:31:24.331: 3152: debug : virLockManagerLogParams:98 :   key=uuid type=uuid
value=92dd267d-3ac5-4338-9d18-328cf3526a88
20:31:24.334: 3152: debug : virDomainFree:2144 : dom=0x7f936c110590, (VM:
name=test, uuid=92dd267d-3ac5-4338-9d18-328cf3526a88), 
20:31:24.334: 3152: debug : virReleaseDomain:238 : release domain
0x7f936c110590 test 92dd267d-3ac5-4338-9d18-328cf3526a88
20:31:24.335: 3154: debug : virDomainLookupByUUID:1922 : conn=0x7f935c000a60,
uuid=92dd267d-3ac5-4338-9d18-328cf3526a88
20:31:24.335: 3154: debug : virDomainFree:2144 : dom=0x7f9364007700, (VM:
name=test, uuid=92dd267d-3ac5-4338-9d18-328cf3526a88), 
20:31:24.335: 3154: debug : virReleaseDomain:238 : release domain
0x7f9364007700 test 92dd267d-3ac5-4338-9d18-328cf3526a88
20:39:45.498: 3155: debug : virDomainFree:2144 : dom=0x7f93600008e0, (VM:
name=test, uuid=92dd267d-3ac5-4338-9d18-328cf3526a88), 
------

 
Notes:
Comments:

		177153 	[Log and debugging] Starting libvirtd with libvirtd.conf containing an unterminated string - bug 728654 	dyuan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging
    Regression

bug:

    No bug found

Actions:

1. Adding the following to libvirtd.conf

log_outputs="1:file:/var/log/libvirt/libvirt_debug.log

2. # service libvirtd restart

3. # libvirtd 

	
Expected Results:

step 2:

Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon: 05:10:16.742: 11484: info : libvirt version: 0.9.4,
package: 5.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>,
2011-08-22-12:26:05, x86-009.build.bos.redhat.com)
05:10:16.742: 11484: error : main:1394 : Can't load config file
'/etc/libvirt/libvirtd.conf' 


step 3:

05:10:35.896: 11488: info : libvirt version: 0.9.4, package: 5.el6 (Red Hat,
Inc. <http://bugzilla.redhat.com/bugzilla>, 2011-08-22-12:26:05,
x86-009.build.bos.redhat.com)
05:10:35.896: 11488: error : main:1394 : Can't load config file
'/etc/libvirt/libvirtd.conf'

 

Notes:
Comments:

		177154 	[Log and debugging] Coredump filter to exclude KVM guest OS memory out of QEMU process -- Bug 822641 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

libvirt just pass the "dumpCore" to qemu-kvm commandline as "dump-guest-core", you can also check the coredump file for further testing related to qemu-kvm function:

Trigger a core dump

# kill -s SIGSEGV `pidof qemu-kvm`

# ll core.${pid}

# gdb /usr/libexec/qemu-kvm ${coredump-file}

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Log and debugging

bug:

    No bug found

Actions:

1.
# virsh dumpxml rhel6
snip..
<memory dumpCore='off' unit='KiB'>1000000</memory>
snip..

# virsh start rhel6

# ps aux|grep qemu-kvm

qemu      7737 31.5  0.3 1627160 30024 ?       Sl   18:26   0:02 /usr/libexec/qemu-kvm -name rhel62 -S -machine rhel6.3.0,**dump-guest-core=off** -enable-kvm -bios /usr/share/seabios/bios.bin -m 977 -smp 2,maxcpus=4,sockets=4,cores=1,threads=1 -uuid e601feea-ebc3-ed65-d4ab-252c3370470b -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/rhel62.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc...

2.
# virsh edit rhel6
set the dumpCore='on': <memory dumpCore='on' unit='KiB'>1000000</memory>

restart the guest, then check the option for qemu-kvm

# ps aux|grep qemu-kvm
qemu      7905 14.0  0.3 1626600 30184 ?       Sl   18:29   0:00 /usr/libexec/qemu-kvm -name rhel62 -S -machine rhel6.3.0,**dump-guest-core=on** -enable-kvm...

set the dumpCore to other values, such as "00","1", "offf", can't update the xml successfully.
# virsh edit rhel6
error: XML error: Bad value 'offf'
Failed. Try again? [y,n,f,?]:

 
	
Expected Results:
Notes:
Disable this one since is duplicated with 222473 [Guest kernel debugging] Coredump filter to exclude KVM guest OS memory out of QEMU process BZ 822641 ---lsu
Comments:

		177155 	[Longevity] 256 pools active on 1 machine for 1 week after 10 times libvirtd reload 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

do cases

158789 [Scalability] 256 autostarted storage pools reloading

first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    longevity

bug:

    No bug found

Actions:

Keep the storage pools for 2 weeks
	
Expected Results:

After 2 weeks, check the pools still active as 2 weeks before and can still generate volume on them
Notes:
Comments:

		177158 	[Longevity] Guest running 1 week with maximum attached disks number (hotplug) 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

do cases

172501 [Scalability] Attach/Detach virtual disks to guest over the top limitation via attach-disk

first and keep the attached disk without detach
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    longevity

bug:

    No bug found

Actions:

Keep guest running 2 weeks
	
Expected Results:

After 2 weeks, check the disks on guest still exists as 2 weeks before
Notes:
Comments:

		177159 	[Longevity] Guest running 1 week with maximum attached interfaces number (hotplug) 	weizhan 	weizhan 	Manual 		Function 	P1 	None 	Edit
Setup:

do cases

173021 [Scalability] Attach/Detach virtual interfaces (vnet) to one guest to the top limitation of the guest

first and keep the attached interface on guest without detach
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    longevity

bug:

    No bug found

Actions:

keep the guest running for 2 weeks
	
Expected Results:

After 2 weeks, check the interfaces still exist on guest as 2 weeks before
Notes:
Comments:

		177161 	[Longevity] Keep guests running 1 week after 100 times migration 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare 15 running guests with different kinds of os

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

Prepare migration environment according to the case

124654 [Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4

2. Do 50 times migration 

#for i in {1..50}; do for guest in {win7-32,win7-64,win2008-32,win2008-64,win2008r2-64,win2003-32,win2003-64,winxp,rhel6u3-32,rhel6u3-64,rhel5u8-32,rhel5u8-64,rhel4u9-32,rhel4u9-64,rhel3u9-32}; do virsh migrate --live --undefinesource --persistent --p2p --tunnelled qemu+tls://{target ip}/system; done; for i in {win7-32,win7-64,win2008-32,win2008-64,win2008r2-64,win2003-32,win2003-64,winxp,rhel6u3-32,rhel6u3-64,rhel5u8-32,rhel5u8-64,rhel4u9-32,rhel4u9-64,rhel3u9-32}; do virsh -c qemu+ssh://{target ip}/system migrate --live --undefinesource --persistent --p2p --tunnelled qemu+tls://{source ip}/system; done; done


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    longevity

bug:

    No bug found

Actions:

After migration, leave the guests running 2 weeks
	
Expected Results:

After 2 weeks, checking:

all guests running normally

libvirtd status is running, and not hang

all the guests can still get ip as 2 weeks before
Notes:
Comments:

		177163 	[Longevity] libvirt python event handler running 1 week both on local and remote 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

do cases

124687 [Scalability] Libvirt event handler investigation - scenario 1

124688 [scalability] Libvirt event handler investigation - scenario 2

first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    longevity

bug:

    No bug found

Actions:

Keep the libvirt event handler running 2 weeks
	
Expected Results:

After 2 weeks, check it can still get events in cases 124687, 124688
Notes:
Comments:

		177168 	[Longevity] Running 1024 guests 1 week - bug 707155 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

run case 173023 [Scalability] Create 1024 guests on different kinds of virtual network (isolated, route , NAT) with DHCP

first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    longevity

bug:

    No bug found

Actions:

1. Record the host memory and cpu status

# top -p `pidof libvirtd`

2. Leave the guest running for 2 weeks
	
Expected Results:

After 2 weeks

check the guest status, all guests should still running

check the network status, all guests should still have network as 2 weeks before

check libvirtd status, libvirtd should still running

check the host cpu and memory usage, should not have big difference, especially

PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND                                                                              
12357 root      20   0  984m  22m 4340 S  0.0  0.3   8:03.44 libvirtd 
Notes:
Comments:

		177171 	[LXC] Application container suspend/resume - bug 773212 842979 857341 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Case should be confirmed after Bug 842979 - [Regression] lxc domain fail to start due to not exist cgroup dir is fixed.

Case should be confirmed after Bug 857341 - fail to start lxc domain is fixed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    Regression

bug:

    842979 - From Run 45525

Actions:

1. prepare an xml
# cat lxc.xml 
<domain type='lxc'>
  <name>toy</name>
  <uuid>d1f4798b-bebf-d93c-1d97-fe1c1cb7c780</uuid>
  <memory>500000</memory>
  <currentMemory>500000</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <interface type='network'>
      <mac address='52:54:00:25:bf:e9'/>
      <source network='default'/>
    </interface>
    <console type='pty'>
      <target type='lxc' port='0'/>
    </console>
  </devices>
</domain>

2.Define and start lxc guest.

# virsh -c lxc:/// define lxc.xml
# virsh -c lxc:/// start toy

3.List the guest status
# virsh -c lxc:/// list

4.Suspend the guest.
# virsh -c lxc:/// suspend toy
# virsh -c lxc:/// list

5. # cat /cgroup/freezer/libvirt/lxc/toy/freezer.state 


6.Resume the guest
# virsh -c lxc:/// resume toy

# virsh -c lxc:/// list --all

7. # cat /cgroup/freezer/libvirt/lxc/toy/freezer.state 

 

	
Expected Results:

2.For bug 842979, Shoud start normally, but get error like:

error: Failed to start domain toy
error: internal error guest failed to start: PATH=/bin:/sbin TERM=linux container=lxc-libvirt container_uuid=d1f4798b-bebf-d93c-1d97-fe1c1cb7c780 LIBVIRT_LXC_UUID=d1f4798b-bebf-d93c-1d97-fe1c1cb7c780 LIBVIRT_LXC_NAME=toy /bin/sh
2012-07-25 07:26:47.994+0000: 1: info : libvirt version: 0.9.13, package: 3.el6 (Red Hat, Inc.<http://bugzilla.redhat.com/bugzilla>, 2012-07-20-03:24:32, x86-010.build.bos.redhat.com)
2012-07-25 07:26:47.994+0000: 1: error : lxcContainerIdentifyCGroups:1282 : Unable to read directory /sys/fs/cgroup: No such file or directory
2012-07-25 07:26:47.996+0000: 18089: info : libvirt version: 0.9.13, package: 3.el6 (Red Hat, Inc.<http://bugzilla.redhat.com/bugzilla>, 2012-07-20-03:24:32, x86-010.build.bos.redhat.com)
2012-07-25 07:26:47.996+0000: 18089: error : virCommandWait:2314 : internal error Child process (ip link set veth1 netns 18090) status unexpected: exit status 2

 

For bug 857341, Shoud start normally, but get error like:

error: Failed to start domain toy
error: internal error guest failed to start: PATH=/bin:/sbin TERM=linux container=lxc-libvirt container_uuid=bb428983-cb9f-4702-0f8d-7d4e143d9aad LIBVIRT_LXC_UUID=bb428983-cb9f-4702-0f8d-7d4e143d9aad LIBVIRT_LXC_NAME=toy /bin/sh
2012-09-17 02:49:39.124+0000: 1: info : libvirt version: 0.10.1, package: 2.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2012-09-13-00:00:13, x86-009.build.bos.redhat.com)
2012-09-17 02:49:39.124+0000: 1: error : lxcContainerMountBasicFS:560 : Failed to mount /selinux on /selinux type selinuxfs: Device or resource busy
2012-09-17 02:49:39.124+0000: 1863: info : libvirt version: 0.10.1, package: 2.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2012-09-13-00:00:13, x86-009.build.bos.redhat.com)
2012-09-17 02:49:39.124+0000: 1863: error : virLXCControllerRun:1426 : error receiving signal from container: Input/output error

 

3. # virsh -c lxc:/// list
 Id Name State
----------------------------------
9591 toy running

4.# virsh -c lxc:/// suspend toy
Domain toy suspended

# virsh -c lxc:/// list
 Id Name                 State
----------------------------------
9591 toy                  paused

5.

FROZEN

 

6.# virsh -c lxc:/// resume toy
Domain toy resumed 

# virsh -c lxc:/// list --all

  Id Name State
----------------------------------
9591 toy running

7. 
THAWED

Notes:
Comments:

		177176 	[LXC] get domain infor via virsh dominfo - bug 819401 	dyuan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    rhel6.4

bug:

    No bug found

Actions:

# virsh -c lxc:/// define /tmp/toy.xml
Domain toy defined from /tmp/toy.xml

# virsh -c lxc:/// dumpxml toy
<domain type='lxc'>
  <name>toy</name>
  <uuid>bb428983-cb9f-4702-0f8d-7d4e143d9aad</uuid>
  <memory unit='KiB'>500000</memory>
  <currentMemory unit='KiB'>500000</currentMemory>
  <vcpu>4</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <console type='pty'>
      <target type='lxc' port='0'/>
    </console>
  </devices>
</domain>

Notes, defined a lxc guest with 4 vcpu.

# virsh -c lxc:/// dominfo toy
Id:             -
Name:           toy
UUID:           bb428983-cb9f-4702-0f8d-7d4e143d9aad
OS Type:        exe
State:          shut off
CPU(s):         4
Max memory:     500000 kB
Used memory:    500000 kB
Persistent:     yes
Autostart:      disable
Managed save:   unknown
Security model: selinux
Security DOI:   0

	
Expected Results:
Notes:
Comments:

		177178 	[LXC] interface devices are defined with a private network namespace 	dyuan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    rhel6.4.0

bug:

    No bug found

Actions:

1. Make sure a good lxc guest

 # virsh --connect lxc:///
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id    Name                           State
----------------------------------------------------
 -     lxc                            shut off

virsh # dumpxml lxc
<domain type='lxc'>
  <name>lxc</name>
  <uuid>2a0c59a1-519b-3865-582b-5339f5e1b95d</uuid>
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <features>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <interface type='network'>
      <mac address='00:16:3e:cb:17:2b'/>
      <source network='default'/>
    </interface>
    <console type='pty'>
      <target type='lxc' port='0'/>
    </console>
  </devices>
</domain>

2. Edit the guest, add the following to guest xml
<features>
  <privnet/>
</features>

virsh # edit lxc
error: internal error unexpected feature privnet

Known issue bug 805361. 

	
Expected Results:
Notes:
Comments:

		177192 	[LXC]define/start LXC Application container BZ#769752 #857341 	zhpeng 	None 	Manual 		Regression 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC

bug:

    No bug found

Actions:

1. Define a LXC guest
# cat single_toy.xml 
<domain type='lxc'>
  <name>single_toy</name>
  <uuid>386f5b25-43ee-9d62-4ce2-62c3809e47c1</uuid>
  <memory>500000</memory>
  <currentMemory>500000</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <interface type='network'>
      <source network='default'/>
    </interface>
    <console type='pty'>
      <target port='0'/>
    </console>
  </devices>
</domain>

# virsh -c lxc:/// define single_toy.xml 
Domain single_toy defined from single_toy.xml


2. Start the LXC guest 
# virsh -c lxc:/// start single_toy

	
Expected Results:

Step 1 & Step 2 succed.
Notes:
Comments:

		177194 	[Managed save] bypass the file system cache when saving and restoring guests 	dyuan 	None 	Manual (Autoproposed) 		Feature 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

1. prepare a running guest, then flush and free caches

# echo 3 > /proc/sys/vm/drop_caches

# head -n4 /proc/meminfo 
MemTotal:        8061616 kB
MemFree:         6529192 kB
Buffers:            1276 kB
Cached:            79276 kB

2. run virsh managedsave guest with --bypass-cache
# virsh managedsave --bypass-cache rhel6

3. check the value of Cache

# cat /proc/meminfo  | head -4 

 
4. start the domain with --bypass-cache
# virsh start rhel6  --bypass-cache 

5. check the value of Cache

# cat /proc/meminfo  | head -4 



6. set BYPASS_CACHE=1 in /etc/sysconfig/libvirt-guests

7. # service libvirt-guests stop
Running guests on default URI:rhel6
Suspending guests on default URI...
Suspending dom: done        

8. # cat /proc/meminfo  | head -4 

9. # service libvirt-guests start
Resuming guests on default URI...
Resuming guest rhel6: done

10. # cat /proc/meminfo  | head -4 

 

	
Expected Results:

step 3:

it didn't almost change.

MemTotal: 8061616 kB
MemFree: 6527232 kB
Buffers: 5760 kB
Cached: 86280 kB


step 5:

it didn't almost change.

MemTotal: 8061616 kB
MemFree: 6527232 kB
Buffers: 5760 kB
Cached: 84363 kB

 

step 8, step 10:

The cached mem should not have big change, just like the step3 and step5.
Notes:
Comments:

		177205 	[Managed save] Don't start libvirt-guests on upgrade libvirt-client - bug 660706 	dyuan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

#chkconfig libvirt-guests off, then test pass
1. # service libvirt-guests status
started

2. # service libvirt-guests stop
Running guests on default URI: no running guests.

3. # chkconfig libvirt-guests off
# chkconfig --list libvirt-guests
libvirt-guests  0:off   1:off   2:off   3:off   4:off   5:off   6:off

4. # rpm -Uvh libvirt-0.8.7-2.el6.x86_64.rpm
libvirt-client-0.8.7-2.el6.x86_64.rpm libvirt-python-0.8.7-2.el6.x86_64.rpm 
Preparing...                ########################################### [100%]
   1:libvirt-client         ########################################### [ 33%]
   2:libvirt                ########################################### [ 67%]
   3:libvirt-python         ########################################### [100%]

5. # service libvirt-guests status

	
Expected Results:

5.

stopped, with saved guests

 
Notes:
actions
4
rpm -Uvh the newest libvirt pkg for test
Expected Results
5.
stopped, with no saved guest
by xizhao
Comments:

		177206 	[Managed save] Ignore the incomplete save file and start guest normally - bug 730750 	dyuan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:



	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

1. Power on a guest.
# virsh start guest

2. Start to save the image.
# virsh managedsave guest

- Before the save file is complete, make a copy of it. 
Then cancel the save process.
  i.e.
  cp /var/lib/libvirt/qemu/save/guest.save /root/
  This simulates a corrupt image.

3.

# virsh managedsave guest
^C

4. - Shutdown the guest
# virsh shutdown guest

5. Copy the incomplete file back
  i.e.
  cp /root/guest.save /var/lib/libvirt/qemu/save/guest.save

6 Now power on guest
# virsh start guest

# ll /var/lib/libvirt/qemu/save/guest.save

	
Expected Results:

6.
domain will boot normally and remove the incomplete save file.

# virsh start geust
Domain guest started

# ll /var/lib/libvirt/qemu/save/guest.save

 ls: cannot access /var/lib/libvirt/qemu/save/guest.save: No such file or directory


Also get the following libvirtd.log:
15:30:36.635: 10074: warning : qemuDomainObjStart:4857 : Ignoring incomplete
managed state /var/lib/libvirt/qemu/save/guest.save

Notes:
duplicate with https://tcms.engineering.redhat.com/case/177209/?from_plan=6578.
Comments:

		177207 	[Managed save] Implement START_DELAY in libvirt-guests before starting next guest 	dyuan 	None 	Manual 		Feature 	P1 	None 	Edit
Setup:

Note: Domains which were running on shutdown are automatically restored via "libvirt-guests" when host boots up regardless on their autostart settings.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

1. Start 5 guests and keep them running
# virsh list --all
 Id Name                 State
----------------------------------
 49 rhel61_1             running
 50 rhel61_2             running
 51 rhel61_3             running
 52 rhel61_4             running
 53 rhel61_5             running

2. Edit /etc/sysconfig/libvirt-guests and set the start delay to 20 seconds
# number of seconds to wait between each guest start
#START_DELAY=0
START_DELAY=20

3. # service libvirt-guests restart



	
Expected Results:

step 3:

The start_delay between resuming the 5 guests are 20 seconds

Running guests on default URI: rhel61_1, rhel61_2, rhel61_3, rhel61_4, rhel61_5
Suspending guests on default URI...
Suspending rhel61_1: done         
Suspending rhel61_2: done         
Suspending rhel61_3: done         
Suspending rhel61_4: done         
Suspending rhel61_5: done        
Resuming guests on default URI...
Resuming guest rhel61_1: done
Resuming guest rhel61_2: done
Resuming guest rhel61_3: done
Resuming guest rhel61_4: done
Resuming guest rhel61_5: done

 

Notes:
Comments:

		177211 	[Managed save] managed save should not crash libvirtd for <cpu mode='host-model'> guest - bug 801160 	dyuan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

1. 
# cat crash-libvirt.sh
cat > /tmp/crashvm.xml <<EOF
<domain type='kvm'>
  <name>crashvm</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>16</vcpu>
  <os>
    <type arch='x86_64' machine='pc'>hvm</type>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <cpu mode='host-model' match='exact'>
 <model fallback='forbid'/>
 <topology sockets='2' cores='4' threads='2'/>
 </cpu>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
</domain>
EOF

2. 
# virsh define /tmp/crashvm.xml

Domain crashvm defined from /tmp/crashvm.xml


# virsh start crashvm

Domain crashvm started


# virsh managedsave crashvm

Domain crashvm state saved by libvirt


3. 
# virsh start crashvm

Domain crashvm started

 

	
Expected Results:
Notes:
Comments:

		177219 	[Managed save] The managed save image is removed when undefining the domain with "--manage-save" flag 	dyuan 	dyuan 	Auto 		Feature 	P1 	None 	Edit
Setup:

Undefine a domain with managed save image, then start a new domain with the doman name.

Confirm the managed save image is removed when undefining the domain with "--manage-save"  flag specified. And can start a new guest with the same domainname.

 Bug 697742 - libvirt: remove managedsave data when a domain is undefined


	
Breakdown:

 

 

 
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. start a domain
# virsh list --al

 Id Name                 State
----------------------------------
1  rhel6                Running

2. managed save the guest
# virsh mananedsave rhel6

# ls /var/lib/libvirt/qemu/save/rhel6.save 
/var/lib/libvirt/qemu/save/rhel6.save

3. undefine guest

# virsh dumpxml rhel6 > rhel6.xml

# virsh undefine rhel6

4. undefine the guest with --managed-save

# virsh undefine rhel6 --managed-save

# virsh list --all
 Id Name                 State
----------------------------------


5. check the save file

# ls /var/lib/libvirt/qemu/save/rhel6.save 


6. start another guest with the same name 

# virsh define rhel6.xml

# virsh start rhel6

	
Expected Results:

step 3:

# virsh undefine rhel6
error: Refusing to undefine while domain managed save image exists

step 4:

# virsh undefine rhel6 --managed-save
Domain rhel6 has been undefined

step 5:

# ls /var/lib/libvirt/qemu/save/rhel6.save
ls: cannot access /var/lib/libvirt/qemu/save/rhel6.save: No such file or directory

step 6:

The guest should be started successfully .
Notes:
Comments:

		177220 	[Managed save]List the domains with managed save state-bug810799 	honzhang 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

1. Start one domain

#virsh start  <domain>

2. Managed save the domain.

# virsh managesave <domain>

3. List the managedsave domains.

# virsh list --managed-save --inactive
 Id    Name                           State
----------------------------------------------------
 -     rhel6u31                       saved

# virsh list --managed-save --all
 Id    Name                           State
----------------------------------------------------
 -     rhel6u31                       saved

	
Expected Results:

Step3. The state of managedsave domain display " saved" .
Notes:
virsh managesave change to virsh managedsave
Comments:

		177234 	[memory management] unit test for memory - bug 808522 & 813972 	dyuan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    memory

bug:

    No bug found

Actions:

1. Create the guest with 1G memory size.

  <memory unit='MB'>1000</memory>
  <currentMemory unit='MB'>1000</currentMemory>


2. 
# virsh dumpxml rhel6 |grep -i memory
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>


3. create the guest with 1.5G memory size with the following xml

 <memory units="GiB">1.5</memory>

<currentMemory unit='GiB'>1.5</currentMemory> 
if the Bug 893936  is fixed,All of the following will be excuting successfully and no reporting the following error 

# virsh edit rhel6
error: XML error: could not parse memory element ./memory[1]

# virsh define /tmp/rhel6.xml 
error: Failed to define domain from /tmp/xp.xml
error: XML error: could not parse memory element ./memory[1]

# virsh create /tmp/rhel6.xml 
error: Failed to create domain from /tmp/xp.xml
error: XML error: could not parse memory element ./memory[1]

4  check the upper setting with the following the command .

# virsh dumpxml rhel6 |grep -i memory
  <memory unit='KiB'>1572864</memory>
  <currentMemory unit='KiB'>1572864</currentMemory>


5. round up the max memory silently to avoid the cur > max

Create a guest with the following xml:
  <memory unit='MB'>1000</memory>
  <currentMemory unit='MB'>1000</currentMemory>

6. 
# virsh dumpxml rhel6|grep mem -i
  <memory unit='KiB'>976896</memory>
  <currentMemory unit='KiB'>976896</currentMemory>

7. log in the guest, check the memory usage :


	
Expected Results:

step 7

in windows: pleas check the memory info in task manager

in linux, please check the memory info with /proc/meminfo

The total memory should the same as currentMemory size.
Notes:
Add checkpoints to check the domain xmls: accept floats in memory sizes,this checkpoint is relative with the bug 893936 and 893933
--zhwang
Comments:

		177235 	[Migration] Do live migration from rhel6.1 to rhel6.2 or later with pci 0:0:2.0 not assign to video - bug 771603 	weizhan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts, 1 is rhel6.1 release version, the other  is rhel6.2 release or later

2. Prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. Prepare a guest with following xml without video device

<domain type='kvm' id='6'>
  <name>test</name>
  <uuid>0201818b-2c72-28c7-7067-60129f29efa2</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.1.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/mnt/test.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:31:14:4b'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/1'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/1'>
      <source path='/dev/pts/1'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c286,c293</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c286,c293</imagelabel>
  </seclabel>
</domain>

2. Do live migration from rhel6.1 to rhel6.2 or later

# virsh migrate --live guest qemu+ssh://{rhel6.2 host ip}/system

 
	
Expected Results:

Migration should succeed without error
Notes:
Comments:

		177239 	[Migration] Break migration with gdb help - bug 795305, 806206 	weizhan 	weizhan 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare a nfs server

write /etc/exports with

/var/lib/libvirt/images    *(rw,no_root_squash,async)

Then start nfs service

# service nfs start

Stop iptables

# iptables -F

2. Prepare 2 hosts and mount nfs dir on both hosts in same dir, and setting the virt_use_nfs boolean on both sides

  # mount  $NFS_IP:/var/lib/libvirt/images /mnt  -o vers=3
  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

  # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    regression

bug:

    No bug found

Actions:

1. Start a guest on host1 with images in /mnt

2. # gdb virsh 

    (gdb)br virDomainMigrateVersion3 
    (gdb)r 

3.  virsh # migrate --live guest qemu+ssh://10.66.85.92/system --persistent 

4. set another breakpoint just after ret = dconn->driver->domainMigratePrepare3

    (gdb) br 4818         
    *NOTE* because the line in different version may be changed so you need to make sure the line with '(gdb) l'  several times
5.(gdb)c  

6. kill qemu-kvm on destination host 

7. quit gdb 

8. Destroy and start the same guest 

	
Expected Results:

Step 3
migration will stop at breakpoint

Step 5
migration will stop at second breakpoint

Step 8
Start guest will succeed without error

Should not get error like "cannot acquire state change lock"

Notes:
Comments:

		177241 	[migration] check the host's uuid before migration 	whuang 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

edit host1 and host2 's /etc/libvirt/libvirtd.conf  Both

 

host_uuid = "00000000-0000-0000-0000-000000000001"

 

then restart libvirtd

 

#service libvirtd restart

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration

bug:

    No bug found

Actions:

1)in host1 start a guest

 

#virsh start mig

 

2)try to migration

#virsh migrate mig qemu+ssh://$host2/system
root@10.66.85.208's password:
error: internal error Attempt to migrate guest to the same host 00000000-0000-0000-0000-000000000001


	
Expected Results:

should show this error !

error: internal error Attempt to migrate guest to the same host 00000000-0000-0000-0000-000000000001

 

 
Notes:
Comments:

		177245 	[Migration] Do libvirt statistics command loop during migration 	weizhan 	weizhan 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. A RHEL6 host with kvm, with a running domain.

2. Another kvm host, which also is a RHEL6.

3. On both source host, and target host, libvirtd is running.

4. On both source and target machine:
    #iptables -F
    #setenforce 1

   #setsebool virt_use_nfs  on


5. Setup nfs service on Source target machine
 
    5.1 add following line into "/etc/exports"

     /var/lib/libvirt/images 10.66.70.144(rw,no_root_squash,async) 127.0.0.1(rw,no_root_squash,async)

    replace "10.66.70.144" to your destinate host IP

    5.2 service nfs start

6. Mount the nfs filesystem on source host to  both destinate host and source host.
    6.1 on destination machine
     Create /var/lib/libvirt/migrate on destinate host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     # mount -t nfs ${source_host_ip}:/var/lib/libvirt/images/  /var/lib/libvirt/migrate/

    6.2 On source machine
     Create /var/lib/libvirt/migrate on source host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     mount localhost:/var/lib/libvirt/images /var/lib/libvirt/migrate


7. After step 6, make sure your migration domain's disk image locate in "/var/lib/libvirt/migrate/" (such as :/var/lib/libvirt/migrate/migrate.img)on source machine(when you migrate the guest during installation,the iso also should be locate in "/var/lib/libvirt/migrate").
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Prepare and Start 15 guests on source

# for i in {1..15}; do virsh start mig-$i; done

2. Open another console and do

# while true; do for i in {1..15};do virsh domblkinfo mig-$i vda; done; done

3. Do migration with

# for i in {1..15}; do virsh migrate --live mig-$i qemu+ssh://{target ip}/system --unsafe & sleep 1; echo mig-$i; done

4. Check the execute status

5. Do the same command on target host only change the ip address

# for i in {1..15}; do virsh migrate --live mig-$i qemu+ssh://{source ip}/system --unsafe & sleep 1;echo mig-$i; done

6. Check the execute status

Repeat above steps and change the statistics command to {dommemstat, dominfo}
	
Expected Results:

step4

Should finished migration without hang

step6

Should finished migration without hang

 
Notes:
Comments:

		177246 	[Migration] Do migration loop with setmaxdowntime - bug 803186 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. A RHEL6 host with kvm, with a running domain.

2. Another kvm host, which also is a RHEL6.

3. On both source host, and target host, libvirtd is running.

4. On both source and target machine:
    #iptables -F
    #setenforce 1

   #setsebool virt_use_nfs  on


5. Setup nfs service on Source target machine
 
    5.1 add following line into "/etc/exports"

     /var/lib/libvirt/images 10.66.70.144(rw,no_root_squash,async) 127.0.0.1(rw,no_root_squash,async)

    replace "10.66.70.144" to your destinate host IP

    5.2 service nfs start

6. Mount the nfs filesystem on source host to  both destinate host and source host.
    6.1 on destination machine
     Create /var/lib/libvirt/migrate on destinate host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     # mount -t nfs ${source_host_ip}:/var/lib/libvirt/images/  /var/lib/libvirt/migrate/

    6.2 On source machine
     Create /var/lib/libvirt/migrate on source host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     mount localhost:/var/lib/libvirt/images /var/lib/libvirt/migrate


7. After step 6, make sure your migration domain's disk image locate in "/var/lib/libvirt/migrate/" (such as :/var/lib/libvirt/migrate/migrate.img)on source machine(when you migrate the guest during installation,the iso also should be locate in "/var/lib/libvirt/migrate").
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Prepare and Start 15 guests on source

# for i in {1..15}; do virsh start mig-$i; done

2. Do migration with

# for i in {1..15}; do virsh migrate --live mig-$i qemu+ssh://{target ip}/system --unsafe & sleep 1; virsh migrate-setmaxdowntime mig-$i 10000; sleep 1;echo mig-$i; done

3. Check the execute status

4. Do the same command on target host only change the ip address

# for i in {1..15}; do virsh migrate --live mig-$i qemu+ssh://{source ip}/system --unsafe & sleep 1; virsh migrate-setmaxdowntime mig-$i 10000; sleep 1;echo mig-$i; done

5. Check the execute status

Repeat above steps with migration && migrate-setspeed
	
Expected Results:

step3

Should finished migration without hang

step5

Should finished migration without hang

 
Notes:
Comments:

		177250 	[migration] general error return on migrate after calling abortjob() - bug 760149 873792 	ydu 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

  # iptables -F
	
Breakdown:

 

	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. Start a guest and do live migration.

# virsh migrate --live $guest_name qemu+ssh://$des_host_ip/system --verbose

2. Before migration finished, in another terminal, abort the migration job.

# virsh domjobabort --domain  $guest_name 

 

Steps from bug 873792:

Prepare a live migration env then:
3. 
# virsh migrate --live vr-rhel6u3-x86_64-kvm qemu+ssh://10.66.84.16/system --verbose & usleep 500000; \
virsh domjobabort vr-rhel6u3-x86_64-kvm

error: operation aborted: migration out: canceled by client

 
	
Expected Results:

Step2:

migration report error error:

operation aborted: migration job: canceled by client

And the migration is canceled. Guest still running on source, there is no guest on target

 
Notes:
Comments:

		177260 	[Migration] Live migration of storage (without moving VM) - bug 638506 	weizhan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Need install qemu-kvm-rhev package instead of qemu-kvm
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Start a guest with qcow2 image
# virsh start guest
# virsh dumpxml guest
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/foo.qcow2'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
...
# qemu-img info /var/lib/libvirt/images/foo.qcow2 
image: /var/lib/libvirt/images/foo.qcow2
file format: qcow2
virtual size: 1.0G (1073741824 bytes)
disk size: 136K
cluster_size: 65536

2. Create disk snapshot
# virsh snapshot-create-as guest --disk-only --diskspec hda,snapshot=external,driver=qcow2
Domain snapshot 1335336433 created

# qemu-img info foo.1335336433
image: foo.1335336433
file format: qcow2
virtual size: 1.0G (1073741824 bytes)
disk size: 136K
cluster_size: 65536
backing file: /var/lib/libvirt/images/foo.qcow2 (actual path: /var/lib/libvirt/images/foo.qcow2)

3. Do blockpull
# virsh blockpull guest /var/lib/libvirt/images/foo.1335336433
check blockpull status with blockjob
# virsh blockjob guest /var/lib/libvirt/images/foo.1335336433

4. Check the domain and image status
# virsh dumpxml guest
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/foo.1335336433'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
...
# qemu-img info /var/lib/libvirt/images/foo.1335336433 
image: /var/lib/libvirt/images/foo.1335336433
file format: qcow2
virtual size: 1.0G (1073741824 bytes)
disk size: 1.0G
cluster_size: 65536

	
Expected Results:
Notes:
Comments:

		177287 	[Migration] Migration with different cache mode - bug 751631 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Start a domain with cache='writeback'

...

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='writeback'/>
      <source dev='/var/lib/libvirt/images/test.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>

...

2. Do migration

# virsh migrate --live guest qemu+ssh://{target ip}/system

3. Do migration

# virsh migrate --live guest qemu+ssh://{target ip}/system --unsafe
	
Expected Results:

Step 2

will report error

error: Unsafe migration: Migration may lead to data corruption if disks use cache != none

Step 3

migration will succeed
Notes:
Comments:

		177288 	[Migration] Migration with inactive iso attached with rhevm- bug 701106 889961 	weizhan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Bug: 889961 VDSM | Libvirt fails the migration process when removing 1st boot device from running VM.

 


1. Prepare 2 machines, one is for RHEL/VDSM, another one is for RHEVM.

2. Follow the instructions to config RHEVM.

http://cleo.tlv.redhat.com/qumrawiki/Integration/RHEVM_RPM_HOWTO

https://docspace.corp.redhat.com/docs/DOC-68146

3. Configure a NFS server with the options below.

# vi /etc/exports

/export    *(rw,sync)

# service nfs restart

# chown 36:36 /export

# ls -ld /export

drwxr-xr-x.   3 vdsm kvm   4096 Jul 20 04:43 /export

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    QE consumption
    regression

bug:

    837485 - From Run 42159
    851397 - From Run 44420
    852668 - From Run 44628
    852668 - From Run 44549
    869956 - From Run 48788
    855729 - From Run 49669
    869956 - From Run 50073
    869956 - From Run 49843
    869956 - From Run 50921

Actions:

1 . Make sure you have a iso domain which is in inactive status

2.  Upload an iso file to this domain with command

rhevm-iso-uploader -i iso-domain -u admin@internal -r 127.0.0.1:8443 upload /var/lib/libvirt/images/virtio-win-1.4.0.iso

3.  Start a domain with attach this iso file on Boot Options

4.  Inactive iso domain

5. Do migration

 

For bug 889961

1. rhevm create VM and select boot from network.
2. While the VM is running delete the network interface from the VM (using rhevm webadmin)
3. From rhevm webadmin migrate the VM.
  

 

 
	
Expected Results:

migration will succeed

 

For bug 889961

Step 3:

Migration should pass. Not fail with log:

Thread-200896::DEBUG::2012-12-24 09:05:43,576::libvirtvm::454::vm.Vm::(_startUnderlyingMigration) vmId=`22ee2cb6-d617-44ce-b650-d16607825589`::starting migration to qemu+tls://orchid-vds1.qa.lab.tlv.redhat.com/system
Thread-200899::DEBUG::2012-12-24 09:05:43,577::libvirtvm::352::vm.Vm::(run) vmId=`22ee2cb6-d617-44ce-b650-d16607825589`::migration downtime thread started
Thread-200900::DEBUG::2012-12-24 09:05:43,580::libvirtvm::380::vm.Vm::(run) vmId=`22ee2cb6-d617-44ce-b650-d16607825589`::starting migration monitor thread
Thread-200896::DEBUG::2012-12-24 09:05:43,850::libvirtvm::367::vm.Vm::(cancel) vmId=`22ee2cb6-d617-44ce-b650-d16607825589`::canceling migration downtime thread
Thread-200896::DEBUG::2012-12-24 09:05:43,851::libvirtvm::417::vm.Vm::(stop) vmId=`22ee2cb6-d617-44ce-b650-d16607825589`::stopping migration monitor thread
Thread-200899::DEBUG::2012-12-24 09:05:43,851::libvirtvm::364::vm.Vm::(run) vmId=`22ee2cb6-d617-44ce-b650-d16607825589`::migration downtime thread exiting
Thread-200896::ERROR::2012-12-24 09:05:43,851::vm::181::vm.Vm::(_recover) vmId=`22ee2cb6-d617-44ce-b650-d16607825589`::internal error boot orders have to be contiguous and starting from 1
Thread-200896::ERROR::2012-12-24 09:05:44,099::vm::262::vm.Vm::(run) vmId=`22ee2cb6-d617-44ce-b650-d16607825589`::Failed to migrate
Traceback (most recent call last):
  File "/usr/share/vdsm/vm.py", line 245, in run
    self._startUnderlyingMigration()
  File "/usr/share/vdsm/libvirtvm.py", line 478, in _startUnderlyingMigration
    None, maxBandwidth)
  File "/usr/share/vdsm/libvirtvm.py", line 518, in f
    ret = attr(*args, **kwargs)
  File "/usr/lib64/python2.6/site-packages/vdsm/libvirtconnection.py", line 83, in wrapper
    ret = f(*args, **kwargs)
  File "/usr/lib64/python2.6/site-packages/libvirt.py", line 1103, in migrateToURI2
    if ret == -1: raise libvirtError ('virDomainMigrateToURI2() failed', dom=self)
libvirtError: internal error boot orders have to be contiguous and starting from 1

 
Notes:
Comments:

		177293 	[Migration] Migration with non-existent xml - bug 773208 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:

   Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    migration

bug:

    No bug found

Actions:

1. Do migration with none exist xml file
# virsh migrate kvm-rhel6u2-x86_64-new --live qemu+ssh://10.66.83.197/system
--xml non-existent-xml

	
Expected Results:

error: file 'none-exist.xml' doesn't exist
Notes:
Comments:

		177294 	[Migration] Migration with non-shared floppy and cdrom with startupPolicy 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Start a guest with floppy disk and cdrom on non-shared dir with startpolicy='requisite'

...

   <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/cdrom.img' startupPolicy='requisite'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/floppy.img' startupPolicy='requisite'/>
      <target dev='fda' bus='fdc'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
...

2. Do migration

# virsh migrate --live guest qemu+ssh://{target ip}/system

3. Repeat the above steps with startupPolicy='optional'
	
Expected Results:

Step 2

Migration succeed

dumpxml on target, the cdrom and floppy disk are dropped

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source startupPolicy='requisite'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='requisite'/>
      <target dev='fda' bus='fdc'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

Step 3

Migration succeed

dumpxml on target, the cdrom and floppy disk are dropped

     <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source startupPolicy='optional'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source startupPolicy='optional'/>
      <target dev='fda' bus='fdc'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
Notes:
Comments:

		177297 	[Migration] Migration with unknown tcp socket - bug 720269 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. start a guest, make sure that the normal migration will succeed

# virsh migrate --live guest qemu+ssh://{target ip}/system

2. Do

# virsh migrate --live guest qemu+ssh://{target ip}/system  tcp://{unknow host ip}

3. Do

# virsh migrate --live guest qemu+ssh://{target ip}/system  tcp://{target ip}
	
Expected Results:

Step 2

report error clear like " Unable to resolve hostname XXXX" or "unable to connect to server at XXXX" but not "An undefined error has occured

Step 3

should succeed without error
Notes:
Comments:

		177299 	[Migration] P2P migration using a TLS enabled URI - Bug 721411 	weizhan 	None 	Auto 		Regression 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. do case "39342 [Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4" first for setting up tls environment

<https://tcms.engineering.redhat.com/case/124654/?from_plan=5066>

3. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    QE consumption

bug:

    No bug found

Actions:

1. install a guest and start on source

2. do migration with

# virsh migrate --p2p --live domain_name qemu+tls://{target hostname}/system

3. check the /var/log/libvirt/libvirtd.log
	
Expected Results:

migration will succeed

there is no Input/output error on /var/log/libvirt/libvirtd.log

 
Notes:
Comments:

		177300 	[Migration] P2P migration using TCP enabled URI without sasl 	weizhan 	None 	Auto 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration

bug:

    No bug found

Actions:

1. Do case

Following case:

[Remote access] Connect to the hypervisor on host using a plain TCP connection without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124649/?from_plan=5066>

Run the case on both source host and target host to enable tcp listen for libvirtd.

 

2. Define and start a guest

3. Do migration with
 # virsh migrate --live --p2p domain_name qemu+tcp://{target ip}/system
	
Expected Results:

migration will succeed with no error
Notes:
Comments:

		177301 	[Migration] P2P reverse migration with TLS enabled URI - bug 722738 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:

1.
- 2 host installed with kvm kernel
- libvirtd service is running on both system
- require package "gnutls-utils" installed

2.
Make sure 2 hosts UTC time was same.
# date -u
if not please set it, or sync time with clock.redhat.com:
# ntpdate clock.redhat.com

3.
Make sure on both of the 2 hosts can resolve each other with FQDN.
Add both ip and hostname to /etc/hosts on both hosts such like :
# cat /etc/hosts
10.66.6.38  libn760.redhat.com              libn760
10.66.5.143 libn755.redhat.com              libn755

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    migration

bug:

    852668 - From Run 44628

Actions:

<1>. build tls environment from source to target host:
server: target (libn760.redhat.com)
client: source (libn755.redhat.com)

	On server (libn760.redhat.com)

	1. Set up a Certificate Authority (CA)

	1.1 # certtool --generate-privkey > cakey.pem

	1.2 self-sign cakey.pem by creating a file with the signature details called ca.info containing:

	cn = libn760.redhat.com
	ca
	cert_signing_key

	1.3 # certtool --generate-self-signed --load-privkey cakey.pem --template ca.info --outfile cacert.pem

	2. Create server certificates

	2.1 # certtool --generate-privkey > serverkey.pem

	2.2 sign that key with the CA's private key by first creating a template file called server.info
		
	organization = Red Hat
	cn = libn760.redhat.com
	tls_www_server
	encryption_key
	signing_key

	2.3 # certtool --generate-certificate --load-privkey serverkey.pem --load-ca-certificate cacert.pem --load-ca-privkey cakey.pem --template server.info --outfile servercert.pem

	3. Copy CA key and server key to correct directory

	3.1 # cp cakey.pem cacert.pem /etc/pki/CA

	3.2 # mkdir -p /etc/pki/libvirt/private

	3.3 # cp serverkey.pem /etc/pki/libvirt/private

	3.4 # cp servercert.pem /etc/pki/libvirt

	4. Copy CA key to client(libn755.redhat.com) into correct directory

	# scp cakey.pem cacert.pem root@10.66.5.143:/etc/pki/CA

	5. Turn on libvird monitor listening in /etc/sysconfig/libvirtd
	  -- uncomment LIBVIRTD_ARGS="--listen"

	6. Edit /etc/libvirt/libvirtd.conf
	  -- uncomment auth_tls = "none"

	7. # service libvirtd restart
		Stopping libvirtd daemon:                                  [  OK  ]
		Starting libvirtd daemon:                                  [  OK  ]

	8. # service iptables stop


	On client (libn755.redhat.com)

	9.  Create client certificates

	9.1 # certtool --generate-privkey > clientkey.pem

	9.2 Act as CA and sign the certificate.  Create client.info containing:

	country = GB
	state = London
	locality = London
	organization = Red Hat
	cn = libn755.redhat.com
	tls_www_client
	encryption_key
	signing_key

	9.3 # certtool --generate-certificate --load-privkey clientkey.pem --load-ca-certificate /etc/pki/CA/cacert.pem --load-ca-privkey /etc/pki/CA/cakey.pem --template client.info --outfile clientcert.pem

	10. Copy client key to correct directory

	10.1 # mkdir -p /etc/pki/libvirt/private

	10.2 # cp clientkey.pem /etc/pki/libvirt/private

	10.3 # cp clientcert.pem /etc/pki/libvirt/

	11. Conect to server(target) hypervisor

	# virsh -c qemu+tls://libn760.redhat.com/system
	
	Connect to target(libn760.redhat.com) successfully.
	
<2>. build tls environment from source to source host with the same cacert.pem and cakey.pem, just generate serverkey.pem and servercert.pem locally and copy to /etc/pki/libvirt/private and /etc/pki/libvirt:
server: source (libn755.redhat.com)
client: source (libn755.redhat.com)

	Do all the following cmds on source (libn755.redhat.com) host.

	12. # certtool --generate-privkey > serverkey.pem

	13. sign that key with the CA's private key ( which were copied from target(libn760) by scp in step <1>.4) by first creating a template file called server.info
		
	organization = Red Hat
	cn = libn755.redhat.com
	tls_www_server
	encryption_key
	signing_key

	14. Use the cacert.pem and and cakey.pem copied from target(libn760) by scp in step <1>.4 and local server.info to generate servercert.pem:
	# certtool --generate-certificate --load-privkey serverkey.pem --load-ca-certificate /etc/pki/CA/cacert.pem --load-ca-privkey /etc/pki/CA/cakey.pem --template server.info --outfile servercert.pem
	
	15. # cp serverkey.pem /etc/pki/libvirt/private

	16. # cp servercert.pem /etc/pki/libvirt
	
	17. Turn on libvird monitor listening in /etc/sysconfig/libvirtd
	  -- uncomment LIBVIRTD_ARGS="--listen"

	18. Edit /etc/libvirt/libvirtd.conf
	  -- uncomment auth_tls = "none"

	19. # service libvirtd restart
		Stopping libvirtd daemon:                                  [  OK  ]
		Starting libvirtd daemon:                                  [  OK  ]

	20. # service iptables stop
	
	21. Conect to server(source) hypervisor

	# virsh -c qemu+tls://libn755.redhat.com/system
	
	Connect to source(libn755.redhat.com) successfully.

<3>. start a guest on target(libn760.redhat.com) host which image is on shared nfs mounted on both sides and
	# setsebool virt_use_nfs 1
	# iptables -F

<4>. do migration on source(libn755.redhat.com) host with command:
	# virsh -c qemu+tls://libn760.redhat.com/system migrate --p2p guest qemu+tls://libn755.redhat.com/system
	error: operation failed: Failed to connect to remote libvirt URI qemu+tls://libn755.redhat.com/system

	
Expected Results:

Step 7:

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

 Step 11:

# virsh -c qemu+tls://libn760.redhat.com/system

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # exit

Step 19:

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

 Step 21:

# virsh -c qemu+tls://libn755.redhat.com/system

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # exit

 Step <4>:

report error:

# virsh -c qemu+tls://libn760.redhat.com/system migrate --p2p guest qemu+tls://libn755.redhat.com/system

error: operation failed: Failed to connect to remote libvirt URI qemu+tls://libn755.redhat.com/system

 And libvirtd are not crashed on both hosts:

# service libvirtd status
libvirtd (pid  6665) is running...

On target(libn760.redhat.com) sides:

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 1     guest                          running

On source(libn755.redhat.com) sides:

# virsh list --all
 Id    Name                           State
----------------------------------------------------

 -     qcow2                          shut off

 
Notes:
Comments:

		177303 	[Migration] Seamless spice migration with graphic guest and windows spice client- bug 783968 	weizhan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

1. Prepare a nfs server

write /etc/exports with

/var/lib/libvirt/images    *(rw,no_root_squash,async)

Then start nfs service

# service nfs start

Stop iptables

# iptables -F

Flush the iptables.
 if you want to stop iptables:
 # service iptables stop

2. Prepare 2 hosts(source:hostA target:hostB) and mount nfs dir on both hosts, and setting the virt_use_nfs boolean on both sides

  # mount  $NFS_IP:/var/lib/libvirt/images /mnt  -o vers=3

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

  # iptables -F

3. Prepare another windows os(hostC/guestC) and install spicec-win on it(both  host and guest are okay for testing).
The  version of spicec-win  client  should be  0.1-25 or above . Old version probably arouse problems.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    regression
    migration
    graphical framebuffers

bug:

    No bug found

Actions:

Actions

1. Define and start a windows guest with spice graphic for migration on host A.
<domain type='kvm'>
  <name>win-mig</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/mnt/win7-x86_64'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <controller type='virtio-serial' index='0'>
    </controller>
    <controller type='ide' index='0'>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:a2:19:5e'/>
      <source network='default'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
    </channel>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes'/>
    <sound model='ich6'>
    </sound>
    <video>
      <model type='qxl' vram='65536' heads='1'/>
    </video>
    <memballoon model='virtio'>
    </memballoon>
  </devices>
</domain>


2. Edit the following line in /etc/libvirt/qemu.conf both in source host(host A) and target host(host B):
from:
# spice_listen = "0.0.0.0"
to
spice_listen = "0.0.0.0"

3. Restart libvirtd service

4. Start the domain on source host (host A)
# virsh start win-mig

On guest win-mig, install qxl-win driver in control panel -> device manager -> vga adapter5.use spicec-win to connect win-mig and watch it.
 

 5.Run spicec-win in host/guest C, fill in the ip of host A and spice port, then click "connect".
Spice port can be obtained by running "virsh dumpxml win-mig" on host A.

7. Play video in guest win-mig

8. Do migration
# virsh migrate --live win-mig qemu+ssh://{target hostB_ip}/system
	
Expected Results:

Step 8

Migration succeed with no error, and spice client still alive, video still play without interrupt, no black connecting interface(the screen like what you seen when run windows spice client) exist during migration on windows spice client
Notes:
Comments:

		177312 	[Migration] Tunnelled migration using a TLS enabled URI 	weizhan 	None 	Auto 		Regression 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. do case " [Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4"first for setting up tls environment

<https://tcms.engineering.redhat.com/case/124654/?from_plan=5066>

3. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. install a guest an start on source

2. do migration with

# virsh migrate --p2p --live --tunnelled domain_name qemu+tls://{target hostname}/system

3. check the /var/log/libvirt/libvirtd.log
	
Expected Results:

migration will succeed with no error
Notes:
Comments:

		177313 	[Migration] Tunnelled migration using TCP enabled URI without sasl 	weizhan 	None 	Auto 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Do case

Following case:

[Remote access] Connect to the hypervisor on host using a plain TCP connection without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124649/?from_plan=5066>

Run the case on both source host and target host to enable tcp listen for libvirtd.
2. Define and start a guest

3. Do migration with
 # virsh migrate --live --p2p --tunnelled domain_name qemu+tcp://{target ip}/system
	
Expected Results:

migration will succeed with no error
Notes:
Comments:

		177320 	[Migration]DNS resolving fails during migration - bug 729567, 738915 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:

Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. Start a guest located on nfs

2. Ensure two hosts have no FQDN(fully qualified domain name).

on hostA: # hostname

test1

on hostB: # hostname

test2

3. Ensure /etc/hosts have no domain name resolve on two hosts

# cat /etc/hosts

127.0.0.1 localhost localhost.localdomain localhost4 loclhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6

4. Do migration from hostA to hostB

# virsh migrate --live guest qemu+ssh://{hostB ip}/system

5. Do migration from hostA to hostB with migrateuri option

# virsh migrate --live guest qemu+ssh://{hostB ip}/system  tcp://{hostB ip}:9001

 
	
Expected Results:

Step4:

will report error like:

error: Unable to resolve address 'test2' service '49158': Name or service not known



Step5:

migration should succeed with no error.

 

Notes:
Comments:

		177326 	[Migration]migrating domain to unreachable destination - bug 723912, 723881 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. install a guest and start on source

2. try do migrating to unreachable destenation.

# virsh migrate guest --live --p2p qemu+tls://1.1.1.1/system
	
Expected Results:

will report error like

error: operation failed: Failed to connect to remote libvirt URI qemu+tls://1.1.1.1/system
Notes:
Comments:

		177327 	[migration]migration from RHEL6.X to RHEL6.Y which X > Y BZ 815503 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Prepare 2 machine with different libvirt version.

like: RHEL6.X RHEL6.Y which X = 3, Y = 2

so libvirt version is like RHEL6.X: libvirt-0.9.10-16; RHEL6.Y: libvirt-0.9.4-23
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    migration

bug:

    No bug found

Actions:

RHEL6u2 --> RHEL6u3 --> RHEL6U2
1, define a guest from RHEL6u2
 # virsh dumpxml guest
...
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
...

2, migrate guest to RHEL6u3
3, check xml on RHEL6u3
 # virsh dumpxml guest
...
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
...
    <controller type='usb' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x2'/>
    </controller>
...
4, migrate guest to RHEL6u2
5, check xml on RHEL6u2
...
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
...
==================================================================
RHEL6u3 --> RHEL6u2 --> RHEL6U3
1, define a guest from RHEL6u3
 # virsh dumpxml guest
...
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
...
    <controller type='usb' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x2'/>
    </controller>
...
2, migrate it to RHEL6u2
virsh # migrate guest --live qemu+ssh://10.66.6.209/system
root@10.66.6.209's password: 
error: internal error Process exited while reading console log output:
Supported machines are:
pc         RHEL 6.2.0 PC (alias of rhel6.2.0)
rhel6.2.0  RHEL 6.2.0 PC (default)
rhel6.1.0  RHEL 6.1.0 PC
rhel6.0.0  RHEL 6.0.0 PC
rhel5.5.0  RHEL 5.5.0 PC
rhel5.4.4  RHEL 5.4.4 PC
rhel5.4.0  RHEL 5.4.0 PC
3, change guest OS type
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
4, check the xml on RHEL6u3
...
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
...
    <controller type='usb' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x2'/>
    </controller>
...
4, migrate guest to RHEL6u2
5, check xml on RHEL6u2 
...
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
...
6, migrate guest to RHEL6u3
7, check the xml on RHEL6u3
...
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
...
    <controller type='usb' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x2'/>
    </controller>
...
========================================================
Add one more usb controller on RHEL6u3
1, add one more usb controller on RHEL6u3:
...
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
...
   <controller type='usb' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x2'/>
    </controller>
    <controller type='usb' index='1'>
      <alias name='usb1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </controller>
...
2, migrate it to RHEL6u2
virsh # migrate guest --live qemu+ssh://10.66.6.209/system
root@10.66.6.209's password: 
error: internal error Unknown controller type 'usb'

3, change guest OS type:
...
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
...
   <controller type='usb' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x2'/>
    </controller>
    <controller type='usb' index='1'>
      <alias name='usb1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </controller>
...
4, migrate it to RHEL6u2
virsh # migrate guest --live qemu+ssh://10.66.6.209/system
root@10.66.6.209's password: 
error: internal error Unknown controller type 'usb'

	
Expected Results:

ALL pass without error
Notes:
Comments:

		177329 	[migration]migration will cause guest IO failure when DST sebool is virt_use_nfs=off BZ 822052 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression
    rhel7

bug:

    No bug found

Actions:

1, prepare a nfs and mount it to host A and B as a shared nfs pool.
2, virt_use_nfs = off (B)
   virt_use_nfs = on  (A)
3, define and start a guest on A
4, migrate guest from A to B
# migrate --live aaa qemu+ssh://10.66.7.230/system
The authenticity of host '10.66.7.230 (10.66.7.230)' can't be established.
RSA key fingerprint is d2:76:01:77:2f:5b4:bf:8f:1a:a1:92:94:c3:e3:2e.
Are you sure you want to continue connecting (yes/no)? yes
root@10.66.7.230's password:
error: internal error Process exited while reading console log output: char device redirected to /dev/pts/1
qemu-kvm: -drive file=/var/lib/libvirt/images/rhel6u2.img,if=none,id=drive-virtio-disk0,format=raw,cache=none: could not open disk image /var/lib/libvirt/images/rhel6u2.img: Permission denied
5, check the screen of guest
  

	
Expected Results:

Step 5, check the screen of guest, wait for a while it will not show some IO Failure message

BUG 822052 is not fixed, so keep the case NEED_UPDATE.

 

Notes:
Comments:

		177333 	[Miscellanea] Ability to specify custom BIOS for QEMU/KVM using <loader> XML (for WHQL testing) - bug 811227 	bili 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:

Prepare a windows guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

1.# virsh edit guest
2.Add <loader>/usr/share/seabios/bios.bin</loader> under <os> element.
3.# virsh start guest.
4.# ps -ef |grep -- -bios 

	
Expected Results:

step 4:

root 4370 3771 0 16:58 pts/1 00:00:00 grep -- -bios

qemu 30365 1 38 15:25 ? 00:35:25 /usr/libexec/qemu-kvm -S -M rhel6.3.0 -enable-kvm -bios /usr/share/seabios/bios.bin -m 2048 -smp 2,sockets=2,cores=1,threads=1 -name win7-64 -uuid edf5b5d5-4b84-f4e7-ecd9-def7a0ffbd09 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/win7-64.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=localtime,driftfix=slew -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/mnt/nfs/Windows_KVM_images/win7-64.raw,if=none,id=drive-ide0-0-0,format=raw,cache=none -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1 -netdev tap,fd=24,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:bc:a4:c7,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -device usb-tablet,id=input0 -vnc 127.0.0.1:0 -vga std -device intel-hda,id=sound0,bus=pci.0,addr=0x4 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5
Notes:
Comments:

		177386 	[NPIV]Automatically generate unique WWN 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NPIV

bug:

    No bug found

Actions:

1.create a vHBA by an XML file without wwnn
specified.
# cat vHBA.xml
<device>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
     </capability>
  </capability>
</device>

# service libvirtd restart 

2 # virsh nodedev-create vHBA.xml 
And create it again

3 # virsh nodedev-create vHBA.xml  

4. Check the WWN value of the created vHBAs  
# virsh nodedev-dumpxml scsi_host9

5 # virsh nodedev-dumpxml scsi_host10 

	
Expected Results:

verify:

after step1:

the libvirtd start successful

Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

 step 2:

Node device scsi_host9 created from vHBA.xml

step 3:

Node device scsi_host10 created from vHBA.xml

 step 4:

<device>
  <name>scsi_host9</name>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <host>9</host>
    <capability type='fc_host'>
      <wwnn>5001a4af1ecab19a</wwnn>
      <wwpn>5001a4a6c8b11b08</wwpn>
      <fabric_wwn>2001000dec9877c1</fabric_wwn>
    </capability>
  </capability>
</device>
step 5:

<device>
  <name>scsi_host10</name>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <host>10</host>
    <capability type='fc_host'>
      <wwnn>5001a4a0ec67fd79</wwnn>
      <wwpn>5001a4aa8d7f64f8</wwpn>
      <fabric_wwn>2001000dec9877c1</fabric_wwn>
    </capability>
  </capability>
</device>

 From the out put XML file, we can get the WWN value for the vHBA.
 The the first nibble is hex 5, and since the hypervisor type is QEMU of my host, 
  the 3-bytes vendor ID is 001a4a, and the left 36 bits are auto-generated uniquely.



 
Notes:
Comments:

		177387 	[NPIV]option to associate a WWN with a VM 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NPIV

bug:

    No bug found

Actions:

1. Create a vHBA with the XML file
# cat vHBA.xml 
<device>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
     </capability>
  </capability>
</device>

# virsh nodedev-create vHBA.xml 
Node device scsi_host14 created from vHBA.xml

2. Check the vHBA's XML and the fabric_name in sysfs

# virsh nodedev-dumpxml scsi_host14
<device>
  <name>scsi_host14</name>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <host>14</host>
    <capability type='fc_host'>
      <wwnn>5001a4a6afd9084e</wwnn>
      <wwpn>5001a4a264a4b7f1</wwpn>
      <fabric_wwn>2001000dec9877c1</fabric_wwn>
    </capability>
  </capability>
</device>

# cat /sys/class/fc_host/host14/fabric_name 
0x2001000dec9877c1

4. The physical HBA's XML

# virsh nodedev-dumpxml scsi_host5
<device>
  <name>scsi_host5</name>
  <parent>pci_0000_04_00_1</parent>
  <capability type='scsi_host'>
    <host>5</host>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9da4e</wwpn>
      <fabric_wwn>2001000dec9877c1</fabric_wwn>
    </capability>
    <capability type='vport_ops' />
  </capability>
</device>

	
Expected Results:

verify in xml have found fabric_wwn element
Notes:
Comments:

		177394 	[NUMA]memory tuning interleave mode 	gren 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

1 on a NUMA machine with more than two nodes, install a guest.

# numactl --show
policy: default
preferred node: current
physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
cpubind: 0 1 2 3
nodebind: 0 1 2 3
membind: 0 1 2 3

	
Breakdown:
	
Attachment:

    nmstat.c

Component:

    libvirt

Tag:

    NUMA

bug:

    No bug found

Actions:

1 edit guest xml description and add following xml between <domain> element

# virsh edit <guest>

  <numatune>
    <memory mode='interleave' nodeset='2-3'/>
  </numatune>

 2. Start the guest and check

# virsh start <guest>
	
Expected Results:

1. Download the c program from Attachment, follow the instruction on it.
# gcc -g -std=gnu99 -o nmstat nmstat.c

# ps -ef|grep qemu-kvm
qemu     55089     1 26 14:10 ?        00:00:37 /usr/libexec/qemu-kvm -S -M rhel6.2.0 -enable-kvm -m 1024 -smp 2,sockets=2,cores=1,threads=1 -name arhel61 -uuid b2910ade-c025-a041-599b-f65d2a41e9a7 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/arhel61.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -drive file=/var/lib/libvirt/images/arhel61.img,if=none,id=drive-virtio-disk0,format=raw,cache=none,aio=threads -device virtio-blk...

# ./nmstat -s 55089

Per-node memory usage (in MBs) for PID 55089:
                      N0          N1          N2          N3
                --------    --------    --------    --------
        huge        0.00        0.00        0.00        0.00
        heap        0.00        0.00      117.79        1.80
       stack        0.00        0.00        0.02        0.02
      shared        0.46        0.35        0.32        1.12
     private        0.00        0.10    10463.91    10465.09
       total        0.46        0.45    10582.04    10468.02

Note very balanced (interleaved) allocation for N2 and N3.  

Notes:
Comments:

		177395 	[NUMA]memory tuning preferred mode 	gren 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

1, on a NUMA machine with more than two nodes, install a guest.

# numactl --show
policy: default
preferred node: current
physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
cpubind: 0 1 2 3
nodebind: 0 1 2 3
membind: 0 1 2 3
	
Breakdown:
	
Attachment:

    nmstat.c

Component:

    libvirt

Tag:

    NUMA

bug:

    No bug found

Actions:

1. Edit guest xml description and add following xml between <domain> element

# virsh edit <guest>
<numatune>
    <memory mode='preferred' nodeset='2'/>
  </numatune>

 2. Start the guest and check

# virsh start <guest>

3. Edit guest xml as following and check preferred mode only support 1 node.
# virsh edit <guest>
...
  <numatune>
    <memory mode='preferred' nodeset='2-3'/>
  </numatune>
....

 # virsh start arhel61
error: Failed to start domain arhel61
error: internal error internal error NUMA memory tuning in 'preferred' mode only supports single node

	
Expected Results:

1. After step 2, Download the c program from Attachment, follow the instruction on it.
# gcc -g -std=gnu99 -o nmstat nmstat.c

# ps -ef|grep qemu-kvm
qemu     4459     1 26 14:10 ?        00:00:37 /usr/libexec/qemu-kvm -S -M rhel6.2.0 -enable-kvm -m 1024 -smp 2,sockets=2,cores=1,threads=1 -name arhel61 -uuid b2910ade-c025-a041-599b-f65d2a41e9a7 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/arhel61.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -drive file=/var/lib/libvirt/images/arhel61.img,if=none,id=drive-virtio-disk0,format=raw,cache=none,aio=threads -device virtio-blk...

# ./nmstat -s 4459

Per-node memory usage (in MBs) for PID 4459:
                      N0          N1          N2          N3
                --------    --------    --------    --------
        huge        0.00        0.00        0.00        0.00
        heap        0.00        0.00        1.51      0.00
       stack        0.00        0.00        0.02        0.00
      shared        0.46        0.35        1.16        0.23
     private        0.00        0.10     5458.34    0.00
       total        0.46        0.45     5460.15     0.45

Note the memory allocation will main on N2.

Notes:
Comments:

		177396 	[NUMA]memory tuning strict mode 	gren 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

1, on a NUMA machine with more than two nodes, install a new guest.

# numactl --show
policy: default
preferred node: current
physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95
cpubind: 0 1 2 3
nodebind: 0 1 2 3
membind: 0 1 2 3
	
Breakdown:
	
Attachment:

    nmstat.c

Component:

    libvirt

Tag:

    NUMA

bug:

    No bug found

Actions:

1. edit the guest xml description, add the <memtune> subnode  between <domain> element

# virsh edit <guest>

  <numatune>
    <memory mode='strict' nodeset='2-3'/>
  </numatune>


2. Start the guest and check 
# virsh start <guest>

 
	
Expected Results:

1. Download the c program from Attachment, follow the instruction on it.
# gcc -g -std=gnu99 -o nmstat nmstat.c


./nmstat -n qemu -v  

Per-node memory usage (in MBs) for PID 1346:
                      N0          N1          N2          N3
                --------    --------    --------    --------
        huge        0.00        0.00        0.00        0.00
        heap        0.00        0.00      117.02        2.57
       stack        0.00        0.00        0.01        0.02
      shared        0.46        0.35        0.29        1.16
     private        0.00        0.10    13005.13     4937.85
       total        0.46        0.45    13122.44     4941.60

Note sometimes the balance is not good because of kernel.

Notes:
Comments:

		177397 	[NUMA]numad testing - bug 769930, 810157, 820461, 869096 	gsun 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

A machine with multiple (>=2) numa nodes.

# numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 40 41 42 43 44 45 46 47 48 49
node 0 size: 131050 MB
node 0 free: 127328 MB
node 1 cpus: 10 11 12 13 14 15 16 17 18 19 50 51 52 53 54 55 56 57 58 59
node 1 size: 131072 MB
node 1 free: 127338 MB
node 2 cpus: 20 21 22 23 24 25 26 27 28 29 60 61 62 63 64 65 66 67 68 69
node 2 size: 131072 MB
node 2 free: 127540 MB
node 3 cpus: 30 31 32 33 34 35 36 37 38 39 70 71 72 73 74 75 76 77 78 79
node 3 size: 131072 MB
node 3 free: 127442 MB
node distances:
node   0   1   2   3 
  0:  10  11  11  11 
  1:  11  10  11  11 
  2:  11  11  10  11 
  3:  11  11  11  10 

  

Also prepare a running guest

Pls make sure your log level is 1 in libvirtd.conf.

After bug fix of:

https://bugzilla.redhat.com/show_bug.cgi?id=869096

Now numad returned node value will not be set to guest vcpu, but the domain main emulator process.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NUMA

bug:

    855218 - From Run 47352
    869096 - From Run 48366

Actions:

1. prepare a domain with vcpu parts set auto placement:
1.1
# virsh dumpxml rhel63
...
  <vcpu placement='auto'>24</vcpu>
...

1.2
# virsh start rhel63
1.3
# virsh dumpxml rhel63 

1.4 check log: 
# cat /var/log/libvirtd.log| grep Nodeset

1.5 
# cat /var/log/libvirt/qemu/rhel63.log 

 

1.6 fetch the domain pid and check (grep qemu process of the domain)
# cat /proc/23518/status

1.7 Modify the bold numbers in the script:

modify the first bold one to node numbers output from step 1.4 

(e.x. "1-2" to "1..2", "1-4" to "1..4", "1,3,8" to "1,3,8"), modify the second with max cpu numbers on the host.

check acctual cpu affinity:
#! /bin/sh
for i in {1..2}; do numactl --hardware | grep "node $i cpus:" >> cpus; done
cat cpus | awk -F':' '{print $2}' > cpus2
for i in {0..79}; do
    if grep "\b$i\b" cpus2 > /dev/null; then
        echo -n "y"
    else
        echo -n "-"
    fi
 done
echo > cpus 

# sh compare-cpu.sh
----------yyyyyyyyyyyyyyyyyyyy--------------------yyyyyyyyyyyyyyyyyyyy----------


1.8
check vcpuinfo: # virsh vcpuinfo rhel63

1.9 check vcpu process


2. add numatune part with memory mode as interleave
2.1
# virsh destroy rhel63
2.2
# virsh edit rhel63
...
  <vcpu placement='auto'>24</vcpu>
  <numatune>
    <memory mode='interleave'/>
  </numatune>
...

2.3
# virsh start rhel63
Domain rhel63 started

2.4
# virsh dumpxml rhel63

2.5
check libvirtd.log:
# cat /var/log/libvirtd.log| grep Nodeset

2.6

# cat /var/log/libvirt/qemu/rhel63.log 


2.7
# cat /proc/24429/status

2.8
modify compare-cpu.sh like in step 1.8 and run:
# sh compare-cpu.sh 

2.9
# virsh vcpuinfo rhel63


2.10 check vcpu process



3. vcpu placement as auto but memory mode as strict
3.1 edit domian 
# virsh edit rhel6u2
...
  <vcpu placement='auto'>24</vcpu>
  <numatune>
    <memory mode='strict' nodeset='0-1,3'/>
  </numatune>
...

3.2
# virsh start rhel63
Domain rhel63 started

3.4
check libvirtd.log:
# cat /var/log/libvirtd.log| grep Nodeset

3.5
# cat /var/log/libvirt/qemu/rhel63.log 

3.6
# cat /proc/24458/status

3.7
modify compare-cpu.sh like in step 1.8 and run:
# sh compare-cpu.sh 

 

3.8
# virsh vcpuinfo rhel63


3.9. check vcpu process




4. vcpu placement as static but memory placement as auto

4.1
# virsh edit rhel6u2
...
  <vcpu placement='static' cpuset='0-11,13-22,66-79'>24</vcpu>
  <numatune>
    <memory mode='interleave' placement='auto'/>
  </numatune>
...

4.2  

# cat /var/log/libvirt/qemu/rhel63.log 

 

	
Expected Results:

 1.

1.3 After start, check:
# virsh dumpxml rhel63 
...
  <vcpu placement='auto'>24</vcpu>
  <numatune>
    <memory mode='strict' placement='auto'/>
  </numatune>
...

1.4 check log:
# cat /var/log/libvirtd.log| grep Nodeset
2012-05-24 09:33:04.300+0000: 20342: debug : qemuProcessStart:3356 : Nodeset returned from numad: 1-2

1.5
# cat /var/log/libvirt/qemu/rhel63.log 
...
2012-05-24 09:33:04.447+0000: 23518: debug : qemuProcessInitCpuAffinity:1731 : Setting CPU affinity
2012-05-24 09:33:04.456+0000: 23518: debug : qemuProcessInitCpuAffinity:1749 : Set CPU affinity with advisory nodeset from numad
2012-05-24 09:33:04.456+0000: 23518: debug : qemuProcessInitNumaMemoryPolicy:1599 : Set NUMA memory policy with advisory nodeset from numad
...
 
1.6
# cat /proc/23518/status
...
check cpu_allow_list:
Cpus_allowed:	003f,fffc0000,3ffffc00
Cpus_allowed_list:	10-29,50-69
Mems_allowed:	00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000006
Mems_allowed_list:	1-2 ...


1.7
# sh compare-cpu.sh 
----------yyyyyyyyyyyyyyyyyyyy--------------------yyyyyyyyyyyyyyyyyyyy----------

The output is the same with vcpuinfo 
So this is working as expected.


1.8
check vcpuinfo:
# virsh vcpuinfo rhel63
VCPU:           0
CPU:            55
State:          running
CPU time:       7.5s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy
...

The CPU Affinity will show guest vcpu process affinity, it will not be affacted by numad returned value, so if no setting before, it should be all available host cpus as all 'y'.


1.9 

# pstree -apnh 23518
qemu-kvm,8441 -name libvirt_test_api -S -M rhel6.4.0 -enable-kvm -m 1024 -smp 2,sockets=2,cores=1,threads=1 -uuid 05867c1a-afeb-300e-e55e-2673391ae080 -nodefconfig -nodefaults -chardevsocket,id=charmonitor,path
  ââ{qemu-kvm},23612
  ââ{qemu-kvm},23613
   ....

 

# grep Cpus_allowed_list /proc/23612/status 
Cpus_allowed_list:	0-79
# grep Cpus_allowed_list /proc/23613/status 
Cpus_allowed_list:	0-79

.......



2. destroy domain and edit domain xml as:

2.4
# virsh dumpxml rhel63
...
  <vcpu placement='auto'>24</vcpu>
  <numatune>
    <memory mode='interleave' placement='auto'/>
  </numatune>
...

2.5
check libvirtd.log:
# cat /var/log/libvirtd.log| grep Nodeset
2012-05-24 09:50:42.481+0000: 20343: debug : qemuProcessStart:3356 : Nodeset returned from numad: 1,3

2.6

...
2012-05-24 09:33:04.447+0000: 23518: debug : qemuProcessInitCpuAffinity:1731 : Setting CPU affinity
2012-05-24 09:33:04.456+0000: 23518: debug : qemuProcessInitCpuAffinity:1749 : Set CPU affinity with advisory nodeset from numad
2012-05-24 09:33:04.456+0000: 23518: debug : qemuProcessInitNumaMemoryPolicy:1599 : Set NUMA memory policy with advisory nodeset from numad
...


2.7
# cat /proc/24429/status
...
Cpus_allowed:	ffc0,0ffc00ff,c00ffc00
Cpus_allowed_list:	10-19,30-39,50-59,70-79
Mems_allowed:	00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,0000000f
Mems_allowed_list:	0-3 ...

2.8
modify compare-cpu.sh and run:
# sh compare-cpu.sh 
----------yyyyyyyyyy----------yyyyyyyyyy----------yyyyyyyyyy----------yyyyyyyyyy

2.9
# virsh vcpuinfo rhel63
VCPU:           0
CPU:            55
State:          running
CPU time:       7.5s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy
...


2.10 
# pstree -apnh 24429
qemu-kvm,24429 -name libvirt_test_api -S -M rhel6.4.0 -enable-kvm -m 1024 -smp 24,sockets=24,cores=1,threads=1 -uuid 05867c1a-afeb-300e-e55e-2673391ae080 -nodefconfig -nodefaults -chardevsocket,id=charmonitor,pa
  ââ{qemu-kvm},26903
  ââ{qemu-kvm},26904
   ......
  
 

# grep Cpus_allowed_list /proc/26903/status 
Cpus_allowed_list:	0-79
# grep Cpus_allowed_list /proc/26904/status 
Cpus_allowed_list:	0-79

.....

3. 
3.4

check libvirtd.log:
# cat /var/log/libvirtd.log| grep Nodeset
2012-05-24 09:50:42.481+0000: 20343: debug : qemuProcessStart:3356 : Nodeset returned from numad: 1,3


3.5

2012-06-04 08:23:02.314+0000: 2830: debug : qemuProcessInitCpuAffinity:1731 : Setting CPU affinity
2012-06-04 08:23:02.316+0000: 2830: debug : qemuProcessInitCpuAffinity:1749 : Set CPU affinity with advisory nodeset from numad
2012-06-04 08:23:02.316+0000: 2830: debug : qemuProcessInitNumaMemoryPolicy:1595 : Set NUMA memory policy with specified nodeset
                                                      
2012-08-07 09:43:14.096+0000: 7932: debug : qemuProcessHook:2617 : Moving process to cgroup
2012-08-07 09:43:14.096+0000: 7932: debug : virCgroupNew:617 : New group /libvirt/qemu/rhel6q

2012-08-07 09:43:14.096+0000: 7932: debug : virCgroupDetect:274 : Detected mount/mapping 3:memory at /cgroup/memory in

2012-08-07 09:43:14.097+0000: 7932: debug : virCgroupMakeGroup:560 : Make controller /cgroup/memory/libvirt/qemu/rhel6q/




3.6

...
Cpus_allowed:	ffc0,0ffc00ff,c00ffc00
Cpus_allowed_list:	10-19,30-39,50-59,70-79
Mems_allowed:	00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,00000000,0000000f
Mems_allowed_list:	0-1,3 ...

3.7

----------yyyyyyyyyy----------yyyyyyyyyy----------yyyyyyyyyy----------yyyyyyyyyy

 


3.8

# virsh vcpuinfo rhel63
VCPU:           0
CPU:            55
State:          running
CPU time:       7.5s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy
...


3.9

# pstree -apnh 24458
qemu-kvm,24458 -name libvirt_test_api -S -M rhel6.4.0 -enable-kvm -m 1024 -smp 24,sockets=24,cores=1,threads=1 -uuid 05867c1a-afeb-300e-e55e-2673391ae080 -nodefconfig -nodefaults -chardevsocket,id=charmonitor,pa
  ââ{qemu-kvm},26903
  ââ{qemu-kvm},26904
   ......

 

# grep Cpus_allowed_list /proc/26903/status 
Cpus_allowed_list:	0-79
# grep Cpus_allowed_list /proc/26904/status 
Cpus_allowed_list:	0-79
...



4. 
4.2
...
2012-06-04 08:20:57.694+0000: 2761: debug : qemuProcessInitCpuAffinity:1731 : Setting CPU affinity
2012-06-04 08:20:57.696+0000: 2761: debug : qemuProcessInitCpuAffinity:1760 : Set CPU affinity with specified cpuset
2012-06-04 08:20:57.696+0000: 2761: debug : qemuProcessInitNumaMemoryPolicy:1599 : Set NUMA memory policy with advisory nodeset from numad
...

 

Notes:
Comments:

		177398 	[PCI and USB device assignment] Remove usb controller and vga device-bug818996. 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare one guest with usb controller and vga device.

this case is duplicated with the case  https://tcms.engineering.redhat.com/case/200241/?from_plan=6578
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.4.0

bug:

    No bug found

Actions:

1.  Edit guest 'xml

# virsh edit rhel6u3

2. Remove the <controller usb> and <video> .

 

3.Start  the guest

# virsh start rhel6u3

4. Dump the guest ' xml

There is no <controller usb> and <video>
	
Expected Results:
Notes:
this case is duplicated with the case https://tcms.engineering.redhat.com/case/200241/?from_plan=6578
Comments:

		177399 	[PCI and USB assignment] USB device can be reassigned to another VM without error - bug743671 	honzhang 	None 	Manual 		Bug verification 	P1 	None 	Edit
Setup:

1. Plug a USB device to host

# lsusb | grep Kingston
Bus 001 Device 004: ID 0951:1625 Kingston Technology DataTraveler 101 I

 
 

 

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    743671
    796059
    825068

Actions:

1. Plug a USB device into host and check its bus and device.
# lsusb | grep Kingston
Bus 001 Device 004: ID 0951:1625 Kingston Technology DataTraveler 101 I

2. Check use device selinux context. 
# ll /dev/bus/usb/001/ -Z
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 001
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 004

3. Install 2 guests or direclty use 2 existing guests.
# Install 2 guests via virt-install or virt-manager such as guest name is guest1, guest2

4. vim edit the following content into usb.xml
# cat usb.xml

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
 <vendor id='0x0951'/> <product id='0x1625'/>
      </source>
    </hostdev>

5. Attach the usb device to the first guest1, note that these 2 guests are shutoff status.
# virsh attach-device guest1 usb.xml --persistent

6. Start the first guest1.
# virsh start guest1
Domain guest1 started

7. Check usb device context.
# ll /dev/bus/usb/001/ -Z
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 001
crw-rw-r--. qemu qemu system_u:object_r:svirt_image_t:s0:c213,c954 004

Repeat 5-7 steps for the second guest2:

8.
# virsh attach-device guest2 usb.xml --persistent

9.
# virsh start guest2

10.
 # ll /dev/bus/usb/001/ -Z
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 001
crw-rw-r--. qemu qemu system_u:object_r:svirt_image_t:s0:c319,c628 004

Repeat 9-10 steps to check whether usb device selinux context will change.

	
Expected Results:

9. Error shows as follows:

error: Failed to start domain guest2
error: Requested operation is not valid: USB device 001:004 is in use by domain guest1

To repeat 9-10 steps and make sure usb device selinux context hasn't been reseted to original one, for example:

# ll /dev/bus/usb/001/ -Z
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 001
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 004

 It should still look like this(see steps 10)

 # ll /dev/bus/usb/001/ -Z
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 001
crw-rw-r--. qemu qemu system_u:object_r:svirt_image_t:s0:c319,c628 004

 

Notes, we may repeatly 9-10 steps for the second guest2 then check selinux context.
Notes:
Comments:

		177401 	[PCI and USB device assignment] Attach 1 usb device when have 2 usb devices with same vendor and product id - bug 815755 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Prepare 2 usb device with same verdor and product id, for example 2 token, both insert into your host

#lsusb

Bus 006 Device 003: ID 15e1:2007 RSA RSA SecurID (R) Authenticator

Bus 006 Device 004: ID 15e1:2007 RSA RSA SecurID (R) Authenticator
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Start a guest

2. Prepare an usb xml with larger device number

# cat usb.xml

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <vendor id='0x15e1'/>
        <product id='0x2007'/>
        <address bus='6' device='4'/> 
      </source>
    </hostdev>

Be sure that above 3 lines must exist concurrently

3. Attach this usb device to guest

# virsh attach-device guest usb.xml

4. Dump domain xml

# virsh dumpxml guest

5. Shutdown and restart guest

# virsh destroy guest

# virsh start guest

6. Attach following usb device to guest

# cat usb2.xml

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <vendor id='0x15e1'/>
        <product id='0x2007'/>
      </source>
    </hostdev>

# virsh attach-device guest usb2.xml
	
Expected Results:

Step4

In the guest, there should have the same usb xml with what we attached

...

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <vendor id='0x15e1'/>
        <product id='0x2007'/>
        <address bus='6' device='4'/> 
      </source>
    </hostdev>

...

Step6

error: Failed to attach device from usb2.xml
error: operation failed: multiple USB devices for 15e1:2007, use <address> to specify one

Notes:
Comments:

		177404 	[PCI and USB device assignment] Check automatic PCI address assignment for USB2 companion controllers-bug820869 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Edit guest'  xml .

# virsh edit rhel6u3
Domain rhel6u3 XML configuration edited.

2. Add the following XML to a guest

<controller type='usb' index='0' model='ich9-ehci1'/>

<controller type='usb' index='0' model='ich9-uhci1'/>

<controller type='usb' index='0' model='ich9-uhci2'/>

<controller type='usb' index='0' model='ich9-uhci3'/>

 

3.  Restart guest

# virsh destroy rhel6u3
Domain rhel6u3 destroyed

# virsh start rhel6u3
Domain rhel6u3 started

 

4. Dump the guest 'xml and check the PCI address  for USB2 companion controllers

# virsh dumpxml rhel6u3

...

    <controller type='usb' index='0' model='ich9-ehci1'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x7'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci1'>
      <alias name='usb0'/>
      <master startport='0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0' multifunction='on'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci2'>
      <alias name='usb0'/>
      <master startport='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x1'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci3'>
      <alias name='usb0'/>
      <master startport='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x2'/>
    </controller>

...
	
Expected Results:

Guest's XML configuration should look like step 4,  in addition, in the guest,

we should can see these usb devices take 1 slot via lspci:

# lspci

00:07.0 USB controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #1 (rev 03)
00:07.1 USB controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #2 (rev 03)
00:07.2 USB controller: Intel Corporation 82801I (ICH9 Family) USB UHCI Controller #3 (rev 03)
00:07.7 USB controller: Intel Corporation 82801I (ICH9 Family) USB2 EHCI Controller #1 (rev 03)

 
Notes:
Comments:

		177411 	[PCI and USB device assignment] Detach device during guest boot- bug 807023 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1.Have executed test case  with the summary  [SR-IOV] Prepare: Enable iommu.

2.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment
    rhel6.5

bug:

    No bug found

Actions:

1. Start a guest with the following hostdev device

...

   <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
                <address bus='5' slot='0' function='0'/>
            </source>
    </hostdev>

...

2. Start a guest

3. At the same time with step2, detach device from the guest asap

# virsh detach-device guest pci.xml

# cat pci.xml

   <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
                <address bus='5' slot='0' function='0'/>
            </source>
    </hostdev>

4. ll /proc/[pid of guest]/fd
	
Expected Results:

Step 3

Should report error like "can not detach device"

Step 4

Device still exist

should see line like

13 -> /system/devices/pci0000:00/0000:00:1c.4/0000:05:00.0/resource0

24 -> /system/devices/pci0000:00/0000:00:1c.4/0000:05:00.0/config
Notes:
Comments:

		177424 	[PCI and USB device assignment] Pass multi usb contorllers unordered - bug 822159 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    upstream
    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Prepare a shut down guest

2. Add following xml to guest xml

<controller index="0" model="ich9-uhci3" type="usb"/>
<controller index="0" model="ich9-ehci1" type="usb"/>
<controller index="0" model="ich9-uhci1" type="usb"/>
<controller index="0" model="ich9-uhci2" type="usb"/>

 3. Start guest
	
Expected Results:

Guest should be started successfully
Notes:
Comments:

		177429 	[PCI and USB device assignment] Support for qemu PCI romfile option 	ajia 	ajia 	Auto 		Function 	P1 	None 	Edit
Setup:

1. Do  124766: [SR-IOV] Prepare: Enable VT-D first

2. For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

modprobe -r kvm_intel

modprobe -r kvm

modprobe kvm allow_unsafe_assigned_interrupts=1

modprobe kvm_intel
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. virsh edit the following xml into guest configuration:

    <interface type='user'>
      <mac address='52:54:00:24:a5:9f'/>
      <model type='virtio'/>
      <rom file='/etc/fake/bootrom.bin'/>
    </interface>
    <interface type='user'>
      <mac address='52:54:00:63:79:db'/>
      <model type='virtio'/>
      <rom bar='on'/>
    </interface>
    <interface type='user'>
      <mac address='52:54:00:ac:b5:23'/>
      <model type='virtio'/>
      <rom bar='off'/>
    </interface>

    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x00' slot='0x19' function='0x0'/>
      </source>
      <rom file='/etc/fake/bootrom.bin'/>
    </hostdev>

Notes: remember to replace your NICs address, and you also add 'bar' and 'file' properity together.

2. create a fake rom file
# mkdir /etc/fake && touch /etc/fake/bootrom.bin

3. start the guest
# virsh start <domain>

4. check qemu-kvm cmdline
# ps -ef|grep qemu-kvm
qemu     16159     1 11 15:25 ?        00:00:01 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name foo
-uuid d93dae01-6865-d193-39f3-c62773b83417 -nodefconfig -nodefaults -chardev
socket,id=charmonitor,path=/var/lib/libvirt/qemu/foo.monitor,server,nowait -mon
chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device
piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive
file=/var/lib/libvirt/images/foo.qcow2,if=none,id=drive-virtio-disk0,format=qcow2
-device
virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1
-netdev tap,fd=22,id=hostnet0,vhost=on,vhostfd=23 -device
virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:7e:ef:a6,bus=pci.0,addr=0x3
-netdev user,id=hostnet1 -device
virtio-net-pci,netdev=hostnet1,id=net1,mac=52:54:00:24:a5:9f,bus=pci.0,addr=0x7,romfile=/etc/fake/bootrom.bin
-netdev user,id=hostnet2 -device
virtio-net-pci,netdev=hostnet2,id=net2,mac=52:54:00:63:79:db,bus=pci.0,addr=0x9,rombar=1
-netdev user,id=hostnet3 -device
virtio-net-pci,netdev=hostnet3,id=net3,mac=52:54:00:ac:b5:23,bus=pci.0,addr=0xa,rombar=0
-chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0
-vnc 127.0.0.1:0 -k en-us -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x4
-device
pci-assign,host=00:19.0,id=hostdev0,configfd=24,bus=pci.0,addr=0x8,romfile=/etc/fake/bootrom.bin
-device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

Notes, rom bar=off ==> rombar=0, rom bar=on ==> rombar=1, rom
file=/etc/fake/bootrom.bin ==> romfile=/etc/fake/bootrom.bin.

5. separately to check rom bar=on|off case for PCI device:
1) virsh edit the following xml into a new guest configuration
<hostdev mode='subsystem' type='pci' managed='yes'>
  <source>
    <address domain='0x0000' bus='0x00' slot='0x19' function='0x0'/>
  </source>
  <rom bar='off'/>
</hostdev>
2) start the new guest
# virsh start <new domain>
3) check qemu-kvm cmdline
# ps -ef|grep qemu-kvm
qemu     13531     1 73 15:03 ?        00:00:11 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name foo
-uuid d93dae01-6865-d193-39f3-c62773b83417 -nodefconfig -nodefaults -chardev
socket,id=charmonitor,path=/var/lib/libvirt/qemu/foo.monitor,server,nowait -mon
chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device
piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive
file=/var/lib/libvirt/images/foo.qcow2,if=none,id=drive-virtio-disk0,format=qcow2
-device
virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1
-netdev tap,fd=22,id=hostnet0,vhost=on,vhostfd=23 -device
virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:7e:ef:a6,bus=pci.0,addr=0x3
-chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0
-vnc 127.0.0.1:0 -k en-us -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x4
-device
pci-assign,host=00:19.0,id=hostdev0,configfd=24,bus=pci.0,addr=0x7,rombar=0
-device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

repeat 1)-3) and replace 'off' to 'on'.

Notes, rom bar=off ==> rombar=0, rom bar=on ==> rombar=1


The following is a specific SR-IOV case, and also check it:
 https://tcms.engineering.redhat.com/case/135970/?from_plan=5066

 
	
Expected Results:

Make sure romfile argument is correctly passed from libvirt to qemu-kvm.

 
Notes:
Comments:

		177431 	[PCI and USB device assignment] USB 2.0 pass-through can boot guest VM for a SECOND time - bug 808459 	bili 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:

Prepare a USB 2.0 device
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Create a linux VM using virsh cmd, guest must have the usb controller driver.

#cat usb_qcow2.xml

<domain type='kvm'>
  <name>usb_qcow2</name>
  <uuid>25e80a96-dfb8-2b4b-1a58-f95104ff3ffa</uuid>
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
  <vcpu>2</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/data/images/qcow2.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
 <controller type='usb' index='0'>
 <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
 </controller>
    <interface type='network'>
      <mac address='52:54:00:3b:1a:8a'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
</domain>

2.# virsh define usb_qcow2.xml

3. Plug the USB 2.0 device to the host usb interface.

4. Get the vendor and product id of the USB2.0 device

# lsusb

Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 002: ID 0557:7000 ATEN International Co., Ltd Hub
Bus 003 Device 003: ID 0557:2213 ATEN International Co., Ltd CS682 2-Port USB 2.0 DVI KVM Switch
Bus 001 Device 009: ID 058f:6387 Alcor Micro Corp. Transcend JetFlash Flash Drive

5. create a xml file, like

#cat usb-device.xml

 <hostdev mode='subsystem' type='usb' managed='yes'>
 <source>
 <vendor id='0x058f'/>
 <product id='0x6387'/>
 </source>
 </hostdev>

6. startup guest

# virsh start usb_qcow2

7. Attach the USB device to the guest

# virsh attach-device usb_qcow2 usb-device.xml --persistent

Device attached successfully

8. You can see the Device you just pass-through to the guest in /media/ directory

in the guest:

# ls /media/ 

New Volume

#lsusb

Bus 001 Device 004: ID 058f:6387 Alcor Micro Corp. Transcend JetFlash Flash Drive 

9. shutdown guest

# virsh destroy guest

10. startup guest

# virsh start guest

The guest could boot normally, and the USB device is working well.

 
	
Expected Results:

Step 10:

The guest could boot normally. And the USB device is work well in guest.

Should not get the error info like:

Error starting domain: Requested operation is not valid: USB device 001:011 is
in use by domain ***

 

 
Notes:
Comments:

		177432 	[PCI and USB device assignment]Missing support for persistent hotplug attach/detach of <hostdev> devices BZ#802856 	whuang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1.enable kernel iommu. edit grub.conf add intel_iommu=on at the end of kernel line.

2.For platform just support vt-d1(host kernel) and host kernel
larger than 171 kernel, do the following steps.
modprobe -r kvm_intel
modprobe -r kvm
modprobe kvm allow_unsafe_assigned_interrupts=1
modprobe kvm_intel 

3.Check device list, prepare hotplug network from host to guest.
computer |
+- pci_0000_00_19_0 
| |
| +- net_eth0_44_37_e6_67_11_a2


4.# virsh nodedev-dumpxml pci_0000_00_19_0

5. Prepare hostdev.xml that like as following
<hostdev mode='subsystem' type='pci' managed='yes'>
<source>
<address domain='0x0000' bus='0x00' slot='0x19' function='0x0'/>
</source>
</hostdev> 

6.# virsh attach-device rhel6 hostdev.xml --persistent
Device attached successfully 

7. In guest, using lspci, and ping to check the host network device is working
fine. 

8. # virsh destroy rhel6 
    # virsh start rhel6

9. The host network device still works fine in guest.

10. # virsh detach-device rhel6 hostdev.xml --persistent
Device detached successfully 

11. # virsh destroy rhel6

12. # virsh start rhel6

	
Expected Results:

Step6
Device attached successfully 


Step10
Device detached successfully
The host network device automatically be re-attached to host.


Step 12 
The host network device don't exist in guest. 

Notes:
Comments:

		177433 	[PCI and USB device assignment]segfault when attempting to detach non-existent network device BZ#802664 	whuang 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Prepare and start a guest named rhel6
# virsh start rhel6

2.Prepare the following xml.
<interface type='user'>
<mac address='02:11:22:33:44:55'/>
<model type='virtio'/>
</interface>

3.# virsh attach-device rhel6 net.xml
Device attached successfully

4.# virsh detach-device rhel6 net.xml
Device detached successfully

5.# virsh detach-device rhel6 net.xml

	
Expected Results:

5. error should be 

error: Failed to detach device from /home/net.xml
error: operation failed: network device 02:11:22:33:44:55 not found

 

 

NOT like :

error: Failed to detach device from net.xml
error: End of file while reading data:Input/Output error

Notes:
Comments:

		177435 	[PCI and USB devices assignment] migration with assigned devices - bug 816519 	dyuan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

https://bugzilla.redhat.com/show_bug.cgi?id=816519
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB devices assignment
    rhel6.4.0

bug:

    No bug found

Actions:

 1. Enable Intel VT-d in BIOS.

2. Append kernel parameter intel_iommu=on to the host machine's
  kernel parameters and then boot the host machine.

3. Install a guest

4. Assign a PCI device and a USB device to the guest

5. start the guest

6. Check whether the device on the host has successfully been
  passed through from the host machine, using lspci/lsusb or virsh
  nodedev-list.

7. # virsh dump guest
# virsh save guest

# virsh snapshot-create-as guest rhel-hello
# virsh migrate guest qemu+ssh://$dstip/system



	
Expected Results:

step 7


all commands can be done correctly without error.
Notes:
Comments:

		177437 	[Miscellanea]Cannot roundtrip blkio parameters due to broken deviceWeight handling BZ#800734 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

setup

remember install the pkg perl-Sys-Virt.x86_64 0:0.10.2-4.el6
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    aide
    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

1 prepare set_blkio.pl as follows:
 # cat set_blkio.pl 
#!/usr/bin/perl
use warnings;
use strict;
use Sys::Virt;

my $uri = "qemu:///system";
my $domname = "vr-rhel6-x86_64-kvm";
my $flags = 0;

my $con = Sys::Virt->new(address => $uri, readonly => 0);
my $dom = $con->get_domain_by_name($domname);
my $weight = 600;

my %blkio = (Sys::Virt::Domain::BLKIO_WEIGHT => $weight);
$dom->set_blkio_parameters(\%blkio);
my $current_weight = $dom->get_blkio_parameters();
print $current_weight->{weight}."\n";

2 execute set_blkio.pl to set blkio value and get blkio value to check:
# perl set_blkio.pl 
600

	
Expected Results:

as the output
Notes:
Comments:

		177438 	[python binding] blkioParameters test BZ#770795 	zhpeng 	None 	Manual 		Regression 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    python binding

bug:

    No bug found

Actions:

1. # virsh blkiotune foo
weight : 200

device_weight :

2. # python
Python 2.6.6 (r266:84292, Sep 12 2011, 14:03:14)
[GCC 4.4.5 20110214 (Red Hat 4.4.5-6)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import libvirt
>>> con = libvirt.open(None)
>>> dom = con.lookupByName('foo')
>>> print dom.blkioParameters(0)

	
Expected Results:

Step 2 result should like Step 1
Notes:
will be covered in test-API.
Comments:

		177439 	[python binding] blockIoTune did not work right with parameters BZ#770683 	zhpeng 	None 	Manual 		Regression 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    python binding

bug:

    No bug found

Actions:

Step 1:
# python
Python 2.6.6 (r266:84292, Sep 12 2011, 14:03:14) 
[GCC 4.4.5 20110214 (Red Hat 4.4.5-6)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import libvirt
>>> conn = libvirt.open(None)
>>> dom = conn.lookupByName("rhel6u2")
>>> dom.blockIoTune('hda', None, 1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: blockIoTune() takes exactly 3 arguments (4 given)
>>> dom.blockIoTune('hda', 0)
libvir: QEMU error : internal error cannot read total_bytes_sec
>>> dom.blockIoTune('hda', 1)
libvir: QEMU error : internal error cannot read total_bytes_sec
>>> dom.blockIoTune('hda', 2)
{'write_bytes_sec': 0L, 'total_iops_sec': 0L, 'read_iops_sec': 0L,
'read_bytes_sec': 0L, 'write_iops_sec': 0L, 'total_bytes_sec': 0L}

	
Expected Results:

As Step 1

# python
Python 2.6.6 (r266:84292, Sep 12 2011, 14:03:14) 
[GCC 4.4.5 20110214 (Red Hat 4.4.5-6)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import libvirt
>>> conn = libvirt.open(None)
>>> dom = conn.lookupByName("rhel6u2")
>>> dom.blockIoTune('hda', None, 1)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: blockIoTune() takes exactly 3 arguments (4 given)
>>> dom.blockIoTune('hda', 0)
libvir: QEMU error : internal error cannot read total_bytes_sec
>>> dom.blockIoTune('hda', 1)
libvir: QEMU error : internal error cannot read total_bytes_sec
>>> dom.blockIoTune('hda', 2)
{'write_bytes_sec': 0L, 'total_iops_sec': 0L, 'read_iops_sec': 0L,
'read_bytes_sec': 0L, 'write_iops_sec': 0L, 'total_bytes_sec': 0L}

Notes:
will be covered in test-API.
Comments:

		177470 	[remote access] virsh connect fails with remote machine which has different libvirt version BZ#760436 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access

bug:

    881552 - From Run 51755

Actions:

Steps: 

1. Local machine has the environment above new-libvirt pkg  etc.

 libvirt-0.9.8-1.el6.x86_64 (new one)


 Remote machine has the same except libvirt is libvirt-0.9.4-23.el6.x86_64 (old one)

 2. local machine connect remote machine  from new to old 
 # virsh -c qemu+ssh://$OLD_Host/system

	
Expected Results:

 connect success
Notes:
Comments:

		177472 	[remote access]libvirt runs qemu with tls options even when certs/keys are not set BZ#790436 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access

bug:

    No bug found

Actions:

set /etc/libvirt/qemu.conf spice_tls = 0 then restart libvirtd 


#virsh define tls.xml 

<domain type='kvm'>
  <name>auto-tls-port</name>
  <memory>65536</memory>
  <os>
    <type arch='x86_64' machine='pc'>hvm</type>
  </os>
  <devices>
    <graphics type='spice' port='5900' tlsPort='-1' autoport='yes' listen='0' keymap='en-us' passwdValidTo='2012-02-14T13:45:54' connected='disconnect'>
      <listen type='address' address='0'/>
      <channel name='main' mode='secure'/>
      <channel name='inputs' mode='secure'/>
    </graphics>
  </devices>
</domain>


# virsh start auto-tls-port

 

	
Expected Results:


Expected results:

# virsh create t.xml 
error: Failed to create domain from t.xml
error: unsupported configuration: spice secure channels set in XML configuration, but TLS is disabled in qemu.conf

Notes:
Comments:

		177473 	[remote access]There are some typos when virsh connect source guest server with ssh PermitRootLogin disabled BZ#822340 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access

bug:

    No bug found

Actions:

 

When use virsh -c connect source guest server with ssh PermitRootLogin disabled, there are some typos in pop up warning message
Here is the warning message:

 

input incorrect passwd 3 times

# virsh -c qemu+ssh://10.66.6.214/system list 

The authenticity of host '10.66.6.214 (10.66.6.214)' can't be established.
RSA key fingerprint is 28:96:e7:fb:a3:0d:2e:bc:07:e2:2c:cc:9d:76:39:30.
Are you sure you want to continue connecting (yes/no)? yes
root@10.66.6.214's password:
root@10.66.6.214's password:
root@10.66.6.214's password:
error: Cannot recv data: Warning: Permanently added '10.66.6.214' (RSA) to the list of known hosts.
Permission denied, please try again.
Permission denied, please try again.
: Connection reset by peerey,gssapi-keyex,gssapi-with-mic,password).

	
Expected Results:

like this : 

#  virsh -c qemu+ssh://10.66.6.217/system list
The authenticity of host '10.66.6.217 (10.66.6.217)' can't be established.
RSA key fingerprint is 12:df:c5:0b:24:1b:b8:f4:e6:22:e2:b6:1c:12:60:fe.
Are you sure you want to continue connecting (yes/no)? yes
root@10.66.6.217's password: 
root@10.66.6.217's password: 
root@10.66.6.217's password: 
error: Cannot recv data: Warning: Permanently added '10.66.6.217' (RSA) to the list of known hosts.
Permission denied, please try again.
Permission denied, please try again.
Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).: Connection reset by peer
error: failed to connect to the hypervisor

Notes:
Comments:

		177477 	[Scalability] 256 autostarted network (isolated, route , NAT) reloading 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

 Please also refer to the following cases.

[Virtual Networks] Route virtual network <https://tcms.engineering.redhat.com/case/124969>

[Virtual Networks] Isolated virtual network <https://tcms.engineering.redhat.com/case/124959>

[Virtual Networks] NAT virtual network <https://tcms.engineering.redhat.com/case/124966>

 

Prepare the following network xml file 

#cat  network.xml

<network>

<name>#netname#</name>

<bridge name="#bridgename#" stp="on" delay="0"/>

<ip address="#ip#" netmask="255.255.255.0">

<dhcp><range start="192.168.1.2" end="192.168.1.254"/></dhcp>

</ip>

</network>

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. sh create-256net.sh

cat  create-256net.sh

#!/bin/sh
name="network"
bridgename="virbr"
ip="192.168"
netfile="network.xml"
for i in {1..100}
do
  sed -i -e "s,#netname#,$name$i,g" $netfile 
  sed -i -e "s,#bridgename#,$bridgename$i,g" $netfile
  sed -i -e "s,#ip#,$ip.$i.1,g" $netfile
  sed -i -e "s,#ipstart#,$ip.$i.2,g" $netfile
  sed -i -e "s,#ipend#,$ip.$i.254,g" $netfile
  virsh net-define $netfile
  sleep 2
  sed -i -e "s,$name$i,#netname#,g" $netfile
  sed -i -e "s,$bridgename$i,#bridgename#,g" $netfile
  sed -i -e "s,$ip.$i.1,#ip#,g" $netfile
  sed -i -e "s,$ip.$i.2,#ipstart#,g" $netfile
  sed -i -e "s,$ip.$i.254,#ipend#,g" $netfile
  virsh net-start $name$i
  virsh net-autostart $name$i
 
done

for i in {101..200}
do
  sed '3i\<forward mode='\''nat'\''/>' $netfile > network1.xml
  sed -i -e "s,#netname#,$name$i,g" network1.xml 
  sed -i -e "s,#bridgename#,$bridgename$i,g" network1.xml
  sed -i -e "s,#ip#,$ip.$i.1,g" network1.xml
  sed -i -e "s,#ipstart#,$ip.$i.2,g" network1.xml
  sed -i -e "s,#ipend#,$ip.$i.254,g" network1.xml
  virsh net-define network1.xml
  sleep 2
  sed -e '3d' network1.xml
  sed -i -e "s,$name$i,#netname#,g" network1.xml
  sed -i -e "s,$bridgename$i,#bridgename#,g" network1.xml
  sed -i -e "s,$ip.$i.1,#ip#,g" network1.xml
  sed -i -e "s,$ip.$i.2,#ipstart#,g" network1.xml
  sed -i -e "s,$ip.$i.254,#ipend#,g" network1.xml
  virsh net-start $name$i
  virsh net-autostart $name$i

done


for i in {201..255}
do
  sed '3i\<forward mode='\''route'\''/>' $netfile > network2.xml
  sed -i -e "s,#netname#,$name$i,g" network2.xml
  sed -i -e "s,#bridgename#,$bridgename$i,g" network2.xml
  sed -i -e "s,#ip#,$ip.$i.1,g" network2.xml
  sed -i -e "s,#ipstart#,$ip.$i.2,g" network2.xml
  sed -i -e "s,#ipend#,$ip.$i.254,g" network2.xml
  virsh net-define network2.xml
  sleep 2
  sed -e '3d' network2.xml
  sed -i -e "s,$name$i,#netname#,g" network2.xml
  sed -i -e "s,$bridgename$i,#bridgename#,g" network2.xml
  sed -i -e "s,$ip.$i.1,#ip#,g" network2.xml
  sed -i -e "s,$ip.$i.2,#ipstart#,g" network2.xml
  sed -i -e "s,$ip.$i.254,#ipend#,g" network2.xml
  virsh net-start $name$i
  virsh net-autostart $name$i

done
for k in {1..10}

do
#reload 256 pool

echo "------${k}-------"

service libvirtd restart
sleep 2

running_net_num=$(virsh net-list --all |grep active |wc -l)
if [[ $running_net_num -lt 256 ]];then
    echo "network reloading failed"
else
    echo "succeed"
fi

done

	
Expected Results:

Make sure you get Succeed return value after performing the script
Notes:
Comments:

		177489 	[Scalability] Attach 1024 virtio scsi disks on 1 guest-bug841519 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Now it has a  bug

Bug 808980 - [FJ6.2 Bug]: We cannot define 221 virtio devices on a KVM guest.

And now qemu-kvm command line can not start guest with so many disks because the command line length limitation

NOTE: The number of attaching disk  could be 990

ï»¿Bug 841519 - virtio scsi disk could not be initialized when hotplug virtio scsi disk more than 990
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Prepare a shutdown guest

2. Add the following xml to guest with "virsh edit"

<controller type='scsi' index='0' model='virtio-scsi'/>

3.  # cat scsi-disk-unit.xml
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/#IMAGE#'/>
      <target dev='sd#ID#' bus='scsi'/>
      <address type='drive' controller='0' bus='0' target='0' unit='#UNIT#'/>
    </disk>

4. Attach 1024 scsi disks to guest

# for i in {0..1023}; do sed -e "s/#IMAGE#/disk-$i/g" -e "s/#UNIT#/$i/g" -e "s/#ID#/$i/g" scsi-disk-unit.xml > scsi-disks-$i.xml ; virsh attach-device desktop scsi-disks-$i.xml --persistent; qemu-img create /var/lib/libvirt/images/disk-$i 10M; done

5.  Start guest

# virsh start guest
	
Expected Results:

Guest can be started successfully, all the disks can be find in the guest
Notes:
Comments:

		177490 	[Scalability] Attach/Detach maximum Vfs to one guest for looping 10 times-bug696877,bug678368 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

VT-D is enabled ( including bios vt-d enable and kernel line "intel_iommu=on" adding for intel host)

1.1.Have executed test case  "[SR-IOV] Prepare: Enable VT-D"

1. For 82576 card, the max function is 8 (0~7)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Refer to the case " [SR-IOV] Assign one VF to one guest "

2. Prepare 8 VF.xml files  with named VF1.xml ..VF8.xml.

          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='66' slot='17' function='5'/>
            </source>
          </hostdev>

3. Prepare the following script.

cat attachDetachMaxVFs.sh


#!/bin/bash
guest="rhel6u3"
path="/var/lib/libvirt/images/VF"
for i in {1..10};
do
  echo "------------${i}--------------"
  current=1
  current2=1
  end=8

  while [ $current -le $end ] 
  do
  echo "virsh attach-device VF${current} successfully"
  virsh attach-device ${guest} ${path}${current}.xml
    sleep 3
    current=`expr $current + 1`

  done

  while [ $current2 -le $end ]
  do 
  echo "virsh detach-device VF${current2} successfully"
  virsh detach-device ${guest} ${path}${current2}.xml
    sleep 3
    current2=`expr $current2 + 1`
  done
done

4. sh  attachDetachMaxVFs.sh

5. On guest, check whether network works fine.

# service network restart

# ifconfig

# ping {host}

 
	
Expected Results:

step 5.

Guest could get ip and ping host successfully
Notes:
Comments:

		177491 	[Scalability] Attach/Detach maxmum ide disks to guest for looping 10 times 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1.KVM is limited to a maximum of four virtualized (emulated) IDE devices per guest.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    No bug found

Actions:

1. Assign 4 ide disks to guest via editing guest ' xml.

..

 <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u3-4.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u3.img'/>
      <target dev='hdb' bus='ide'/>
      <alias name='ide0-0-1'/>
      <address type='drive' controller='0' bus='0' target='0' unit='1'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u3-3.img'/>
      <target dev='hdc' bus='ide'/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u3.img'/>
      <target dev='hdd' bus='ide'/>
      <alias name='ide0-1-1'/>
      <address type='drive' controller='0' bus='1' target='0' unit='1'/>
    </disk>
..

2. virsh start guest

 

3. login to the guest to check

# fdisk -l | grep /dev/hd

 

4. virsh destroy guest

 

5. virsh edit  guest and add the following xml

 <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/test.qcow2'/>
      <target dev='hde' bus='ide'/>
      <address type='drive' controller='1' bus='1' target='1' unit='0'/>
 </disk>

6. virsh start guest.

 

7. Detach 4 ide disks from guest via editing guest ' xml . And delete the following xml part.

..

 <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u3-4.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u3.img'/>
      <target dev='hdb' bus='ide'/>
      <alias name='ide0-0-1'/>
      <address type='drive' controller='0' bus='0' target='0' unit='1'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u3-3.img'/>
      <target dev='hdc' bus='ide'/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u3.img'/>
      <target dev='hdd' bus='ide'/>
      <alias name='ide0-1-1'/>
      <address type='drive' controller='0' bus='1' target='0' unit='1'/>
    </disk>
..

 

8. Execute above steps for looping 10 times.

 
	
Expected Results:

3. All the disks can be accessed in guest

 

5.

error: Failed to start domain rhel6u3
error: internal error Only 1 ide controller is supported
Notes:
Comments:

		177492 	[Scalability] Attach/Detach virtual disk which is 1TB to guest for looping 100 times - large machine 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

Creating your public key pair and copy to guest
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{guest ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Define and start a guest  

2. qemu-img create /var/lib/libvirt/images/disk.img   1T

3 . Prepare  script.

#cat attach.sh

#!/bin/sh

for i in {1..100}

do

echo "--------${i}-----------"

virsh attach-disk guest  /var/lib/libvirt/images/disk.img vdb

sleep 1

ssh {guest ip} "fdisk -l |grep vd |tee -a disk.log"

virsh detach-disk guest  vdb

sleep 1

done

4. Run the  attach.sh  in host  almost at the same time .

# sh attach.sh
	
Expected Results:

Check the disk.log in guest  and the  disk changed information  will be recorded in log. 

There is no any error.
Notes:
Comments:

		177493 	[Scalability] Attach/Detach virtual disks to guest over the top limitation via attach-device 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1.  Presently, guests are limited to a maximum of 32 PCI devices. Some PCI devices are critical for the guest to run and these devices cannot be removed.

The default, required devices are:   video device / usb / ide controller /memballoon. Remove the other pci devices  from guest. Note: in rhel6.4 , video device and ide controller can be removed.

https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/chap-Virtualization_Host_Configuration_and_Guest_Installation_Guide-PCI_Assignment.html

"PCI devices are limited by the virtualized system architecture. Out of the 32 PCI devices for a guest, 4 are always defined for a KVM guest, and are not removable. This means there are up to 28 PCI slots available for additional devices per guest. Note, however, that only up to 8 devices per guest are supported. Each PCI device in a guest can have up to 8 functions."

 

Note : There is the Bug 808980 [FJ6.2 Bug]: We cannot define 221 virtio devices on a KVM guest.

2. Creating your public key pair and copy to guest
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{guest ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    No bug found

Actions:

 

1.  start one guest

# vrish start guest

 

2. Run script

2.1 # cat disk-template.xml

<disk type='file' device='disk'>

<driver name='qemu' type='raw'/>

<source file='#SOURCEFILE#'/>

<target dev='#TARGETDEV#' bus='usb'/>

</disk>

 

2.2 # cat attachdevice28disk.sh

#!/bin/sh

for  loop  in {1..10}  ; do


for i in {a..z}

do

qemu-img create /var/lib/libvirt/images/disk-$i.img 10M

sourcefile="/var/lib/libvirt/images/disk-$i.img"

cp disk-template.xml disk-$i.xml

sed -i -e "s,#SOURCEFILE#,$sourcefile,g"  \

     -e "s,#TARGETDEV#,vd$i,g" disk-$i.xml

virsh attach-device $1 disk-$i.xml

sleep 1

done

 

for i  in {a..b}

do

qemu-img create /var/lib/libvirt/images/disk-vda$i.img 10M

sourcefile="/var/lib/libvirt/images/disk-vda$i.img"

cp disk-template.xml disk-vda$i.xml

sed -i -e "s,#SOURCEFILE#,$sourcefile,g"  \

     -e "s,#TARGETDEV#,vda$i,g" disk-vda$i.xml

virsh attach-device $1 disk-vda$i.xml

sleep 1

done

 

ssh {guest ip} "fdisk -l |grep vd |tee -a disk-attach.log"

 

for i in {a..z}

do 

virsh detach-device $1  disk-$i.xml

sleep 1

done

 

for i in {a..b}

do 

virsh detach-device $1 disk-vda$i.xml

sleep 1

done

done

 

ssh {guest ip} "fdisk -l |grep vd |tee -a disk-detach.log" 

 

2.3 # sh  attachdevice29disk.sh  guest

=========================================================================================================================

Test 232 virtual disks with multifunction

1. Prepare the following xml

 # cat 232disk.xml
<disk type="file" device="disk">
<driver name="qemu" type="raw"/>
<source file="/var/lib/libvirt/images/disks/#file#"/>
<address type="pci" domain="0" bus="0" #slot# #function# multifunction="on"/>
<target dev="#dev#" bus="virtio"/>
</disk>

 

# cat 232domain.xml
<domain type="kvm" id="16">
<name>usb</name>
<uuid>b86b1082-e42b-07c1-b014-14c3c7f91780</uuid>
<memory unit="KiB">524288</memory>
<currentMemory unit="KiB">524288</currentMemory>
<vcpu>1</vcpu>
<os><type arch="x86_64" machine="rhel6.2.0">hvm</type>
<boot dev="hd"/>
</os>
<features>
<acpi/><apic/>
<pae/>
</features>
<clock offset="utc"/>
<on_poweroff>destroy</on_poweroff>
<on_reboot>restart</on_reboot>
<on_crash>restart</on_crash><devices>
<emulator>/usr/libexec/qemu-kvm</emulator>
<disk type="block" device="disk">
<driver name="qemu" type="raw" cache="none"/>
<source dev="/var/lib/libvirt/images/test.img"/>
<target dev="hda" bus="ide"/>
<alias name="ide0-0-0"/>
<address type="drive" controller="0" bus="0" target="0" unit="0"/>
</disk>
<controller type="usb" index="0">
<alias name="usb0"/>
<address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x2"/>
</controller>
<controller type="ide" index="0">
<alias name="ide0"/>
<address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x1"/>
</controller>
<memballoon model="none"></memballoon>

2. Run the following script to create domain xml .

# cat create232disk.sh
#!/bin/sh
netfile="232disk.xml"
domfile="232domain.xml"
k=0
s=2
for i in {a..i};do
  for j in {a..z};do
    echo "--i${i}--j${j} --"
        qemu-img create /var/lib/libvirt/images/disks/usb-$i-$j.img 10M
        sed -i -e "s,#file#,usb-$i-$j.img,g" $netfile  
        sed -i -e "s,#slot#,slot=\"$s\",g" $netfile
        sed -i -e "s,#dev#,sd$i$j,g" $netfile
        if [ $k -le 7 ]; then
         sed -i -e "s,#function#,function=\"$k\",g" $netfile
         cat $netfile
         cat $netfile >> $domfile
         sed -i -e "s,slot=\"$s\",#slot#,g" $netfile
        else
         k=0
         s=`expr $s + 1`

         sed -i -e "s,#slot#,slot=\"$s\",g" $netfile

         sed -i -e "s,#function#,function=\"$k\",g" $netfile
         cat $netfile
         cat $netfile >> $domfile
         sed -i -e "s,slot=\"$s\",#slot#,g" $netfile
        fi

        sed -i -e "s,usb-$i-$j.img,#file#,g" $netfile  
        sed -i -e "s,sd$i$j,#dev#,g" $netfile
        sed -i -e "s,function=\"$k\",#function#,g" $netfile       
        k=`expr $k + 1`
       
   done
done
sleep 3
r=$(sed -n '$=' $domfile)
sed $(($r-12+1)),${r}d $domfile
echo "</devices>" >> $domfile
echo "</domain>" >> $domfile

3. Define the domain with new created xml file.

#virsh define guest

4. Start the guest

#virsh start guest

in guest, check the disks status
	
Expected Results:

Step 2.There is no error occurs.

check the log file in guest to make sure that the attach/detach succeed

==================================================================================================================================

Test 232 virtual disks with multifunction

Step 4. There is no error occurs. Domain can successfully start.

You can find all disks in the guest
Notes:
Comments:

		177494 	[Scalability] Attach/Detach virtual disks to guest over the top limitation via attach-disk 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1.  Presently, guests are limited to a maximum of 32 PCI devices. Some PCI devices are critical for the guest to run and these devices cannot be removed.

The default, required devices are:   video device / usb / ide controller/ memballoon . Remove the other pci devices  from guest. Note: in rhel6.4 , video device and ide controller can be removed.

https://access.redhat.com/knowledge/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/chap-Virtualization_Host_Configuration_and_Guest_Installation_Guide-PCI_Assignment.html

"PCI devices are limited by the virtualized system architecture. Out of the 32 PCI devices for a guest, 4 are always defined for a KVM guest, and are not removable. This means there are up to 28 PCI slots available for additional devices per guest. Note, however, that only up to 8 devices per guest are supported. Each PCI device in a guest can have up to 8 functions."

 

Note : There is the Bug 808980 [FJ6.2 Bug]: We cannot define 221 virtio devices on a KVM guest.

2. Creating your public key pair and copy to guest
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{guest ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    870344 - From Run 51862

Actions:

1.  start one guest

# vrish start guest

 

2. Run script

# cat attach28disk.sh

#!/bin/sh

for i in {a..z}

do

qemu-img create /var/lib/libvirt/images/disk-$i.img 1M

virsh attach-disk $1 /var/lib/libvirt/images/disk-$i.img vd$i

sleep 1

done

 

for i in {a..b}

do

qemu-img create /var/lib/libvirt/images/disk-vda$i.img 1M

virsh attach-disk $1 /var/lib/libvirt/images/disk-vda$i.img vda$i

sleep 1

done

 ssh {guest ip} "fdisk -l |grep vd |tee -a disk-attach.log"

for i in {a..z}

do

virsh detach-disk $1  vd$i

sleep 1

done

 

for i in {a..b}

do

virsh detach-disk $1  vda$i

sleep 1

done

 ssh {guest ip} "fdisk -l |grep vd |tee -a disk-detach.log"

3. sh  attach28disk.sh  guest 

=========================================================================================================================

Test 232 virtual disks with multifunction

1. Prepare the following xml

 # cat 232disk.xml
<disk type="file" device="disk">
<driver name="qemu" type="raw"/>
<source file="/var/lib/libvirt/images/disks/#file#"/>
<address type="pci" domain="0" bus="0" #slot# #function# multifunction="on"/>
<target dev="#dev#" bus="virtio"/>
</disk>

 

# cat 232domain.xml
<domain type="kvm" id="16">
<name>usb</name>
<uuid>b86b1082-e42b-07c1-b014-14c3c7f91780</uuid>
<memory unit="KiB">524288</memory>
<currentMemory unit="KiB">524288</currentMemory>
<vcpu>1</vcpu>
<os><type arch="x86_64" machine="rhel6.2.0">hvm</type>
<boot dev="hd"/>
</os>
<features>
<acpi/><apic/>
<pae/>
</features>
<clock offset="utc"/>
<on_poweroff>destroy</on_poweroff>
<on_reboot>restart</on_reboot>
<on_crash>restart</on_crash><devices>
<emulator>/usr/libexec/qemu-kvm</emulator>
<disk type="block" device="disk">
<driver name="qemu" type="raw" cache="none"/>
<source dev="/var/lib/libvirt/images/test.img"/>
<target dev="hda" bus="ide"/>
<alias name="ide0-0-0"/>
<address type="drive" controller="0" bus="0" target="0" unit="0"/>
</disk>
<controller type="usb" index="0">
<alias name="usb0"/>
<address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x2"/>
</controller>
<controller type="ide" index="0">
<alias name="ide0"/>
<address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x1"/>
</controller>
<memballoon model="none"></memballoon>

2. Run the following script to create domain xml .

# cat create232disk.sh
#!/bin/sh
netfile="232disk.xml"
domfile="232domain.xml"
k=0
s=2
for i in {a..i};do
  for j in {a..z};do
    echo "--i${i}--j${j} --"
        qemu-img create /var/lib/libvirt/images/disks/usb-$i-$j.img 10M
        sed -i -e "s,#file#,usb-$i-$j.img,g" $netfile  
        sed -i -e "s,#dev#,sd$i$j,g" $netfile
        if [ $k -le 7 ]; then
         sed -i -e "s,#function#,function=\"$k\",g" $netfile
         cat $netfile
         cat $netfile >> $domfile
         sed -i -e "s,slot=\"$s\",#slot#,g" $netfile
        else
         k=0
         s=`expr $s + 1`

         sed -i -e "s,#slot#,slot=\"$s\",g" $netfile

         sed -i -e "s,#function#,function=\"$k\",g" $netfile
         cat $netfile
         cat $netfile >> $domfile
         sed -i -e "s,slot=\"$s\",#slot#,g" $netfile
        fi
        sed -i -e "s,usb-$i-$j.img,#file#,g" $netfile  
        sed -i -e "s,sd$i$j,#dev#,g" $netfile
        sed -i -e "s,function=\"$k\",#function#,g" $netfile       
        k=`expr $k + 1`
       
   done
done
sleep 3
r=$(sed -n '$=' $domfile)
sed $(($r-12+1)),${r}d $domfile
echo "</devices>" >> $domfile
echo "</domain>" >> $domfile

3. Define the domain with new created xml file.

#virsh define guest

4. Start the guest

#virsh start guest

in guest, check the disks status 
	
Expected Results:

There is no error occurs.

check the log file in guest to make sure that the attach/detach succeed 

===============================================================================================================================

Test 232 virtual disks with multifunction

Step 4. There is no error occurs. Domain can successfully start.

 You can find all disks in the guest
Notes:
Comments:

		177495 	[Scalability] Attach/Detach virtual interfaces (vnet) to one guest to the top limitation of the guest 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

1.  Presently, guests are limited to a maximum of 32 PCI devices. Some PCI devices are critical for the guest to run and these devices cannot be removed.

The default, required devices are:   video device / usb / ide controller . Remove the other pci devices  from guest.

Note : There is the Bug 808980 [FJ6.2 Bug]: We cannot define 221 virtio devices on a KVM guest.

2. Creating your public key pair and copy to guest
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{guest ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    870344 - From Run 51862

Actions:

1.  start one guest

# vrish start guest

 

2. Run script

#cat detachinterface.sh

  #!/bin/sh
  for i in {1..10};do
  for i in {1..28}
  do virsh attach-interface $1 network default --target vnet$i
  sleep 2
  done  

 ssh {guest ip} "ifconfig -a |grep eth | tee -a interface-attach.log"

  for i in {1..28}
  do
  interface=$( virsh dumpxml $1 | grep "mac address" | head -1|cut -c 21-37)
  echo "$interface"
  virsh detach-interface $1 network --mac $interface
  sleep 2
  done

 ssh {guest ip} "ifconfig -a |grep eth | tee -a interface-detach.log"

  done

3. sh detachinterface.sh guest 

==============================================================================================================================

Test 232 virtual interface with multifunction

1. Prepare the following xml

# cat 232interface.xml
<interface type="network">
      <mac address=#mac#/>
      <source network="default"/>
      <target dev="#dev#"/>
      <address type="pci" domain="0" bus="0" #slot# #function# multifunction="on"/>
</interface>

# cat 232interdomain.xml.bak
<domain type="kvm" id="16">
<name>usb</name>
<uuid>b86b1082-e42b-07c1-b014-14c3c7f91780</uuid>
<memory unit="KiB">524288</memory>
<currentMemory unit="KiB">524288</currentMemory>
<vcpu>1</vcpu>
<os><type arch="x86_64" machine="rhel6.2.0">hvm</type>
<boot dev="hd"/>
</os>
<features>
<acpi/><apic/>
<pae/>
</features>
<clock offset="utc"/>
<on_poweroff>destroy</on_poweroff>
<on_reboot>restart</on_reboot>
<on_crash>restart</on_crash><devices>
<emulator>/usr/libexec/qemu-kvm</emulator>
<disk type="block" device="disk">
<driver name="qemu" type="raw" cache="none"/>
<source dev="/var/lib/libvirt/images/test.img"/>
<target dev="hda" bus="ide"/>
<alias name="ide0-0-0"/>
<address type="drive" controller="0" bus="0" target="0" unit="0"/>
</disk>
<controller type="usb" index="0">
<alias name="usb0"/>
<address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x2"/>
</controller>
<controller type="ide" index="0">
<alias name="ide0"/>
<address type="pci" domain="0x0000" bus="0x00" slot="0x01" function="0x1"/>
</controller>
<memballoon model="none"></memballoon>


2.  Run the following script to create domain xml.

# cat create232inter.sh
#!/bin/sh

netfile="232interface.xml"
domfile="232interdomain.xml"
macaddr()
{
   for i in {1..232};do
    local random
    local MAC
    random=$(echo $RANDOM | md5sum | sed 's/\(..\)/&:/g'| cut -d\  -f1)
    MAC=52:54:00:${random:0:8}
    echo $MAC
    done
}

i=0
k=0
s=2
for macAdd in $(macaddr);do

echo "====$macAdd===="

        sed -i -e "s,#mac#,\"$macAdd\",g" $netfile
        sed -i -e "s,#slot#,slot=\"$s\",g" $netfile
        sed -i -e "s,#dev#,vnet$i,g" $netfile
        if [ $k -le 7 ]; then
         sed -i -e "s,#function#,function=\"$k\",g" $netfile
         cat $netfile
         cat $netfile >> $domfile
         sed -i -e "s,slot=\"$s\",#slot#,g" $netfile
        else
         k=0
         sed -i -e "s,#function#,function=\"$k\",g" $netfile
         cat $netfile
         cat $netfile >> $domfile
         sed -i -e "s,slot=\"$s\",#slot#,g" $netfile
         s=`expr $s + 1`
        fi
        sed -i -e "s,\"$macAdd\",#mac#,g" $netfile  
        sed -i -e "s,vnet$i,#dev#,g" $netfile
        sed -i -e "s,function=\"$k\",#function#,g" $netfile       
        k=`expr $k + 1`
        i=`expr $i + 1`
done
echo "</devices>" >> $domfile
echo "</domain>" >> $domfile

3. Define the domain with new created xml file.

#virsh define guest

4. Start the guest

#virsh start guest

login guest to check interface status
	
Expected Results:

step 3. There is no error occurs.

 Log in to guest to check the logs to make sure that attach/detach succeed

===============================================================================

Test 232 virtual interface with multifunction

Step 4. There is no error occurs. Domain can successfully start.

 All interface can be find in guest
Notes:
Comments:

		177496 	[Scalability] Bi-directional migration for looping 500 times through TLS connection --tunnelled -bug807996 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Finish case:

[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4

Run the case on both source host and target host to enable tls listen for libvirtd.

3. Prepare 10 guests on each side (20 total) with shared image on nfs named guest0 ~ guest9 on source and guest10 ~ guest19 on target which include 16 kinds of guests including

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp (32/64) 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

4. Dispatch ssh public key on both host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host and do it on both host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    No bug found

Actions:

1.Start all the guest on both sides

   on host1:

   for i in {0..9}; do virsh start mig$i; done

   on host2:

   for i in {10..19}; do virsh start mig$i; done

2.Prepare scprit

# cat bi-mig.sh
#!/bin/sh
for i in {1..500};do
echo "--------${i}-----------"
if [ $(($i % 2)) = 1 ]; then
    current=0
    while [ $current -le 9 ];
    do
      ssh root@host2 ip ./migrate-cmd.sh guest1$current
     
      ./migrate-cmd.sh guest$current

      sleep 2
      virsh list --all
      current=`expr $current + 1`
    done
  else 
    current2=0
    while [ $current2 -le 9 ];
    do
      ssh root@host2 ip ./migrate-cmd.sh guest$current2
     
      ./migrate-cmd.sh guest1$current2
     
      sleep 2
      virsh list --all
      current2=`expr $current2 + 1`
    done
  fi 
done

 

cat migrate-cmd.sh ( have this sh on both side with setting different ip of their target )

#!/bin/sh
virsh migrate --tunnelled --p2p --live $1 qemu+tls://10.66.83.198/system  &
wait
virsh migrate-setmaxdowntime $1 10000

3. Start migration for multiple guests between host1 and host2 bidrectional at the same time.

on host1, do:

sh bi-mig.sh 2>&1|tee mig.log

 
	
Expected Results:

1. The migration of all guests on both sides should be successful.

2. Log in all the guests to check that they all work fine include network

3. Check the disk type, cpu and mem on all the guest are the same as before migration
Notes:
Comments:

		177499 	[Scalability] Check performance when 'cpu' controller with many sub-groups created on large SMP systems (96 cpus)-bug623712 - large machine 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

1. Define 150 guests using following scripts

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 

2. The host should has 96 cpus.

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1.# time virsh list --all > /dev/null

real 0m0.642s

user 0m0.077s

sys 0m0.228s

 

2. # cd /cgroup/cpu/libvirt/qemu/

3. # for i in `seq 1 150` ; do mkdir kvm-$i ; done

4. # time virsh list --all > /dev/null

real 0m0.746s

user 0m0.162s

sys 0m0.027s  
	
Expected Results:

4. The time spending should be similar with step1 result, and no big difference between them
Notes:
Comments:

		177500 	[Scalability] Check running total number of 256 vm's disks-bug740899 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

It duplicated with case https://tcms.engineering.redhat.com/case/124698/?from_plan=5066

1.Check the aio limit .If  the /proc/sys/fs/aio-max-nr is 65536 , then then there is a regression for bug  740899

#cat /proc/sys/fs/aio-max-nr
1048576

2. Prepare 12 guests.

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    No bug found

Actions:

1. Attach 11 guests with 23 disks and 1 guest with 3 disks. You may can using the following scpript.

#cat attach.sh

#!/bin/sh

for i in {a..w}

do
qemu-img create /var/lib/libvirt/images/disk-$i.img 10M
virsh attach-disk $1 /var/lib/libvirt/images/disk-$i.img vd$i
sleep 1
done

# sh attach.sh <guest>

2. Start 12 guests with total 256 disks  and aio=native
	
Expected Results:

2. All the guest can started successfully
Notes:
Comments:

		177501 	[Scalability] Check the libvirtd status when qemu process are unresponsive-bug692663 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. change on /etc/libvirt/libvirtd.conf

max_clients = max_workers + 1 (at least)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability
    libvirtd

bug:

    No bug found

Actions:

1. Start 20 guest

#for i in {1..20}; do virsh start guest$i ; done

2. STGSTOP to all the qemu processes

# for i in `ps aux | grep qemu | grep -v grep | awk '{print $2}'`; do kill -19 $i;done

3. sh memstat.sh

cat memstat.sh

#!/bin/bash for i in {1..20} do virsh dommemstat foo_$i & done

4. do virsh list 

# virsh list
	
Expected Results:

4. virsh list works fine. It don't should hang.
Notes:
Comments:

		177502 	[Scalability] Check virDomainCreateXML status after start multi guests with vdsm-bug691514 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Install vdsm . refer to the following wiki.

https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/Install%20Linux%20version%20of%20RHEVM
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    No bug found

Actions:

1. Save the belows script as 691514.sh, and execute in command line.

# ./691514.sh

------------------
#!/bin/bash

times=20

macaddr()
{
    local random
    local MAC

    random=$(echo $RANDOM | md5sum | sed 's/\(..\)/&:/g'| cut -d\  -f1)
    MAC=52:54:00:${random:0:8}

    echo $MAC
}

for i in $(seq 1 $times)
do
    vdsClient -s 0 create /dev/null vmId=$(uuidgen) vmName=vm${i} memSize=256 macAddr=$(macaddr) bridge=rhevm nicModel=pv display=vnc
done
------------------


2. Open another console, execute the following command.

# vdsClient -s 0 list table
cde0d6f7-00ce-4ac6-9125-3ae82522ddc9   7771  vm19                 Up                                       
388b6a04-f12a-4cd0-ac4d-f22d7948c550   6735  vm6                  Up                                       
176f8a0c-ef67-4e43-83cd-e53de14a8b6b   7538  vm16                 Up                                       
f5fba8ac-481e-46fe-9d20-670ba43b672f   6913  vm8                  Up                                       
e1b02f19-00d6-48fa-b8d2-d32782b72e2a   7619  vm17                 Up                                       
65cc45ff-829a-4a56-8daf-b6e663b6fff1   6998  vm9                  Up                                       
6fe3c8be-2029-4b29-949c-2f24fdc72718   6640  vm5                  Up                                       
19016c55-412c-463c-8c9e-133437724daf   7149  vm11                 Up                                       
b6239a26-1c12-4c93-a3f7-68d0f6f14bb6   7074  vm10                 Up                                       
01f17fc3-e79b-46d5-b2b9-1d206b9f97c9   7460  vm15                 Up                                       
a54b3d24-4e65-4866-a4c7-62c810bd4088   7380  vm14                 Up                                       
2b87831e-10b6-4720-9701-e0c1f956bfa4   7302  vm13                 Up                                       
b808ffa7-0e89-4341-9a3a-18836dda290c   7848  vm20                 Up                                       
4ae12ea7-8b71-47d7-be0c-69c547d69ca6   6244  vm2                  Up                                       
b01cb46c-3b05-4e46-acfc-0013974a9a8b   6819  vm7                  Up                                       
57a80086-e9d7-468d-9956-ce301a7be688   7697  vm18                 Up                                       
40e15f57-62ff-49b3-90a1-aa83b801ad87   6378  vm3                  Up                                       
80e5a82e-fb09-4c6e-92be-2f428d3d1094   6109  vm1                  Up                                       
4fb68774-afee-4809-a0c2-b3a67bd30d95   7227  vm12                 Up                                       
77e91f40-1165-4a57-95cf-1b78449b5a2c   6513  vm4                  Up

	
Expected Results:

Before running the script, there is no vm running in the vdsm host
After execute script, wait the status to change from WaitForLunch to Up
Notes:
Comments:

		177503 	[Scalability] Concurrent migrate 514 guests through TLS connection using live,undefinesource,persistent with ipv6 - large machine 	weizhan 	weizhan 	Manual 		Function 	P1 	None 	Edit
Setup:
1. Finish the following case:

[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv6

<https://tcms.engineering.redhat.com/case/124653/?from_plan=5066>

2. Prepare the following os image

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

2. Clone 514 guests using following scripts and above image

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 3. A pair of large machines which have more than 512G mem for each
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    846013 - From Run 51862

Actions:

1.Start 514 domain on source host

# for i in {1..514};do virsh start  guest$i;done

 

2. Prepare  scripte. 

#cat migcon514.sh

#!/bin/sh
for i in {0..9} ; do
       let j=i*50+1
       k=`expr $j + 50`
       while [ $j -le $k ]
       do
       virsh migrate --live  --undefinesource --persistent guest$j qemu+tls://[target host ipv6 address]/system  &

       sleep  2     

       j=`expr $j + 1`
       done

      wait
done

for l in {502..514} ; do

virsh migrate --live  --undefinesource --persistent guest$l qemu+tls://[target host ipv6 address]/system  &      

sleep 2

done

3. Start migration

sh migcon514.sh 2>&1|tee mig.log

4. List  domains on source and target host.

on source host

# virsh list --all

on target host

# virsh list --all

Domains are running

# service libvirtd restart

# virsh list --all

Domains are running
	
Expected Results:

There is no error occurs.

Source host: domains are undefined

Target host: domains are running
Notes:
Comments:

		177504 	[Scalability] Create 1024 guests on different kinds of virtual network (isolated, route , NAT) with DHCP - large machine 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare the following os image

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

 

2.  Create NAT/ isolated / route network  .Please refer to the following cases

[Virtual Networks] Route virtual network <https://tcms.engineering.redhat.com/case/124969>

[Virtual Networks] Isolated virtual network <https://tcms.engineering.redhat.com/case/124959>

[Virtual Networks] NAT virtual network <https://tcms.engineering.redhat.com/case/124966>

 

3. Clone 1024 guests using following scripts

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 

3. Disable memballoon for each guest (depends on bug 626958)

# virsh dumpxml guest |grep memballoon

  <memballoon model='none'>

 

4. Change the max_processes and max_files to 65535

# vi /etc/libvirt/qemu.conf |grep max_processes

max_processes=65535

max_files=65535

 

5. Change the LIBVIRTD_NOFILES_LIMIT to 65535

# vi /etc/sysconfig/libvirtd

LIBVIRTD_NOFILES_LIMIT=65535

 

6. After starting parts of guests, you may need to release some memory on host

# echo 3 > /proc/sys/vm/drop_caches
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Start 1024 guests

# for i in {1..1024} ; do virsh start  guest$i ; done

2. List all guest.

#  virsh list --all

3. Check ip with nmap and  make sure it can  scan total 1024 ip addresses.

for example
# nmap 192.168.144.*

# nmap 10.0.0.*

# nmap 192.168.100.* 

......
	
Expected Results:

Nmap scan report for 192.168.144.2
Host is up (0.00045s latency).
Not shown: 999 filtered ports
PORT   STATE SERVICE
22/tcp open  ssh
MAC Address: 52:54:00:92:93:2F (QEMU Virtual NIC)

Nmap scan report for 192.168.144.4
Host is up (0.00043s latency).
Not shown: 999 filtered ports
PORT   STATE SERVICE
22/tcp open  ssh
MAC Address: 52:54:00:13:FD:92 (QEMU Virtual NIC)


.....

nmap scan 1024 ip addresses.
Notes:
Comments:

		177505 	[Scalability] Define domain with 232 usb disks with multifunction-bug807145, 808980 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare the guest xml  and get the xml from following.

wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/usb-all.xml

Note : There is the Bug 808980 [FJ6.2 Bug]: We cannot define 221 virtio devices on a KVM guest.


2. Create disk images with

# mkdir /var/lib/libvirt/images/usb-img

# for i in {a..i}; do for j in {a..z}; do qemu-img create /var/lib/libvirt/images/usb-img/usb-$i-$j.img 10M; done; done
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    No bug found

Actions:

1. Define guest using the xml

#virsh define usb-all.xml

2. Dump xml of the guest.

#virsh dumpxml guest

3. Start guest

# virsh start guest
	
Expected Results:

1. Successfully define guest. No  errors as follows

error: Failed to define domain from usb-all.xml
error: Unable to encode message payload
error: Reconnected to the hyperviso

2 .Successfully dump the xml of guest. No errors as follows

error: Unable to encode message payload
error: Reconnected to the hypervisor

 3. Guest can be started successfully, login to the guest, there are 232 disks.
Notes:
Comments:

		177506 	[Scalability] Do bidirection migration with rhevm - bug 722282, 638285 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare 3 machines, 2 big machines for RHEL/VDSM with at least 20G mem, another one is for RHEVM.

2. Follow the instructions to config RHEVM.

http://cleo.tlv.redhat.com/qumrawiki/Integration/RHEVM_RPM_HOWTO

https://docspace.corp.redhat.com/docs/DOC-68146

you can also access the exist rhevm environment with

10.66.4.232 , username: admin  passwd:redhat

3. Configure a NFS server with the options below.

# vi /etc/exports

/export   *(rw,no_root_squash,async)

# service nfs restart

# chown 36:36 /export

# ls -ld /export

drwxr-xr-x.   3 vdsm kvm   4096 Jul 20 04:43 /export
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    scalability

bug:

    No bug found

Actions:

1. Create Data Cernters, clusters, Hosts, Storage, Virtual Machines as

https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/Install%20Linux%20version%20of%20RHEVM
shows, add both 2 large machines to "Hosts"

2. Right click your new vm, choose "Make Template" and create a templete

3. Click "Pools" tab, click "New", create a new Pool with template you just created, and write "100" on "Number of VMs", and select vms runing on 1 machine with setting "Host".

Click "Pools" tab, click "New", create another new Pool with template you just created, and write "100" on "Number of VMs", and select vms runing on the other machine with setting "Host".

4. Click the pool you just created, and choose "Virtual Machines" in the same tab on the bottom, choose all the vms with "shift+mouse" and click "Detach"

5. Click "Virtual Machines" tab, choose all vms you just created, right click "run" to start all the vms
6. Choose all vms, right click "Migrate" and select the "Select Host Automatically" option.

7. On the mchine source and destination, check libvirtd

# service libvirtd status
	
Expected Results:

Step 3:

Check that the guest generated on specific hosts which you configured.
Step 6:

all the vms can be migrated successfully, make sure that all the guests migrated from one machine to another with running status

Step 7:

libvirtd still running on both source and dest host without crash
Notes:
Comments:

		177507 	[Scalability] Do concurrent migration with rhevm - bug 722282, 638285 	weizhan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

1. Prepare 3 machines, 2 big machines for RHEL/VDSM with at least 20G mem, another one is for RHEVM.

2. Follow the instructions to config RHEVM.

http://cleo.tlv.redhat.com/qumrawiki/Integration/RHEVM_RPM_HOWTO

https://docspace.corp.redhat.com/docs/DOC-68146

you can also access the exist rhevm environment with

10.66.4.232 , username: admin  passwd:redhat

3. Configure a NFS server with the options below.

# vi /etc/exports

/export   *(rw,no_root_squash,async)

# service nfs restart

# chown 36:36 /export

# ls -ld /export

drwxr-xr-x.   3 vdsm kvm   4096 Jul 20 04:43 /export
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability
    migration
    regression

bug:

    No bug found

Actions:

1. Create Data Cernters, clusters, Hosts, Storage, Virtual Machines as

https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/Install%20Linux%20version%20of%20RHEVM
shows, add both 2 large machines to "Hosts"

login http://rhevm ip:8080/RHEVManager/WPFClient.xbap

2. Right click your new vm, choose "Make Template" and create a templete

3. Click "Pools" tab, click "New", create a new Pool with template you just created, and write "30" on "Number of VMs", also specific all the vms runing on 1 machine with setting "Host".

4. Click the pool you just created, and choose "Virtual Machines" in the same tab on the bottom, shoose all the vms with "shift+mouse" and click "Detach"

5. Click "Virtual Machines" tab, choose all vms you just created, right click "run" to start all the vms

6. After all vms started, right click "suspend" to pause all the vms

7. After all vms paused, right click "run" to resume all the vms

8. Loop step 6-7 for 2 times

9. On the machine which running vms, do

# lsof -p $(pidof libvirtd)

10. Choose all vms, right click "Migrate"

11. On the mchine source and destination, check libvirtd

# service libvirtd status
	
Expected Results:

Step 9:

check that there is no items like

libvirtd 6632 root   32r   BLK             253,23 0x90000000     143969  /dev/dm-23

Step 10:

all the vms can be migrated successfully

Step 11:

libvirtd still running on both source and dest host without crash
Notes:
Comments:

		177508 	[Scalability] Do libvirt statistics command to fetch the dom/blk/mem/list info about 1024 guests-bug617286,bug687907 - large machine 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

1. Clone 1024 guests using following scripts

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 

2. Disable memballoon for each guest (depends on bug 623096)

# virsh dumpxml guest |grep memballoon

  <memballoon model='none'>

 

3. Change the max_processes and max_files to 65535

# vi /etc/libvirt/qemu.conf |grep max_processes

max_processes=65535

max_files=65535

 

4. Change the LIBVIRTD_NOFILES_LIMIT to 65535

# vi /etc/sysconfig/libvirtd

LIBVIRTD_NOFILES_LIMIT=65535

 

5. After starting parts of guests, you may need to release some memory on host

# echo 3 > /proc/sys/vm/drop_caches
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Start 1024 guests

# for i in {1..1024} ; do virsh start  guest$i ; done

2. List all guest.

#  virsh list --all

3. Run the following command to fetch dom/blk/mem/ info

# for i in {1..1024} ; do virsh dominfo guest$i ; sleep 2 ; done

# for i in {1..1024} ; do virsh domblkinfo guest$i  /nfsdata/guest$i ; sleep 2 ; done


4. List all guest.

#  virsh list --all

Note: Pay attention to the performance, those operation should not take too much times.

Bug 617286 - [libvirt] virsh list on host running 70 guests takes 2 minutes to return with full list

 





	
Expected Results:

Step 3. The domain info and  domain block info all display fine.

step 4. List all domains.
Notes:
Comments:

		177509 	[Scalability] Kill SIGKILL/SIGSEGV qemu processes 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    No bug found

Actions:

1. Start 20 guests
# for i in {1..20}  ; do virsh start guest$i ; done

2. kill -9 to all qemu processes

# ps -ef |grep guest$i

qemu     10843     1  2 May30 ?        01:23:03 /usr/libexec/qemu-kvm -S -M rhel6.3.0

# kill -9 10843

3. Perform following command to check libvirtd status

# service libvirtd status

# for i in {1..20}  ; do virsh start guest$i ; done

# virsh list --all

4. kill - SIGSEGV to all qemu processes

# kill -SIGSEGV  PID


5.Perform following command to check libvirtd status

# service libvirtd status

# virsh list --all

 



	
Expected Results:

Step 3 

libvirtd is running

The state of the domains is shutoff.

 

Step5

libvirtd is running

The state of the domains is shutoff. (may need to wait some time)

 virsh list --all could list all of the guest immidietely

 
Notes:
Comments:

		177512 	[Scalability] Migrate 514 guests through TCP connection --tunnelled with ipv4 - large machine 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

Prepare

1.Following case:

[Remote access] Connect to the hypervisor on host using TCP connection with SASL via ipv6 (Do not need to add sasl for this test, just change to none)

<https://tcms.engineering.redhat.com/case/124650/?from_plan=5066>

 

Run the case on both source host and target host to enable tcp listen for libvirtd.

 

2. Define 514 guests using following scripts

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 

3. A pair of large machines which have more than 512G mem for each

 

 

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    846013 - From Run 51862

Actions:

1. Start 514 domain on source host with diferent os and configuration including

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

# for i in {1..514};do virsh start  guest$i;done

2. Start migration

#for i in {1..514};do time virsh migrate --live guest$i qemu+tcp://[target host ipv6 address]/system --verbose --p2p --tunnelled;done

Note Pay attention to the performance of migration.

Bug 807910 - Tunnelled migration speed is much slower than without it

 

1. # time virsh migrate --live rhel5u8 qemu+tcp://10.66.104.94/system --verbose --p2p --tunnelled

2. # time virsh migrate --live rhel5u8 qemu+tcp://10.66.104.94/system --verbose

Compare the speed . they  should be close.


 

 
	
Expected Results:

2. Migration should be fininshed successfully.

Output

.....

Migration: [100 %]

real    0m4.656s
user    0m0.011s
sys    0m0.009s
Migration: [100 %]

real    0m5.483s
user    0m0.010s
sys    0m0.009s
Migration: [100 %]

real    0m7.480s
user    0m0.008s
sys    0m0.011s
Migration: [100 %]

real    0m5.448s
user    0m0.011s
sys    0m0.008s

 

No the following error occurs

Migration: [ 43 %]error: internal error received hangup / error event on socket

 

The migration speed should be the similar with non-tunnelled migration
Notes:
Comments:

		177513 	[Scalability] Migrate 514 guests through TLS connection --live --persistent with ipv4 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:
1. Finish the following case:

[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124654/?from_plan=5066>

1. Prepare the following os image

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

2. Clone 1024 guests using following scripts and above image

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 3. A pair of large machines which have more than 512G mem for each
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    migration

bug:

    No bug found

Actions:

1.Start 514 domain on source host

# for i in {1..514};do virsh start  guest$i;done

 

2. Start migration

#for i in {1..514};do  virsh migrate --live  --persistent guest$i qemu+tls://[target host ipv4 address]/system ;done

 

3. Wait util step 2 finished, then check domain status both on source and target host

    # virsh list --all

 

4. On target host, destroy domain

    # for i in {1..514};do virsh destroy  guest$i;done

 

5. On target host, check if domain is defined.

     # virsh list --all

 

6. On target host. start domain

     # for i in {1..514};do virsh start  guest$i;done

 

 
	
Expected Results:

step 3:

       Source host: domain are shutoff

       Target host: domain are running

step 5:

       Domains are defined.

step 6:

       Domains can be started successfully.
Notes:
Comments:

		177514 	[Scalability] Migrate 514 guests through TLS connection --live --undefinesource with ipv4 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:
1. Finish the following case:

[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124654/?from_plan=5066>

2. Prepare the following os image

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

2. Clone 1024 guests using following scripts and above image

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 3. A pair of large machines which have more than 512G mem for each


 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability
    migration

bug:

    No bug found

Actions:

1.Start 514 domain on source host

# for i in {1..514};do virsh start  guest$i;done

 

2. Start migration

#for i in {1..514};do  virsh migrate --live  --undefinesource guest$i qemu+tls://[target host ipv4 address]/system ;done

 

3. Wait util step 2 finished, then check domain status both on source and target host

On source host, check if domains are undefined.

# virsh list --all

 

4. On target host, check if domains are started.

# virsh list --all

 

 

 

 

 
	
Expected Results:

step 3:

       All domains are undefined.

step 4:

       Domains can be started successfully.
Notes:
Comments:

		177525 	[Scalability] qemu-kvm process status monitoring kill -SIGSEGV/19/9 qemu pid 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability

bug:

    No bug found

Actions:

1. Run more than 20 vms

2. Get the qemu process id. 
#ps -aef | grep guest1

2. kill -SIGSTOP to 5 qemu processes 
# kill -SIGSTOP guest1 

3. Perform following command to check libvirtd status
# service libvirtd status
# virsh list --all 

4.# kill -SIGSEGV  guest2

5.# kill -9  guest3




	
Expected Results:

3.1 libvirtd is running

3.2 virsh list --all could list all of the guest immidietely

The guest still remain the status(running) before kill.

4. make sure libvirtd could recognize guest.

5. The guest be destroyed.
Notes:
Comments:

		177526 	[Scalability] Reloading 1024 running guests for looping 100 times - large machine 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

1. Clone 1024 guests using following scripts

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 Refer to

https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/scalability%20and%20longevity%20test%20document

2. Disable memballoon for each guest (depends on bug 623096)

# virsh dumpxml guest |grep memballoon

  <memballoon model='none'>

 

3. Change the max_processes and max_files to 65535

# vi /etc/libvirt/qemu.conf |grep max_processes

max_processes=65535

max_files=65535

4.# service cgconfig stop 

5. Change the LIBVIRTD_NOFILES_LIMIT to 65535

# vi /etc/sysconfig/libvirtd

LIBVIRTD_NOFILES_LIMIT=65535

 

6. After starting parts of guests, you may need to release some memory on host

# echo 3 > /proc/sys/vm/drop_caches
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Start 1024 guests

# for i in {1..1024} ; do virsh start  guest$i ; done

2. List all guest.

#  virsh list --all

3. 

# for i in {1..100} ; do service libvirtd restart ; sleep 8 ; done

4. List all domains.

#  virsh list --all
	
Expected Results:

step 4.  All domains are running.
Notes:
Comments:

		177527 	[Scalability] Restart vdsmd service after start 200 guests with rhevm-bug723811 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare 3 machines, 2 big machines for RHEL/VDSM with at least 20G mem, another one is for RHEVM.

2. Follow the instructions to config RHEVM.

http://cleo.tlv.redhat.com/qumrawiki/Integration/RHEVM_RPM_HOWTO

https://docspace.corp.redhat.com/docs/DOC-68146

you can also access the exist rhevm environment with

10.66.4.232 , username: admin  passwd:redhat

3. Configure a NFS server with the options below.

# vi /etc/exports

/export   *(rw,no_root_squash,async)

# service nfs restart

# chown 36:36 /export

# ls -ld /export

drwxr-xr-x.   3 vdsm kvm   4096 Jul 20 04:43 /export

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Create Data Cernters, clusters, Hosts, Storage, Virtual Machines as

https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/Install%20Linux%20version%20of%20RHEVM
shows, add both 2 large machines to "Hosts"

2. Right click your new vm, choose "Make Template" and create a templete

3. Click "Pools" tab, click "New", create a new Pool with template you just created, and write "200" on "Number of VMs", also specific all the vms runing on 1 machine with setting "Host".

4. Click the pool you just created, and choose "Virtual Machines" in the same tab on the bottom, shoose all the vms with "shift+mouse" and click "Detach"

5. Click "Virtual Machines" tab, choose all vms you just created, right click "run" to start all the vms

6. After 200 guests started, restart vdsm service.

#  for i in {1..10} ; do service vdsmd restart ; sleep 10 ; done

7. Check libvirtd status

# service libvirtd status

libvirtd (pid 66950) is running...


	
Expected Results:
Notes:
Comments:

		177529 	[Scalability] Start a guest with host free memory size-bug825133 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare one big machine without large swap .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Get host free mem

# free
                            total       used       free     shared    buffers     cached
Mem:     529253468   13144480  516108988          0     189648    2430244
-/+ buffers/cache:   10524588  518728880
Swap:      1048568          0    1048568

 

2. Edit guest mem add in domain xml in <memory>XXX</memory> and <currentMemory>XXX<currentMemory>

# virsh edit guest

<domain type='kvm'>
  <name>rhel6.3</name>
  <uuid>a48b8aec-e7cd-2d8b-8e38-621621c4211d</uuid>
  <memory unit='KiB'>516108988</memory>
  <currentMemory unit='KiB'>516108988</currentMemory>
  <vcpu placement='auto'>4</vcpu>
  <numatune>
    <memory mode='strict' placement='auto'/>
  </numatune>
  <os>
 ...

</domain>

 

3. Start the guest

# virsh start guest
	
Expected Results:

Step 3 . guest can be started successfully.
Notes:
Comments:

		177530 	[Scalability] Start/Destroy 1024 guests including 16 kinds of os - bug 769500, 769503,806752 - large machine 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

1. Prepare the following os image

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

 

2. Clone 1024 guests using following scripts

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 

3. Disable memballoon for each guest (depends on bug 623096)

# virsh dumpxml guest |grep memballoon

  <memballoon model='none'>

 

4. Change the max_processes and max_files to 65535

# vi /etc/libvirt/qemu.conf |grep max_processes

max_processes=65535

max_files=65535

 

5. Change the LIBVIRTD_NOFILES_LIMIT to 65535

# vi /etc/sysconfig/libvirtd

LIBVIRTD_NOFILES_LIMIT=65535

 

6. After starting parts of guests, you may need to release some memory on host

# echo 3 > /proc/sys/vm/drop_caches

 

7. For fully started all guest, you may need to start some of them and then sleep some time and start again
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Start 1024 guests

# for i in {1..1024} ; do virsh start  guest$i ; done

2. List all guest.

#  virsh list --all

# virsh list |wc -l

3. check domain id list using python api

# python

Python 2.6.6 (r266:84292, Sep 12 2011, 14:03:14)

[GCC 4.4.5 20110214 (Red Hat 4.4.5-6)] on linux2

Type "help", "copyright", "credits" or "license" for more information.

>>> import libvirt

>>> conn = libvirt.open(None)

>>> id = conn.listDomainsID() 

>>> len(id)

4. Destroy 1024 guests

# for i in {1..1024} ; do virsh destroy guest$i ; done

5.  List all guest

#  virsh list --all

virsh list --all has inactive domain number limit, there is  a bug 806752


	
Expected Results:

1.  There is no any error occurs.

2.  Can list  all  guests with running state.

the count of line should be 1027, include head and tail space

3. The result should be 1024

5.  Can list all guests with shutoff state.
Notes:
Comments:

		177531 	[Scalability] Start/Destroy 1024 guests with empty image 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

Similar with 172096 [Scalability] Start/Destroy 1024 guests including 16 kinds of os, so disable it

1. Prepare one emply image

 

2. Define 1024 guests using following scripts

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 

3. Disable memballoon for each guest (depends on bug 623096)

# virsh dumpxml guest |grep memballoon

  <memballoon model='none'>

 

4. Change the max_processes and max_files to 65535

# vi /etc/libvirt/qemu.conf |grep max_processes

max_processes=65535

max_files=65535

 

5. Change the LIBVIRTD_NOFILES_LIMIT to 65535

# vi /etc/sysconfig/libvirtd

LIBVIRTD_NOFILES_LIMIT=65535

 

6. After starting parts of guests, you may need to release some memory on host

# echo 3 > /proc/sys/vm/drop_caches

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Start 1024 guests

# for i in {1..1024} ; do virsh start  guest$i ; done

2. List all guest.

#  virsh list --all

3. Destroy 1024 guests

# for i in {1..1024} ; do virsh destroy guest$i ; done

4.  List all guest

#  virsh list --all

virsh list --all has inactive domain number limit, there is  a bug 806752
	
Expected Results:

1.  There is no any error occurs.

 

2. There is no any error coccurs.
Notes:
Comments:

		177532 	[Scalability] Start/Destroy one guest with 1TB mem/160 vCPUs for looping 10 times - bug 748946 - large machine 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

1.Prepare one windows xp guest  - 1TB mem / 160 vCPUs

 

2.The host needs 160 cores CPU/1TB mem.

 

3. Give more swap  to the host if your host only have 512G/1T memory, else your guest start up will be very slow

# mkswap /dev/sda[n]

# swapon /dev/sda[n]

check the swap info

# swapon -s

 

4. For sometimes the ksmtunned process may occupy the large cpu usage, just stop this service

# service ksmtunned stop



	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Start/Destroy the guest for looping 10 times .

Start the guest.

#virsh start  guest

 

Destroy the guest

# virsh destroy guest

2. Login guest and do some operations.
	
Expected Results:

1. There is no any error occurs.

 

2. The guest  works fine
Notes:
Comments:

		177533 	[Scalability] Start/Destroy one guest with the minimum memory for looping 500 times 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:

Prepare one windows xp guest  - 128MB mem/1 vcpu
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1.  Prepare the following script.

cat  test.sh

#!/bin/sh

for i in {1..500};do
  echo "------${i}-------"
  virsh start winxp-x86_64
  sleep 8
  virsh destroy winxp-x86_64
done

2. sh test.sh

3. Log in the guest and do some operations.
	
Expected Results:

2. There is no any error occurs.

 3. The guest  works fine
Notes:
Comments:

		177534 	[Scalability] Start/Stop 100 guests for 10 times in rhevm, check the libvirtd status-bug670848 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare 3 machines, 2 big machines for RHEL/VDSM with at least 20G mem, another one is for RHEVM.

2. Follow the instructions to config RHEVM.

http://cleo.tlv.redhat.com/qumrawiki/Integration/RHEVM_RPM_HOWTO

https://docspace.corp.redhat.com/docs/DOC-68146

you can also access the exist rhevm environment with

10.66.4.232 , username: admin  passwd:redhat

3. Configure a NFS server with the options below.

# vi /etc/exports

/export   *(rw,no_root_squash,async)

# service nfs restart

# chown 36:36 /export

# ls -ld /export

drwxr-xr-x.   3 vdsm kvm   4096 Jul 20 04:43 /export
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Create Data Cernters, clusters, Hosts, Storage, Virtual Machines as

https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/Install%20Linux%20version%20of%20RHEVM
shows, add both 2 large machines to "Hosts"

2. Right click your new vm, choose "Make Template" and create a templete

3. Click "Pools" tab, click "New", create a new Pool with template you just created, and write "100" on "Number of VMs", also specific all the vms runing on 1 machine with setting "Host".

4. Click the pool you just created, and choose "Virtual Machines" in the same tab on the bottom, shoose all the vms with "shift+mouse" and click "Detach"

5. Click "Virtual Machines" tab, choose all vms you just created, right click "run" to start all the vms

6. After 200 guests started,  choose all vms you just created, right click "stop" to stop all the vms

7. Repeat step5 ~ step6 for 10 times.

8. Check libvirtd status

# service libvirtd status

libvirtd (pid 66950) is running...


 
	
Expected Results:
Notes:
Comments:

		177536 	[Scalability]Concurrent migrate 514 guests through TLS connection using live,undefinesource,persistent with ipv4 - large machine 	honzhang 	None 	Manual 		Stress 	P1 	None 	Edit
Setup:
1. Finish the following case:

[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124654/?from_plan=5066>

2. Prepare the following os image

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

2. Clone 1024 guests using following scripts and above image

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

3. A pair of large machines which have more than 512G mem for each

4.

a. Change the max_client and max_works number to 4*{guests num}+1

# cat /etc/libvirt/libvirtd.conf |grep -E "max_clients|max_workers"

max_clients = 2057

max_workers=2057

b. The current guests number is about 60-70 guests at one time(for my test), but not all the guests

 

Note: Limit the number of guests in concurrent migration below 60 ,Or the following error may occur.
error: internal error process exited while connecting to monitor: char device redirected to /dev/pts/169
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1.Start 514 domain on source host

# for i in {1..514};do virsh start  guest$i;done

 

2. Prepare  scripte. 

#cat migcon514.sh

#!/bin/sh
for i in {0..9} ; do
       let j=i*50+1
       k=`expr $j + 50`
       while [ $j -le $k ]
       do
       virsh migrate --live  --undefinesource --persistent  guest$j qemu+tls://[target host ipv4 address]/system  &

       sleep  2     

       j=`expr $j + 1`
       done

      sleep   8
done

for l in {502..514} ; do

virsh migrate --live  --undefinesource --persistent  guest$l qemu+tls://[target host ipv4 address]/system  &      

sleep 2

done

3. Start migration

sh migcon514.sh 2>&1|tee mig.log

4. List  domains on source and target host.

on source host

# virsh list --all

on target host

# virsh list --all

Domains are running

# service libvirtd restart

# virsh list --all

Domains are running

 
 

 
	
Expected Results:

There is no error occurs.

Source host: domains are undefined

Target host: domains are running

 
Notes:
Comments:

		177546 	[snapshot] create snapshot for a paused guest at the second time will take long time BZ#771626 	whuang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

install a guest

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

Steps to Reproduce:
1. Suspend a running guest
# virsh suspend test
Domain test suspended

# virsh list
 Id Name                 State
----------------------------------
  7 test                 paused


2. Create snapshot for the guest
# time virsh snapshot-create-as test s1
Domain snapshot s1 created

real        0m9.139s
user        0m0.005s
sys        0m0.005s
3. Create snapshot for the guest again
# time virsh snapshot-create-as test s2
Domain snapshot s2 created

real        4m59.982s
user        0m0.006s
sys        0m0.008s


4. At step3, if cancel the snapshot-create job, the snapshot file will be
created by libvirt, and the guest can't resume back
  4.1 during creating the snapshot guest process, type Ctrl+c to cancel it
  4.2 check the new create snapshot file 
# virsh snapshot-list test
 Name                 Creation Time             State
------------------------------------------------------------
 s1                   2012-01-04 15:24:49 +0800 paused
 s2                   2012-01-04 15:25:20 +0800 paused

# qemu-img info /var/lib/libvirt/images/test.qcow2 
image: /var/lib/libvirt/images/test.qcow2
file format: qcow2
virtual size: 6.0G (6442450944 bytes)
disk size: 2.8G
cluster_size: 65536
Snapshot list:
ID        TAG                 VM SIZE                DATE       VM CLOCK
1         s1                     349M 2012-01-04 15:36:53   00:04:57.177

here, s2 didn't create by qemu,

  4.3 try to resume the guest back # virsh resume test error: Failed to resume domain test
error: Timed out during operation: cannot acquire state change lock

# virsh list
 Id Name                 State
----------------------------------
  7 test                 paused

	
Expected Results:

snaphost does not  take so long time

guest can resume
Notes:
https://bugzilla.redhat.com/show_bug.cgi?id=771626 closed NOTABUG
Comments:

		177553 	[snapshot] Expose 'virDomainSnapshotListChildrenNames' API in python binding - bug 766553 	gsun 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1. define a guest with qcow2 image

# qemu-img create -f qcow2 /var/lib/libvirt/images/disk.img 100M

# cat > /root/demo.xml <<EOF
<domain type='qemu'>
  <name>demo</name>
  <memory>219200</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
    <boot dev='hd'/>
  </os>
  <devices>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/disk.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes' listen='0.0.0.0'/>
  </devices>
</domain>
EOF

# virsh define /root/demo.xml

 



2. create a snapshot for the guest

# virsh snapshot-create-as demo


3. run interact python to try libvirt python API




# python
Python 2.6.6 (r266:84292, Sep 12 2011, 14:03:14) 
[GCC 4.4.5 20110214 (Red Hat 4.4.5-6)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import libvirt 
>>> con = libvirt.open(None) 
>>> dom = con.lookupByName("demo") 
>>> snap_name = dom.snapshotListNames(0)[0] 
>>> snap = dom.snapshotLookupByName(snap_name, 0) 
>>> dir(snap)
['__del__', '__doc__', '__init__', '__module__', '_dom', '_o', 'delete',
'domain', 'getConnect', 'getDomain', 'getName', 'getParent', 'getXMLDesc',
'listChildrenNames', 'numChildren']
>>> snap.getName()
'1323685119'
>>> snap.getDomain().name()
'demo'
>>> snap.listChildrenNames(0)

	
Expected Results:

1.
Formatting '/var/lib/libvirt/images/disk.img', fmt=qcow2 size=104857600
encryption=off cluster_size=65536

Domain demo defined from /root/demo.xml

2.

Domain snapshot 1323685119 created


3.

Actual results:

>>> snap.listChildrenNames(0)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/lib64/python2.6/site-packages/libvirt.py", line 3733, in
listChildrenNames
    ret = libvirtmod.virDomainSnapshotListChildrenNames(self._o, flags)
AttributeError: 'module' object has no attribute
'virDomainSnapshotListChildrenNames'

Expected results: 
it should work

 

Notes:
Comments:

		177555 	[snapshot] libvirt should check for ABI compatibility of snapshots --bug 738676 	nzhang 	None 	Manual (Autoproposed) 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    snapshot

bug:

    No bug found

Actions:

[root@localhost ~]# virsh start foo
Domain foo started

[root@localhost ~]# virsh snapshot-create-as foo snap
Domain snapshot snap created

[root@localhost ~]# virsh snapshot-list foo
 Name                 Creation Time             State
------------------------------------------------------------
 snap                 2010-02-08 20:58:16 -0500 running

[root@localhost ~]# virsh snapshot-dumpxml foo snap
<domainsnapshot>
  <name>snap</name>
  <state>running</state>
  <creationTime>1265680696</creationTime>
<domain type='kvm'>
  <name>foo</name>
  <uuid>5035868c-6f23-63d1-d05c-1fc5ff963134</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/foo.img'/>
      <target dev='vda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='virtio-serial' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </controller>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:11:df:7f'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <channel type='pty'>
      <target type='virtio' name='org.linux-kvm.port.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <input type='tablet' bus='usb'/>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04'
function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02'
function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06'
function='0x0'/>
    </memballoon>
  </devices>
</domain>
</domainsnapshot>

[root@localhost ~]# virsh destroy foo
Domain foo destroyed

[root@localhost ~]# virsh setmaxmem foo 1048576

[root@localhost ~]# virsh start foo
Domain foo started

[root@localhost ~]# virsh snapshot-revert foo snap
error: unsupported configuration: Target domain max memory 524288 does not
match source 1048576

	
Expected Results:

The error is normal result, because the incompatible ABI was captured.
Notes:
Comments:

		177556 	[snapshot] libvirt should not leave stale snapshot metadata behind after domain disappears --bug 735457 	nzhang 	None 	Manual (Autoproposed) 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    snapshot

bug:

    No bug found

Actions:

# qemu-img create -f qcow2 /var/lib/libvirt/images/foo.img 10M
Formatting '/var/lib/libvirt/images/foo.img', fmt=qcow2 size=10485760
encryption=off cluster_size=65536 

# qemu-img info /var/lib/libvirt/images/foo.img 
image: /var/lib/libvirt/images/foo.img
file format: qcow2
virtual size: 10M (10485760 bytes)
disk size: 140K
cluster_size: 65536

# cat > /root/demo.xml <<EOF
<domain type='qemu'>
  <name>demo</name>
  <memory>219200</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <devices>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/foo.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes' listen='0.0.0.0'/>
  </devices>
</domain>
EOF

# virsh define /root/demo.xml
Domain demo defined from /root/demo.xml

# virsh snapshot-list demo
 Name                 Creation Time             State
------------------------------------------------------------

# cat > /root/snap.xml <<EOF
<domainsnapshot>
  <state>shutoff</state>
</domainsnapshot>
EOF

# virsh snapshot-create demo /root/snap.xml 
Domain snapshot 1318054922 created from '/root/snap.xml'

# virsh snapshot-list demo
 Name                 Creation Time             State
------------------------------------------------------------
 1318054922           2011-10-08 14:22:02 +0800 shutoff

# virsh snapshot-list demo --parent
 Name                 Creation Time             State           Parent
------------------------------------------------------------
 1318054922           2011-10-08 14:22:02 +0800 shutoff

# virsh snapshot-list demo --roots
 Name                 Creation Time             State
------------------------------------------------------------
 1318054922           2011-10-08 14:22:02 +0800 shutoff

# virsh snapshot-list demo --parent --roots
error: --parent and --roots are mutually exclusive

Notes, without typo issue for 'exclusive' words.

# ls /var/lib/libvirt/qemu/snapshot/demo/1318054922.xml 
/var/lib/libvirt/qemu/snapshot/demo/1318054922.xml

# virsh undefine demo
error: Failed to undefine domain demo
error: Requested operation is not valid: cannot delete inactive domain with 1
snapshots

Notes, this is a expected behaviour.

# ls /var/lib/libvirt/qemu/snapshot/demo/1318054922.xml 
/var/lib/libvirt/qemu/snapshot/demo/1318054922.xml

Notes, snapshot metadata still exists, this is a expected result.

# virsh undefine --snapshots-metadata demo
Domain demo has been undefined

# ls /var/lib/libvirt/qemu/snapshot/demo/1318054922.xml 
ls: cannot access /var/lib/libvirt/qemu/snapshot/demo/1318054922.xml: No such
file or directory

	
Expected Results:

Check each of steps in "Actions".
Notes:
Comments:

		177557 	[snapshot] libvirt should reject snapshots of autodestroy domains --bug 733806 	nzhang 	None 	Manual (Autoproposed) 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    snapshot

bug:

    No bug found

Actions:

[root@localhost ~]# virsh
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # create foo.xml --autodestroy
Domain foo created from foo.xml

virsh # suspend foo
Domain foo suspended

virsh # snapshot-create foo
error: Requested operation is not valid: domain is marked for auto destroy

virsh # save foo /tmp/foo
error: Failed to save domain foo to /tmp/foo
error: Requested operation is not valid: domain is marked for auto destroy

virsh # quit

	
Expected Results:

Check if the output is consistent with 'Actions' listed.
Notes:
Comments:

		177558 	[snapshot] libvirtd crash when executing "virsh snapshot-create-as" command sometimes --bug 737010 	nzhang 	None 	Manual (Autoproposed) 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    snapshot

bug:

    No bug found

Actions:

# for name in s1 s2 s3 s4; do virsh snapshot-create-as foo $name; done
Domain snapshot s1 created
Domain snapshot s2 created
Domain snapshot s3 created
Domain snapshot s4 created

# virsh snapshot-list foo
 Name                 Creation Time             State
------------------------------------------------------------
 s1                   2010-02-08 19:25:28 -0500 shutoff
 s2                   2010-02-08 19:25:34 -0500 shutoff
 s3                   2010-02-08 19:25:39 -0500 shutoff
 s4                   2010-02-08 19:25:45 -0500 shutoff

	
Expected Results:

No errors occurs like the following.

error: End of file while reading data: Input/output error

Notes:
Comments:

		177564 	[snapshot] virsh should have better snapshot support --bug 735495 	nzhang 	None 	Manual 		Function 	P2 	None 	Edit
Setup:

Continuously create 2 snapshots for exsiting domain.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    snapshot

bug:

    No bug found

Actions:

1. # virsh snapshot-current rhel6 --name
s4

2. # virsh snapshot-parent rhel6 s4
s3

3. # virsh qemu-monitor-command rhel6 --hmp info block
drive-virtio-disk0: type=hd removable=0 file=/var/lib/libvirt/images/rhel6.img
ro=0 drv=qcow2 encrypted=0

4. # virsh list
 Id Name                 State
----------------------------------
  1 rhel6                running

# virsh undefine rhel6
Domain rhel6 has been undefined

# virsh list
 Id Name                 State
----------------------------------
  1 rhel6                running

# virsh destroy rhel6
Domain rhel6 destroyed

# virsh list --all
 Id Name                 State
----------------------------------

	
Expected Results:

Check the outputs in "Actions".
Notes:
Comments:

		177565 	[snapshot] [RFE] support domain offline --disk-only snapshots Bug#817226 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Pls confirm this case uilt bug Bug 817226 Verified
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    upstream
    snapshot

bug:

    No bug found

Actions:

1)
# virsh domstate test
shut off

2)
# virsh snapshot-create test --atomic --disk-only
error: unsupported configuration: disk snapshots of inactive domains not
implemented yet


Actual results:
"disk snapshots of inactive domains not implemented yet"

	
Expected Results:

Expected results:
Support offline --disk-only snapshots.

virsh command will successfully 

Notes:
Comments:

		177566 	[snapshot]Check for guest agent presence when issuing command BZ 808527 	zhpeng 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:

Build case ENV like 
[snapshot]snapshot with quiesce option BZ 804210
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1. kill the GA in guest

2.

# virsh snapshot-create --disk-only --quiesce rhel6-q
error: internal error Guest agent not available for now

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 2     rhel6-q                        running

3. # virsh suspend rhel6-q
Domain rhel6-q suspended

4. # virsh snapshot-create --disk-only --quiesce rhel6-q

error: internal error Guest agent not available for now

5. # virsh list 
 Id    Name                           State
----------------------------------------------------
 2     rhel6-q                        paused

6. # virsh resume rhel6-q
Domain rhel6-q resumed

 
	
Expected Results:

Follow all step results
Notes:
Comments:

		177567 	[snapshot]Delete snapshot parent will crash libvirtd BZ#790744 	whuang 	whuang 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

have a  qcow2 image running guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

1. qemu-img create -f qcow2 /var/lib/libvirt/images/foo.qcow2 10M
2. virsh edit guest configuration and let guest disk source point to the image
file
3. # virsh snapshot-create-as foo snap
Domain snapshot snap created

# virsh snapshot-list foo
 Name                 Creation Time             State
------------------------------------------------------------
 snap                 2012-02-15 17:51:18 +0800 running

# virsh snapshot-create foo
Domain snapshot 1329299507 created

# virsh snapshot-create foo
Domain snapshot 1329299512 created

# virsh snapshot-list foo --parent
 Name                 Creation Time             State           Parent
------------------------------------------------------------
 1329299507           2012-02-15 17:51:47 +0800 running         snap
 1329299512           2012-02-15 17:51:52 +0800 running         1329299507
 snap                 2012-02-15 17:51:18 +0800 running

4. # virsh snapshot-delete --children foo snap

	
Expected Results:

Not  show  ERROR like this  and libvirt is still alive 
 # virsh snapshot-delete --children foo snap
error: Failed to delete snapshot snap
error: End of file while reading data: Input/output error

Notes:
Comments:

		177570 	[snapshot]libvirtd crashed with SIGSEGV in __strcmp_ssse3() BZ 808371 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1. # qemu-img create -f qcow2 /var/lib/libvirt/images/test.img 10M
Formatting '/var/lib/libvirt/images/test.img', fmt=qcow2 size=10485760
encryption=off cluster_size=65536

2. # cat > /tmp/test.xml <<EOF
<domain type='qemu'>
  <name>test</name>
  <memory>219200</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
    <boot dev='hd'/>
  </os>
  <devices>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/test.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <channel type='pty'>
      <target type='virtio'/>
    </channel>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes' listen='0.0.0.0'/>
  </devices>
</domain>
EOF

3. # virsh define /tmp/test.xml
Domain test defined from /tmp/test.xml

4. # virsh snapshot-list test
 Name                 Creation Time             State
------------------------------------------------------------

5. # virsh snapshot-create-as test hello
Domain snapshot hello created

6. # virsh snapshot-list test
 Name                 Creation Time             State
------------------------------------------------------------
 hello                2012-03-30 17:30:13 +0800 shutoff

7. # virsh snapshot-current test hello
Snapshot hello set as current

8. # /etc/init.d/libvirtd status
libvirtd (pid  26619) is running...

	
Expected Results:

Follow all the step results
Notes:
Comments:

		177571 	[snapshot]list snapshots with options BZ 807545 807555 842947 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

A qcow2 guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    842947 - From Run 51756

Actions:

1 , virsh # list 

 Id    Name                           State
----------------------------------------------------

2, virsh # snapshot-create-as aaa --name "111"
Domain snapshot 111 created
3, virsh # snapshot-create-as aaa --name "222"
Domain snapshot 222 created
4, virsh # snapshot-list aaa --parent 
 Name                 Creation Time             State           Parent
------------------------------------------------------------
 111                  2012-03-31 19:31:31 +0800 shutoff
 222                  2012-03-31 19:31:38 +0800 shutoff         111

5, virsh # snapshot-list aaa --roots --from "111"
error: --roots and --from are mutually exclusive

6,#  virsh snapshot-list aaa --parent --roots

error: --parent and --roots are mutually exclusive

# virsh snapshot-list foo --parent --tree

error: --parent and --tree are mutually exclusive

# virsh snapshot-list foo --roots --tree
error: --roots and --tree are mutually exclusive

Prepare 2 machines one with the old libvirt

7) perpare a guest with snapshot in old server (libvirt-0.9.10-21.el6_XX)
#rpm -q libvirt
libvirt-0.9.10-21.el6_3.5.x86_64

8) connect old server with new client
#rpm -q libvirt
libvirt-0.10.2-7.el6.x86_64


# virsh -c qemu+ssh://10.66.82.251/system
root@10.66.82.251's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # snapshot-list t
 Name                 Creation Time             State
------------------------------------------------------------
 s1                   2012-07-25 10:05:07 +0800 shutoff

virsh # snapshot-list t --from s1
error: End of file while reading data: : Input/output error

9) old server libvirtd NOT crashed
# /etc/init.d/libvirtd status
libvirtd (pid  8516) is running...


 

	
Expected Results:

step 5  :

virsh # snapshot-list aaa --roots --from "111"
error: --roots and --from are mutually exclusive

 
Step9:

libvirtd not crashed after BZ 842947 fixed

Notes:
Comments:

		177573 	[snapshot]Live snapshots must be consistent (fail or succeed for all the drives) BZ#782457 	whuang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

 

use qemu-kvm-rhev pkg 

latest version qemu-kvm not support live snaphost  

 

 

 

make sure libvirt support this command 

 

#virsh snapshot-create snapshot2 disk2.xml --disk-only --reuse-external

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

step 1 

virsh # start aaa
Domain aaa started

virsh # dumpxml aaa
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/tmp/test1.qcow2'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/tmp/test2.qcow2'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/tmp/test3.qcow2'/>
      <target dev='vdc' bus='virtio'/>
      <alias name='virtio-disk2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </disk>
...
virsh # snapshot-list aaa
 Name                 Creation Time             State
------------------------------------------------------------
# cat disk.xml

 <domainsnapshot>
   <disks>
     <disk name='vda' snapshot='external'>
       <driver type='qcow2'/>
       <source file='/mnt/snapshot-a.new'/>
     </disk>
     <disk name='vdb' snapshot='external'>
       <driver type='qcow2'/>
       <source file='/mnt/snapshot-b.new'/>
     </disk>
     <disk name='vdc' snapshot='external'>
       <driver type='qcow2'/>
       <source file='/mnt/snapshot-c.new'/>
     </disk>
   </disks>
 </domainsnapshot>

step 2 


# qemu-img create -f qcow2 /mnt/snapshot-a.new 1G
# qemu-img create -f qcow2 /mnt/snapshot-b.new 1G
# qemu-img create -f qcow2 /mnt/snapshot-c.new 1G
# chmod 000 /mnt/snapshot-b.new

step 3

virsh # snapshot-create aaa disk.xml --disk-only --reuse-external 
error: internal error unable to execute QEMU command 'transaction': Could not open '/mnt/snapshot-b.new'



virsh # dumpxml aaa
...
 <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/tmp/test1.qcow2'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/tmp/test2.qcow2'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/tmp/test3.qcow2'/>
      <target dev='vdc' bus='virtio'/>
      <alias name='virtio-disk2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </disk>
...
the xml looks good.

virsh # snapshot-list aaa
 Name                 Creation Time             State
------------------------------------------------------------

	
Expected Results:

step 3

it will report error

virsh # snapshot-create aaa disk.xml --disk-only --reuse-external 
error: internal error unable to execute QEMU command 'transaction': Could not open '/mnt/snapshot-b.new'

 
Notes:
Comments:

		177579 	[snapshot]snapshot with quiesce option BZ 804210 	zhpeng 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:

the guest need install qemu-kvm-guest-agent pkg
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1, prepare a guest with qcow2 and install qemu-guest-agent in guest

2, add channel

    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/f16x86_64.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
3, start guest and start qemu-guest-agent

# qemu-ga -d

4. if you take snapshot when guest is running, pls use trace to check the agent status

in guest:

# strace -p `pidof qemu-ga`

in host:

# virsh snapshot-create --disk-only --quiesce f16

Domain snapshot 1332225869 created

5.  suspend guest and try to make snapshot again

# virsh suspend f16

Domain f16 suspended

# virsh snapshot-create --disk-only --quiesce f16

error: internal error Guest agent not available for now

 
	
Expected Results:

step 4 : successed

 

 

 Step 5  will report error like :

error: internal error Guest agent not available for now

 
Notes:
Comments:

		177584 	[Snapshot]Use snapshot-edit to change snaphsot name - bug 744724 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1.# virsh snapshot-create rhel6

2.# virsh snapshot-list rhel6

3.# virsh snapshot-edit rhel6 1318280132
*Change
<domainsnapshot>
<name>1318280132</name>
...
to
<domainsnapshot>
<name>1318280130</name>
...
then save*
	
Expected Results:

1. # virsh snapshot-create rhel6
Domain snapshot 1318280132 created

2.# virsh snapshot-list rhel6
Name Creation Time State
------------------------------------------------------------
1318280132 2011-10-11 04:55:32 +0800 shutoff

3.

error: Must use --rename or --clone to change  1318280132 to 1318280130
Notes:
Comments:

		177585 	[snapshot]virsh heap corruption due to bad memmove BZ 819636 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

#valgrind virsh -c test:///default  snapshot-create-as test --print-xml --diskspec vda,file=a,,b

...
==29018== HEAP SUMMARY:
==29018==     in use at exit: 127,906 bytes in 1,362 blocks
==29018==   total heap usage: 6,496 allocs, 5,134 frees, 850,278 bytes allocated
==29018== 
==29018== LEAK SUMMARY:
==29018==    definitely lost: 0 bytes in 0 blocks
==29018==    indirectly lost: 0 bytes in 0 blocks
==29018==      possibly lost: 0 bytes in 0 blocks
==29018==    still reachable: 127,906 bytes in 1,362 blocks
==29018==         suppressed: 0 bytes in 0 blocks
==29018== Rerun with --leak-check=full to see details of leaked memory
==29018== 
==29018== For counts of detected and suppressed errors, rerun with: -v
==29018== Use --track-origins=yes to see where uninitialised values come from
==29018== ERROR SUMMARY: 45 errors from 10 contexts (suppressed: 8 from 6)

	
Expected Results:

Notice there is not  "invalid write of size 1" error

Notes:
Comments:

		177611 	[SR-IOV] Install guest through a attached VF - bug 781562 - only on 82576 	ajia 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

NOTE: this case can only be executed on 82576 card, for 82599 card, can not build rom file

1. Do  124766: [SR-IOV] Prepare: Enable VT-D first

2. For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

modprobe -r kvm_intel

modprobe -r kvm

modprobe kvm allow_unsafe_assigned_interrupts=1

modprobe kvm_intel
	
Breakdown:
	
Attachment:

    ipxe-808610ca.rom

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. List  pci device with SR-IOV function in the box

# lspci |grep 82576

2. Confirm the driver is built as a module

# lsmod |grep igb

3.  max_vfs value controls the number  of virtual network card
Valid Range:   0-7

# modprobe -r igb

# modprobe igb max_vfs=7

4. Relist the pci device relevant Intel 82576 network device

# lspci |grep 82576

5. Show the VF product ID and vendor ID

# lspci -n | grep 03:10.7

6. List availiable node devices on host

# virsh nodedev-list

7. Dump VF detail info

# virsh nodedev-dumpxml pci_0000_03_10_7

8. Add this VF xml info to VM config xml (kvm.xml)

...
          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='0x03' slot='0x10' function='0x07'/>
            </source>         
            <boot order='1'/>         
           <rom bar='on' file='/usr/share/seabios/ipxe-808610ca.rom'/>
          </hostdev>
...

The ipxe-808610ca.rom in the attachment and please put it in dir /usr/share/seabios/

And also remove following line in <os> </os> element.

  <boot dev='hd'/>

9. # virsh define kvm.xml

10. # virsh start kvm

11. Check the qemu command

# ps aux |grep qemu-kvm

12. Start the guest

13. Destroy the guest and change the xml with

...
          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='0x03' slot='0x10' function='0x07'/>
            </source>         
            <boot order='1'/>         
           <rom bar='off' file='/usr/share/seabios/ipxe-808610ca.rom'/>
          </hostdev>
...

be sure that the vm image has os in it

14. Start the guest

 

Of course, you may also create a ipxe-808610ca.rom according to the following link(see "Source code" and "Replace an existing PXE ROM"):

http://ipxe.org/download

Notes that, you must compile codes on SR-IOV host.
 
	
Expected Results:

Step 11

/usr/libexec/qemu-kvm .... -device pci-assign,host=28:10.7,id=vf-7,iommu=1,multifunction=on,romfile=/usr/share/seabios/ipxe-808610ca.rom,bootindex=1

Step 12

The guest can be started through ipxe,  seeing the pxe profile choosing interface is ok

Step 14

The guest just started from local disk, without ipxe start process
Notes:
Comments:

		177624 	[stable guest ABI] Do live migration from rhel6.x latest release version to rhel6.x newest version - bug 796063 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts, host 1 is rhel6.x latest release version, host 2  is newest rhel6.x version

2. Prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    stable guest ABI

bug:

    851395 - From Run 44420
    860573 - From Run 47432
    860573 - From Run 50664

Actions:

1. Prepare a guest locate on host 1 and start, check the qemu-kvm command

the disk xml part should be

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/mnt/kvm-rhel6u2-x86_64-new.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </disk>

...

2. Do live migration from host 1 to host 2

# virsh migrate --live guest qemu+ssh://{host 2 ip}/system

3. Check the qemu-kvm command on target, should have scsi=off

4. Login guest to check the guest status
	
Expected Results:

Step 1

 /usr/libexec/qemu-kvm -S -M rhel6.3.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name tt -uuid 727a861c-25e6-c2f2-544b-97d1bd1d9cc2 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/tt.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -drive file=/mnt/kvm-rhel6u2-x86_64-new.img,if=none,id=drive-virtio-disk0,format=raw,cache=none -device virtio-blk-pci,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=26,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:b9:4d:60,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -vnc 127.0.0.1:1 -vga cirrus -device intel-hda,id=sound0,bus=pci.0,addr=0x4 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

Step 2

Migration should succeed without error

Step 3

/usr/libexec/qemu-kvm -S -M rhel6.3.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name tt -uuid 727a861c-25e6-c2f2-544b-97d1bd1d9cc2 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/tt.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/mnt/kvm-rhel6u2-x86_64-new.img,if=none,id=drive-virtio-disk0,format=raw,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x6,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=24,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:b9:4d:60,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -vga cirrus -device intel-hda,id=sound0,bus=pci.0,addr=0x4 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -incoming tcp:0.0.0.0:49155 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

Step 4

Guest should work well
Notes:
Comments:

		177626 	[Stable guest ABI] Stable guest ABI when migrate domain to higher version qemu-kvm 	ydu 	None 	Manual 		Feature 	P3 	None 	Edit
Setup:

Migrate guest from host A to host B.

1. Host A installed a lower version qemu-kvm;

2. Host B installed the lastest version qemu-kvm.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Ensure that a little lower version qemu-kvm is installed on host A.

2. Install a guest on host A.

3. Check the XML configuration of the guest.
    
    #virsh dumpxml $guestName |grep machine

4. Note all the pci address allocation in the guest.

    #lspci

5. Migrate the guest to host B which has installed the lastest version qemu-kvm

    #virsh migrate --live $guestName qemu+ssh://$host_B_ip/system

6. On host B, check the XML configuration of the guest.
 
    #virsh dumpxml $guestName |grep machine.

7. Check all the pci address allocation again, and compare the pci address with step 4.

	
Expected Results:

1.

2. Guest is installed successfully.

3. #virsh dumpxml $guestName |grep machine

       <type arch='x86_64' machine='rhel6.2.0'>hvm</type>

4.

5. Migrate successfully.

6. #virsh dumpxml $guestName |grep machine

       <type arch='x86_64' machine='rhel6.2.0'>hvm</type>

7. All the pci address allocation is not chaged.
Notes:
Comments:

		177662 	[storage] Incorrect hardcoded sector size in disk storage pool --bug 735441 	nzhang 	None 	Manual 		Function 	P2 	None 	Edit
Setup:

1. Find a test machine in Beaker: dell-per300-01.rhts.eng.bos.redhat.com

2. Confirm it has a disk with 4k sector.

# fdisk -l
Note: sector size is 4096 (not 512)

Disk /dev/sdb: 162.8 GB, 162842222592 bytes
255 heads, 63 sectors/track, 2474 cylinders
Units = cylinders of 16065 * 4096 = 65802240 bytes
Sector size (logical/physical): 4096 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disk identifier: 0x000d2129

   Device Boot      Start         End      Blocks   Id  System

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage
    Regression

bug:

    No bug found

Actions:

Steps:
1. define disk pool on /dev/sdb
2. build and start /dev/sdb
3. create a vol in the disk pool using vol.xml:
<volume>
  <name>sdb1</name>
  <key>/dev/sdb1</key>
  <source>
    <device path='/dev/sdb'>
    </device>
  </source>
  <capacity unit='M'>10240</capacity>
  <target>
    <path>/dev/sdb1</path>
  </target>
</volume>

# virsh vol-create sdb vol.xml

4. check the vol info
# virsh vol-info sdb1 --pool sdb

	
Expected Results:

Results:
Step 3:
the vol created successfully.

Step 4:
libvirt-0.9.4-7.2 (Incorrect):
# virsh vol-info sdb1 --pool sdb
Name:           sdb1
Type:           block
Capacity:       1.25 GB
Allocation:     1.25 GB

libvirt-0.9.4-21 (Correct):
# virsh vol-info sdb1 --pool sdb
Name:           sdb1
Type:           block
Capacity:       10.00 GB
Allocation:     10.00 GB

Notes:
Comments:

		177672 	[Storage]check storage pool permission -- Bug: 815644 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    815644 - From Run 54156

Actions:

step :
1: prepare a new env. , no default pool exist
2: cat pool.xml
  <pool type='dir'> 
<name>default</name> 
<uuid>736c6f23-2afe-0f4f-294b-3d8864f5872f</uuid> 
<capacity unit='bytes'>30963351552</capacity>
 <allocation unit='bytes'>20410916864</allocation>
 <available unit='bytes'>10552434688</available>
 <source> </source>
 <target> 
<path>/var/lib/libvirt/images</path>
 <permissions> 
<owner>-1</owner> 
<group>-1</group>
 </permissions> 
</target> 
</pool>

3: define and start pool
    #virsh pool-define pool.xml
    #virhs pool-start default
4: check pool permissions
    #virsh pool-dumpxml default
 
   #ls -al /var/lib/libvirt 

 

	
Expected Results:

verify :

after step4 :

      <permissions>
      <mode>0755</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>

 

drwxr-xr-x. 19 root root 81920 Nov 15 02:05 images
Notes:
Comments:

		177673 	[Storage]Create 128 partition volume on disk pool 	vbian 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

 

 

1.Please make sure your /dev/sdb is a gpt partition , if not do

# parted /dev/sdb
GNU Parted 2.1
Using /dev/sdb
Welcome to GNU Parted! Type 'help' to view a list of commands.
(parted) mklabel                                                          
New disk label type? gpt                                                  
(parted) quit                                                             
Information: You may need to update /etc/fstab.                           
#

Tips:Most box here have only single disk and not gpt partition too , so a free usb(or useless whatever ) is a better choice to be the sdb

         instead of looking for a machine have two disk.

pool.xml
     <pool type="disk">
        <name>dsk_pool</name>
        <source>
             <device path='/dev/sdb'/>

             <format type='gpt'/>


        </source>
        <target>
             <path>/dev</path>
        </target>
     </pool>

vol-template.xml
  <volume>
    <name>XXXX</name>
    <key>/dev/XXXX</key>
    <source>
        <device path='/dev/sdb'>
        </device>
    </source>
    <capacity>10689</capacity>
    <allocation>10689</allocation>
    <target>
        <path>/dev/XXXX</path>
        <format type='none'/>
        <permissions>
           <mode>0660</mode>
           <owner>0</owner>
           <group>6</group>
           <label>system_u:object_r:fixed_disk_device_t:s0</label>
        </permissions>
    </target>
  </volume>

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

Knowledge base:
1. vol-create make a GPT partition table partition
2. fdisk doesn't support GPT.
3. GPT supports a maximum of 128 primary partitions

Do following things to create partition volumns
1. Create disk based pool
   # virsh pool-create pool.xml
2. Create volumn
   # for i in {1..128}; do sed -i -e "s/XXXX/sdb$i/g" vol-template.xml ; virsh vol-create dsk_pool vol-template.xml; sed -i -e "s/sdb$i/XXXX/g" vol-template.xml ; done
3. Look up the creation result
   # parted -s /dev/sdb print
4. Look up the libvirtd status and pool status
   # service libvirtd status
   # virsh pool-list --all

	
Expected Results:

[Expected result]
2. you could create 128 volumn successfully
3. parted could show the 128 partition for /dev/sdb
4. libvirtd keeps runnign, and pool-keeps active status

Notes:
Comments:

		177674 	[Storage]create guest on lvm volume groups that contains mirrored LVM 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

prepare two disk (sdb,sdc)

1: create four pv(sdb5, sdb6,sdb7,sdc)

sdb5 is data disk, sdb6 for log, sdb7 for backup, sdc is the mirror of sdb5

create partition

fdisk /dev/sdb , create sdb5 sdb6 sdb7

partprobe

pvcreate /dev/sdb5

pvcreate /dev/sdb6

pvcreate /dev/sdb7

pvdisplay

 2: create vg

vgcreate vg0 /dev/sdb5 /dev/sdb6 /dev/sdc

3: create mirror  /dev/vg0/mirror

lvcreate -L 1000M -m1 -n mirror vg0 /dev/sdb5 /dev/sdc /dev/sdb6

lvdisplay /dev/vg0/mirror

lvs -a -o +devices

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:


step:
 1 Setup a storage pool config for the VG that contains mirrored LVM.

virtcluster.xml 
<pool type="logical">
   <name>vg0</name>
   <source>
     <name>vg0</name>
     <format type="lvm2"/>
   </source>
   <target>
     <path>/dev/vg0</path>
   </target>
 </pool>

Then run 

virsh pool-define virtvg

virsh pool-list --all
Name                 State      Autostart 
-----------------------------------------
default              active     yes       
vg0         active   no     

virsh pool-start vg0

2: install a guest on sdb5

3: broken sdb5
dd if=/dev/zero of=/dev/sdb5 count=10

lvs -a -o +devices
 lvscan

umount /mirror/ 
 mount /dev/vg0/mirror /mirror/
 cd /mirror/

vgreduce --removemissing --force vg0

4: backup date

vgextend vg0 /dev/sdb7
lvconvert -m1 /dev/vg0/mirror /dev/sdb7 /dev/sdc /dev/sdb6

lvs -a -o +devices

lvdisplay /dev/vg0/mirror

	
Expected Results:

after step 2 ,verify the guest can install successful.

after step 3, verify sdb5 is unknow

after step 4, verify sdb7 instead of sdb5, and the guest can worked well
Notes:
Comments:

		177675 	[Storage]Define a storage pool with supported format types 	vbian 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Example XML , please save it as pool.xml
<pool type='disk'>
  <name>sdb</name>
  <source>
    <device path='/dev/sdb'>
    </device>
    <format type='XXXX'/>
  </source>
  <target>
    <path>/dev</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

1. Try to create the storage pool with the given xml and following command:
   # for i in {bsd,dos,dvh,gpt,mac,pc98,sun}; do sed -i -e "s/XXXX/$i/g" pool.xml ; virsh pool-define pool.xml; virsh pool-dumpxml sdb|grep format; virsh pool-undefine sdb; sed -i -e "s/$i/XXXX/g" pool.xml ; done
2. Check libvirtd status
   # service libvirtd status

	
Expected Results:

1. Could create storage pool with supported format types successfully
2. The format type output is NOT "unknown"
3. After create storage pool , libvirtd is still running
4. verify in every format , the guest should be installed and worked well.
Notes:
Comments:

		177676 	[Storage]Define a storage pool with unsupported format type 	vbian 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

[Setup]
Example XML
<pool type='disk'>
  <name>sdb</name>
  <source>
    <device path='/dev/sdb'>
    </device>
    <format type='auto'/>
  </source>
  <target>
    <path>/dev</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

1. Try to create the storage pool with the given xml
   # virsh pool-define pool.xml
2. Check libvirtd status
   # service libvirtd status

	
Expected Results:

1.Could be told the storage format type is not supported
2.Libvirtd is running still

Notes:
Comments:

		177678 	[storage]define multiple pools with the same target--bug: 817219 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

prepare three xml:
# virsh pool-dumpxml A.xml
<pool type='iscsi'>
  <name>A</name>
  <uuid>ba58183c-9cff-022a-4204-b161cd67a750</uuid>
  <capacity unit='bytes'>0</capacity>
  <allocation unit='bytes'>0</allocation>
  <available unit='bytes'>0</available>
  <source>
    <host name='192.168.58.160'/>
    <device path='iqn.2011-01.com.usi:storage.iscsi1'/>
  </source>
  <target>
    <path>/dev/disk/by-path</path>
    <permissions>
      <mode>0700</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>

# virsh pool-dumpxml B.xml
<pool type='iscsi'>
  <name>B</name>
  <uuid>d2cd5eb1-32bc-833f-c93a-f56b0b85853c</uuid>
  <capacity unit='bytes'>0</capacity>
  <allocation unit='bytes'>0</allocation>
  <available unit='bytes'>0</available>
  <source>
    <host name='192.168.58.160'/>
    <device path='iqn.2011-01.com.usi:storage.iscsi1'/>
  </source>
  <target>
    <path>/dev/disk/by-path</path>
    <permissions>
      <mode>0700</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>

# virsh pool-dumpxml C.xml
<pool type='iscsi'>
  <name>C</name>
  <uuid>f145fcaa-d53d-a4f1-67bb-3a28831250b5</uuid>
  <capacity unit='bytes'>0</capacity>
  <allocation unit='bytes'>0</allocation>
  <available unit='bytes'>0</available>
  <source>
    <host name='192.168.58.160'/>
    <device path='iqn.2011-01.com.usi:storage.iscsi2'/>
  </source>
  <target>
    <path>/dev/disk/by-path</path>
    <permissions>
      <mode>0700</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

step:
1:# virsh pool-define A.xml 
Pool A defined from A.xml

2:# virsh pool-define B.xml 

3:# virsh pool-define C.xml 
Pool C defined from C.xml

4:# virsh pool-define B.xml 




	
Expected Results:

verify:
after step 2:

    error: Failed to define pool from B.xml
    error: operation failed: Storage source conflict with pool: 'A'
after step 4:

    error: Failed to define pool from B.xml
    error: operation failed: Storage source conflict with pool: 'A'

 

 

 

Notes:
Comments:

		177679 	[Storage]find-storage-pool-sources/ find-storage-pool-sources-as for all pool type -- Bug 823850 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

create nfs pool:

follow case : 177668 [Storage] netfs based storage pool - bug 878400

create iscsi pool:

follow case: 177663 [Storage] iSCSI based storage pool

create logical type pool:

follow case: 177666 [Storage] Logical based storage pool
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Storage

bug:

    No bug found

Actions:

step:

   1:prepare nfs ,iscsi,logical type pool

2:virsh # pool-list
Name                 State      Autostart
-----------------------------------------
bbb                  active     yes      ------> logical pool
default              active     yes      
iscsi                active     no       ------> iscsi pool
mnt                  active     yes      
nfs                  active     no       ------> netfs pool

virsh # find-storage-pool-sources logical
<sources>
  <source>
    <device path='/dev/sdb'/>
    <name>bbb</name>
    <format type='lvm2'/>
  </source>
</sources>

 3: prepare netfs & iscsi pool
#virsh pool-list --all
Name                 State      Autostart 
-----------------------------------------
A                    active     no          ----iscsi 
default              active     no        
mpath                inactive   no        
mylvm                active     yes         ----logical
nfs                  active     yes         ----netfs

#virsh find-storage-pool-sources iscsi


4: #virsh find-storage-pool-sources iscsi iscsi.xml




5: #virsh find-storage-pool-sources netfs


6: #virsh find-storage-pool-sources netfs nfs.xml

 

 
	
Expected Results:

after step 2 ,verify:

for logical:

<sources>
  <source>
    <device path='/dev/sdb'/>
    <name>bbb</name>
    <format type='lvm2'/>
  </source>
</sources>

after step 3 , verify

error: Failed to find any iscsi pool sources
error: invalid argument: hostname and device path must be specified for iscsi sources

after step 4, verify

<sources>
  <source>
    <host name='10.66.90.100'/>
    <device path='iqn.2001-05.com.equallogic:0-8a0906-12a1f7d03-0daf49b25a84ee02-s3-kyla-131842'/>
  </source>
  <source>
    <host name='10.66.90.100'/>
    <device path='iqn.2001-05.com.equallogic:0-8a0906-9951f7d03-34cf49b25f04f94b-libvirt-2-150313'/>
  </source>
  <source>
    <host name='10.66.90.100'/>
    <device path='iqn.2001-05.com.equallogic:0-8a0906-6eb1f7d03-30cf49b25f24f94d-libvirt-1-150313'/>
  </source>
</sources>


after step 5, verify

error: Failed to find any netfs pool sources
error: invalid argument: hostname must be specified for netfs sources

after step 6, verify
 

<sources>
  <source>
    <host name='10.66.6.166'/>
    <dir path='/common'/>
    <format type='nfs'/>
  </source>
</sources>

 

 

 
Notes:
Comments:

		177680 	[Storage]large udevadm settle waiting time when connecting to libvirtd 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage
    rhel6.5

bug:

    No bug found

Actions:

1. Define at least one storage pool with a LVM volume group.
2. Disconnect virt-manager
3. Try to reconnect virt-manager
=========TBD========

	
Expected Results:

Connecting without delay.
libvirt should:
a.) distinguish in more detail when udevadm needs to be called. Certainly not
on a connect of a client.
b.) pass a "--timeout 15" argument or similar to "udevadm settle" and allow
clients to give the user visual feedback about the ongoing settle operation

=============TBD===========
Bug 789766 - #570359 patch introduces large udevadm settle waiting time when connecting to libvirtd

Notes:
Comments:

		177682 	[Storage]lvm volume groups that contains snapshot lvm 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

step:

create vg contains snapshot lvm.

   1: need two disk partation for pvcreate     

          # pvcreate /dev/sda2 /dev/sda13

  Writing physical volume data to disk "/dev/sda2"
  Physical volume "/dev/sda2" successfully created
  Writing physical volume data to disk "/dev/sda13"
  Physical volume "/dev/sda13" successfully created
2: add two pv into a vg

 vgcreate vg_virt /dev/sda2 /dev/sda13

 Volume group "vg_virt" successfully created
3: create lv lv_virt
# lvcreate --size 10GB --name lv_virt vg_virt

4: create snapshot lv
# lvcreate -s -L 1GB -n ss_virt /dev/vg_virt/lv_virt

check snapshot lv created
#lvdisplay

5: Setup a storage pool config for the VG that contains snapshot LVM.

# virsh -d 4 pool-create-as vg_virt logical --target /dev/vg_virt


6: virsh pool-list --all


7: virsh pool-dumpxml vg_virt

	
Expected Results:

verify:

after step 4:

--- Logical volume ---
  LV Name                /dev/vg_virt/ss_virt
  VG Name                vg_virt
  LV UUID                gIeCD8-WsI5-BcC9-9gyl-3jiR-UGVV-FAtS2d
  LV Write Access        read/write
  LV snapshot status     active destination for /dev/vg_virt/lv_virt
  LV Status              available
  # open                 0
  LV Size                10.00 GiB
  Current LE             2560
  COW-table size         1.00 GiB
  COW-table LE           256
  Allocated to snapshot  0.00%
  Snapshot chunk size    4.00 KiB
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:1

after step 5:

Pool vg_virt created.

no error

after step 6:

Name                 State      Autostart
-----------------------------------------
   
boot-scratch         active     yes      
default              active     yes       
vg_virt              active     no      

after step 7:

<pool type='logical'>
  <name>vg_virt</name>
  <uuid>7fafd73a-4d20-ec7c-1619-085616af7513</uuid>
  <capacity unit='bytes'>28231860224</capacity>
  <allocation unit='bytes'>11811160064</allocation>
  <available unit='bytes'>16420700160</available>
  <source>
    <name>vg_virt</name>
    <format type='unknown'/>
  </source>
  <target>
    <path>/dev/vg_virt</path>
    <permissions>
      <mode>0700</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>

Notes:
Comments:

		177683 	[storage]use pool-create/pool-create-as with --build option to create pool -- Bug:830056 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage
    upstream

bug:

    No bug found

Actions:

should test this with all type pool

step:

  1: don't create dir : /var/lib/libvirt/dir

       dir pool

      virsh # pool-create dirpool.xml--build

   

  2:  disk pool

       virsh # pool-create diskpool.xml--build

  

  3: fs pool

       virsh # pool-create fspool.xml --build

      

 
	
Expected Results:

verify:

 all the pool can create without error

==========TBD============

refer bug: https://bugzilla.redhat.com/show_bug.cgi?id=830056
Notes:
Comments:

		177684 	[Storage]vol-create-as should fail when allocate a malformed size image --Bug 823362 	zpeng 	None 	Manual 		Negative test 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

Steps 
1, define and start a nfs pool
virsh # pool-info nfs
Name:           nfs
UUID:           9401f015-e2cb-2cb2-2c05-83ba92c8cb4b
State:          running
Persistent:     yes
Autostart:      yes
Capacity:       98.43 GB       ----------> pool capacity
Allocation:     58.37 GB
Available:      40.06 GB

2,create a malformed size image

virsh # vol-create-as nfs big 10000000000000000000000000000000000000G
error: Malformed size 10000000000000000000000000000000000000G


virsh # vol-create-as nfs big 100000000000G
error: Malformed size 100000000000G
error: Failed to create vol big error: cannot extend file '/nfs/big': File too large

virsh # vol-info /nfs/big --pool nfs
error: failed to get vol 'nfs/big'
error: Storage volume not found: no storage vol with matching path


3, create a over commit image in the pool
virsh # vol-create-as nfs big6 100G
Vol big6 created                     ------------> a long time

virsh # vol-info --pool nfs big6
Name:           big6
Type:           file
Capacity:       100.00 GB           -------> expect
Allocation:     401.18 MB

	
Expected Results:

verify:

1,virsh command should fail when create a malformed size image.




 
Notes:
Comments:

		177686 	[Supported hypervisors]support hyper-v --Bug 884430 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    supported hypervisors

bug:

    No bug found

Actions:

#  virsh -c hyperv://<ip address>/?transport=http (over HTTP)
#  virsh -c hyperv://<ip address>/                (over HTTPS)

=====TBD======

https://bugzilla.redhat.com/show_bug.cgi?id=884430
	
Expected Results:
Notes:
Comments:

		177687 	[Supported hypervisors]connection to vCenter server --Bug 819729 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

prepare vCenter server

10.66.6.123
10.66.6.48

administrator/123qweP

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    supported hypervisors

bug:

    No bug found

Actions:

step:

make sure 10.66.6.123 is vCenter server
1 #virsh -c esx://10.66.6.123/?no_verify=1
input correct user&password




	
Expected Results:

error msg:

error: internal error 10.66.6.123 is neither an ESX 3.5, 4.x nor 5.x host
error: failed to connect to the hypervisor

 

 
Notes:
Comments:

		177688 	[Supported hypervisors]Probing for guest capabilities --bug 813735 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    supported hypervisors

bug:

    No bug found

Actions:

step
first point:
    1: modprobe -r kvm_intel
    2: modprobe -r kvm
    3: virsh capabilities
  
 second point:
     1:make sure kvm module loaded
      # lsmod | grep kvm
kvm_intel              52890  0 
kvm                   314931  1 kvm_intel
     2:# mv /usr/libexec/qemu-kvm /usr/bin/qemu-system-x86_64
     3: virsh capabilities
   

 
	
Expected Results:

verify:

after first point step3:

 ......
     <guest>
    <os_type>hvm</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.3.0</machine>
   ......
     <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
   ......

 after second point step 3

 .......
     <domain type='kvm'>
        <emulator>/usr/bin/qemu-system-x86_64</emulator>
      </domain>
    .......

 
Notes:
Comments:

		177689 	[Supported hypervisors]libvirt need to support ESXi5.0 - bug 758231 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Have a ESXi 5.0 and ESXi 5.1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    supported hypervisors
    Regression

bug:

    864384 - From Run 47366
    873538 - From Run 49806

Actions:

1. Connect to ESXi 5.0.
virsh -c esx://<ESXi5.0ServerIP>/?no_verify=1

# virsh -c esx://10.66.73.152/?no_verify=1

2. List all ESXi 5.0 guest.

# virsh -c esx://10.66.73.152/?no_verify=1 list --all

3. Start a guest in ESXi 5.0 Server

# virsh -c esx://<ESXi5.0ServerIP>/?no_verify=1  start [guestname]

4.Dumpxml of guest.

 # virsh -c esx://10.66.73.152/?no_verify=1 dumpxml rhel6-20111005.1-x64

 5.Perpare a guest xml from step 4 and change the <Name>,<UUID> and disk.Define a guest.

 virsh -c esx://10.66.73.152/?no_verify=1 define esxi5.xml

 6.Undefine the guest.
# virsh -c esx://10.66.73.152/?no_verify=1 undefine test

7.Destroy a guest.

# virsh -c esx://10.66.73.152/?no_verify=1 destroy 1

8. repeat step 1- 7 for ESXi5.1
	
Expected Results:

1.
# virsh -c esx://10.66.73.152/?no_verify=1
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  - rhel6-20111005.1-x64 shut off
  - rhel6.2-20111013.1-x64 shut off

2.
# virsh -c esx://10.66.73.152/?no_verify=1 list --all
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
 Id Name                 State
----------------------------------
  - rhel6-20111005.1-x64 shut off
  - rhel6.2-20111013.1-x64 shut off

3.
# virsh -c esx://10.66.73.152/?no_verify=1 start rhel6-20111005.1-x64
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
Domain rhel6-20111005.1-x64 started

# virsh -c esx://10.66.73.152/?no_verify=1 list 
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
 Id Name                 State
----------------------------------
  1 rhel6-20111005.1-x64 running

4.
# virsh -c esx://10.66.73.152/?no_verify=1 dumpxml rhel6-20111005.1-x64 
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
<domain type='vmware' id='1'>
  <name>rhel6-20111005.1-x64</name>
  <uuid>564d1c04-8b93-0e12-95d5-d9112af53fd6</uuid>
  <memory>2097152</memory>
....

5.
# virsh -c esx://10.66.73.152/?no_verify=1 define esxi5.xml
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
Domain test defined from esxi5.xml

# virsh -c esx://10.66.73.152/?no_verify=1 list --all
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
 Id Name                 State
----------------------------------
  1 rhel6-20111005.1-x64 running
  - test                 shut off

6.Undefine the guest.
# virsh -c esx://10.66.73.152/?no_verify=1 undefine test 
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
Domain test has been undefined

7.Destroy a guest.
# virsh -c esx://10.66.73.152/?no_verify=1 destroy 1
Enter username for 10.66.73.152 [root]: 
Enter root's password for 10.66.73.152: 
Domain 1 destroyed

8. verify result same with connect ESXi5.0

Notes:
Comments:

		177690 	[Supported hypervisors]Unclear error message when wrong ESX url is passed-781718 	tzheng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Make sure you have prepared a Virtualcenter.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    supported hypervisors
    Regression

bug:

    No bug found

Actions:

1.When connect ESXi server,pass the url of the Virtualcenter.

eg:virsh -c esx://<VirtualcenterIP>/?no_verify=1
Enter username for ip [root]:
Enter administrator's password for ip:


	
Expected Results:

There should be clear error messages given like:the host is neither an ESX 3.5, 4.x nor 5.x host

eg:# virsh -c esx://10.66.6.48/?no_verify=1
Enter username for 10.66.6.48 [root]: administrator
Enter administrator's password for 10.66.6.48:  123qweP
error: internal error 10.66.6.48 is neither an ESX 3.5, 4.x nor 5.x host
error: failed to connect to the hypervisor
Notes:
Comments:

		177691 	[sVirt] Allow a base label to be specified in dynamic labelling mode 	gsun 	None 	Manual 		--default-- 	P3 	None 	Edit
Setup:

prepare a running guest

Make sure selinux is enforcing

# setenforce 1

# getenforce

Enforcing

 

Base on commit:

http://libvirt.org/git/?p=libvirt.git;a=commit;h=4ebfc42716bfe7f78b996c13183f4d01e5824ebd
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    No bug found

Actions:

1. shutdown the guest

# virsh destroy $guest_name

2. edit the guest xml

# virsh edit $guest_name

add following part:

...

   <seclabel type='dynamic' model='selinux'>
     <baselabel>system_u:system_r:svirt_t:s0</baselabel>
   </seclabel>

...

3. start the guest

# virsh start $guest_name

4. check sVirt part in guest xml

# virsh dumpxml $guest_name

 
	
Expected Results:

2.

# virsh edit adsd
Domain adsd XML configuration edited.


3.

# virsh start adsd
Domain adsd started

4.

...

  <seclabel type='dynamic' model='selinux' relabel='yes'>
    <label>unconfined_u:object_r:virt_t:s0:c128,c321</label>
    <imagelabel>unconfined_u:object_r:svirt_image_t:s0:c128,c321</imagelabel>
    <baselabel>system_u:object_r:virt_t:s0</baselabel>
  </seclabel>

...
Notes:
Comments:

		177692 	[sVirt] Allow for resource relabelling with static labels 	gsun 	None 	Manual 		--default-- 	P1 	None 	Edit
Setup:

Prepare a running guest

Make sure selinux is enforcing

# setenforce 1

# getenforce

Enforcing

 

 

Base on commit:

http://libvirt.org/git/?p=libvirt.git;a=commit;h=6321fd979851a29da974823dddd6b2a4a7e43f83
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    No bug found

Actions:

1. shutdown the guest

# virsh destroy $guest_name

2. edit the guest xml (with relabel = 'yes')

# virsh edit $guest_name

add following part:

...

  <seclabel type='static' model='selinux' relabel='yes'>
    <label>system_u:system_r:svirt_t:s0:c392,c662</label>
  </seclabel>
...

3. start the guest

# virsh start $guest_name

4. check sVirt part in guest xml

# virsh dumpxml $guest_name

5. redo step 2 to 4, with relabel = 'no' in xml

Also modify the label context to make it different

...

  <seclabel type='static' model='selinux' relabel='no'>
    <label>system_u:system_r:svirt_t:s0:c311,c611</label>
  </seclabel>
...

 
	
Expected Results:

2.

# virsh edit adsd
Domain adsd XML configuration edited.


3.

# virsh start adsd
Domain adsd started

4.

...

  <seclabel type='static' model='selinux' relabel='yes'>
    <label>system_u:system_r:svirt_t:s0:c392,c662</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c392,c662</imagelabel>
  </seclabel>
...

5.

# virsh start adsd
error: Failed to start domain adsd
error: internal error Process exited while reading console log output: 14:43:56.132: 22719: info : libvirt version: 0.9.4, package: 0rc1.2.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2011-08-01-23:37:12, x86-003.build.bos.redhat.com)
char device redirected to /dev/pts/11
qemu-kvm: -drive file=/var/lib/libvirt/images/adsd.img,if=none,id=drive-ide0-0-0,format=raw,cache=none,aio=threads: could not open disk image /var/lib/libvirt/images/adsd.img: Permission denied

Notes:
Comments:

		177694 	[sVirt] create live snapshot (disk-only) with guest's image backing file on NFS storage - bug 822015 	gsun 	gsun 	Manual 		--default-- 	P3 	None 	Edit
Setup:

Using qemu-kvm-rhev package

Prepare an NFS server: 

1.NFS server   *(rw,sync,no_root_squash)


mount $NFS   /mnt/mig -o vers=3

#mount
...
10.66.5.12:/mnt/data/images/ on /mnt/mig type nfs (rw,vers=3,addr=10.66.5.12)

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt
    rhel6.5

bug:

    No bug found

Actions:

1.install a guest in the NFS server named mig-qcow2.img

#qemu-img info /mnt/mig/mig-qcow2.img
image: /mnt/mig/mig-qcow2.img
file format: qcow2
virtual size: 8.0G (8589934592 bytes)
disk size: 3.4G
cluster_size: 65536

2. create a image which backing file is /mnt/mig/mig-qcow2.img

#qemu-img create -f qcow2 -b /mnt/mig/mig-qcow2.img  /var/lib/libvirt/images/q.img  8G

# qemu-img info /var/lib/libvirt/images/q.img
image: /var/lib/libvirt/images/q.img
file format: qcow2
virtual size: 8.0G (8589934592 bytes)
disk size: 31M
cluster_size: 65536
backing file: /mnt/mig/mig-qcow2.img (actual path: /mnt/mig/mig-qcow2.img)

3.  start a guest  with  q.img
# virsh dumpxml q
...
<disk type='file' device='disk'>
<driver name='qemu' type='qcow2' cache='none'/>
<source file='/var/lib/libvirt/images/q.img'>
<seclabel relabel='no'/>
</source>
<target dev='vda' bus='virtio'/>
<alias name='virtio-disk0'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
</disk>
...

# virsh domstate q
running


4. try to create live snapshot
# virsh snapshot-create q --disk-only

	
Expected Results:

4.

# virsh snapshot-create q --disk-only 

error: internal error unable to execute QEMU command 'transaction': Could not open '/var/lib/libvirt/images/q.1337057407

  # tail -f /var/log/libvirt/libvirtd.log

  2012-05-15 04:50:07.331+0000: 12026: error : qemuMonitorJSONCheckError:338 : internal error unable to execute QEMU command 'transaction': Could not open '/var/lib/libvirt/images/q.1337057407'

 

Expected result:

This should work.
Notes:
Comments:

    #1 ydu@redhat.com 2012-06-29 14:26:04
    Bug still not fix. BTW, i think the Expected result still need update after bug fix. So move to NEED_UPDATE

		177708 	[sVirt] qemu-kvm cannot access -mem-path if umask is 027 - bug 815206 	gsun 	gsun 	Manual 		Function 	P1 	None 	Edit
Setup:

Make sure selinux is enabled:

# sestatus
SELinux status:                 enabled
SELinuxfs mount:                /selinux
Current mode:                   enforcing
Mode from config file:          enforcing
Policy version:                 24
Policy from config file:        targeted

Prepare a domain.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    No bug found

Actions:

 

1. Disable transparent hugepages (such as adding 'transparent_hugepage=never' to the kernel command line) 
# vim /etc/grub.conf
...
        kernel /boot/vmlinuz-2.6.32-262.el6.x86_64 ro root=UUID=7cac5d85-15de-4fa7-ad22-9987cbf144b2 rd_NO_LUKS rd_NO_LVM rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYBOARDTYPE=pc KEYTABLE=us crashkernel=128M rhgb quiet transparent_hugepage=never

...

2. Set 'umask=027' in /etc/rc.d/init.d/functions
# vim /etc/rc.d/init.d/functions


3. mkdir /var/hugepages
# mkdir /var/hugepages


4. Mount hugetlbfs via /etc/fstab ("hugetlbfs /var/hugepages hugetlbfs")
# cat /etc/fstab 

#
# /etc/fstab
# Created by anaconda on Tue Jul  5 20:23:44 2011
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
UUID=7cac5d85-15de-4fa7-ad22-9987cbf144b2 /                       ext4    defaults        1 1
tmpfs                   /dev/shm                tmpfs   defaults        0 0
devpts                  /dev/pts                devpts  gid=5,mode=620  0 0
sysfs                   /sys                    sysfs   defaults        0 0
/dev/sda7        /mnt            ext4    defaults    0 0
proc                    /proc                   proc    defaults        0 0
hugetlbfs        /var/hugepages        hugetlbfs



5. Set 'vm.nr_hugepages=516' in /etc/sysctl.conf  (to create hugepages)
Adding vm.nr_hugepages=516 at the end of the file.



6. Reboot host

7. Define a virtual machine, with the following XML added
    <memoryBacking> 
    <hugepages/> 
    </memoryBacking> 

# vish dumpxml $dom
...
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
  <memoryBacking> 
  <hugepages/> 
  </memoryBacking> 

...


8. Start the VM

  9. check permission and error

9.1 libvirt dir 
# ll -d /var/hugepages/libvirt/
drwxr-x--x. 3 root root 0 May  3 19:48 /var/hugepages/libvirt/

  9.2 qemu dir

  # ll -d /var/hugepages/libvirt/qemu

  9.3 check meminfo

  # vim /proc/meminfo

  9.4 check in $dom.log

  # vim /var/log/libvirt/qemu/rhel6u2.log

 
	
Expected Results:

8.

Domain dom_test started

9.

9.1

drwxr-x--x. 3 root root 0 May  3 19:48 /var/hugepages/libvirt/

dir permission is 751 not 750 

 

9.2

 drwxr-x---. 2 qemu qemu 0 May 3 20:32 /var/hugepages/libvirt/qemu

750 qemu:qemu

9.3

...
HugePages_Total:     516
HugePages_Free:      320
HugePages_Rsvd:      316
HugePages_Surp:        0
Hugepagesize:       2048 kB
...

hugepage is used

 9.4

No 'statfs: Permission denied' errors in /var/log/libvirt/qemu/vmname.log

NOTE:when you found something like "alloc_mem_area: can't mmap hugetlbfs pages: Cannot allocate memory" in the log , 
            you can shrink the guest's memory and restart the guest.
           Technical said , the guest's memory should less equal   HugePages_Total: ( 516) * Hugepagesize: (2048 kB)



 

 





 

 
Notes:
Comments:

		177715 	[sVirt] selinux should not block start virtual network - bug 788985 	gsun 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

For machine with libvirt and virt-manager, stop default virtual network
# virsh net-list --all
Name                 State      Autostart

-----------------------------------------
default              active     yes

# virsh net-destroy default
Network default destroyed

Make sure selinux is enabled:

# sestatus
SELinux status:                 enabled
SELinuxfs mount:                /selinux
Current mode:                   enforcing
Mode from config file:          enforcing
Policy version:                 24
Policy from config file:        targeted

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    No bug found

Actions:

1. remove the file for record 'default' network pid (if it exists)

# ll /var/run/libvirt/network/default.pid -Z
-rw-r--r--. root root system_u:object_r:dnsmasq_var_run_t:s0 /var/run/libvirt/network/default.pid

# rm -f /var/run/libvirt/network/default.pid

2. start 'default' network

# virsh net-start default

3. check the pid file secontxt

# ll -Z /var/run/libvirt/network/default.pid


	
Expected Results:

2.

Network default started

3.

-rw-r--r--. root root unconfined_u:object_r:dnsmasq_var_run_t:s0 /var/run/libvirt/network/default.pid

 NOT unconfined_u:object_r:virt_var_run_t
Notes:
Comments:

		177716 	[sVirt] set security_default_confined in qemu.conf - bug 823857 	gsun 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. prepare a stopped domain

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     dom                            shut off

 

check domaim img file

# ll -Z /var/lib/libvirt/images/qed1.img 

-rw-r--r--. root root system_u:object_r:virt_image_t:s0 /var/lib/libvirt/images/qed1.img
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    851491 - From Run 44483

Actions:

1. add security_default_confined = 0 in qemu.conf

# vim /etc/libvirt/qemu.conf

Add security_default_confined = 0 at the bottom, save and quit, then restart libvirtd.

# service libvirtd restart

Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

2. edit domain xml with seclable type as none.

# virsh edit dom

...

    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='none'/>
</domain>

3. start domain

# virsh start dom

4. check domain

# virsh list

# virsh dumpxml dom

# ll -Z /var/lib/libvirt/images/qed1.img

5. prepare another unused img file

# qemu-img info /var/lib/libvirt/images/dom
image: /var/lib/libvirt/images/dom
file format: raw
virtual size: 5.0G (5368709120 bytes)
disk size: 0

dump domain xml and modify domain name and img file location:

# virsh dumpxml dom > new.xml

# vim new.xml

...

  <name>dom_1</name>
...

      <source file='/var/lib/libvirt/images/dom'/>
...

And make sure there is no seclabe in xml.

6. define a new domain from new.xml

# virsh define new.xml
Domain dom_1 defined from new.xml

7. start dom_1

# virsh start dom_1

8.

# virsh dumpxml dom_1

 
	
Expected Results:

3.

Domain dom started

4.

# virsh list
 Id    Name                           State
----------------------------------------------------
 1     dom                            running

# virsh dumpxml dom

...

    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='none'/>
</domain>

-rw-r--r--. qemu qemu system_u:object_r:virt_image_t:s0 /var/lib/libvirt/images/qed1.img

no change of context

7.

Domain dom_1 started

8.

...

  </devices>
  <seclabel type='none' model='selinux'/>
</domain>

...

default seclabe set as none now.
Notes:
Comments:

		177717 	[sVirt] set security_driver in /etc/libvirt/qemu.conf 	gsun 	None 	Manual (Autoproposed) 		--default-- 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    No bug found

Actions:

1. Edit /etc/libvirt/qemu.conf, add

security_driver = "none"

2. restart libvirtd

# service libvirtd restart

3. Install a guest by virt-manager or libvirt-test-API

4. check the guest image selinux label

# ll -Z /var/lib/libvirt/images/$guest_img

5. destroy the guest

# virsh destroy $guest_name

6. Edit /etc/libvirt/qemu.conf, add

security_driver = "selinux"

7. restart libvirtd

# service libvirtd restart

8. start the guest

# virsh start $guest_name

9. recheck the guest img

# ll -Z /var/lib/libvirt/images/$guest_img

10. destroy the guest and set  security_driver = "none", after restart libvirtd, recheck guest img

# ll -Z /var/lib/libvirt/images/$guest_img
	
Expected Results:

4. no specified selinux lable

# ll -Z /var/lib/libvirt/images/dom_test
-rw-r--r--. qemu qemu unconfined_u:object_r:virt_image_t:s0 /var/lib/libvirt/images/dom_test

9.

# ll -Z /var/lib/libvirt/images/dom_test
-rw-r--r--. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c831,c864 /var/lib/libvirt/images/dom_test

10.

# ll -Z /var/lib/libvirt/images/dom_test
-rw-r--r--. qemu qemu system_u:object_r:virt_image_t:s0 /var/lib/libvirt/images/dom_test

Notes:
Comments:

		177718 	[sVirt] set security_require_confined in qemu.conf 	gsun 	gsun 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. prepare a stopped domain

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     dom                            shut off

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    851395 - From Run 44483

Actions:

1. add security_require_confined = 1 in qemu.conf

# vim /etc/libvirt/qemu.conf

Add security_require_confined = 1 at the bottom, save and quit, then restart libvirtd.

# service libvirtd restart

Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

2. edit domain xml with seclable type as none.

# virsh edit dom

...

    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='none'/>
</domain>

3. start domain

# virsh start dom
	
Expected Results:

3.

error: Failed to start domain dom
error: unsupported configuration: Unconfined guests are not allowed on this host

Notes:
Comments:

		177719 	[sVirt] set set_process_name in /etc/libvirt/qemu.conf 	gsun 	None 	Manual (Autoproposed) 		--default-- 	P3 	None 	Edit
Setup:

Prepare a guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt

bug:

    No bug found

Actions:

1. Edit /etc/libvirt/qemu.conf

set_process_name = 1

2. Restart libvirtd

# service libvirtd restart

3. start the guest

# virsh start $guest_name

4. check the qemu process

# ps aux|grep qemu

5. destroy the guest

# virsh destroy $guest_name

6. Edit /etc/libvirt/qemu.conf

set_process_name = 0

7. Restart libvirtd

# service libvirtd restart

8. start the guest

# virsh start $guest_name

9. check qemu process

# ps aux|grep qemu
	
Expected Results:

4.

qemu     24345 39.2  4.2 1308824 341436 ?      Sl   13:48   0:18 /usr/libexec/qemu-kvm -S -M rhel6.2.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name dom_test,process=qemu:dom_test -uuid 0aae4b03-93bc-401a-fb1b-a5450668e115 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/dom_test.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -drive file=/var/lib/libvirt/images/dom_test,if=none,id=drive-ide0-0-0,format=raw,cache=none -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1 -netdev tap,fd=22,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=54:52:00:32:68:58,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4

diff part:

... -name dom_test,process=qemu:dom_test ...

9.

qemu     24482 12.2  0.2 1298608 23936 ?       Sl   13:49   0:00 /usr/libexec/qemu-kvm -S -M rhel6.2.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name dom_test -uuid 0aae4b03-93bc-401a-fb1b-a5450668e115 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/dom_test.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -drive file=/var/lib/libvirt/images/dom_test,if=none,id=drive-ide0-0-0,format=raw,cache=none -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1 -netdev tap,fd=22,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=54:52:00:32:68:58,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4

diff part:

... -name dom_test ...

 
Notes:
Comments:

		177727 	[Update device flags] Cannot insert CD (update device) to VM when LIVE constant is not used in updateDeviceFlags command - bug 716826 	gsun 	None 	Manual 		--default-- 	P2 	None 	Edit
Setup:

Prepare a guest with a cdrom.

1. Install win7-32 guest with the following cdrom xml:
# virsh dumpxml win7-32
...
<disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/a.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
</disk>

 2. prepare 3 different cdrom img files

# mkisofs -o /var/lib/libvirt/images/b.iso /tmp/

edit a new file into /tmp/, then

# mkisofs -o /var/lib/libvirt/images/c.iso /tmp/

edit a new file into /tmp/, then

# mkisofs -o /var/lib/libvirt/images/d.iso /tmp/
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks
    Regression

bug:

    No bug found

Actions:

1. Execute the following update1.py:
# cat update1.py
import libvirt

xml = '''<disk device="cdrom" type="file">
<source file="/var/lib/libvirt/images/b.iso"/>
<target dev="hdc"/>
</disk>
'''

conn = libvirt.open('qemu:///system')
dom = conn.lookupByName('vr-win7-i386-kvm')
dom.updateDeviceFlags(xml, libvirt.VIR_DOMAIN_DEVICE_MODIFY_FORCE |
libvirt.VIR_DOMAIN_DEVICE_MODIFY_LIVE)

#python update1.py

2. check the guest xml
# virsh dumpxml win7-32

3. Execute the following update2.py:
# cat update2.py
import libvirt

xml = '''<disk device="cdrom" type="file">
<source file="/var/lib/libvirt/images/c.iso"/>
<target dev="hdc"/>
</disk>
'''

conn = libvirt.open('qemu:///system')
dom = conn.lookupByName('vr-win7-i386-kvm')
dom.updateDeviceFlags(xml, libvirt.VIR_DOMAIN_DEVICE_MODIFY_FORCE |
libvirt.VIR_DOMAIN_DEVICE_MODIFY_CURRENT)

#python update2.py

4. check the guest xml
# virsh dumpxml win7-32


5. Execute the following update3.py:
# cat update3.py
import libvirt

xml = '''<disk device="cdrom" type="file">
<source file="/var/lib/libvirt/images/d.iso"/>
<target dev="hdc"/>
</disk>
'''

conn = libvirt.open('qemu:///system')
dom = conn.lookupByName('vr-win7-i386-kvm')
dom.updateDeviceFlags(xml, libvirt.VIR_DOMAIN_DEVICE_MODIFY_FORCE)

#python update3.py

6. check the guest xml
# virsh dumpxml win7-32

	
Expected Results:

2.

cdrom img updated

...
<disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/b.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
</disk>
...

 4.

cdrom img updated

...
<disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/c.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
</disk>
...

6.

cdrom img updated

...
<disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/d.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
</disk>
...

 
Notes:
Comments:

		177730 	[Update device flags] control link up/down state of guest NICs via XML & on the fly - bug 643373 	gsun 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

prepare a running domain with a interface:

# virsh start rhel6u2
Domain rhel6u2 started

# virsh dumpxml rhel6u2

...

    <interface type='network'>
      <mac address='54:52:00:1f:d1:38'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
...

Pay attention with the model type, make sure it's the same with the xml for update.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Open guest console, login in and run

# ping www.google.com

keep the window open

2. set the interface link down

# virsh domif-setlink rhel6u2 vnet0 down

Switch to the guest console, check ping state

3. update the interface link status with iface1.xml:

    <interface type='network'>
      <mac address='54:52:00:1f:d1:38'/>
      <source network='default'/>
      <target dev='vnet0'/> 

      <model type='virtio'/>
      <link state='up'/>
      <alias name='net0'/>
    </interface>

# virsh update-device rhel6u2 iface.xml

recheck ping status in guest

4. update the interface link status with iface2.xml

    <interface type='network'>
      <mac address='54:52:00:1f:d1:38'/>
      <source network='default'/>
      <target dev='vnet0'/> 

      <model type='virtio'/>
      <link state='down'/>
      <alias name='net0'/>
    </interface>

# virsh update-device rhel6u2 iface2.xml

recheck ping status in guest

5. use command bring the link up

# virsh domif-setlink rhel6u2 vnet0 up

recheck ping status in guest
	
Expected Results:

1.

ping success

2.

Device updated successfully

In guest, ping fail

3.

Device updated successfully

ping works fine now

4.

Device updated successfully

In guest, ping fail

5.

Device updated successfully

In guest, ping success
Notes:
Comments:

		177733 	[Update device flags] persistent option of virsh update-device command never work - bug 598792 	gsun 	None 	Auto 		--default-- 	P2 	None 	Edit
Setup:

1.a healthy guest, which is shutoff

2.for floppy test, if the guest os is rhel6, pls do modprobe first when login into the guest.

#modprobe floppy

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks
    virsh-rail

bug:

    740702 - From Run 43003

Actions:

1. Define and start a domain with an CDROM device, and a media (temp.iso) is in
this CDROM device.
...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/temp.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>
...
2. Try to change the media in the existing CDROM device, and keep this change
on the persisted domain configuration.
# cat cdrom1.xml 
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/temp1.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>
# virsh update-device rhel61_x86_64 cdrom1.xml --persistent

3.# virsh destroy rhel61_x86_64
Domain rhel61_x86_64 destroyed

 # virsh dumpxml rhel61_x86_64



----------- test with floppy device ---------------
1. Define and start a domain with a floppy device
...
    <disk type='block' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source dev='/var/lib/libvirt/images/floppy1.img'/>
      <target dev='fda' bus='fdc'/>
      <readonly/>
    </disk>
...
2. Try to change the media in the existing floppy device, and keep this change
on the persisted domain configuration.
# cat floppy2.xml 
    <disk type='block' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source dev='/var/lib/libvirt/images/floppy2.img'/>
      <target dev='fda' bus='fdc'/>
    </disk>
# virsh update-device rhel61_x86_64 floppy2.xml --persistent

3.# virsh destroy rhel61_x86_64
Domain rhel61_x86_64 destroyed
# virsh dumpxml rhel61_x86_64

	
Expected Results:

2.

Device updated successfully

 3.

...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/temp1.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
...

the temp1.iso still in the guest

 ----------- test with floppy device ---------------

2.

Device updated successfully


3. 

...
    <disk type='block' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source dev='/var/lib/libvirt/images/floppy2.img'/>
      <target dev='fda' bus='fdc'/>
      <readonly/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
...

The floppy2.img still in the guest.

 
Notes:
Comments:

		177739 	[Update device flags] using virsh command to insert & eject media - bug 713932 	gsun 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. prepare a running domain with cdrom 
# virsh list
 Id    Name                           State
----------------------------------------------------
 3     rhel6u2                        running

# virsh dumpxml rhel6u2
...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/tmp/temp2.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
...

2. prepare two iso file:
# mkisofs -o /var/lib/libvirt/images/aaa.iso /tmp
# mkdir /tmp/changetest
# vim /tmp/changetest/dsa
# mkisofs -o /var/lib/libvirt/images/bbb.iso /tmp

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. eject media
# virsh change-media rhel6u2 hdc --eject 

2. insert media
# virsh change-media rhel6u2 hdc /var/lib/libvirt/images/aaa.iso --insert

3. change media
# virsh change-media rhel6u2 hdc /var/lib/libvirt/images/bbb.iso --update


4. eject media
# virsh change-media rhel6u2 hdc --eject

in guest:

#mount /dev/cdrom /mnt

then in host:

# virsh change-media rhel6u2 hdc --eject

 

in guest:

#umount /mnt

in host:

# virsh change-media rhel6u2 hdc --eject


5. insert media with update
# virsh change-media rhel6u2 hdc /var/lib/libvirt/images/aaa.iso --update


6. force eject
# virsh change-media rhel6u2 hdc --eject --force


7. update media with --config
# virsh change-media rhel6u2 hdc /var/lib/libvirt/images/aaa.iso --update
--config


destroy domain and check xml
# virsh destroy rhel6u2

# virsh dumpxml rhel6u2

8. update media with --current
# virsh change-media rhel6u2 hdc /var/lib/libvirt/images/bbb.iso --update
--current


# virsh dumpxml rhel6u2


So, this is working.

	
Expected Results:

 

1.

succeeded to complete action eject on media

in guest cdrom ejected

2.

succeeded to complete action insert on media

log in guest with x windows started, cdrom mounted and opened with nautilus

 

3.

succeeded to complete action update on media

open cdrom in guest, content changed

 

4.

while the cdrom is mounted, the following error will be seen while running eject command.

error: Failed to complete action eject on media
error: internal error unable to execute QEMU command 'eject': Device
'drive-ide0-1-0' is locked

 succeeded to complete action eject on media, after umount cdrom in guest

5.

succeeded to complete action update on media

in guest cdrom opened

 

6.

succeeded to complete action eject on media

in guest cdrom ejected

 

7.

succeeded to complete action update on media

Domain rhel6u2 destroyed

...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/aaa.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
...

 

 

8.

 

succeeded to complete action update on media

 

... ...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/bbb.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
...

 

 

 
Notes:
Comments:

		177740 	[update device flags] virsh can't detach virtio-scsi CD-ROM (passed through from host) - Bug 809783 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    809783 - From Run 46172
    809783 - From Run 49499
    809783 - From Run 50422
    809783 - From Run 54154

Actions:

1. Fresh install the guest(should be 6.3 or later), then shutdown it and add the CDROM
by 'virsh edit' command, and the CDROM XML is:
#cat cdrom.xml
<disk type='block' device='lun'>
  <driver name='qemu' type='raw'/>
  <source dev='/dev/cdrw'/>
  <target dev='sdc' bus='scsi'/>
  <serial>scsi-lun-1</serial>   
</disk>

2. Change the boot disk to scisi bus:

 <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/test4.img'/>
      <target dev='sda' bus='scsi'/>
      <alias name='scsi0-0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>

ï»¿and also edit the scsi controller, just add model="virtio-scsi": 

<controller type="scsi" index="0" model="virtio-scsi">

3. Make sure there's DVD in the host's CDROM, and start the guest.
4. Log in the guest, and run#scsi-rescan(need install sg3_utils package)
5. On the host, hotunplug the CDROM device by:
# virsh detach-disk test4 sdc
Disk detached successfully
6. Check again in the guest.

 
	
Expected Results:

4. in guest,

# scsi-rescan 
Host adapter 0 (ata_piix) found.
Host adapter 1 (ata_piix) found.
Host adapter 2 (virtio_scsi) found.
Scanning SCSI subsystem for new devices
Scanning host 0 for  SCSI target IDs  0 1 2 3 4 5 6 7, all LUNs
Scanning host 1 for  SCSI target IDs  0 1 2 3 4 5 6 7, all LUNs
Scanning host 2 for  SCSI target IDs  0 1 2 3 4 5 6 7, all LUNs
Scanning for device 2 0 0 0 ...
OLD: Host: scsi2 Channel: 00 Id: 00 Lun: 00
      Vendor: QEMU     Model: QEMU HARDDISK    Rev: 0.12
      Type:   Direct-Access                    ANSI SCSI revision: 05
Scanning for device 2 0 1 0 ...
OLD: Host: scsi2 Channel: 00 Id: 01 Lun: 00
      Vendor: PLDS     Model: DVD+-RW DH-16AAS Rev: JD12
      Type:   CD-ROM                           ANSI SCSI revision: 05
0 new device(s) found.               
0 device(s) removed.         

6. should NO CDROM detected.

 
Notes:
Comments:

		177747 	[virsh cmd]Some issues about virsh -h usage BZ 817244 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh cmd
    Regression

bug:

    No bug found

Actions:

1. #man virsh
Man page:
-h, --help
Ignore all other arguments, and behave as if the help command were
           given instead.


2. # virsh -h -v -V
0.9.10

3. # virsh -h -V -v
Virsh command line tool of libvirt 0.9.10
See web site at http://libvirt.org/

Compiled with support for:
 Hypervisors: QEmu/KVM LXC ESX Test
 Networking: Remote Daemon Network Bridging Netcf Nwfilter VirtualPort
 Storage: Dir Disk Filesystem SCSI Multipath iSCSI LVM
 Miscellaneous: SELinux Secrets Debug DTrace Readline

	
Expected Results:

Step2 and Step3, -h should ignore all other arguments.

BZ https://bugzilla.redhat.com/show_bug.cgi?id=817244 is not fixed. so set it to NEEDUPDATE

 

 

MOVE IT TO test plan 5067
Notes:
Comments:

		177760 	[virtio-serial]libvirtd dead when create a guest with "--channel pty,target_type=virtio" by virt-install BZ#790745 	whuang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtio-serial

bug:

    No bug found

Actions:

1). Create a guest by virt-install:
# virt-install -n demo1 -r 1024 --disk
path=/var/lib/libvirt/images/demo1.img,size=5 --channel pty,target_type=virtio
-l
http://fileshare.englab.nay.redhat.com/pub/redhat/rhel/rel-eng/RHEL-6.2/RHEL6.2-20111117.0/x86_64/os/
--debug


	
Expected Results:

No show error  like this and  make sure libvirtd is still alive 

Actual results:
Wed, 15 Feb 2012 02:42:09 ERROR    End of file while reading data: Input/output
error
Wed, 15 Feb 2012 02:42:09 DEBUG    Traceback (most recent call last):
  File "/usr/sbin/virt-install", line 629, in start_install
    noboot=options.noreboot)
  File "/usr/lib/python2.6/site-packages/virtinst/Guest.py", line 1223, in
start_install
    noboot)
  File "/usr/lib/python2.6/site-packages/virtinst/Guest.py", line 1291, in
_create_guest
    dom = self.conn.createLinustart_xml or final_xml, 0)
  File "/usr/lib64/python2.6/site-packages/libvirt.py", line 2413, in
createLinux
    if ret is None:raise libvirtError('virDomainCreateLinu) failed', conn=self)
libvirtError: End of file while reading data: Input/output error
Wed, 15 Feb 2012 02:42:09 DEBUG    Domain installation does not appear to have
been successful.
If it was, you can restart your domain by running:
  virsh --connect qemu:///system start demo1
otherwise, please restart your installation.
Domain installation does not appear to have been successful.
If it was, you can restart your domain by running:
  virsh --connect qemu:///system start demo1
otherwise, please restart your installation.

Check the libvirtd status:
# service libvirtd status
libvirtd dead but pid file exists

Notes:
Comments:

		177761 	[Virtio] control the event index support in virtio and vhost-net - BZ#725448 	nzhang 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

run the following command first

#lsmod|grep vhost_net

if there isn't any result, then run the following command please:

   #modprobe vhost_net
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks
    virtual networks
    Regression

bug:

    No bug found

Actions:

1. Set the value of event_idx in guest, make sure the disk and network model type is "virtio".
# virsh dumpxml <domain>
...
    <disk type='file' device='disk'>
      <driver name='qemu' event_idx='on' .../>
      ...
       <target dev='vda' bus='virtio'/>
      ...
    </disk>
    <interface type='network'>
      <driver name='vhost' event_idx='off' .../>
      ...
      <model type='virtio'/>
      ...
    </interface>
...

2. View the process of qemu-kvm.
# ps -ef|grep qemu-kvm
...
-device virtio-blk-pci,event_idx=on
...
-device virtio-net-pci,event_idx=off

 
	
Expected Results:

Check the qemu-kvm process if there are the parameters [event_idx=on/off] was set.
Notes:
Comments:

		177776 	[Virtio] Support for KVM virtio console 	nzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Prepare a exsitent guest, and make it in shutdown status.# virsh list --all
 Id Name                 State
----------------------------------
  - <domain>                 shut off
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices

bug:

    No bug found

Actions:

# virsh edit <domain>
...
    <console type='pty'>
      <target type='virtio' port='0'/>
    </console>
...

# virsh start <domain>

# ps -ef | grep qemu-kvm
...
-chardev pty,id=charconsole0 -device virtconsole,chardev=charconsole0,id=console0
...
	
Expected Results:
Notes:
Comments:

		177777 	[Virtio] Support for setting IOeventFD for virtio block 	nzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Prepare a exsitent guest, and make it in shutdown status.# virsh list --all
 Id Name                 State
----------------------------------
  - <domain>                 shut off
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

# virsh edit <domain>
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none' ioeventfd='on'/>
      <source file='/var/lib/libvirt/images/test.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
...

# virsh start <domain>

# ps -ef | grep qemu-kvm
...
-drive
file=/var/lib/libvirt/images/test.img,if=none,id=drive-virtio-disk0,format=raw,cache=none
-device
virtio-blk-pci,ioeventfd=on,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1
...

	
Expected Results:
Notes:
Comments:

		177778 	[Virtio] Support for virtio-net tuning option 	nzhang 	None 	Manual 		Function 	P2 	None 	Edit
Setup:

Prepare a exsitent guest, and make it in shutdown status.

# virsh list --all
 Id Name                 State
----------------------------------
  - <domain>                 shut off
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. # virsh edit <domain>, and add <driver> tag like the following.

    <interface type='network'>
      <mac address='52:54:00:68:aa:2e'/>
      <source network='default'/>
      <model type='virtio'/>
      <driver txmode='iothread'/>
      <tune>
        <sndbuf>1024</sndbuf>
      </tune>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>

# virsh start <domain>

# ps -ef|grep qemu-kvm
... -device virtio-net-pci,tx=bh, ...


2. # virsh edit <domain>, and add <driver> tag like the following.

    <interface type='network'>
      <mac address='52:54:00:68:aa:2e'/>
      <source network='default'/>
      <model type='virtio'/>
      <driver txmode='timer'/>
      <tune>
        <sndbuf>1024</sndbuf>
      </tune>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>

# virsh start <domain>

# ps -ef|grep qemu-kvm
... -device virtio-net-pci,tx=timer, ...

	
Expected Results:
Notes:
Comments:

		177779 	[virtio]Controllers do not support virsh attach/detach-device --persistent BZ#804601 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtio

bug:

    No bug found

Actions:

1. Start a guest named rhel6u3
2. Prepare a controller xml
# cat controller.xml 
<controller type='scsi' model='virtio-scsi'/>

3. Attach controller persistently 
# virsh attach-device --persistent rhel6u3 controller.xml 

4. Check domain xml
# virsh dumpxml rhel6u3
# virsh dumpxml rhel6u3 --inactive 

5. Attach scsi disk in the guest
# cat scsi-disk.xml
 <disk type='file' device='disk'>
 <driver name='qemu' type='raw' cache='none'/>
 <source file='/var/lib/libvirt/images/local.img'/>
 <target dev='sdb' bus='scsi'/>
  </disk>
# virsh attach-device rhel6u3 scsi-disk.xml 

6. Detach controller
# virsh detach-device --persistent rhel6u3 controller.xml 

7. Detach controller after detach scsi disk
# virsh detach-device rhel6u3 scsi-disk.xml 

# virsh detach-device --persistent rhel6u3 controller.xml 

8. Check domain xml
# virsh dumpxml rhel6u3 

# virsh dumpxml rhel6u3 --inactive 

 

	
Expected Results:

Step 3

Device attached successfully

Step 4
a. 
...
 <controller type='scsi' index='0' model='virtio-scsi'> 
...
b. 

...
 <controller type='scsi' index='0' model='virtio-scsi'> 
...

Step 5

Device attached successfully

Step 6

error: Failed to detach device from controller.xml
error: operation failed: device cannot be detached: device is busy

Step 7

a. Device detached successfully

b. Device detached successfully

 

Step 8

a. no scsi controller

b. no scsi controller

 

 



Notes:
Comments:

		177780 	[sVirt][FEAT RHEL6.3]: Grant KVM guests retain arbitrary capabilitiesBZ#767425 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

 

 


set /etc/libvirt/qemu.conf

 user = "root" 
group = "root" 
clear_emulator_capabilities  as default   Do not edit it 

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt

bug:

    No bug found

Actions:

clean you audit.log 

# > /var/log/audit/audit.log


#sdb is another physical disk 
1) #virsh attach-disk VM /dev/sdb vdb --driver qemu --type lun --rawio
--persistent
 

NOTE:if there is not AVC error in your audit.log you should skip step 2 .

# grep  AVC /var/log/audit/audit.log
# 

 

2)SELinux policy to grant qemu sys_rawio,then start a guest 
if there is any AVC type in /var/log/audit/audit.log  about sys_rawio 

#grep  AVC /var/log/audit/audit.log | audit2allow -M mypol


#semodule -i mypol.pp

then run virsh cmd again

3) then I check it proc in the host 
#cat /proc/`pidof qemu-kvm`/status



	
Expected Results:

 there are some infomation like this :

Name: qemu-kvm
State: S (sleeping)
Tgid: 26845
Pid: 26845
PPid: 1
TracerPid: 0
Uid: 0 0 0 0
Gid: 0 0 0 0
Utrace: 0
FDSize: 64
Groups: 0 1 2 3 4 6 10 
VmPeak:  1707308 kB
VmSize:  1460296 kB
VmLck:        0 kB
VmHWM:   282092 kB
VmRSS:   279940 kB
VmData:  1318484 kB
VmStk:       88 kB
VmExe:     2436 kB
VmLib:    16824 kB
VmPTE:      540 kB
VmSwap:        0 kB
Threads: 2
SigQ: 2/62761

...
CapPrm: fffffffc00020000
 CapEff: fffffffc00020000 
CapBnd: fffffffc00020000
....

 
Notes:
Comments:

		177781 	[Virtual disks] Attach a disk with specific name - bug 829246 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. prepare a running guest

2. perpare 2 images in /var/lib/libvirt/images

# qemu-img create /var/lib/libvirt/images/local.img 1G

# qemu-img create /var/lib/libvirt/images/attch.img 1G

NOTE: The dir and image names must be the same as above list, but not defined freely
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Attach a disk with specific name

# virsh attach-disk guest /var/lib/libvirt/images/local.img vdb

2. Detach the same disk

# virsh detach-disk guest vdb

3. Repeat the above steps with image " /var/lib/libvirt/images/attch.img"
	
Expected Results:

1. Disk can be attached successfully

check in xml and login to the guest, can see the disk

2. Disk can be detached successfully

check in xml and login to the guest, can not see the disk anymore

3. The result should be similar with step 1&2 result
Notes:
Comments:

		177782 	[virtual disks] Attach scsi disks with wwn and serial numbers - bug 831099 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Install sg3_utils package  in guest
	
Breakdown:

 

	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. edit guest xml, adding serial and wwn to scsi disk and cdrom

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/scsi.img'/>
      <target dev='sda' bus='scsi'/>           
     <serial>cdcdcd</serial>
     <wwn>5000c60016ea71ad</wwn>     
  </disk>
 <disk type='file' device='cdrom'>
    <driver name='qemu' type='raw'/> 
    <source file='/var/lib/libvirt/images/images/tt.img'/>
    <target dev='sdc' bus='scsi'/> 
    <readonly/>
    <serial>aababab</serial> 
   <wwn>5001c60016ea71ad</wwn>
</disk>    
 <controller type='scsi' index='0' model='virtio-scsi'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
 </controller>

 

2. Start guest

3. login guest and check

# sg_inq -p 0x83 /dev/sda

# sg_inq -p 0x83 /dev/sr0
	
Expected Results:

Step 3

# sg_inq -p 0x83 /dev/sda 
VPD INQUIRY: Device Identification page
  Designation descriptor number 1, descriptor length: 10
    designator_type: vendor specific [0x0],  code_set: ASCII
    associated with the addressed logical unit
 vendor specific: cdcdcd
  Designation descriptor number 2, descriptor length: 12
    designator_type: NAA,  code_set: Binary
    associated with the addressed logical unit
      NAA 5, IEEE Company_id: 0xc60
      Vendor Specific Identifier: 0x16ea71ad
 [0x5000c60016ea71ad]

 

# sg_inq -p 0x83 /dev/sr0 
VPD INQUIRY: Device Identification page 
Designation descriptor number 1, descriptor length: 11 
  designator_type: vendor specific [0x0], code_set: ASCII 
  associated with the addressed logical unit 
    vendor specific: aababab 
Designation descriptor number 2, descriptor length: 12 
  designator_type: NAA, code_set: Binary 
  associated with the addressed logical unit 
    NAA 5, IEEE Company_id: 0x1c60 
    Vendor Specific Identifier: 0x16ea71ad 
    [0x5001c60016ea71ad] 



Notes:
Comments:

		177785 	[Virtual disks] Entry into S4 state after hotplug disks - bug 808463 	weizhan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

1. Install and start a guest with xml

    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/rhel6.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>

 If your seabios version is higher than 0.6.1.2-18, still need to add the following Bold xml to guest

  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <loader>/usr/share/seabios/bios-pm.bin</loader>
    <boot dev='cdrom'/>
  </os>

2. Install qemu-guest-agent in guest

3. Start qemu-ga service with

# /usr/bin/qemu-ga start
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks
    rhel6.5

bug:

    No bug found

Actions:

1. Attach a disk to above running guest

# virsh attach-disk guest /var/lib/libvirt/images/tmp.img vda

2. Do

# virsh dompmsuspend guest --target disk

3. Check the disk in domain xml
	
Expected Results:

Step 3

Disk still exist

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/var/lib/libvirt/images/tmp.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>

Notes:
Comments:

		177786 	[Virtual disks] Hotplug/unplug Hotplug/unplug disk with scsi bus type 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare partition /dev/sdc (or other block disks depends on your host)

2. Install sg3_utils package on guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. Prepare a guest with the following controller

<controller type="scsi" index="0" model="virtio-scsi"/>

2. Start the guest and update the guest kernel to newest version

3. Prepare scsi-disk.xml

<disk type='block' device='lun'>
  <driver name='qemu' type='raw'/>
  <source dev='/dev/sdc'/>
  <target dev='sdb' bus='scsi'/>
</disk>

5. Do hotplug

# virsh attach-device guest scsi-disk.xml

6. Check the device on guest

find which is virtio-scsi host

# cat /sys/class/scsi_host/host[N]/proc_name

scan this host

# echo "- - -" > /sys/class/scsi_host/host[N]/scan

find the scsi disk

# scsi-rescan

# fdisk -l

7. Detach disk

# virsh detach-device guest scsi-disk.xml

 

 
	
Expected Results:

Step 5

Attach device should succeed without error

Step 6

You can find the device sdb after # fdisk -l

Step 7

Device should be removed, there is no sdb in guest
Notes:
Comments:

		177787 	[Virtual disks] Support disk file name with comma - bug 801970 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Start a guest with

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/tt,tt.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>

2. Check qemu command

# ps aux |grep <domain name>
	
Expected Results:

Step 1

Domain started without error

Step 2

Should have

-drive file=/var/lib/libvirt/images/tt,,tt.img,if=none,id=drive-ide0-0-0,format=raw,cache=none 

 
Notes:
Comments:

		177788 	[Virtual disks] Support virtio-scsi disk - bug 782034, 801772 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Prepare  /dev/sdb (or other block disks)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Start guest with 

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6u2.qcow2'/>
      <target dev='sda' bus='scsi'/>
      <alias name='scsi0-0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <disk type='block' device='lun'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/dev/sdb'/>
      <target dev='sdb' bus='scsi'/>
      <alias name='scsi0-0-0-1'/>
      <address type='drive' controller='0' bus='0' target='0' unit='1'/>
    </disk>
   <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/boot.iso'/>
      <target dev='sdc' bus='scsi'/>
      <readonly/>
      <alias name='scsi0-0-1-0'/>
      <address type='drive' controller='0' bus='0' target='1' unit='0'/>
    </disk>
    <controller type='scsi' index='0' model='virtio-scsi'>
      <alias name='scsi0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </controller>


2. Update the guest kernel to the newest one if you want to find the disk in guest

3. Start guest with 

    <disk type='block' device='lun'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/dev/sdb'/>
      <target dev='sda' bus='usb'/>
    </disk>

    <controller type='scsi' index='0' model='virtio-scsi'>       <alias name='scsi0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06'
function='0x0'/>
    </controller>

 

 

	
Expected Results:

Step1:

Guest can be started successfullty

Check the qemu command

-drive file=/var/lib/libvirt/images/rhel6u2.qcow2,if=none,id=drive-scsi0-0-0-0,format=qcow2,cache=none -device scsi-hd,bus=scsi0.0,channel=0,scsi-id=0,lun=0,drive=drive-scsi0-0-0-0,id=scsi0-0-0-0,bootindex=1 
-drive file=/dev/sdb,if=none,id=drive-scsi0-0-0-1,format=raw,cache=none -device scsi-block,bus=scsi0.0,channel=0,scsi-id=0,lun=1,drive=drive-scsi0-0-0-1,id=scsi0-0-0-1
-drive file=/var/lib/libvirt/images/boot.iso,if=none,id=drive-scsi0-0-1-0,readonly=on,format=raw -device scsi-cd,bus=scsi0.0,channel=0,scsi-id=1,lun=0,drive=drive-scsi0-0-1-0,id=scsi0-0-1-0

Step 2:

   you can find these 2 disks in the guest like sdb and sdc

Step 3:

  Should report error

error: Failed to start domain XX
error: unsupported configuration: disk device='lun' is not supported for
bus='usb'

 
Notes:
Comments:

		177789 	[Virtual disks]Attach disk with --cache - bug 829562 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. Prepare a running guest

2. Generate 7 disk images for attaching

# qemu-img create /var/lib/libvirt/images/disk1.img

# mkfs.ext3 /var/lib/libvirt/images/disk1.img

# for i in {2..7}; do cp /var/lib/libvirt/images/disk1.img /var/lib/libvirt/images/disk$i.img; done
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Attach disk with --cache

# virsh attach-disk guest /var/lib/libvirt/images/disk1.img vdb --cache none

# virsh attach-disk guest /var/lib/libvirt/images/disk2.img vdc --cache default

# virsh attach-disk guest /var/lib/libvirt/images/disk3.img vdd --cache writethrough

# virsh attach-disk guest /var/lib/libvirt/images/disk4.img vde --cache writeback

# virsh attach-disk guest /var/lib/libvirt/images/disk5.img vdf --cache unsafe

# virsh attach-disk guest /var/lib/libvirt/images/disk6.img vdg --cache directsync

2. Attach disk with abnormal parameter of --cache

# virsh attach-disk guest /var/lib/libvirt/images/disk7.img vdh --cache sdkfhskhf
	
Expected Results:

1. Check the xml of guest

# virsh dumpxml guest

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/var/lib/libvirt/images/disk1.img'/>
      <target dev='vdb' bus='virtio'/>
    </disk>

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' />
      <source dev='/var/lib/libvirt/images/disk2.img'/>
      <target dev='vdc' bus='virtio'/>
    </disk>

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='writethrough'/>
      <source dev='/var/lib/libvirt/images/disk3.img'/>
      <target dev='vdd' bus='virtio'/>
    </disk>

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='writeback'/>
      <source dev='/var/lib/libvirt/images/disk4.img'/>
      <target dev='vde' bus='virtio'/>
    </disk>

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='unsafe'/>
      <source dev='/var/lib/libvirt/images/disk5.img'/>
      <target dev='vdf' bus='virtio'/>
    </disk>

For # virsh attach-disk guest /var/lib/libvirt/images/disk6.img vdg --cache directsync, it qemu not supported, it will report error

error: Failed to attach disk
error: unsupported configuration: disk cache mode 'directsync' is not supported by this QEMU

else, it will succeed and can see the following

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='directsync'/>
      <source dev='/var/lib/libvirt/images/disk6.img'/>
      <target dev='vdg' bus='virtio'/>
    </disk>

login guest to check that all the disks exists

2. Should report error

error: Failed to attach disk
error: internal error unknown disk cache mode 'sdkfhskhf'
Notes:
Comments:

		177818 	[virtual networks] Restart libvirtd after create networks - bug 819416 	weizhan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    rhel6.5

bug:

    No bug found

Actions:

1. Create a network with xml
<network>
   <name>fp_commission</name>
</network>

2.  list all network

 # virsh net-list --all

Name                 State      Autostart
-----------------------------------------
default              active     yes      
fp_commission        active     no       

# brctl show
bridge name    bridge id        STP enabled    interfaces
virbr0        8000.52540040e79a    yes        virbr0-nic
virbr1        8000.525400f2c450    yes        virbr1-nic

3.  restart libvirtd

# service libvirtd restart

4. virsh net-list --all

Name                 State      Autostart
-----------------------------------------
default              active     yes      

5. brctl --show



	
Expected Results:

step 5

should only show

# brctl show
bridge name    bridge id        STP enabled    interfaces
virbr0        8000.52540040e79a    yes        virbr0-nic
Notes:
Comments:

		177826 	[virtual networks]Libvirt with NAT setup nukes port forwarding into the internal network BZ#812441 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

 

Make sure this bug have been fixed 

https://bugzilla.redhat.com/show_bug.cgi?id=812441

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    upstream

bug:

    No bug found

Actions:

1) get a running guest   NAT virtual network , and dhcp get its IP  like : 192.168.122.10 

 

2)  run iptables cmd  

 

#iptables -A PREROUTING -p tcp -i eth0 -d $PUBLIC_IP --dport 80 -j DNAT --to
192.168.122.10:80

#iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

#iptables -A FORWARD -p tcp -i eth0 -d 192.168.122.10 --dport 80 -j ACCEPT

#iptables -A FORWARD -p tcp -i eth0 -d 192.168.122.10 --dport 22 -j ACCEPT

#iptables -A INPUT -j REJECT --reject-with icmp-host-prohibited

3) restart libvirtd    

#/etc/init.d/libvirtd restart 

then check iptables 

# iptables -L FORWARD -v 

 

 

	
Expected Results:

OUTPUT DO NOT like this : 

 

Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination 
    0     0 ACCEPT     all  --  any    virbr0  anywhere            
192.168.122.0/24    state RELATED,ESTABLISHED 
    0     0 ACCEPT     all  --  virbr0 any     192.168.122.0/24     anywhere    
    0     0 ACCEPT     all  --  virbr0 virbr0  anywhere             anywhere    
 0 0 REJECT all -- any virbr0 anywhere anywhere reject-with icmp-port-unreachable 

   0     0 REJECT     all  --  virbr0 any     anywhere             anywhere           reject-with icmp-port-unreachable 
    1    48 ACCEPT     tcp  --  eth0   any     anywhere             dev-virt-1         tcp dpt:http 
 1 60 ACCEPT tcp -- eth0 any anywhere dev-virt-1 tcp dpt:ssh 

 

Notes:
Comments:

    #1 ydu@redhat.com 2012-06-29 13:31:20
    Bug still not fixed, and the Expected Results should be updated, "NOT like" should be avoided. So move to NEED_UPDATE

		177852 	[Watchdog device] Watchdog device "i6300esb" - dump 	ydu 	None 	Auto 		Feature 	P3 	None 	Edit
Setup:

1. Prepare a guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh-rail
    watchdog
    watchdog device

bug:

    No bug found

Actions:

1. Edit the config file /etc/libvirt/qemu.conf and uncomment the following line:

auto_dump_path = "/var/lib/libvirt/qemu/dump"


2. Add a watchdog device through edit the guest's XML file, and start the guest.

<watchdog model='i6300esb' action='dump'/>

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, check the watchdog dirvier is installed:

# dmesg | grep i6300

# /sbin/lsmod | grep i6300esb

# lspci

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 100
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest and check if there's a dump file in "/var/lib/libvirt/qemu/dump"
	
Expected Results:

1.

2.

3. there are some values about watchdog device, like:

  -device i6300esb

  -watchdog-action pause                      *QEMU has no support for a 'dump' action*

4. The output is similar to the following:

# dmesg | grep i6300

i6300ESB timer: Intel 6300ESB WatchDog Timer Driver v0.04
i6300ESB timer: initialized (0xffffc90001218000). heartbeat=30 sec (nowayout=0)

# /sbin/lsmod | grep i6300esb

i6300esb     5603     0

#lspci

...

00:05.0 System peripheral: Intel Corporation 6300ESB Watchdog Timer

5.watchdong software is installed successfully.

6.

7.

8. Guest still running and there is a dump file of the guest in "/var/lib/libvirt/qemu/dump"

#cd /var/lib/libvirt/qemu/dump

#ll
total 279684
-rw-------. 1 root root 286391153 Jul 25 10:50 test-1311562222
Notes:
Comments:

		183803 	[libvirt domain event handler] Bug 839661 - libvirt: support QMP event for S4 - Bug 872420 878966 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Bug 878966 - virsh setmem then dompmsuspend to disk will hang forever

Bug 872420 - virsh setmem then dompmsuspend to disk will hang forever

Bug 839661 - libvirt: support QMP event for S4   

depends on :
 Bug 827499 - RFE: QMP notification for S3/S4 events

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    domain event handler

bug:

    No bug found

Actions:

open 2 console

console 1 :

python /usr/share/doc/libvirt-python-0.9.10/events-python/event-test.py
Using uri:qemu:///system

console 2 :

add this in guest

...

 <pm>
    <suspend-to-mem enabled='yes'/>
    <suspend-to-disk enabled='yes'/>
  </pm>
...

....

 <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/demo.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
....

 

1) install a guest

2) login the guest

3)  do S4  (suspend to disk) then check the event handler

#virsh dompemsuspend $domain disk

 

 

 
	
Expected Results:

console 2:

3)  there should be some info about QMP event about S4 like :

myDomainEventCallback1 EVENT: Domain bug(24) PMSuspended Disk
myDomainEventCallback2 EVENT: Domain bug(24) PMSuspended Disk
myDomainEventCallback1 EVENT: Domain bug(24) Shutdown Finished
myDomainEventCallback2 EVENT: Domain bug(24) Shutdown Finished
myDomainEventCallback1 EVENT: Domain bug(24) Stopped Shutdown
myDomainEventCallback2 EVENT: Domain bug(24) Stopped Shutdown

Notes:
There is a related bug 872420, so changing to NEED_UPDATE by bili

Bug 872420 is move to pm-utils, bug 878966 clone from it.-- zhpeng
There is a related bug 878966, so keep NEED_UPDATE -- zhpeng
Comments:

		183804 	[configuration] can not start vdsmd service after update the libvirt packages - bug 837485 	bili 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Prepare a host with healthy vdsm and libvirt packages.

Case need to be confirmed after bug 837485 is fixed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration
    Regression

bug:

    No bug found

Actions:

1. Upgrade libvirt packages (just install the requisite packages which has dependency problems) to the newest version:

# rpm -Uvh libvirt-*xxxx.el6.x86_64.rpm

2. Restart libvirtd:

# service libvirtd restart
Stopping libvirtd daemon: libvirtd: libvirtd is managed by upstart and started, use initctl instead

3. Restart vdsmd:

# service vdsmd restart

	
Expected Results:

Step 3:

Should succeed and not get error like:

Shutting down vdsm daemon:
vdsm watchdog stop                                         [  OK  ]
vdsm stop                                                  [  OK  ]
Stopping libvirtd daemon: libvirtd: libvirtd is managed by upstart and started, use initctl instead
vdsm: libvirt already configured for vdsm                  [  OK  ]
Starting wdmd...
Starting wdmd:                                             [  OK  ]
Starting sanlock...
Starting sanlock:                                          [  OK  ]
Starting iscsid:
diff: : No such file or directory /bin/cp: cannot stat `': No such file or directory vdsm: one of the dependent services did not start, error co[FAILED]

 
Notes:
Comments:

		183805 	[libvirt domain event handler]Bug 834927 - virConnectDomainEventRegisterAny won't register the same callback for the same event but for different domains 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

#cat repro.c 


#include <libvirt/libvirt.h>
#include <libvirt/virterror.h>
#include <stdio.h>
#include <stdlib.h>
#include <signal.h>
#include <string.h>

int run = 1;

static void stop(int sig)
{
    printf("Exiting on signal %d\n", sig);
    run = 0;
}

const char *eventToString(int event) {
    const char *ret = "";
    switch ((virDomainEventType) event) {
        case VIR_DOMAIN_EVENT_DEFINED:
            ret ="Defined";
            break;
        case VIR_DOMAIN_EVENT_UNDEFINED:
            ret ="Undefined";
            break;
        case VIR_DOMAIN_EVENT_STARTED:
            ret ="Started";
            break;
        case VIR_DOMAIN_EVENT_SUSPENDED:
            ret ="Suspended";
            break;
        case VIR_DOMAIN_EVENT_RESUMED:
            ret ="Resumed";
            break;
        case VIR_DOMAIN_EVENT_STOPPED:
            ret ="Stopped";
            break;
        case VIR_DOMAIN_EVENT_SHUTDOWN:
            ret = "Shutdown";
            break;
    }
    return ret;
}

static const char *eventDetailToString(int event, int detail) {
    const char *ret = "";
    switch ((virDomainEventType) event) {
        case VIR_DOMAIN_EVENT_DEFINED:
            if (detail == VIR_DOMAIN_EVENT_DEFINED_ADDED)
                ret = "Added";
            else if (detail == VIR_DOMAIN_EVENT_DEFINED_UPDATED)
                ret = "Updated";
            break;
        case VIR_DOMAIN_EVENT_UNDEFINED:
            if (detail == VIR_DOMAIN_EVENT_UNDEFINED_REMOVED)
                ret = "Removed";
            break;
        case VIR_DOMAIN_EVENT_STARTED:
            switch ((virDomainEventStartedDetailType) detail) {
            case VIR_DOMAIN_EVENT_STARTED_BOOTED:
                ret = "Booted";
                break;
            case VIR_DOMAIN_EVENT_STARTED_MIGRATED:
                ret = "Migrated";
                break;
            case VIR_DOMAIN_EVENT_STARTED_RESTORED:
                ret = "Restored";
                break;
            case VIR_DOMAIN_EVENT_STARTED_FROM_SNAPSHOT:
                ret = "Snapshot";
                break;
            case VIR_DOMAIN_EVENT_STARTED_WAKEUP:
                ret = "Event wakeup";
                break;
            }
            break;
        case VIR_DOMAIN_EVENT_SUSPENDED:
            switch ((virDomainEventSuspendedDetailType) detail) {
            case VIR_DOMAIN_EVENT_SUSPENDED_PAUSED:
                ret = "Paused";
                break;
            case VIR_DOMAIN_EVENT_SUSPENDED_MIGRATED:
                ret = "Migrated";
                break;
            case VIR_DOMAIN_EVENT_SUSPENDED_IOERROR:
                ret = "I/O Error";
                break;
            case VIR_DOMAIN_EVENT_SUSPENDED_WATCHDOG:
                ret = "Watchdog";
                break;
            case VIR_DOMAIN_EVENT_SUSPENDED_RESTORED:
                ret = "Restored";
                break;
            case VIR_DOMAIN_EVENT_SUSPENDED_FROM_SNAPSHOT:
                ret = "Snapshot";
                break;
            }
            break;
        case VIR_DOMAIN_EVENT_RESUMED:
            switch ((virDomainEventResumedDetailType) detail) {
            case VIR_DOMAIN_EVENT_RESUMED_UNPAUSED:
                ret = "Unpaused";
                break;
            case VIR_DOMAIN_EVENT_RESUMED_MIGRATED:
                ret = "Migrated";
                break;
            case VIR_DOMAIN_EVENT_RESUMED_FROM_SNAPSHOT:
                ret = "Snapshot";
                break;
            }
            break;
        case VIR_DOMAIN_EVENT_STOPPED:
            switch ((virDomainEventStoppedDetailType) detail) {
            case VIR_DOMAIN_EVENT_STOPPED_SHUTDOWN:
                ret = "Shutdown";
                break;
            case VIR_DOMAIN_EVENT_STOPPED_DESTROYED:
                ret = "Destroyed";
                break;
            case VIR_DOMAIN_EVENT_STOPPED_CRASHED:
                ret = "Crashed";
                break;
            case VIR_DOMAIN_EVENT_STOPPED_MIGRATED:
                ret = "Migrated";
                break;
            case VIR_DOMAIN_EVENT_STOPPED_SAVED:
                ret = "Failed";
                break;
            case VIR_DOMAIN_EVENT_STOPPED_FAILED:
                ret = "Failed";
                break;
            case VIR_DOMAIN_EVENT_STOPPED_FROM_SNAPSHOT:
                ret = "Snapshot";
                break;
            }
            break;
        case VIR_DOMAIN_EVENT_SHUTDOWN:
            switch ((virDomainEventShutdownDetailType) detail) {
            case VIR_DOMAIN_EVENT_SHUTDOWN_FINISHED:
                ret = "Finished";
                break;
            }
            break;
    }
    return ret;
}
static int myDomainEventCallback2(virConnectPtr conn,
                                  virDomainPtr dom,
                                  int event,
                                  int detail,
                                  void *opaque)
{
    printf("%s EVENT: Domain %s(%d) %s %s\n", __func__, virDomainGetName(dom),
           virDomainGetID(dom), eventToString(event),
           eventDetailToString(event, detail));
    return 0;
}

static void myFreeFunc(void *opaque)
{
    char *str = opaque;
    printf("%s: Freeing [%s]\n", __func__, str);
    free(str);
}

int main(int argc, char *argv[]) {
    virConnectPtr conn = NULL;
    virDomainPtr dom1 = NULL, dom2 = NULL;
    int callback1ret = -1, callback2ret = -1;
    int ret = -1;
    struct sigaction action_stop;

    memset(&action_stop, 0, sizeof(action_stop));
    action_stop.sa_handler = stop;
    sigaction(SIGTERM, &action_stop, NULL);
    sigaction(SIGINT, &action_stop, NULL);

    virEventRegisterDefaultImpl();

    conn = virConnectOpenAuth(argc > 1 ? argv[1] : NULL, virConnectAuthPtrDefault, 0);

    if (!conn) {
       fprintf(stderr, "conn\n");
       return -1;
    }

    dom1 = virDomainLookupByName(conn, argc > 2 ? argv[2] : "f16");
    dom2 = virDomainLookupByName(conn, argc > 3 ? argv[3] : "f17");

    if (!dom1 || !dom2) {
        fprintf(stderr, "dom\n");
        goto cleanup;
    }

    callback1ret = virConnectDomainEventRegisterAny(conn,
                                                    dom1,
                                                    VIR_DOMAIN_EVENT_ID_LIFECYCLE,
                                                    VIR_DOMAIN_EVENT_CALLBACK(myDomainEventCallback2),
                                                    strdup("cb1"), myFreeFunc);
    callback2ret = virConnectDomainEventRegisterAny(conn,
                                                    dom2,
                                                    VIR_DOMAIN_EVENT_ID_LIFECYCLE,
                                                    VIR_DOMAIN_EVENT_CALLBACK(myDomainEventCallback2),
                                                    strdup("cb2"), myFreeFunc);

    if (callback1ret < 0) {
        fprintf(stderr, ":( cb1\n");
        goto cleanup;
    }

    if (callback2ret < 0) {
        fprintf(stderr, ":( cb2\n");
        goto cleanup;
    }

    if (virConnectSetKeepAlive(conn, 5, 3) < 0) {
        fprintf(stderr, "ka\n");
        goto cleanup;
    }

    while (run && virConnectIsAlive(conn) == 1) {
        if (virEventRunDefaultImpl() < 0) {
            fprintf(stderr, "event\n");
        }
    }


    ret = 0;

cleanup:
    if (callback1ret >= 0)
        virConnectDomainEventDeregisterAny(conn, callback1ret);
    if (callback2ret >= 0)
        virConnectDomainEventDeregisterAny(conn, callback2ret);

    if (dom1)
        virDomainFree(dom1);
    if (dom2)
        virDomainFree(dom2);

    if (conn)
        virConnectClose(conn);

    return ret;
}

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt domain event handler

bug:

    834927 - From Run 53038

Actions:

1)  gcc repro.c -o repro -lvirt 


2) chmod+x ;  ./repro   


3) open another console to do  dom virsh opertion  


NOTE: domain name should f16 and f17 or you need edit the C code instead your domain name 

do start/suspend/resume/destroy/save/restore than opertion 


	
Expected Results:

step2 )  Both calls should work and register the callback correctly.

 there is no error about register
Notes:
Comments:

		183806 	[snapshot] check snapshot-info BZ 842966 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

#rpm -q libvirt     version >= 0.9.13

libvirt-0.9.13-2.el6.x86_64
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1) create shutdown snapshot

[root@intel-q9400-4-2 ~]# virsh snapshot-create-as r7 s1
Domain snapshot s1 created

2) check it

[root@intel-q9400-4-2 ~]# virsh snapshot-info r7 s1
Name:           s1
Domain:         r7
Current:        yes
State:          shutoff
Parent:         -
Children:       0
Descendants:    0
Metadata:       yes

[root@intel-q9400-4-2 ~]# virsh start r7
Domain r7 started

3) create running  snapshot
[root@intel-q9400-4-2 ~]# virsh snapshot-create-as r7 s2-running
Domain snapshot s2-running created

4) check it
[root@intel-q9400-4-2 ~]# virsh snapshot-info r7 s2-running
Name:           s2-running
Domain:         r7
Current:        yes
State:          running
Parent:         s1
Children:       0
Descendants:    0
Metadata:       yes

[root@intel-q9400-4-2 ~]# rpm -q qemu-kvm-rhev
qemu-kvm-rhev-0.12.1.2-2.295.el6.x86_64

5) create disk snapshot

[root@intel-q9400-4-2 ~]# virsh snapshot-create-as r7 s3-disk --disk-only
Domain snapshot s3-disk created

6) check it

[root@intel-q9400-4-2 ~]# virsh snapshot-info r7 s3-disk
Name:           s3-disk
Domain:         r7
Current:        yes
State:          disk-snapshot
Parent:         s2-running
Children:       0
Descendants:    0
Metadata:       yes

Test is with 2 machine, one with old libvirt pkg
1) perpare a guest with snapshot in old server 
#rpm -q libvirt
libvirt-0.9.10-21.el6_3.3.x86_64

2) connect old server with new client
#rpm -q libvirt
libvirt-0.9.13-3.el6.x86_64

# virsh -c qemu+ssh://10.66.5.12/system
root@10.66.5.12's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # snapshot-info qcow2 --current
Name:           1319695362
Domain:         qcow2
Current:        yes
State:          running
Parent:         -
Children:       0
Descendants:    0
Metadata:       yes

virsh # snapshot-info qcow2 1319695362

Name:           1319695362
Domain:         qcow2
Current:        yes
State:          running
Parent:         -
Children:       0
Descendants:    0
Metadata:       yes

 

virsh # snapshot-info qcow2 
error: --snapshotname or --current is required

NO :error: unknown procedure: 272



	
Expected Results:

no error and  snapshot-info is currect  like  cmd output
Notes:
Comments:

		183817 	[Miscellanea] Upgrade libvirt packages cause guest show list wrong - bug 837787 	bili 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Bug 837787 - guest is shut down by upgrading libvirt

Case need to be confirmed after bug is fixed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea
    Regression

bug:

    No bug found

Actions:

1. Install libvirt-0.9.10-21.el6_3.1.x86_64 packages:
 # rpm -Uvh libvirt-*0.9.10-21.el6_3.1.x86_64.rpm
Preparing...                ########################################### [100%]
   1:libvirt-client         ########################################### [ 17%]
   2:libvirt                ########################################### [ 33%]
   3:libvirt-lock-sanlock   ########################################### [ 50%]
   4:libvirt-devel          ########################################### [ 67%]
   5:libvirt-python         ########################################### [ 83%]
   6:libvirt-debuginfo      ########################################### [100%]


2. # service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]
 
3. # virsh start test
Domain test started

4. # virsh list --all
 Id    Name                           State
----------------------------------------------------
 1     test                           running

5. Upgrade packages to libvirt-0.9.13-2.el6.x86_64
# rpm -Uvh /root/Downloads/libvirt-*0.9.13-2.el6.x86_64.rpm
Preparing...                ########################################### [100%]
   1:libvirt-client         ########################################### [ 13%]
   2:libvirt-daemon         ########################################### [ 25%]
   3:libvirt-docs           ########################################### [ 38%]
   4:libvirt-devel          ########################################### [ 50%]
   5:libvirt                ########################################### [ 63%]
   6:libvirt-lock-sanlock   ########################################### [ 75%]
   7:libvirt-python         ########################################### [ 88%]
   8:libvirt-debuginfo      ########################################### [100%]

6. # service libvirtd status
libvirtd (pid  24189) is running...

7. # virsh list

8. # service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

9. # virsh list --all

10. # ps aux | grep qemu-kvm

 

	
Expected Results:

Step 7: list should get the running guest's right statu:

# virsh list 
 Id    Name                           State
----------------------------------------------------
 1     test                           running

 

Step 9: list should get right status of guest (running not shut off).

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 1     test                           running

 

Step 10: qemu-kvm process is still running.

# ps aux | grep qemu-kvm 

qemu     12398 56.4  1.8 1821084 70164 ?       Sl   13:55   0:05 /usr/libexec/qemu-kvm -S -M rhel6.3.0 -enable-kvm -m 1024 -smp 4,sockets=4,cores=1,threads=1 -name test -uuid 65c542c7-daa6-56d7-450b-9d5ae55372eb -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/test.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x7 -drive file=/data/images/test.img,if=none,id=drive-virtio-disk0,format=raw,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=23,id=hostnet0,vhost=on,vhostfd=24 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:49:07:b8,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -device usb-tablet,id=input0 -vnc 127.0.0.1:0 -k en-us -vga qxl -global qxl-vga.vram_size=67108864 -device intel-hda,id=sound0,bus=pci.0,addr=0x4 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6

Notes:
Comments:

		183892 	[Migration] Migration with xml and persistent - bug 835300 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

 Prepare 2 hosts and setting the virt_use_nfs boolean on both sides, here do not need nfs

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. Install a guest and start on source, dump the guest xml to file

# virsh dumpxml guest > guest.xml

2. Change guest.xml with different image name, for example

change the

<source file='/var/lib/libvirt/migrate/guest.img'/>

to

<source file='/var/lib/libvirt/migrate/guest2.img'/>

3. Create empty image

# qemu-img create /var/lib/libvirt/migrate/guest2.img 8G

4. Do migration

# virsh migrate --live --persistent --copy-storage-all --xml guest.xml guest qemu+ssh://{target ip}/system

5. Check the inactive xml on target

# virsh dumpxml --inactive guest
	
Expected Results:

Step 4.

Migration should succeed without error

Step 5.

The image name should be

<source file='/var/lib/libvirt/migrate/guest2.img'/>
Notes:
bug still in assign status
Comments:

		184054 	[libvirtd] overriding the default configuration file 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Create the configration file for libvirtd(can copy the default one and edit it)

   # /etc/libvirt/libvirtd.conf /tmp/my-libvirtd.conf

edit the file, such as changing the default log file to anpther dir

   log_outputs="1:file:/tmp/libvirtd.log"

2. Stop libvirtd service

   # service libvirtd stop

3. start libvirtd by using the new ï»¿configration file to overriding the default one

   # libvirtd -f ï»¿ /tmp/my-libvirtd.conf

4. check the libvirtd process and the new configration entered into force

   # ps axu|grep libvirtd

   root     25314  1.0  0.2 513108  8988 pts/3    Sl+  14:00   0:00 libvirtd -f /tmp/my-libvirtd.conf

# ls /tmp/libvirtd.log

/tmp/libvirtd.log

ï»¿

	
Expected Results:

4. libvirtd can start successfully, and the new configration entered into force
Notes:
Comments:

		184055 	[libvirtd] overriding the default pid file 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Stop libvirtd service

   # service libvirtd stop

3. start libvirtd by giving a new pid file to overriding the default one

   # libvirtd -p ï»¿ /tmp/my-libvirtd.pid

4. check the libvirtd process and the pid file

  # ps aux|grep libvirtd

root     25796  1.0  0.2 513108  8992 pts/3    Sl+  14:26   0:00 libvirtd -p /tmp/my-libvirtd.pid

# cat /tmp/my-libvirtd.pid

25796

ï»¿

	
Expected Results:

4. libvirtd can start successfully, and the new pid file is correct.
Notes:
Comments:

		184058 	[lock manager] Sanlock locking failed for readonly devices - Bug 837659 	ydu 	ydu 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Make sure sanlock, libvirt-sanlock installed.

2. Make sure wdmd and sanock services are running.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    lock manager

bug:

    No bug found

Actions:

1. config the /etc/libvirt/qemu.conf and /etc/libvirt/qemu-sanlock.conf.
#egrep -v ^# /etc/libvirt/qemu.conf
lock_manager = "sanlock"

#egrep -v ^# /etc/libvirt/qemu-sanlock.conf
auto_disk_leases = 0
require_lease_for_disks = 0
2. setsebool virt_use_sanlock=on and restart libvirtd.
3. install a guest via virt-manager(or virt-install), the install method is CD-ROM.

 
	
Expected Results:

3.

No error like:

error : virCommandHandshakeWait:2503 : internal error unsupported configuration: Readonly leases are not supported

   and guest can install successfully.
Notes:
Comments:

		184112 	[Migration] Migration with copy-storage-inc 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts in /var/lib/libvirt/migrate (created by yourself), and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable , selinux on both sides

   # iptables -F

It has a bug Bug 822015 - libvirt can not create live snapshot( disk-only) if guest's image has backing file in the NFS server so that migration will come across the error like "permission deny", for work around, need to "setenforce 0"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Prepare an image with rhel6u3 guest installed with qcow2 type and put it in shared nfs

#qemu-img info /var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2

/var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2
file format: qcow2
virtual size: 20G (21474836480 bytes)
disk size: 2.9G
cluster_size: 65536

2. Create 2 image base on above image on both hosts in unshared dir with same name

# qemu-img create /var/lib/libvirt/images/rhel6u3-mig.qcow2 -f qcow2 -b /var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2

# qemu-img info  /var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2

image: /var/lib/libvirt/images/rhel6u3-mig.qcow2
file format: qcow2
virtual size: 20G (21474836480 bytes)
disk size: 136K
cluster_size: 65536
backing file: /var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2

3. Start a guest with /var/lib/libvirt/images/rhel6u3-mig.qcow2 on source host

# virsh start rhel6u3

4. Log into the guest, dd a big file with 100M

# dd if=/dev/zero of=test count=100000 bs=1024

check the image info

qemu-img info  /var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2image: /var/lib/libvirt/images/rhel6u3-mig.qcow2
file format: qcow2
virtual size: 20G (21474836480 bytes)
disk size: 134M
cluster_size: 65536
backing file: /var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2

5. Do migration with

# virsh migrate --live --copy-storage-inc rhel6u3 qemu+ssh://{target ip}/system
	
Expected Results:

Step 5

Migration succeed without error

check image info on target

qemu-img info -b /var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2image: /var/lib/libvirt/images/rhel6u3-mig.qcow2
file format: qcow2
virtual size: 20G (21474836480 bytes)
disk size: 186M
cluster_size: 65536
backing file: /var/lib/libvirt/migrate/RHEL-Server-6.3-64-virtio.qcow2

The disk size changed basically the increased size but not all
Notes:
Comments:

		184404 	[Stable Guest ABI]Check domain disk ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-disk.xml

2.Add a disk to the test-disk.xml

    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>

3.

#virsh migrate --live --xml test-disk.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

4.1 device='disk'     >  "Target disk device %s does not match source %s"

4.1.1dumpxml a new xml

#virsh dumpxml guest > test-disk1.xml

4.1.2 change the device type to others in test-disk1.xml

like  :  device='disk' change to device='cdrom'

4.1.3

#virsh migrate --live --xml test-disk1.xml guest qemu+ssh://10.66.5.120/system

4.2 bus='ide'            >  "Target disk bus %s does not match source %s"

4.3 target dev='hda' >  "Target disk %s does not match source %s"

4.4 serial  > "Target disk serial %s does not match source %s"

4.5 access mode > Target disk access mode does not match source"

 4.6.Also check address type and address elements   <address type='drive' controller='0' bus='1' target='0' unit='0'/>

4.6.1

  address type='drive' >Target device address type %s does not match source %s

  controller , bus or unit > "Target device drive address %d:%d:%d does not match source %d:%d:%d"

4.6.2

 Change your disk't type to virtio first

address type='virtio' > Target device address type %s does not match source %s

controller , bus or port > "Target device virtio serial address %d:%d:%d does not match source %d:%d:%d"

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 



	
Expected Results:

3.

error: unsupported configuration: Target domain disk count 3 does not match source 2

Notes:
Comments:

		184407 	[Stable Guest ABI]Check domain device info ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-info.xml

2.change address type in the xml

This depends on related bus , like
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>

change to

<address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>

3.

#virsh migrate --live --xml test-info.xml guest qemu+ssh://10.66.5.120:/system

4. Other elements need check and corresponding expected error message are below .

drive address > Target device drive address %d:%d:%d does not match source %d:%d:%d

pci address > Target device PCI address %04x:%02x:%02x.%02x does not match source %04x:%02x:%02x.%02x

virtio address > Target device virtio serial address %d:%d:%d does not match source %d:%d:%d

ccid address > Target device ccid address %d:%d does not match source %d:%d

Note:

Only one error will occur once whatever how many xml codes be changes ,

so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element.
	
Expected Results:

3.

error: unsupported configuration: Target device address type pci does not match source drive

Notes:
Comments:

		184410 	[Stable Guest ABI]Check domain controller ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-controller.xml

2.add a new controller to test-contoller.xml

 <controller type='ide' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>

3.

#virsh migrate --live --xml test-controller.xml guest qemu+ssh://10.66.5.120/system

4.Other elements NEED check and corresponding expected error message are below and please read note first.

 <controller type='virtio-serial' index='1'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0a' function='0x0'/>
 </controller>

4.1 controller type  > Target controller type %s does not match source %s

4.1.1 dumpxml a new xml

#virsh dumpxml guest > test-controller1.xml

4.1.2 change the controller type to others

controller tpye='ide' changes to conrtoller type='virtio-serial'

4.1.3

#virsh migrate --live --xml test-conrtoller1.xml qemu+ssh://10.66.5.120/system

4.2 index > Target controller index %d does not match source %d

4.3 model > Target controller model %d does not match source %d

4.4

<controller type='virtio-serial' index='0' ports='16' vectors='4'/>
4.4.1 port > Target controller ports %d does not match source %d
4.4.2 vectors > Target controller vectors %d does not match source %d

4.5 Also check address type and address elements   <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>

4.5.1

domain , bus, slot or function > "Target device PCI address %04x:%02x:%02x.%02x does not match source %04x:%02x:%02x.%02x

Note:

Because controller only support type pci  so that can't test controller address type now.


 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 

 
	
Expected Results:

3

Target domain disk controller count %d does not match source %d
Notes:
Comments:

    #1 bili@redhat.com 2012-09-11 17:20:52
    Should add anther non-use controller to test.
    And address type should be "pci" can not modify to others.

		184441 	[configuration] enable or disable s3/s4 per VMs 	dyuan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
VERIFIED Bug 836462 - Config s3/s4 per VM - in libvirt
WONTFIX Bug 836465 - Config s3/s4 per VM - in python-virtinst
WONTFIX Bug 836463 - Config s3/s4 per VM - in virt-manager
Bug 890648 virsh cmd hang when excute s3/s4 operation for the windows guest which running the guest agent service
Please test both Linux and windows guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration

bug:

    No bug found

Actions:

Linux Guest

0. the xml

 <pm> <suspend-to-mem enabled='yes'/> <suspend-to-disk enabled='yes'/>
 <suspend-to-hybrid enabled='yes'/>
 </pm>

depends on your machine support.

 

1.check the S3/S4 support in your machine

virsh # capabilities  (depends on what suspend type your machine support)
...
    <power_management>
      <suspend_mem/>
      <suspend_disk/>
      <suspend_hybrid/>
    </power_management>
...

2. defina a guest with S3/S4(with)

# virsh dumpxml myRHEL6
<domain type='kvm' id='1'>
  <name>myRHEL6</name>
  ......
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <loader>/usr/share/seabios/bios-pm.bin</loader>
    <boot dev='hd'/>
  </os>
  ......
  <pm>
    <suspend-to-mem enabled='yes'/>
    <suspend-to-disk enabled='yes'/>
    <suspend-to-hybrid enabled='yes'/>
 </pm>
  ......
  <devices>
  ......
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/myRHEL6.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
  ......
  </devices>
</domain>

 3. in guest install qemu-ga and start it.

4. # virsh dompmsuspend domain --target mem

5. # virsh dompmwakeup domain

6. # virsh dompmsuspend domain --target disk

7. # virsh start domain

8. # virsh dompmsuspend domain --target hybrid

9. # virsh dompmwakeup domain

 10. edit guest xml with: then restart guest.

  <pm>
    <suspend-to-mem enabled='no'/>
    <suspend-to-disk enabled='no'/>
    <suspend-to-hybrid enabled='no'/>
 </pm>

# virsh dompmsuspend myRHEL6 --target mem
error: Domain myRHEL6 could not be suspended
error: internal error S3 state is disabled for this domain

# virsh dompmsuspend myRHEL6 --target disk
error: Domain myRHEL6 could not be suspended
error: internal error S4 state is disabled for this domain

# virsh dompmsuspend myRHEL6 --target hybrid
error: Domain myRHEL6 could not be suspended
error: internal error S3 state is disabled for this domain 

 

Windows Guest:

1 Install the virtio-win-1.5.4-1.el6.noarch pkg to get the virtio-serial and spice+qxl drivers
The virtio-serial driver was in
# ls /usr/share/virtio-win/virtio-win-1.5.4.iso
/usr/share/virtio-win/virtio-win-1.5.4.iso

The spice+qxl driver was in
# ls /usr/share/virtio-win/   -----you need make a iso file for this directory
#mkisofs -o /var/lib/libvirt/images/virtiowin.iso /usr/share/virtio-win/

2 Prepare a windows guest with the virtio-serial and spice+qxl driver installed
# virsh dumpxml win7x86
  <domain type='kvm'>
  <name>win7-32</name>
  <uuid>ad61420e-b3c6-b50e-16ab-73009cbf9b6d</uuid>
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='i686' machine='rhel6.4.0'>hvm</type>
    <loader>/usr/share/seabios/bios.bin</loader>
    <boot dev='hd'/>
  </os>

---
  <pm>
    <suspend-to-mem enabled='yes'/>
    <suspend-to-disk enabled='yes'/>
  </pm>
---
  <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/win7-32.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>
    <graphics type='spice' autoport='yes'/>
    <video>
      <model type='qxl' vram='65536' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>

----

3 Install the qemu-guest-agent-win32-0.12.1.2-2.346.el6.x86_64 on a rhel host and get the executable
# ll /usr/share/qemu-kvm/qemu-ga-win32/
total 464
-rwxr-xr-x. 1 root root 467160 Dec 14 12:23 qemu-ga.exe
-r--r--r--. 1 root root   1155 Dec 14 12:16 README.txt

4 Install the qemu-ga service in guest
mkdir a folder named qemu-ga in the windows guest then put the qemu-ga.exe  and other three dll file which needed in README.TXT to the qemu-ga directory
#c:\qemu-ga> dir
qemu-ga.exe
iconv.dll
libglib-2.0.0.dll
libintl-8.dll
README.txt

#c:\qemu-ga\qemu-ga.exe --service install

5 Check the qemu-ga  service statu with the command  services.msc
#c:\services.msc
6 operation s3/s4 on the host
# virsh dompmsuspend win7 --target mem
Domain win7 successfully suspended
 # virsh dompmwakeup domain

7. # virsh dompmsuspend domain --target disk

. # virsh start domain

8. # virsh dompmsuspend domain --target hybrid

. # virsh dompmwakeup domain

9 edit guest xml with: then restart guest.

  <pm>
    <suspend-to-mem enabled='no'/>
    <suspend-to-disk enabled='no'/>
    <suspend-to-hybrid enabled='no'/>
 </pm>

 10.

# virsh dompmsuspend myRHEL6 --target mem
error: Domain myRHEL6 could not be suspended
error: internal error S3 state is disabled for this domain

# virsh dompmsuspend myRHEL6 --target disk
error: Domain myRHEL6 could not be suspended
error: internal error S4 state is disabled for this domain

# virsh dompmsuspend myRHEL6 --target hybrid
error: Domain myRHEL6 could not be suspended
error: internal error S3 state is disabled for this domain

 


	
Expected Results:

Linux and Windows

Step 4-9 success without error.

Step 10 fail

Libvirt will not hang or crash

 
Notes:
Add Windows part
Comments:

		184489 	[Stable Guest ABI]Check domain FS ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

TBD since qemu-rhev does not support virtFS.

 

 

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-fs.xml

 3.

#virsh migrate --live --xml test-controller.xml guest qemu+ssh://10.66.5.120/system

4.Other elements NEED check and corresponding expected error message are below and please read note first.

 

4.1 controller type  > Target controller type %s does not match source %s

4.1.1 dumpxml a new xml

#virsh dumpxml guest > test-controller1.xml

4.1.2 change the controller type to others

controller tpye='ide' changes to conrtoller type='virtio-serial'

4.1.3

#virsh migrate --live --xml test-conrtoller1.xml qemu+ssh://10.66.5.120/system

4.2 index > Target controller index %d does not match source %d

4.3 model > Target controller model %d does not match source %d

4.4

<controller type='virtio-serial' index='0' ports='16' vectors='4'/>
4.4.1 port > Target controller ports %d does not match source %d
4.4.2 vectors > Target controller vectors %d does not match source %d

4.5 Also check address type and address elements   <address type='drive' controller='0' bus='1' target='0' unit='0'/> you can refer https://tcms.engineering.redhat.com/case/184407/?from_plan=6578

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

          2. Check the element which existed in label <controller><controller/> , means if there are type drive and virtio

          then type ccid , pci can be ommited


 
	
Expected Results:

3

Target domain disk controller count %d does not match source %d
Notes:
Comments:

		184490 	[Stable Guest ABI]Check domain net ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-net.xml

2.Add a net card  to the test-net.xml

  <interface type='network'>
      <mac address='52:54:00:02:84:73'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

3.

#virsh migrate --live --xml test-net.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

    <interface type='network'>
      <mac address='52:54:00:02:84:73'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <model type='e1000'/>
      <alias name='net0'/>
    <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

4.1 mac address    >  Target network card mac %02x:%02x:%02x:%02x:%02x:%02x" "does not match source%02x:%02x:%02x:%02x:%02x:%02x

4.1.1dumpxml a new xml

#virsh dumpxml guest > test-net1.xml

4.1.2 change the device type to others in test-disk1.xml

like  :  52:54:00:02:84:73 --> 52:54:00:02:84:00

4.1.3

#virsh migrate --live --xml test-net1.xml guest qemu+ssh://10.66.5.120/system

4.2 model type='e1000'      >  "Target disk bus %s does not match source %s"

4.3Also check address type and address elements   <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>

4.3.1

address type > "Target device address type %s does not match source %s

domain , bus, slot or function > "Target device PCI address %04x:%02x:%02x.%02x does not match source %04x:%02x:%02x.%02x

 

 

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 



	
Expected Results:

3.

error: unsupported configuration: Target domain net card count 3 does not match source 2

Notes:
Comments:

		184491 	[Stable Guest ABI]Check domain input ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-input.xml

2.Add a input usb device to the test-input.xml

 <input type='mouse' bus='usb'/>

3.

#virsh migrate --live --xml test-net.xml guest qemu+ssh://10.66.5.120/system

 

4.Other elements NEED check and corresponding expected error message are below and please read note first.

type -> "Target input device type %s does not match source %s"

 

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

          2. For now, can not test bus, will update case after that is completed.

    

	
Expected Results:

3.

error: unsupported configuration: Target domain input device count %d does not match source %d

Notes:
Comments:

    #1 bili@redhat.com 2012-09-11 18:20:40
    Can not get error like: Target input device bus %s does not match source %s

		184496 	[Stable Guest ABI]Check domain sound ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-sound.xml

2.Add a sound to the test-sound.xml

 <sound model='ich6'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>

3.

#virsh migrate --live --xml test-sound.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

 <sound model='ich6'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>

4.1 sound model='ich6'     >  Target sound card model %s does not match source %s

4.1.1dumpxml a new xml

#virsh dumpxml guest > test-sound1.xml

4.1.2 change the device type to others in test-sound1.xml

like  :  model='ich6' change to model=''ac97'

4.1.3

#virsh migrate --live --xml test-sound1.xml guest qemu+ssh://10.66.5.120/system

 

 

4.2 Also check address type and address elements   <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>

4.2.1

address type > "Target device address type %s does not match source %s

domain , bus, slot or function > "Target device PCI address %04x:%02x:%02x.%02x does not match source %04x:%02x:%02x.%02x

 

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.



	
Expected Results:

3.

error: unsupported configuration: Target domain sound card count %d does not match source %d
Notes:
Comments:

		184497 	[Stable Guest ABI]Check domain video ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-video.xml

2.Add a disk to the test-video.xml

    <video>
        <model type='cirrus' vram='9216' heads='1'/>
              <alias name='video0'/>
                    <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
                        </video>

3.

#virsh migrate --live --xml test-video.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

<video>
        <model type='cirrus' vram='8192' heads='1'/>
        <alias name='video0'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
</video>

4.1 type='cirrus'     >  Target video card model %s does not match source %s

4.1.1dumpxml a new xml

#virsh dumpxml guest > test-video1.xml

4.1.2 change the device type to others in test-disk1.xml

like  : type='cirrus'   change to type='vga' 

4.1.3

#virsh migrate --live --xml test-sound1.xml guest qemu+ssh://10.66.5.120/system

4.2 vram='8192'            >  "Target video card vram %u does not match source %u")

4.3 heads='1' >  ""Target video card heads %u does not match source %u"

4.4 acceleration accel3d='yes' accel2d='yes'l  > "Target video card acceleration does not match source

4.5 accel3d > Target video card 3d accel %u does not match source %u

4.6 accel2d > Target video card 2d accel %u does not match source %u"

 

 4.7 Also check address type and address elements   <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>

4.7.1

address type > "Target device address type %s does not match source %s

domain , bus, slot or function > "Target device PCI address %04x:%02x:%02x.%02x does not match source %04x:%02x:%02x.%02x

 

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 



	
Expected Results:

3.

error: unsupported configuration: Target domain video card count 2 does not match source 1


Notes:
Comments:

		185340 	[Stable Guest ABI]Check domain HostDev ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

We can't test this with migration way since snapshot for running guest doesn't support by official.

So change to disable. If there is any change that make it possible  , please propose and re-write the case/

 

 

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

TBD.Because migrate does not support host device

 

 

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-hostdev.xml

2.Add a host device to the test-disk.xml

<hostdev mode='subsystem' type='usb' managed='yes'>
          <source>
                  <vendor id='0x17ef'/>
                   <product id='0x6019'/>
           </source>
 </hostdev>

 

3.

#virsh migrate --live --xml test-hostdev.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

 

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

          2. Check the <address type>  element which existed in label <disk><disk/> , f there are type drive and virtio

          then type ccid , pci can be ommited and vice versa



	
Expected Results:

3.

error: unsupported configuration: Target domain host device count 2 does not match source 0



Notes:
Comments:

		185342 	[Stable Guest ABI]Check domain smart card ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

Tips:Because qemu only support one smartcard in guest now , so the check point of count is omitted .

 

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-sm.xml

2.Add a smartcard to the test-disk.xml


   <smartcard mode='passthrough' type='spicevmc'/>

 

 3 .Check address type and address elements   <address type='ccid' controller='0' slot='0'/>

address type > "Target device address type %s does not match source %s"

controller or slot > "Target device ccid address %d:%d does not match source %d:%d"

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 



	
Expected Results:

 



Notes:
Comments:

    #1 bili@redhat.com 2012-09-12 11:59:18
    Is there other checkpoints?

		185343 	[Stable Guest ABI]Check domain serial ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-serial.xml

2.Add a disk to the test-disk.xml

<serial type='pty'>
      <source path='/dev/pts/3'/>
      <target port='0'/>
    </serial>

 

3.

#virsh migrate --live --xml test-serial.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

4.1 <target port='0'/>   >  Target serial port %d does not match source %d

4.1.1dumpxml a new xml

#virsh dumpxml guest > test-serial1.xml

4.1.2 change the device type to others in test-serial1.xml

like  : target port='0 change to target port='1'

4.1.3

#virsh migrate --live --xml test-serial1.xml guest qemu+ssh://10.66.5.120/system

 

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 
	
Expected Results:

3.

error: unsupported configuration:Target domain serial port count %d does not match source %d

Notes:
Comments:

		185345 	[Stable Guest ABI]Check domain Parallel ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-parallel.xml

2.Add a disk to the test-parallel.xml

<parallel type='pty'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
    </parallel>

 

3.

#virsh migrate --live --xml test-paraell.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

4.1 target port='0'    >  Target serial port %d does not match source %d

4.1.1dumpxml a new xml

#virsh dumpxml guest > test-parallel.xml

4.1.2 change the prot type to others in test-parallel.xml

like  :  target port='0'   change to target port='1' 

4.1.3

#virsh migrate --live --xml test-parallel.xml guest qemu+ssh://10.66.5.120/system

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

      



	
Expected Results:

3.

error: unsupported configuration: Target domain parallel port count 1 does not match source 0


Notes:
Comments:

		185347 	[Stable Guest ABI]Check domain Channel ABI Stability - bug 842557 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-channel.xml

2.Add a channel to the test-disk.xml

<channel type='pty'>
      <target type='virtio' name='arbitrary.virtio.serial.port.name'/>
    </channel>

 3.

#virsh migrate --live --xml test-channel.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

4.1target type='virtio   >  "Target channel type %s does not match source %s"

4.1.1dumpxml a new xml

#virsh dumpxml guest > test-channel1.xml

4.1.2 change the device type to others in test-disk1.xml

like  : target type='virtio'  --> target type='guestfwd'

4.1.3

#virsh migrate --live --xml test-channel1.xml guest qemu+ssh://10.66.5.120/system

4.2 name='arbitrary.virtio.serial.port.name'      >  Target channel name %s does not match source %s"

4.3 address='10.0.2.1' >  "Target channel addr %s does not match source %s

4.4 Stop the change from pty to spicevm during migrate

4.4.1

#virsh dumpxml <guest>
...
  <channel type='pty'>
      <source path='/dev/pts/15'/>
      <target type='virtio'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
...

4,4.2Virsh dumpxml guest xml,then edit xml like this:
#vim 
...
    <channel type='spicevmc'>
      <target type='virtio' />
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
...

4.4.3.Do migration.
# virsh migrate --live --xml kvm-win7-i386.xml kvm-win7-i386 qemu+ssh://10.66.5.10/system
root@10.66.5.10's password: 

error: unsupported configuration: Changing device type to/from spicevmc would change default target channel name

 

 4.5.Also check address type and address elements  <address type='virtio-serial' controller='0' bus='0' port='1'/>

4.5.1

address type > Target device address type %s does not match source %s

controller , bus or port > "Target device virtio serial address %d:%d:%d does not match source %d:%d:%d"

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 



	
Expected Results:

3.

error: unsupported configuration: Target domain channel count %d does not match source %d

Notes:
Comments:

		185348 	[Stable Guest ABI]Check domain console ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Start a guest with the following xml, and please delete other serial or console elements in the guest:

    <serial type='stdio'>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='stdio'>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <console type='pty'>
      <source path='/dev/pts/2'/>
      <target type='virtio' port='1'/>
      <alias name='console1'/>
    </console>

2.Dumpxml for guest , on one host

# virsh dumpxml guest > test-console.xml

3.Delete a console in the file test-console.xml

4. # virsh migrate --live --xml test-console.xml guest qemu+ssh://10.66.5.120/system

5.

Other elements NEED check and corresponding expected error message are below and please read note first.

5.1 console type='serial'     >  Target console type %s does not match source %s

5.1.1dumpxml a new xml

#virsh dumpxml guest > test-console1.xml

5.1.2 change the console target type to others in test-console1.xml

like  :  console type='serial' change to console type='virtio'

5.1.3

#virsh migrate --live --xml test-console1.xml guest qemu+ssh://10.66.5.120/system

 

 Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 



	
Expected Results:

4.

error: unsupported configuration: Target domain console count 1 does not match source 2

5.1.3

error: unsupported configuration: Target console type virtio does not match source serial

Notes:
Comments:

    #1 bili@redhat.com 2012-09-12 15:15:06
    Step 3:
    error: unsupported configuration: Only the first console can be a serial port

		185349 	[Stable Guest ABI]Check domain watchdog ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-dog.xml

2.Add a watchdog device to the test-dog.xml

  <watchdog model='i6300esb'/>

 3.

#virsh migrate --live --xml test-dog.xml guest qemu+ssh://10.66.5.120/system

 

4.

4.1 Also check address type and address elements   <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>

4.1.1

address type > "Target device address type %s does not match source %s

domain , bus, slot or function > "Target device PCI address %04x:%02x:%02x.%02x does not match source %04x:%02x:%02x.%02x

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 



	
Expected Results:

3.

Target domain watchdog count %d does not match source %d

Notes:
Comments:

		185352 	[Stable Guest ABI]Check domain memballon ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-mem.xml

2.

2.1 model='virtio     >  "Target balloon model %s does not match source %s"

2.1.1dumpxml a new xml

#virsh dumpxml guest > test-mem1.xml

2.1.2 change the device type to others in test-disk1.xml

like  :  model='virtio  --> model='none'

2.1.3

#virsh migrate --live --xml test-mem1.xml guest qemu+ssh://10.66.5.120/system

2.2 Also check address type and address elements   <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>

2.2.1

address type > "Target device address type %s does not match source %s

domain , bus, slot or function > "Target device PCI address %04x:%02x:%02x.%02x does not match source %04x:%02x:%02x.%02x

 

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

           2. Since not support remove the memballon xml from the guest now , so the count check is not possible now

 



	
Expected Results:

2.1.3

error: unsupported configuration: Target balloon model %s does not match source %s
Notes:
Comments:

    #1 bili@redhat.com 2012-09-12 16:41:19
    Can not get error:
    Target domain memory balloon count %d does not match source %d

		185354 	[Stable Guest ABI]Check domain hub ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

We can't test this with migration way since snapshot for running guest doesn't support by official.

So change to disable. If there is any change that make it possible  , please propose and re-write the case/

 

 

 

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

TBD.Because "The hub element has one mandatory attribute, the type whose value can only be 'usb'." from libvirt.org

 

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-hub.xml

2.Add a disk to the test-disk.xml

   <hub type='usb'/>

 

3.

#virsh migrate --live --xml test-hub.xml guest qemu+ssh://10.66.5.120/system

 
	
Expected Results:

3.

error: unsupported configuration: Target domain hub count 3 does not match source 2

Notes:
Comments:

		185356 	[Stable Guest ABI]Check domain clock ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

TBD.

name="tsc" not support
	
Expected Results:

3.

error: unsupported configuration: Target domain disk count 3 does not match source 2

Notes:
Comments:

		185360 	[Stable Guest ABI]Check domain status ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-status.xml

2.

change <domain type='kvm'> to <domain type='xen'> in test-status.xml

3.

#virsh migrate --live --xml test-status.xml guest qemu+ssh://10.66.5.120/system

 

4.

Other elements NEED check and corresponding expected error message are below and please read note first.

4.1 uuid   >  "Target domain uuid %s does not match source %s"

4.2 max memory       >  Target domain max memory %lld does not match source %lld")

4.3 current memory >  Target domain current memory %lld does not match source %lld

4.4 hugepage backing  > Target domain huge page backing %d does not match source %d

4.5 vcpu count > Target domain vpu count %d does not match source %d

 4.6.max vcpu >  Target domain vpu max %d does not match source %d

4.7 os type > Target domain OS type %s does not match source %s"

4.8 .os arch > Target domain architecture %s does not match source %s

4.9 smbios > Target domain SMBIOS mode %s does not match source %s

4.10 .features > Target domain features %d does not match source %d

4.11 sysinfo > Target sysinfo %s does not match source %s

 

Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.

 
	
Expected Results:

3.

error: unsupported configuration: Target domain virt type %s does not match source %s

Notes:
Comments:

		185362 	[Stable Guest ABI]Check domain CPU ABI Stability 	lsu 	lsu 	Manual 		Feature 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts A,B and prepare a nfs which is mounted on both hosts

2.Do this both on host A and B

#setsebool virt_use_nfs 1

#iptables -F

#mount -t nfs 10.66.90.121:/vol/S3/libvirtmanual /mnt

3.Prepare a image which installed os , make sure function migrate works well both from A to B and reverse

on A#virsh migrate --live guest qemu+ssh://10.66.5.120/system

on B#virsh migrate --live guest qemu+ssh://10.66.5.140/system
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1.Dumpxml for guest , on one host

#virsh dumpxml guest > test-disk.xml

2.Add a disk to the test-disk.xml

<cpu>
    <topology sockets='1' cores='2' threads='1'/>
  </cpu>

 3.

#virsh migrate --live --xml test-disk.xml guest qemu+ssh://10.66.5.120/system

4. Other elements NEED check and corresponding expected error message are below and please read note first.

4.1 tpye

Till now , the type not currently exposed in the XML configuration. So you can skip it.

4.2 mode

"Target CPU mode %s does not match source %s

4.3 arch

Target CPU arch %s does not match source %s"

4.4 model

Target CPU model %s does not match source %s"

4.5 vendor

"Target CPU vendor %s does not match source %s"

4.6 vendor_id

Target CPU model %s does not match source %s

4.7 sockets

Target CPU sockets %d does not match source %d

4.8 core

Target CPU cores %d does not match source %d

4.9 threads

"Target CPU threads %d does not match source %d

4.10 features count

"Target CPU feature count %zu does not match source %zu

4.11 feature

Target CPU feature %s does not match source %s

4.12 feature policy

"Target CPU feature policy %s does not match source %s


Note: 1. Only one error will occur once whatever how many xml codes be changes ,

           so for testing others you need do  /* #virsh dumpxml guest  */ to get a new xml and only change one element everytime.



 
	
Expected Results:

3.

Target CPU does not match source
Notes:
Comments:

    #1 bili@redhat.com 2012-09-12 19:56:43
    How to get error:
    Target CPU type %s does not match source %s

		185483 	[CPU Management] libvirt should support AMD Abu-Dhabi (AMD Opteronâ¢ 63xx series) processor-bug838129 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Bug 838129 - [FEAT RHEL6.4] Include support for AMD Abu-Dhabi (AMD Opteronâ¢ 63xx series) processor
Shipping the 11G PowerEdge servers (R715, R815 and M915) need support for this processor refresh.

Waiting for qemu adding the cpu model.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    838129

Actions:

1. Check definition for this cpu model:

# cat /usr/share/libvirt/cpu_map.xml

 

2. check the cpu model in host.


# virsh capabilities
	
Expected Results:
Notes:
The qemu bug this one depends on has been closed as duplicate of the bug requesting support for AMD Seoul. I'm doing the same to be in sync.

*** This bug has been marked as a duplicate of bug 838127 ***
Comments:

		185485 	[CPU Management] libvirt should support AMD Seoul (AMD Opteronâ¢ 4xxx series) processor -bug838127 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
 Bug 838127 - [FEAT RHEL 6.4] Include support for AMD Seoul (AMD Opteronâ¢ 4xxx series) processor

 

Reserve the test machine from beaker with the following filter:

System/Model contains: Seoul

such as: amd-pence-01.lab.bos.redhat.com 

 
The qemu bug this one depends on has been closed as duplicate of the bug requesting support for AMD Seoul. I'm doing the same to be in sync. *** This bug has been marked as a duplicate of bug 838127 *** 

Bug 838129 - [FEAT RHEL6.4] Include support for AMD Abu-Dhabi (AMD Opteronâ¢ 63xx series) processor

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    838127

Actions:

1. Check definition for this cpu model:

# cat /usr/share/libvirt/cpu_map.xml

 

2. check the cpu model in host.

# virsh capabilities

 

3. # virsh edit rhel6.4
<domain>
.....
<cpu mode='host-model'>
    <model fallback='forbid'/>
  </cpu>
......
</domain>

# virsh start rhel6.4
Domain rhel6.4 started

4. # ps -ef|grep rhel6.4


5. On sandybridge machine , start one guest with Opteron_G5 cpu model

# virsh start rhel6.4
	
Expected Results:

step 1,

......
    <model name='Opteron_G5'>
      <model name='Opteron_G4'/>
      <feature name='f16c'/>
      <feature name='fma'/>
      <feature name='tbm'/>
    </model>
......

step 2,

<capabilities>

  <host>
    <uuid>00001111-0000-2222-0000-888800009999</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Opteron_G5</model>
      <vendor>AMD</vendor>
      <topology sockets='1' cores='8' threads='2'/>
      <feature name='bmi1'/>
      <feature name='perfctr_nb'/>
      <feature name='perfctr_core'/>
      <feature name='topoext'/>
      <feature name='nodeid_msr'/>
      <feature name='tce'/>
      <feature name='lwp'/>
      <feature name='wdt'/>
      <feature name='skinit'/>
      <feature name='ibs'/>
      <feature name='osvw'/>
      <feature name='cr8legacy'/>
      <feature name='extapic'/>
      <feature name='cmp_legacy'/>
      <feature name='fxsr_opt'/>
      <feature name='mmxext'/>
      <feature name='osxsave'/>
      <feature name='monitor'/>
      <feature name='ht'/>
      <feature name='vme'/>
    </cpu>
......

step 4,

qemu     28710     1 65 02:32 ?        00:00:09 /usr/libexec/qemu-kvm -name rhel6.3 -S -M rhel6.4.0 -cpu Opteron_G5,+bmi1,+perfctr_nb,+perfctr_core,+topoext,+nodeid_msr,+tce,+lwp,+wdt,+skinit,+ibs,+osvw,+cr8legacy,+extapic,+cmp_legacy,+fxsr_opt,+mmxext,+osxsave,+monitor,+ht,+vme -enable-kvm -m 1024 -smp 1,maxcpus=7,sockets=7,cores=1 ......


step 5,

error: Failed to start domain rhel6.4
error: unsupported configuration: guest and host CPU are not compatible: Host CPU does not provide required features: tbm, fma4, xop, 3dnowprefetch, misalignsse, sse4a, abm, svm, pdpe1gb, f16c, fma
Notes:
Comments:

		185510 	[CPU Management] vcpupin after the status of physical cpu change from offline to online-bug838070 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu
    rhel6.5

bug:

    838070

Actions:

1.# ll /sys/devices/system/cpu/
total 0
drwxr-xr-x 8 root root    0 Jul 18 01:40 cpu0
drwxr-xr-x 8 root root    0 Jul 10 16:36 cpu1
drwxr-xr-x 8 root root    0 Jul 10 16:36 cpu2
drwxr-xr-x 8 root root    0 Jul 24 14:49 cpu3

2. Offline the host cpu 3

# echo 0 > /sys/devices/system/cpu/cpu3/online

3. Pin the first vcpu onto  physical cpu 3

# virsh vcpupin rhel6 0 3
error: Physical CPU 3 doesn't exist.
error: cpulist: Invalid format.

4. Online the cpu 3

# echo 1 > /sys/devices/system/cpu/cpu3/online

5. Check the status of host cpu

# cat /sys/devices/system/cpu/online
0-3

6. # virsh vcpupin rhel6 0 3
error: cannot set CPU affinity on process 17770: Invalid argument

 
	
Expected Results:

Step 6 . Successfully pin cpu.
Notes:
Comments:

		185526 	[CPU Management] Check error message when change VM's processor count to 0-bug834365 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    834365

Actions:

1. Set the maximum cpu number to 4
# virsh dumpxml test
......
<vcpu placement='static' current='1'>4</vcpu>
.....

2. Hotplug vcpus 
# virsh setvcpus test 2 


3.Check the guest xml again
# virsh dumpxml test
......
<vcpu placement='static' current='2'>4</vcpu>
.....

4.Set vcpus to 0
# virsh setvcpus  test 0

error: Invalid number of virtual CPUs

	
Expected Results:

Step 4 . The following error message never occur.

error: invalid argument: virDomainSetVcpus

 
Notes:
Comments:

		189340 	[libvirtd] Include the default listen address in the live guest XML 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Prepare a guest, and make sure there's no 'listen' attribute or <listen> element is set in the
guest XML.

2. Start the guest, check the guest XML.
	
Expected Results:

2. There should be a  <listen> element in the guest XML

 <graphics type='vnc' port='5900' autoport='yes' listen='127.0.0.1' keymap='en-us'>
      <listen type='address' address='127.0.0.1'/>
 </graphics>

 
Notes:
Comments:

		189612 	[Virtual disks] Split ide-drive into ide-cd and ide-hd - BZ 801772 #c6 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Need to check if qemu-kvm support ide-cd and ide-hd, check with

/usr/libexe/qemu-kvm -device ?

you can find

name "ide-cd", bus IDE, desc "virtual IDE CD-ROM"
name "ide-hd", bus IDE, desc "virtual IDE disk"

in the list, else it is not supported, and this case has no sense
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Start a guest with an ide disk and an ide cdrom

# virsh dumpxml guest

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/tmp/idedisk.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
     <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/tmp/cdrom.iso'/>
      <target dev='hdc' bus='ide'/>
    </disk>
    <controller type='ide' index='0' />
    <controller type='ide' index='1' />

	
Expected Results:

Check the qemu-kvm command line, should be

-drive file=/tmp/idedisk.img,if=none,id=drive-ide0-0-0,format=raw -device ide-hd,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1

-drive file=/tmp/cdrom.iso,if=none,id=drive-ide0-1-0,format=raw -device ide-cd,bus=ide.1,unit=0,drive=drive-ide0-1-0,id=ide0-1-0
Notes:
qemu still not supported it ,so here just for recording, not sure when will test
Comments:

		190040 	[Managed save]Checking the status of libvirt-guests service 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

1

# service libvirtd stop

Stopping libvirtd daemon:        [ OK ]

2

# service libvirt-guests status

started

3

# service libvirt-guests stop

Can't connect to default. Skipping.

# service libvirt-guests status

stopped, with no saved guests

4

# service libvirtd start

Starting libvirtd daemon:      [ OK ]

# service libvirt-guests start

# virsh list --all

Id Name State ----------------------------------------------------

5 rhel6 running

6 win running

 

6

# service libvirt-guests stop

Running guests on default URI: rhel6, win

Suspending guests on default URI... Suspending rhel6: done

Suspending win: done

 

# service libvirt-guests status

stopped, with saved guests
	
Expected Results:
Notes:
Comments:

		190041 	[Managed save]Checking the status of libvirt-guests service-bug816448 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    816448

Actions:

1

# service libvirtd stop

Stopping libvirtd daemon:        [ OK ]

2

# service libvirt-guests status

started

3

# service libvirt-guests stop

Can't connect to default. Skipping.

# service libvirt-guests status

stopped, with no saved guests

4

# service libvirtd start

Starting libvirtd daemon:      [ OK ]

# service libvirt-guests start

# virsh list --all

Id Name State ----------------------------------------------------

5 rhel6 running

6 win running

 

6

# service libvirt-guests stop

Running guests on default URI: rhel6, win

Suspending guests on default URI... Suspending rhel6: done

Suspending win: done

 

# service libvirt-guests status

stopped, with saved guests
	
Expected Results:
Notes:
Comments:

		190935 	[Migration] Migration with sheepdog 	weizhan 	None 	Manual 		Function 	P2 	None 	Edit
Setup:

NOTE: This may be not tested on rhel6.4 because it need qemu>0.13, here just to record it so that may be it can be used in rhel7
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    rhel7

bug:

    No bug found

Actions:

1. Prepare 2 hosts in same subnet

2. Install sheepdog and corosync on both host

3. cp /etc/corosync/corosync.conf.example to /etc/corosync/corosync.conf and change "bindnetaddr" to your host ip on both host

4. Start service in sequence on both hosts

# service corosync start

# service sheepdog start

5. Add 2 host in 1 cluster and check on 1 host

# collie node list
# collie cluster format --copies=2
# collie cluster info

6. Create image on 1 host

# qemu-img create sheepdog:demo 5G

7. Install guest with this disk on 1 host

<disk type='network' device='disk'>
  <driver name='qemu' type='raw'/>
  <source protocol='sheepdog' name='demo'/>
  <target dev='vdb' bus='virtio'/>
</disk>

8. Do migration to the other host

# virsh migrate --live guest qemu+ssh://{target ip}/system

9. Do migration to the other host

# virsh migrate --live guest qemu+ssh://{target ip}/system --unsafe
	
Expected Results:


Step 4 . Service should be started successfully, and you can check with "service xxx status"

Step 5.

# collie node list
   Idx - Host:Port              Number of vnodes
------------------------------------------------
     0 - 192.168.1.2:7000        64
*    1 - 192.168.1.3:7000        64

# collie cluster info
running

Ctime                Epoch Nodes
2011-10-11 10:50:01      1 [192.168.1.2:7000, 192.168.1.3:7000]

Step 8.

   Migration should report error

   error: Unsafe migration: Migration may lead to data corruption if disks use cache != none

Step 9.

    Migration should succeed without error


Notes:
Comments:

		191604 	[stable guest ABI] Do live migration from rhel6.1.z release version to rhel6.x newest version and back --bug 856864 863059 	weizhan 	weizhan 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts, host 1 is rhel6.1.z release version, host 2  is newest rhel6.x version

   Please update kernel , qemu-kvm and libvirt package to z-stream on rhel6.1.z host

   For now ,

   Kernel :  kernel-2.6.32-131.32.1.el6  

   qemu-kvm : qemu-kvm-0.12.1.2-2.160.el6_1.9

   libvirt:  libvirt-0.8.7-18.el6_1.5

  How to find the newest z-stream package:

  Search kernel-2.6.32-131* , qemu-kvm-0.12.1.2-2.160.el6* and  libvirt-0.8.7-18.el6*  separately with Build option in brewweb.

 

2. Prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    856864 - From Run 47365

Actions:

1. Prepare a guest locate on host 1 and start, check the qemu-kvm command

the disk xml part should be

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/$SHARED_DIR/kvm-rhel6u2-x86_64-new.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>

...

2. Do live migration from host 1 to host 2

# virsh migrate --live guest qemu+ssh://{host 2 ip}/system

3. Login guest to check the guest status

4. Do live migration from host 2 to host 1

# virsh migrate --live guest qemu+ssh://{host 1 ip}/system

5. Login guest to check the guest status

 
	
Expected Results:

Step 2

Migration should succeed without error

Step 3

Guest should work well

Step 4

Migration should succeed without error

Step 5

Guest should work well

 
Notes:
Comments:

		191606 	[stable guest ABI] Do live migration from rhel6.2.z release version to rhel6.x newest version and back 	weizhan 	weizhan 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts, host 1 is rhel6.2.z release version, host 2  is newest rhel6.x version

    Please update kernel , qemu-kvm and libvirt package to z-stream on rhel6.2.z host

   For now ,

   Kernel : kernel-2.6.32-220.26.1.el6

   qemu-kvm :qemu-kvm-0.12.1.2-2.209.el6_2.5

   libvirt: libvirt-0.9.4-23.el6_2.9

  How to find the newest z-stream package:

  Search kernel-2.6.32-220* , qemu-kvm-0.12.1.2-2.209.el6* and  libvirt-0.9.4-23.el6*  separately with Build option in brewweb.

 

2. Prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Prepare a guest locate on host 1 and start, check the qemu-kvm command

the disk xml part should be

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/$SHARED_DIR/kvm-rhel6u2-x86_64-new.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>

...

2. Do live migration from host 1 to host 2

# virsh migrate --live guest qemu+ssh://{host 2 ip}/system

3. Login guest to check the guest status

4. Do live migration from host 2 to host 1

# virsh migrate --live guest qemu+ssh://{host 1 ip}/system

5. Login guest to check the guest status

 
	
Expected Results:

Step 2

Migration should succeed without error

Step 3

Guest should work well

Step 4

Migration should succeed without error

Step 5

Guest should work well

 
Notes:
Comments:

		191608 	[stable guest ABI] Do live migration from rhel6.3.z release version to rhel6.x newest version and back 	weizhan 	weizhan 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare 2 hosts, host 1 is rhel6.3.z release version, host 2  is newest rhel6.x version

    Please update kernel , qemu-kvm and libvirt package to z-stream on rhel6.3.z host

   For now ,

   Kernel : kernel-2.6.32-279.9.1.el6

   qemu-kvm :qemu-kvm-0.12.1.2-2.295.el6_3.2

   libvirt: libvirt-0.9.10-21.el6_3.3

  How to find the newest z-stream package:

  Search kernel-2.6.32-279* , qemu-kvm-0.12.1.2-2.295.el6* and  libvirt-0.9.10-21.el6*  separately with Build option in brewweb.

   
 

 

2. Prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Prepare a guest locate on host 1 and start, check the qemu-kvm command

the disk xml part should be

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/$SHARED_DIR/kvm-rhel6u2-x86_64-new.img'/>
      <target dev='sda' bus='scsi'/>
    </disk>

    <controller type='scsi' index='0' model='virtio-scsi' />

...

2. Do live migration from host 1 to host 2

# virsh migrate --live guest qemu+ssh://{host 2 ip}/system

3. Login guest to check the guest status

4. Migration back from host 2 to host 1

# virsh migrate --live guest qemu+ssh://{host 1 ip}/system

5. Login guest to check the guest status
	
Expected Results:

Step 2

Migration should succeed without error

Step 3

Guest should work well

Step 4

Migration should succeed without error

Step 5

Guest should work well
Notes:
Comments:

		191663 	[migration]error messages may not be propagated properly during failed migration 844378 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression
    rhel6.5

bug:

    No bug found

Actions:

1. change log level to 1 on src and dst host, restart libvirtd
2. prepare a guest with allocated virtio raw disk
3. hot unplug the disk
4. migrate and fail
5. check the error info and log

	
Expected Results:

This case related bug 844378 and bug 807023, pls update step4 and step5 after they are verified.
Notes:
Comments:

		192670 	[migration]Add live migration support for USB - bug 843560 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Prepare a usb disk and plug into source host

Prepare 2 hosts, 1 source , 1 target

Prepare nfs server

# cat /etc/exports

/var/lib/libvirt/images *(rw,no_root_squash,async)

# service nfs restart

# iptables -F

On both hosts, do

# mount {nfs ip}:/var/lib/libvirt/images/ /var/lib/libvirt/migrate -o vers=3

# iptables -F

# setsebool virt_use_nfs 1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Start a guest  on source host with

 <hostdev mode='subsystem' type='usb' managed='yes'>
 <source startupPolicy='requisite'>
 <vendor id='0x0951'/>
 <product id='0x1646'/>
 </source>
 </hostdev>

and shared disk like /var/lib/libvirt/migrate/guest.img

2. Do migration

# virsh migrate --live qemu+ssh://{target host ip}/system --verbose

3. Repeat the steps 1,2  with startPolicy='optional'

4. Repeat the steps 1,2  with startPolicy='mandatory'

5. Start guest with usb ,pull out the usb disk and plug into target host, do migration with startPolicy='requisite'

6. Start guest with usb ,pull out the usb disk and plug into target host, do migration with startPolicy='optional'

7. Start guest with usb ,pull out the usb disk and plug into target host, do migration with startPolicy='mandatory'
	
Expected Results:

Step 1

Guest can be started successfully

Step 2

Migration can succeed without error

the xml in target changed to
 <hostdev mode='subsystem' type='usb' managed='yes'>
      <source startupPolicy='requisite' missing='yes'>
        <vendor id='0x0951'/>
        <product id='0x1646'/>
        <address bus='2' device='6'/>
      </source>
      <alias name='hostdev0'/>
    </hostdev> 

Step 3

Guest can be started successfully

Migration can succeed without error

the xml in target changed to
 <hostdev mode='subsystem' type='usb' managed='yes'>
      <source startupPolicy='optional' missing='yes'>
        <vendor id='0x0951'/>
        <product id='0x1646'/>
        <address bus='2' device='6'/>
      </source>
      <alias name='hostdev0'/>
    </hostdev> 

Step 4

Guest can be started successfully

Migration will report error

error: internal error Did not find USB device 951:1646

Step 5

Migration can succeed and on target

the xml in target changed to
 <hostdev mode='subsystem' type='usb' managed='yes'>
      <source startupPolicy='requisite'>
        <vendor id='0x0951'/>
        <product id='0x1646'/>
        <address bus='2' device='7'/>      # the address here will changed according to you target host usb id, which can be get with lsusb
      </source>
      <alias name='hostdev0'/>
    </hostdev> 

 Step 6

similar with Step5, only the startupPolicy in xml changed to 'optional'

 Step 7

similar with Step5, only the startupPolicy in xml changed to 'mandatory'
Notes:
Comments:

		193546 	[network filter]DHCP Snooping 	lsu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
 Bug 693884 - [6.4 FEAT] libvirt: Support DHCP Snooping and Dynamic ARP Inspection
is ON_QA now , so i'm not sure all the result is expected .

 

 

Prepare a shut down guest with bridge type interface

#virsh net-dumpxml default

<network>
  <name>default</name>
  <uuid>e7198291-0c2a-45f4-8f75-4a9fceebdd81</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='52:54:00:34:94:B6'/>
  <ip address='192.168.123.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.123.1' end='192.168.123.254' />
    </dhcp>
  </ip>
</network>

Make sure there is a ip address with <dhcp> label

Guest interface is below

   <interface type='bridge'>
      <mac address='52:54:00:71:87:37'/>
      <source bridge='virbr0'/>
      <target dev='vnet0'/>
      <filterref filter='clean-traffic'>
        <parameter name='CTRL_IP_LEARNING' value='dhcp'/>
      </filterref>
    </interface>

 

#yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1.

#virsh start guest

#ps aux | grep dnsmasq

#ebtables -t nat -L

2.login into the guest to check if the network work fine

in guest

#ping www.baidu.com

3.

   On host  do : while :; do kill -SIGHUP `pidof libvirtd` ; echo "HUP $RANDOM"; sleep 20; done

  On guest do: while :; do kill -SIGTERM `pidof dhclient`; dhclient eth0 ; ifconfig eth0; done

  Then on the host do

#cat /var/run/libvirt/network/nwfilter.leases ; date +%s
#ebtables -t nat -L

 

4.

Terminate the command above both on guest and host whatever :-) . BTW  keep the guest alive pls .....

4.1 change the dhcp server's range out of before

 #virsh net-dumpxml default

<network>
  <name>default</name>
  <uuid>e7198291-0c2a-45f4-8f75-4a9fceebdd81</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='52:54:00:34:94:B6'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.122.1' end='192.168.122.254' />
    </dhcp>
  </ip>
</network>

4.2 restart the network

#virsh net-destroy default

#virsh net-start default

#ps aux | grep dnsmasq

4.3 detach and attach the interface

dumpxml the interface to a xml first , like

#cat interface.xml

   <interface type='bridge'>
      <mac address='52:54:00:71:87:37'/>
      <source bridge='virbr0'/>
      <target dev='vnet0'/>
      <filterref filter='clean-traffic'>
        <parameter name='CTRL_IP_LEARNING' value='dhcp'/>
      </filterref>
    </interface>

#virsh detach-interface guest bridge

#virsh attach-device guest interface.xml

4.4

Now repeat setp 1-3

Check

4.4.1 #ebtables -t nat -L

 4.4.2 Network status on the guest , find is expected definenlly.

4.4.3 Guest's IP not change through  step 3

 

 
	
Expected Results:

#ps aux | grep dnsmasq

nobody   32451  0.0  0.0  12880   648 ?        S    10:53   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --listen-address 192.168.123.1 --dhcp-range 192.168.123.1,192.168.123.254 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=254 --dhcp-no-override

#ebtables -t nat -L

Make sure have something like this whose are guest's ip and mac

.....

Bridge chain: I-vnet0-ipv4-ip, entries: 3, policy: ACCEPT
-p IPv4 --ip-src 0.0.0.0 --ip-proto udp -j RETURN
-p IPv4 --ip-src 192.168.123.236 -j RETURN
-j DROP


Bridge chain: I-vnet0-arp-mac, entries: 2, policy: ACCEPT
-p ARP --arp-mac-src 52:54:0:71:87:37 -j RETURN
-j DROP

Bridge chain: I-vnet0-arp-ip, entries: 2, policy: ACCEPT
-p ARP --arp-ip-src 192.168.123.236 -j RETURN
-j DROP

Bridge chain: I-vnet0-rarp, entries: 2, policy: ACCEPT
-p 0x8035 -s 52:54:0:71:87:37 -d Broadcast --arp-op Request_Reverse --arp-ip-src 0.0.0.0 --arp-ip-dst 0.0.0.0 --arp-mac-src 52:54:0:71:87:37 --arp-mac-dst 52:54:0:71:87:37 -j ACCEPT
-j DROP

Bridge chain: O-vnet0-rarp, entries: 2, policy: ACCEPT
-p 0x8035 -d Broadcast --arp-op Request_Reverse --arp-ip-src 0.0.0.0 --arp-ip-dst 0.0.0.0 --arp-mac-src 52:54:0:71:87:37 --arp-mac-dst 52:54:0:71:87:37 -j ACCEPT
-j DROP

2.

[root@test1 ~]# ping www.baidu.com
PING www.a.shifen.com (220.181.111.147) 56(84) bytes of data.
64 bytes from 220.181.111.147: icmp_seq=1 ttl=51 time=15.6 ms
^C
--- www.a.shifen.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 830ms

3.

The guest's IP and Mac should Not  change.And always can access to the internet.

#cat /var/run/libvirt/network/nwfilter.leases ; date +%s should like
1344398038 07f6d2bd-9309-43eb-ede2-c57f07fac57e-52:54:00:71:87:37 192.168.123.236 192.168.123.1
1344396416

ebtables's rule should like step 1 abvoe.

4.

4.2

#ps aux | grep dnsmasq

nobody   32451  0.0  0.0  12880   648 ?        S    10:53   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --listen-address 192.168.122.1 --dhcp-range 192.168.122.1,192.168.122.254 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=254 --dhcp-no-override
Notes:
Comments:

		193547 	[network filter]Multiple IP address support to DHCP snooping 	lsu 	lsu 	Manual 		Function 	P1 	None 	Edit
Setup:
 Bug 693884 - [6.4 FEAT] libvirt: Support DHCP Snooping and Dynamic ARP Inspection

is ON_QA now , so i'm not sure all the result is expected .

Prepare a shut down guest with bridge type interface

#virsh net-dumpxml default

<network>
  <name>default</name>
  <uuid>e7198291-0c2a-45f4-8f75-4a9fceebdd81</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='52:54:00:34:94:B6'/>
  <ip address='192.168.123.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.123.1' end='192.168.123.254' />
    </dhcp>
  </ip>
</network>

Make sure there is a ip address with <dhcp> label

Guest interface is below

   <interface type='bridge'>
      <mac address='52:54:00:71:87:37'/>
      <source bridge='virbr0'/>
      <target dev='vnet0'/>
      <filterref filter='clean-traffic'>
        <parameter name='CTRL_IP_LEARNING' value='dhcp'/>
      </filterref>
    </interface>

 

#yum install ebtalbes -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1.Create a new network, target dev is same as default

#virsh net-dumpxml

<network>
  <name>net</name>
  <uuid>0b2be444-cfc5-4fcb-b9f3-d5b7cd7f8c12</uuid>
  <forward dev='vnet0' mode='nat'>
    <interface dev='vnet0'/>
  </forward>
  <bridge name='virbr1' stp='on' delay='0' />
  <mac address='52:54:00:3F:32:F4'/>
  <ip address='192.168.100.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.100.128' end='192.168.100.254' />
    </dhcp>
  </ip>
</network>
#virsh net-start net

#brctl show

bridge name    bridge id        STP enabled    interfaces
virbr0        8000.5254003494b6    yes        virbr0-nic
virbr1        8000.5254003f32f4    yes        virbr1-nic
                            vnet0

2.

#virsh start guest

#ps aux | grep dnsmasq

#ebtables -t nat -L

 

3.

#brctl delif virbr0 vnet0

#brctl addif virbr1 vnet0

On the guest

#killall dhclient

#dhclient

 

4.

4.1 #ps aux | grep dnsmasq

4.2 #cat /var/run/libvirt/network/nwfilter.leases

4.3 #ebtalbs -t nat -L

4.4 #cat /var/log/libvirtd.log

4.5 on the guest , ping www.baidu.com
	
Expected Results:

2.

#ps aux | grep dnsmasq

nobody   32451  0.0  0.0  12880   652 ?        S    10:53   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --listen-address 192.168.123.1 --dhcp-range 192.168.123.1,192.168.123.254 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=254 --dhcp-no-override

#ebtables -t nat -L

Bridge table: nat

Bridge chain: PREROUTING, entries: 1, policy: ACCEPT
-i vnet0 -j libvirt-I-vnet0

Bridge chain: OUTPUT, entries: 0, policy: ACCEPT

Bridge chain: POSTROUTING, entries: 1, policy: ACCEPT
-o vnet0 -j libvirt-O-vnet0

Bridge chain: libvirt-I-vnet0, entries: 9, policy: ACCEPT
-j I-vnet0-mac
-p IPv4 -j I-vnet0-ipv4-ip
-p IPv4 -j ACCEPT
-p ARP -j I-vnet0-arp-mac
-p ARP -j I-vnet0-arp-ip
-p ARP -j ACCEPT
-p 0x8035 -j I-vnet0-rarp
-p 0x835 -j ACCEPT
-j DROP

Bridge chain: libvirt-O-vnet0, entries: 4, policy: ACCEPT
-p IPv4 -j O-vnet0-ipv4
-p ARP -j ACCEPT
-p 0x8035 -j O-vnet0-rarp
-j DROP

Bridge chain: I-vnet0-mac, entries: 2, policy: ACCEPT
-s 52:54:0:e4:9c:37 -j RETURN
-j DROP

Bridge chain: I-vnet0-ipv4-ip, entries: 3, policy: ACCEPT
-p IPv4 --ip-src 0.0.0.0 --ip-proto udp -j RETURN
-p IPv4 --ip-src 192.168.123.171 -j RETURN
-j DROP

Bridge chain: O-vnet0-ipv4, entries: 1, policy: ACCEPT
-j ACCEPT

Bridge chain: I-vnet0-arp-mac, entries: 2, policy: ACCEPT
-p ARP --arp-mac-src 52:54:0:e4:9c:37 -j RETURN
-j DROP

Bridge chain: I-vnet0-arp-ip, entries: 2, policy: ACCEPT
-p ARP --arp-ip-src 192.168.123.171 -j RETURN
-j DROP

Bridge chain: I-vnet0-rarp, entries: 2, policy: ACCEPT
-p 0x8035 -s 52:54:0:e4:9c:37 -d Broadcast --arp-op Request_Reverse --arp-ip-src 0.0.0.0 --arp-ip-dst 0.0.0.0 --arp-mac-src 52:54:0:e4:9c:37 --arp-mac-dst 52:54:0:e4:9c:37 -j ACCEPT
-j DROP

Bridge chain: O-vnet0-rarp, entries: 2, policy: ACCEPT
-p 0x8035 -d Broadcast --arp-op Request_Reverse --arp-ip-src 0.0.0.0 --arp-ip-dst 0.0.0.0 --arp-mac-src 52:54:0:e4:9c:37 --arp-mac-dst 52:54:0:e4:9c:37 -j ACCEPT
-j DROP

4.

4.1

nobody     872  0.0  0.0  12880   656 ?        S    10:59   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --pid-file=/var/run/libvirt/network/net.pid --conf-file= --except-interface lo --listen-address 192.168.100.1 --dhcp-range 192.168.100.128,192.168.100.254 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/net.leases --dhcp-lease-max=127 --dhcp-no-override
root     15176  0.0  0.0 103244   856 pts/6    S+   14:07   0:00 grep dnsmasq
nobody   32451  0.0  0.0  12880   652 ?        S    10:53   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --listen-address 192.168.123.1 --dhcp-range 192.168.123.1,192.168.123.254 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=254 --dhcp-no-override

4.2

1344398038 07f6d2bd-9309-43eb-ede2-c57f07fac57e-52:54:00:71:87:37 192.168.123.236 192.168.123.1
1344398441 07f6d2bd-9309-43eb-ede2-c57f07fac57e-52:54:00:71:87:37 192.168.100.236 192.168.100.1

4.3

something like below

Bridge chain: I-vnet0-ipv4-ip, entries: 3, policy: ACCEPT
-p IPv4 --ip-src 0.0.0.0 --ip-proto udp -j RETURN
-p IPv4 --ip-src 192.168.100.236 -j RETURN

-j DROP

 

4.4

Should Not show error like that

2012-08-08 02:50:35.518+0000: 31477: error : virNWFilterDHCPSnoopEnd:2113 : internal error ifname "vnet0" not in key map

 

4.5

Network status is fine
Notes:
Comments:

		193550 	[network filter]Ip set API 	lsu 	lsu 	Manual 		Function 	P1 	None 	Edit
Setup:

1.Prepare three guests and their's ip are 192.168.123.172-test1 , 192.168.123.236-test2 , 192.168.123.250-test3 respectively

2.yum install ipset -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1.create a ipset rule
#ipset -N blacklist iphash
#ipset -A blacklist 192.168.123.236

2.create a new filter
#virsh nwfilter-dumpxml ipset-test
<filter name='ipset-test' chain='root'>
<uuid>f525debe-e8dc-98d3-022f-7f661e08a292</uuid>
<rule action='drop' direction='in' priority='500'>
<all ipset='blacklist' ipsetflags='src,src'/>
</rule>
</filter>

3.add to the test1
#virsh dumpxml test1 | grep filter
<filterref filter='ipset-test'/>

 

4.Check if it works
#virsh start test1
login to the guest
#ping 192.168.123.236

#ping 192.168.123.250

	
Expected Results:

4.

ping 192.168.123.236 should lost , and there is no effect on other ips
Notes:
Comments:

		193551 	[SR-IOV] Hotplug/unplug network device with managed='yes' - bug 844287, 843016 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1.Have executed test case  with the summary [SR-IOV] Create up to MAX VFs
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV
    rhel6.5

bug:

    No bug found

Actions:

1. Check the VFs info
# lspci|grep 82576
03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.7 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)


2. Enable unsafe assignemnt for kvm
# modprobe -r kvm_intel kvm
# modprobe kvm allow_unsafe_assigned_interrupts=1
# modprobe kvm_intel


3. Start a guest
# virsh start myVF
Domain myVF started

4. Check the driver of 1 vf in host
# readlink -f /sys/bus/pci/devices/0000\:03\:10.1/driver/
/sys/bus/pci/drivers/igbvf


5. Attach this vf to guest
# cat hostdev.xml 
    <interface type='hostdev' managed='yes'>
      <source>
        <address type='pci' domain='0x0000' bus='0x03' slot='0x10' function='0x1'/>
      </source>
    </interface>

# virsh attach-device myVF hostdev.xml 

6. Check the interface in guest
# virsh dumpxml myVF
# readlink -f /sys/bus/pci/devices/0000\:03\:10.1/driver/

7. Detach interface
# virsh detach-device myVF hostdev.xml

8. Check the vf driver in host
# readlink -f /sys/bus/pci/devices/0000\:03\:10.1/driver/

9. Attach again
# virsh attach-device myVF hostdev.xml

10. Detach interface with detach-interface, the mac is get from guest xml as step6 output shows
# virsh detach-interface myVF hostdev 52:54:00:ec:52:44

11. Check the vf driver in host
# readlink -f /sys/bus/pci/devices/0000\:03\:10.1/driver/



	
Expected Results:

Step 4.

# readlink -f /sys/bus/pci/devices/0000\:03\:10.1/driver/
/sys/bus/pci/drivers/igbvf 

Step 5.

Device attached successfully

Step 6.

# virsh dumpxml myVF

......

    <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:ec:52:44'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x03' slot='0x10' function='0x1'/>
      </source>
      <alias name='hostdev0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>

......

# readlink -f /sys/bus/pci/devices/0000\:03\:10.1/driver/
 /sys/bus/pci/drivers/pci-stub 

Step 7

  Detach should succeed without error

Step 8

# readlink -f /sys/bus/pci/devices/0000\:03\:10.1/driver/
/sys/bus/pci/drivers/igbvf

Step 9.

Device attached successfully

 Step 10

 Interface detached successfully 

 Step 11.

# readlink -f /sys/bus/pci/devices/0000\:03\:10.1/driver/
/sys/bus/pci/drivers/igbvf

 
Notes:
Comments:

		193554 	[libvirtd] Libvirt should never be double-closing an fd - bug 845893 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    874897 - From Run 51211
    874897 - From Run 54163

Actions:

1. On one host A, stop libvirtd service and start it by a non-root user
#service libvirtd stop
#su tester
$libvirtd -d

2. On another host B, try to connect that libvirtd
#virsh -c qemu+ssh://tester@${hostA_ip}/session
or
#virsh -c qemu+ssh://tester@${hostA_ip}/system
or 
#virsh -c qemu+ssh://tester@${hostA_ip}/aaaaaa

	
Expected Results:

For now, bug still now fix,  the actual result is"

Step2:

error: End of file while reading data: : Input/output error
error: failed to connect to the hypervisor

and no such error messages:

error: End of file while reading data: 2012-08-06 02:41:17.381+0000: 7888: info : libvirt version: 0.10.0, package: 0rc0.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2012-08-02-03:44:27, x86-007.build.bos.redhat.com)
2012-08-06 02:41:17.381+0000: 7888: warning : virFileClose:65 : Tried to close invalid fd 7: Input/output error
error: failed to connect to the hypervisor

Notes:
Add some steps for new bug 874897 by bili
Clean steps for bug 874897 as it was remove to rhel-6.5.0 - bili
Comments:

		193555 	[libvirtd] destroy a creating domain - bug 843716 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Prepare a XML for creating domain.

# virsh create --console abc.xml

2. Destroy the creating domain

#virsh destroy abc

3. Check the libvirtd status

#service libvirtd status

#virsh list --all

	
Expected Results:

3.

1) libvirtd still running

2) the new create guest abc has gone.
Notes:
Comments:

		194213 	[Log and debugging]Enable stack traces in log messages 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging

bug:

    No bug found

Actions:

1.  Configure the following settings  in /etc/libvirt/libvirtd.conf.

log_level=1
log_filters="1:+libvirt 1:util 1:qemu"
log_outputs="1:file:/var/log/libvirt/libvirtd.log"

2.  Restart libvirtd

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

 
	
Expected Results:

Check the log file. It should  includes stack traces 

 

2012-08-09 06:52:15.990+0000: 37394: debug : do_open:1130 : name "qemu:///system" to URI components:
  scheme qemu
  server (null)
  user (null)
  port 0
  path /system

/usr/lib64/libvirt.so.0(virLogMessage+0x7c)[0x374585192c]
/usr/lib64/libvirt.so.0[0x37458dd6f9]
/usr/lib64/libvirt.so.0(virConnectOpen+0x56)[0x37458de836]
libvirtd[0x447f61]
/usr/lib64/libvirt.so.0(virStateInitialize+0x7b)[0x37458de96b]
libvirtd[0x420dc1]
/usr/lib64/libvirt.so.0[0x374585b9a9]
/lib64/libpthread.so.0[0x370b407851]
/lib64/libc.so.6(clone+0x6d)[0x370b0e767d]

2012-08-09 06:52:15.991+0000: 37394: debug : do_open:1177 : trying driver 0 (Test) ...
/usr/lib64/libvirt.so.0(virLogMessage+0x7c)[0x374585192c]
/usr/lib64/libvirt.so.0[0x37458dd7ec]
/usr/lib64/libvirt.so.0(virConnectOpen+0x56)[0x37458de836]
libvirtd[0x447f61]
/usr/lib64/libvirt.so.0(virStateInitialize+0x7b)[0x37458de96b]
libvirtd[0x420dc1]
/usr/lib64/libvirt.so.0[0x374585b9a9]
/lib64/libpthread.so.0[0x370b407851]
/lib64/libc.so.6(clone+0x6d)[0x370b0e767d]

 

2012-08-09 06:52:15.990+0000: 37394: debug : virConnectGetConfigFile:956 : Loading config file '/etc/libvirt/libvirt.conf'
/usr/lib64/libvirt.so.0(virLogMessage+0x7c)[0x374585192c]
/usr/lib64/libvirt.so.0[0x37458de372]
/usr/lib64/libvirt.so.0(virConnectOpen+0x56)[0x37458de836]
libvirtd[0x447f61]
/usr/lib64/libvirt.so.0(virStateInitialize+0x7b)[0x37458de96b]
libvirtd[0x420dc1]
/usr/lib64/libvirt.so.0[0x374585b9a9]
/lib64/libpthread.so.0[0x370b407851]
/lib64/libc.so.6(clone+0x6d)[0x370b0e767d]

 
Notes:
Comments:

		194276 	[storage]create a rbd pool and create vol in the pool 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

rhel6 not support ceph and sheepdog
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. prepare ceph env. ,check it worked well
#ceph -s
HEALTH_WARN 208 pgs degraded; 208 pgs stuck unclean; recovery 30/60 degraded (50.000%)
   monmap e1: 1 mons at {0=192.168.122.252:6789/0}, election epoch 0, quorum 0 0
   osdmap e7: 1 osds: 1 up, 1 in
    pgmap v49: 208 pgs: 208 active+degraded; 16660 bytes data, 101 MB used, 771 MB / 1000 MB avail; 30/60 degraded (50.000%)
   mdsmap e4: 1/1/1 up {0=0=up:active}
 v0) v1 ==== 415+0+0 (336046976 0 0) 0x7f0f10001150 con 0x19ddb80
   health HEALTH_WARN 208 pgs degraded; 208 pgs stuck unclean; recovery 30/60 degraded (50.000%)
   monmap e1: 1 mons at {0=192.168.122.252:6789/0}, election epoch 0, quorum 0 0
   osdmap e7: 1 osds: 1 up, 1 in
    pgmap v49: 208 pgs: 208 active+degraded; 16660 bytes data, 101 MB used, 771 MB / 1000 MB avail; 30/60 degraded (50.000%)
   mdsmap e4: 1/1/1 up {0=0=up:active}

2. create a rados pool
#rados mkpool rbdpool
#rados lspools
data
metadata
rbdpool

3. make sure the pool can worked, create a rbd volume named foo
#qemu-img create -f rbd rbd:rbdpool/foo 1G

# qemu-img info rbd:rbdpool/foo
image: rbd:rbdpool/foo
file format: raw
virtual size: 1.0G (1073741824 bytes)
disk size: unavailable

4. create a rbd pool xml

<pool type="rbd">
    <name>myrbdpool</name>
      <source>
      <name>rbdpool</name>
      <host name='192.168.122.252' port='6789'/>
       <auth username='admin' type='ceph'>
       <secret uuid='2ec115d7-3a88-3ceb-bc12-0ac909a6fd87'/>
       </auth>
      </source>
</pool>

5. define the pool
#virsh pool-define rbd.xml

6. prepare a vol xml

<volume>
         <name>myvol</name>
         <key>rbd/myvol</key>
         <source>
         </source>
         <capacity unit='bytes'>53687091200</capacity>
         <allocation unit='bytes'>53687091200</allocation>
         <target>
           <path>rbd:rbd/myvol</path>
           <format type='unknown'/>
           <permissions>
             <mode>00</mode>
             <owner>0</owner>
             <group>0</group>
           </permissions>
         </target>
       </volume>

7. define the vol

 
	
Expected Results:

verify the pool can be defined.

=====have a bug ======
Notes:
Comments:

		194446 	[CPU Management] libvirt support Haswell new instructions- bug843087 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
 Bug 843087 - [Intel 6.4 FEAT] Haswell new instructions support for libvirt

Feature Description:
Haswell CPU will support more new instructions. For example,
 - FP fused Multiply Add
 - 256-bit Integer vectors
 - MOVBE support
 - HLE/HLE+
 - etc.


Reserve the Haswell machine from beaker using the following filter:
Key/Value - System/Model  contains Haswell
 
such as: intel-sharkbay-dh-01.lab.bos.redhat.com
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    843087

Actions:

1. Check definition for this cpu model:

# cat /usr/share/libvirt/cpu_map.xml

 

2. check the cpu model in host.

# virsh capabilities

 

3. # virsh edit rhel6.4
<domain>
.....
<cpu mode='host-model'>
    <model fallback='forbid'/>
  </cpu>
......
</domain>


# virsh start rhel6.4
Domain rhel6.4 started


4. # ps -ef|grep rhel6.4


5. On sandybridge machine , start one guest with Haswell cpu model

# virsh start rhel6.4
	
Expected Results:

step 1,

    <model name='Haswell'>
      <model name='SandyBridge'/>
      <feature name='fma'/>
      <feature name='pcid'/>
      <feature name='movbe'/>
      <feature name='fsgsbase'/>
      <feature name='bmi1'/>
      <feature name='hle'/>
      <feature name='avx2'/>
      <feature name='smep'/>
      <feature name='bmi2'/>
      <feature name='erms'/>
      <feature name='invpcid'/>
      <feature name='rtm'/>
    </model>

 

step 2,

<capabilities>

  <host>
    <uuid>001320fb-61bb-0013-20fb-61bb001320fb</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Haswell</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='abm'/>
      <feature name='pdpe1gb'/>
      <feature name='rdrand'/>
      <feature name='f16c'/>
      <feature name='osxsave'/>
      <feature name='pcid'/>
      <feature name='pdcm'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='smx'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='dtes64'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>
......

</capabilities>

step 4,

qemu     32696     1 83 22:39 ?        00:00:27 /usr/libexec/qemu-kvm -name rhel6.4 -S -M rhel6.4.0 -cpu Haswell, +abm,+pdpe1gb,+rdrand,+f16c,+osxsave,+pcid,+pdcm,+xtpr,+tm2,+est,+smx,+vmx,+ds_cpl,+monitor,+dtes64,+pbe,+tm,+ht,+ss,+acpi,+ds,+vme -enable-kvm -m 1024 -smp 1,maxcpus=7,sockets=7,cores=1,threads=1 -uuid 4781b892-413e-9f07-3400-12694227becb -nodefconfi

step 5,

error: Failed to start domain rhel6.4
error: unsupported configuration: guest and host CPU are not compatible: Host CPU does not provide required features: rtm, invpcid, erms, bmi2, smep, avx2, hle, bmi1, fsgsbase, movbe, fma
Notes:
Comments:

		194549 	[Miscellanea] libvirt's text interfaces are locale-dependent - 845493 	ajia 	None 	Manual 		Bug verification 	P2 	None 	Edit
Setup:

If libvirtd is running, please stop it.

# service libvirtd status
libvirtd (pid  12648) is running...

# service libvirtd stop
Stopping libvirtd daemon:                                  [  OK  ]

Start libvird with "sv_SE.UTF-8" locle.

# LC_ALL="sv_SE.UTF-8" libvirtd -d -v
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    845493

Actions:

1. Start an existing guest or install a new guest via virt-manager or virt-install,

# virsh start myRHEL6
Domain myRHEL6 started

 

2. Change guest memory such as 524288Kib

# virsh setmem myRHEL6 524288
	
Expected Results:

Check steps2 virsh output, you can see the following error message:

# virsh setmem myRHEL6 524288
error: internt fel Unexpected JSON reply '{"error": {"class": "QMPBadInputObject", "desc": "Expected 'object' in QMP input", "data": {"expected": "object"}}}'
Notes:
Comments:

		194551 	[virtual networks] RFE: [required for VMFEX support] Add ability to dynamically change a libvirt network - BZ#844404 	ajia 	ajia 	Manual (Autoproposed) 		Bug verification 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:
1. Prepare a network for modifing.

# virsh net-list
Name                 State      Autostart     Persistent
--------------------------------------------------
default              active     yes           yes
net-1                active     no            yes

# virsh net-dumpxml net-1
<network>
  <name>net-1</name>
  <uuid>d7f1e011-6347-5c14-1941-46d7ff1b5e57</uuid>
  <forward mode='nat'/>
  <bridge name='net-test' stp='on' delay='0' />
  <mac address='52:54:00:E7:56:E9'/>
  <ip address='192.168.200.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.200.2' end='192.168.200.254' />
    </dhcp>
  </ip>
</network>

2. Modify the network via net-update

# virsh net-update net-1 modify bridge "<bridge name='net-update' stp='on' delay='0' />"

3. Delete the 'bridge' section of the network via net-update

# virsh net-update net-1 delete bridge "<bridge name='net-test' stp='on' delay='0' />"

ï»¿4. Prepare a xml file for 'portgroup' section updating(add, delete, modify)

# cat portgroup.xml
  <portgroup name='engineering' default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='test'/>
    </virtualport>
    <bandwidth>
      <inbound average='1000' peak='5000' burst='5120'/>
      <outbound average='1000' peak='5000' burst='5120'/>
    </bandwidth>
  </portgroup>

4.1 Add 'portgroup' seciton for the network

# virsh net-update net-1 add-first portgroup portgroup.xml 

# virsh net-dumpxml net-1 

4.2 Modify 'portgroup' section of the network

# cat portgroup-new.xml

<portgroup name='engineering' default='no'>
  <virtualport type='802.1Qbh'>
    <parameters profileid='test'/>
  </virtualport>
  <bandwidth>
    <inbound average='2000' peak='5000' burst='5120'/>
    <outbound average='3000' peak='5000' burst='5120'/>
  </bandwidth>
</portgroup>

# virsh net-update net-1 modify portgroup portgroup-new.xml 
Updated network net-1 live state

# virsh net-dumpxml net-1

4.3 Add the section portgroup

# cat new-portgroup.xml 
  <portgroup name='sales' default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='salestest'/>
    </virtualport>
    <bandwidth>
      <inbound average='500' peak='2000' burst='2560'/>
      <outbound average='128' peak='256' burst='256'/>
    </bandwidth>
  </portgroup>

#  virsh net-update net-1 add-last portgroup new-portgroup.xml 

# virsh net-dumpxml net-1

4.4 Delete one portgroup

# cat new-portgroup.xml 
  <portgroup name='sales' default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='salestest'/>
    </virtualport>
    <bandwidth>
      <inbound average='500' peak='2000' burst='2560'/>
      <outbound average='128' peak='256' burst='256'/>
    </bandwidth>
  </portgroup>

# virsh net-update net-1 delete portgroup new-portgroup.xml 

# virsh net-dumpxml net-1

5. Repeat the steps2~4 to update network with '--config' option(both network running and shut off)

6. Prepare a xml file for 'ip-dhcp-host' section updating(add, delete, modify)

#virsh net-dumpxml net-1
<network>
  <name>net-1</name>
  <uuid>d7f1e011-6347-5c14-1941-46d7ff1b5e57</uuid>
  <forward mode='nat'/>
  <bridge name='net-test' stp='on' delay='0' />
  <mac address='52:54:00:E7:56:E9'/>
  <ip address='192.168.200.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.200.2' end='192.168.200.254' />
      <host mac='52:54:00:5a:a0:8b' ip='192.168.200.150' />
    </dhcp>
  </ip>
</network>

6.1

6.1.1
# virsh net-update net-1 modify ip-dhcp-host  "<host mac='52:54:00:5a:a0:8b' ip='192.168.200.152' />"
Updated network net-1 live state

then start a guest network using net-1, check its ip is 192.168.200.152  (expected result) 

6.1.2

# virsh net-update net-1 delete ip-dhcp-host  "<host mac='52:54:00:5a:a0:8b' ip='192.168.200.152' />"
Updated network net-1 live state

6.1.3
#virsh net-update net-1 add ip-dhcp-host  "<host mac='52:54:00:5a:a0:8b' ip='192.168.200.152' />"
Updated network net-1 live state


6.2
# virsh net-update net-1 add forward-interface "<interface dev='eth2'/>"
Updated network net-1 live state

then start a guest check guest network does not work well

 # virsh net-update net-1 del forward-interface "<interface dev='eth2'/>"
Updated network net-1 live state

 # virsh net-update net-1 add forward-interface "<interface dev='eth0'/>"
Updated network net-1 live state

check guest network works well


# virsh net-update net-1 add-first ip-dhcp-range  "<range start='192.168.200.100' end='192.168.200.150' />"
Updated network net-1 live state

then check guest's ip is 192.168.200.100

 

 

 

	
Expected Results:

2. 

# virsh net-update net-1 modify bridge "<bridge name='net-update' stp='on' delay='0' />"
error: Failed to update network net-1
error: this function is not supported by the connection driver: can't update 'bridge' section of network 'net-1'

3.

# virsh net-update net-1 delete bridge "<bridge name='net-test' stp='on' delay='0' />"
error: Failed to update network net-1
error: this function is not supported by the connection driver: can't update 'bridge' section of network 'net-1'

 4.1

# virsh net-update net-1 add-first portgroup portgroup.xml 

Updated network net-1 live state

# virsh net-dumpxml net-1
<network>
  <name>net-1</name>
  <uuid>d7f1e011-6347-5c14-1941-46d7ff1b5e57</uuid>
  <forward mode='nat'/>
  <bridge name='net-test' stp='on' delay='0' />
  <mac address='52:54:00:E7:56:E9'/>
  <ip address='192.168.200.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.200.2' end='192.168.200.254' />
    </dhcp>
  </ip>
  <portgroup name='engineering' default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='test'/>
    </virtualport>
    <bandwidth>
      <inbound average='1000' peak='5000' burst='5120'/>
      <outbound average='1000' peak='5000' burst='5120'/>
    </bandwidth>
  </portgroup>
</network>

 4.2

# virsh net-update net-1 modify portgroup portgroup-new.xml 
Updated network net-1 live state

 

# virsh net-dumpxml net-1
<network>
  <name>net-1</name>
  <uuid>d7f1e011-6347-5c14-1941-46d7ff1b5e57</uuid>
  <forward mode='nat'/>
  <bridge name='net-test' stp='on' delay='0' />
  <mac address='52:54:00:E7:56:E9'/>
  <ip address='192.168.200.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.200.2' end='192.168.200.254' />
    </dhcp>
  </ip>
  <portgroup name='engineering'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='test'/>
    </virtualport>
    <bandwidth>
      <inbound average='2000' peak='5000' burst='5120'/>
      <outbound average='3000' peak='5000' burst='5120'/>
    </bandwidth>
  </portgroup>
</network>

4.3 

#  virsh net-update net-1 add-last portgroup new-portgroup.xml 
Updated network net-1 live state
# virsh net-dumpxml net-1
<network>
  <name>net-1</name>
  <uuid>d7f1e011-6347-5c14-1941-46d7ff1b5e57</uuid>
  <forward mode='nat'/>
  <bridge name='net-test' stp='on' delay='0' />
  <mac address='52:54:00:E7:56:E9'/>
  <ip address='192.168.200.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.200.2' end='192.168.200.254' />
    </dhcp>
  </ip>
  <portgroup name='engineering'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='test'/>
    </virtualport>
    <bandwidth>
      <inbound average='2000' peak='5000' burst='5120'/>
      <outbound average='3000' peak='5000' burst='5120'/>
    </bandwidth>
  </portgroup>
  <portgroup name='sales' default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='salestest'/>
    </virtualport>
    <bandwidth>
      <inbound average='500' peak='2000' burst='2560'/>
      <outbound average='128' peak='256' burst='256'/>
    </bandwidth>
  </portgroup>
</network>

 4.4

# virsh net-update net-1 delete portgroup new-portgroup.xml 
Updated network net-1 live state
# virsh net-dumpxml net-1
<network>
  <name>net-1</name>
  <uuid>d7f1e011-6347-5c14-1941-46d7ff1b5e57</uuid>
  <forward mode='nat'/>
  <bridge name='net-test' stp='on' delay='0' />
  <mac address='52:54:00:E7:56:E9'/>
  <ip address='192.168.200.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.200.2' end='192.168.200.254' />
    </dhcp>
  </ip>
  <portgroup name='engineering'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='test'/>
    </virtualport>
    <bandwidth>
      <inbound average='2000' peak='5000' burst='5120'/>
      <outbound average='3000' peak='5000' burst='5120'/>
    </bandwidth>
  </portgroup>
</network>

5. 'config' option work well

6. As setp shows.

 

Notes:
Comments:

		195111 	[snapshot] Return a specific error when qemu-ga is missing or unusable during a live snapshot (quiesce) BZ# 845635 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Check https://bugzilla.redhat.com/show_bug.cgi?id=845635   status . make sure it is fixed
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1) when qemu-ga is working 

# virsh snapshot-create-as q64 g1 --disk-only --quiesce
Domain snapshot g1 created



2) when qemu-ga is missing 
# virsh snapshot-create-as q64 g2 --disk-only --quiesce 
error: Guest agent is not responding: Guest agent not available for now

libvirtd.log
2012-09-21 07:40:19.000+0000: 10200: error : qemuAgentSend:888 : Guest agent is not responding: Guest agent not available for now
2012-09-21 07:40:24.000+0000: 10200: error : qemuAgentSend:888 : Guest agent is not responding: Guest agent not available for now

	
Expected Results:

As steps
Notes:
Comments:

		195113 	[snapshot] snapshot-list --descendants --from will core dumped BZ#845468 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Check https://bugzilla.redhat.com/show_bug.cgi?id=845468  status first.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1.  prepare a domain with snapshot
# virsh snapshot-list q3
 Name                 Creation Time             State
------------------------------------------------------------
 d1                   2012-07-26 14:43:43 +0800 disk-snapshot
 d2                   2012-07-26 14:43:49 +0800 disk-snapshot
 d3                   2012-07-26 14:43:56 +0800 disk-snapshot
 s1                   2012-07-26 11:06:19 +0800 running
 s2                   2012-07-26 14:34:18 +0800 shutoff
 s3                   2012-07-26 14:34:56 +0800 shutoff


2.

2.# virsh snapshot-list q3 --descendants --from d1

 Name                 Creation Time             State
------------------------------------------------------------
 d2                   2012-07-26 14:43:49 +0800 disk-snapshot
 d3                   2012-07-26 14:43:56 +0800 disk-snapshot
 s1                   2012-07-26 11:06:19 +0800 running
 s2                   2012-07-26 14:34:18 +0800 shutoff
 s3                   2012-07-26 14:34:56 +0800 shutoff


 [root@intel-q9400-4-2 ~]# virsh snapshot-list q3 --descendants --from s1

 Name                 Creation Time             State
------------------------------------------------------------
 s2                   2012-07-26 14:34:18 +0800 shutoff
 s3                   2012-07-26 14:34:56 +0800 shutoff




	
Expected Results:

step 2 : 

Expected results:

no core dumped

Notes:
Comments:

		195114 	[snapshot] disk-only snapshot create external file even if snapshot command failed BZ#843372 	whuang 	None 	Manual (Autoproposed) 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1.# prepare a qcow2 domain with 2 disk
...
  <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/q2.new2'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/q2-2-new2'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </disk>

...

2.  try to create disk-only snapshot with xml   file
<domainsnapshot>
<description>Snapshot of OS install and updates</description>
<disks>
<disk name='/var/lib/libvirt/images/q2.new2'>
<source file='/var/lib/libvirt/images/q2.new3'/>
</disk>
<disk name='/var/lib/libvirt/images/q2-2-new2'>
<source file='/var/lib/libvirt/images/q2-2-new3'/>
</disk>
</disks>
</domainsnapshot>

3. do some operation  to make snapshot failed , you can do other option to make snapshot fail .too
# chmod 000 /var/lib/libvirt/images/q2-2-new2

4.  check whether file is exist before  the run the virsh snapshot cmd
#ll /var/lib/libvirt/images/*new3*
ls: cannot access /var/lib/libvirt/images/*new3*: No such file or directory

# virsh snapshot-create q2 --disk-only snap-disk.xml
error: internal error unable to execute QEMU command 'transaction': Could not open '/var/lib/libvirt/images/q2-2-new3': Permission denied


# ll /var/lib/libvirt/images/*new3*
ls: cannot access /var/lib/libvirt/images/*new3*: No such file or directory

 NOTE: snapshot cmd failed and q2.new3 file is not created .

	
Expected Results:

Expected results:

as steps

Notes:
Comments:

		195115 	[snapshot] snapshot-edit will report error message but return 0 when do not update xml BZ#843324 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Check https://bugzilla.redhat.com/show_bug.cgi?id=843324 status
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1. create a snapshot for domain
#virsh snapshot-create-as q3 s1
Domain snapshot s1 created


2. run  snapshot-edit but do not modify any info in the xml then quit it

#virsh snapshot-edit q3 s1 
Snapshot snap1 XML configuration not changed.


[root@intel-q9400-4-2 ~]# echo $?
0

	
Expected Results:

step 2
Expected results:
no error message likeï¼

# virsh snapshot-edit q3 s1 Snapshot s1 XML configuration not changed. error: Failed to update s1

when return 0 

Notes:
Comments:

		197204 	[Migration]migrated VM not visible to non-root user on the target host BZ 847822 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression
    rhel6.5

bug:

    No bug found

Actions:

1. create a normal user "redhat" on both machines
[root@zhpeng ~]# id redhat
uid=500(redhat) gid=500(redhat) groups=500(redhat),10(wheel)

2. create a guest with qemu:///session

[redhat@zhpeng ~]$ virsh list
 Id Name                 State
----------------------------------
  4 sss                  running

[redhat@zhpeng ~]$ virsh uri
qemu:///session

3. migrate it to dst machine
$ virsh migrate --live sss qemu+tcp://redhat@10.66.7.230/session
$ virsh list --all
 Id Name                 State
----------------------------------
  - sss                  shut off

or add auth_unix_rw = "none" to libvirtd.conf and use qemu+ssh
The result is the same.


4. on dst:
# virsh list --all
 Id Name                 State
----------------------------------
$ virsh list --all
 Id Name                 State
----------------------------------

  3 sss                  running

 

	
Expected Results:

Step 4:

guest running in session mode

BZ 847822 Not fixed yet,pls update steps later.
Notes:
Comments:

		197477 	[storage] create volumes with virsh vol-create-as command for mutipath 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage
    rhel6.5

bug:

    No bug found

Actions:

1. install  rhel and boot from local storage
2. enable multipath after the installation is finished
3. add 50-100GB SAN LUN
4. use parted command to create a GPT disk label on the disk
5. create storage pool configuration file
6. use "virsh pool-define" command with pool configuration file to attch the device
7. start the storage pool using "pool-start" and "pool-autostart" commands
8. verify the storage pool configuration using "pool-info"
9. creat 10G volume using "virsh vol-create-as Storage vol1 10G" command
10. create another 10 volume using the same command
11. failed to created vol error shows up

	
Expected Results:

 

refer:https://bugzilla.redhat.com/show_bug.cgi?id=846564
Notes:
Comments:

		197505 	[NUMA]Run virsh cpu-stats command on NUMA machine - bug 846629 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

NUMA machine with two or more nodes
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NUMA
    Regression

bug:

    No bug found

Actions:

1.Define a guest on NUMA machine

#virsh dumpxml <guest>

<domain type='kvm' id='11'>
  <name>yuping-rhel6</name>
  <uuid>94f74de5-6704-ceba-4625-54ac40f90602</uuid>
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/RHEL63.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </disk>
    <controller type='usb' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
    </controller>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:22:36:35'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/1'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/1'>
      <source path='/dev/pts/1'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5900' autoport='yes' listen='127.0.0.1'>
      <listen type='address' address='127.0.0.1'/>
    </graphics>
    <sound model='ich6'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux' relabel='yes'>
    <label>system_u:system_r:svirt_t:s0:c605,c733</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c605,c733</imagelabel>
  </seclabel>
</domain>

2.Start guest.

3.Check the CPU stats.

# virsh cpu-stats yuping-rhel6

4.cat /cgroup/cpuacct/libvirt/qemu/yuping-rhel6/cpuacct.usage_percpu

5.Use qemu-kvm to create a 5 more guests with the same name.

e.g.#/usr/libexec/qemu-kvm -enable-kvm -m 1G -smp 2 -uuid b2e2b41c-d562-4f7f-82bb-1d45c1919221 -M rhel6.3.0 -name yuping-rhel6 -drive file=/var/lib/libvirt/images/RHEL63.img,if=none,id=virtio,format=raw,cache=none,werror=stop,rerror=stop -device ide-drive,drive=virtio,id=drive-virtio0-0-0 -spice port=5930,disable-ticketing -vga qxl -monitor stdio

Change the uuid and spice port to create 4 more same name guests.

6.Run one day,then check cpu-stats.

# virsh cpu-stats yuping-rhel6
	
Expected Results:

3.# virsh cpu-stats yuping-rhel6
CPU0:
    cpu_time             0.001560378 seconds
    vcpu_time            0.001560378 seconds
CPU1:
    cpu_time             0.041017379 seconds
    vcpu_time            0.041017379 seconds
CPU2:
    cpu_time             0.001444663 seconds
    vcpu_time            0.001444663 seconds
CPU3:
    cpu_time             0.000000000 seconds
    vcpu_time            0.000000000 seconds
CPU4:
    cpu_time             0.000000000 seconds
    vcpu_time            0.000000000 seconds
CPU5:
    cpu_time             0.000000000 seconds
    vcpu_time            0.000000000 seconds
CPU6:
    cpu_time             0.000000000 seconds

.....

CPU158:
    cpu_time             0.000040425 seconds
    vcpu_time            0.000040425 seconds
CPU159:
    cpu_time             0.000078792 seconds
    vcpu_time            0.000078792 seconds
Total:
    cpu_time             0.691177568 seconds
    user_time            0.000000000 seconds
    system_time          0.000000000 seconds
 

4.cat /cgroup/cpuacct/libvirt/qemu/yuping-rhel6/cpuacct.usage_percpu
167099473 47727917 86583323 919207277 584596 0 0 1208447 0 0 2778079936 198603386 327874418 42432905 0 0 0 71522021 792260 0 1562261703 4372655484 961567061 450674454 8798765 2960690 158450 6527503 0 1740837 449682375 2423626473 38683045 1277405 0 0 367171 0 0 30878 725290783 240625838 1109906548 0 0 30524 0 0 0 323638 232244491 389693681 332080 0 0 0 494471 0 0 0 82828230 1012232 0 347316 0 0 0 0 0 0 3511786156 231472373 1190804361 22511513 0 0 0 0 0 0 3165786 3724623 3198772 0 0 0 0 0 0 0 1716485071 24285408 0 0 0 0 0 0 0 0 1562540957 1284154690 69805525 0 0 0 0 0 0 0 225786271 1003562684 1403113 0 0 0 0 0 0 10045294 477548202 71134154 1487068 0 0 0 0 0 0 8146556 63656031 4421425 0 22095129 0 1057089 1314229 0 0 11233421 8049518 0 0 0 0 0 0 0 3495472 2147907 87864840 40805430 9078167 1369425 0 0 0 2371537 424215256 42256987

6.Should list CPU stats successfully like step 3.

No error like this pop up:

# virsh cpu-stats yuping-rhel6
error: Failed to virDomainGetCPUStats()

error: Failed to read file '/cgroup/cpuacct/libvirt/qemu/yuping-rhel6/cpuacct.usage_percpu': Value too large for defined data type


Notes:
Comments:

		197542 	[CPU Management] libvirt support PV EOI feature- bug848185,bug860971 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu

bug:

    848185

Actions:

1.
# virsh start newrhel6
Domain newrhel6 started

# virsh dumpxml newrhel6
<domain type='kvm' id='7'>
 ......
  <features>
    <acpi/>
    <apic eoi='on'/>
    <pae/>
  </features>
   <clock offset='localtime'> 
      <timer name='kvmclock' present='no'/> 
   </clock>
 ......
</domain>

# ps -ef|grep qemu-kvm
qemu     10813     1 24 13:47 ?        00:00:15 /usr/libexec/qemu-kvm -name newrhel6 -S -M rhel6.3.0 -cpu qemu64,-kvmclock, qemu64,+kvm_pv_eoi -enable-kvm ......


2.Edit the domain ,change the value of eoi as 'off'. and remove timer part.
# virsh edit newrhel6
<domain type='kvm' id='10'>
...... 
    <apic eoi='off'/>
......
</domain>

Domain newrhel6 XML configuration edited.

# virsh destroy newrhel6
Domain newrhel6 destroyed

# virsh start newrhel6
Domain newrhel6 started

# ps -ef|grep qemu-kvm
qemu     11162     1  7 13:55 ?        00:00:00 /usr/libexec/qemu-kvm -name newrhel6 -S -M rhel6.3.0 -cpu qemu64,-kvm_pv_eoi -enable-kvm ......

 
	
Expected Results:

The expected result is the same as the result of command.
Notes:
Comments:

		198948 	[libvirt domain event handler] Watch for an event to indicate guest stopped using a hotunplugged device - bug 813752 	bili 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virtual disks
    rhel6.5

bug:

    No bug found

Actions:

The case just created for tracking the bug Bug 813752 - Watch for an event to indicate guest stopped using a hotunplugged device.
	
Expected Results:
Notes:
Comments:

		199599 	[libvirtd] libvirtd memory leak BZ#851401 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

yum install -y valgrind
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. Stop libvirtd service.
2. run # valgrind libvirtd -d


# valgrind libvirtd -d
==26158== Memcheck, a memory error detector
==26158== Copyright (C) 2002-2010, and GNU GPL'd, by Julian Seward et al.
==26158== Using Valgrind-3.6.0 and LibVEX; rerun with -h for copyright info
==26158== Command: libvirtd -d
==26158== 
==26158== Conditional jump or move depends on uninitialised value(s)
==26158==    at 0x334AD2E2E0: __strcasestr_sse42 (in /lib64/libc-2.12.so)
==26158==    by 0x3361404004: numa_node_size64 (in /usr/lib64/libnuma.so.1)
==26158==    by 0x3361405A48: ??? (in /usr/lib64/libnuma.so.1)
==26158==    by 0x334A40E534: _dl_init (in /lib64/ld-2.12.so)
==26158==    by 0x334A400B39: ??? (in /lib64/ld-2.12.so)
==26158==    by 0x1: ???
==26158==    by 0x7FF00075E: ???
==26158==    by 0x7FF000767: ???
==26158== 
==26158== Conditional jump or move depends on uninitialised value(s)
==26158==    at 0x334AD2E2E4: __strcasestr_sse42 (in /lib64/libc-2.12.so)
==26158==    by 0x3361404004: numa_node_size64 (in /usr/lib64/libnuma.so.1)
==26158==    by 0x3361405A48: ??? (in /usr/lib64/libnuma.so.1)
==26158==    by 0x334A40E534: _dl_init (in /lib64/ld-2.12.so)
==26158==    by 0x334A400B39: ??? (in /lib64/ld-2.12.so)
==26158==    by 0x1: ???
==26158==    by 0x7FF00075E: ???
==26158==    by 0x7FF000767: ???
==26158== 
==26158== Conditional jump or move depends on uninitialised value(s)
==26158==    at 0x334AD2E2EC: __strcasestr_sse42 (in /lib64/libc-2.12.so)
==26158==    by 0x3361404004: numa_node_size64 (in /usr/lib64/libnuma.so.1)
==26158==    by 0x3361405A48: ??? (in /usr/lib64/libnuma.so.1)
..............

[root@intel-w3520-8-2 ~]# ps aux|grep libvirtd
root     26160  3.4  1.7 536276 141892 ?       Sl   13:02   0:06 valgrind libvirtd -d
root     26286  0.0  0.0 103244   860 pts/2    S+   13:05   0:00 grep libvirtd
[root@intel-w3520-8-2 ~]# kill 26160
[root@intel-w3520-8-2 ~]# ==26160== 
==26160== HEAP SUMMARY:
==26160==     in use at exit: 956,598 bytes in 9,669 blocks
==26160==   total heap usage: 179,210 allocs, 169,541 frees, 451,276,713 bytes allocated
==26160== 
==26160== LEAK SUMMARY:
==26160==    definitely lost: 0 bytes in 0 blocks
==26160==    indirectly lost: 0 bytes in 0 blocks
==26160== possibly lost: 4,048 bytes in 11 blocks
==26160==    still reachable: 952,550 bytes in 9,658 blocks
==26160==         suppressed: 0 bytes in 0 blocks
==26160== Rerun with --leak-check=full to see details of leaked memory
==26160== 
==26160== For counts of detected and suppressed errors, rerun with: -v
==26160== Use --track-origins=yes to see where uninitialised values come from
==26160== ERROR SUMMARY: 49 errors from 12 contexts (suppressed: 29 from 9)

 

	
Expected Results:

there is not lost bytes in the message

 

==26160== LEAK SUMMARY:
==26160==    definitely lost: 0 bytes in 0 blocks
==26160==    indirectly lost: 0 bytes in 0 blocks
==26160== possibly lost: 4,048 bytes in 11 blocks
==26160==    still reachable: 952,550 bytes in 9,658 blocks
==26160==         suppressed: 0 bytes in 0 blocks
==26160== Rerun with --leak-check=full to see details of leaked memory
==26160== 

 
Notes:
Comments:

		199601 	[storage] the <format> element of a disk pool can not edit by pool-edit command in active status BZ# 851078 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage

bug:

    No bug found

Actions:

1. Prepare a USB storage.

2. create a disk pool with xml similar with following:

      <pool type="disk">
        <name>dsk_pool</name>
        <source>
          <device path='/dev/sdb'/>
        </source>
        <target>
          <path>/dev</path>
        </target>
      </pool>

  # virsh pool-define dsk_pool.xml
  # virsh pool-start dsk_pool

3. # virsh pool-dumpxml dsk_pool
<pool type='disk'>
  <name>dsk_pool</name>
  <uuid>faeced35-e569-cf95-4000-1aba5ebdda17</uuid>
  <capacity>16022241280</capacity>
  <allocation>0</allocation>
  <available>16022240768</available>
  <source>
    <device path='/dev/sdb'>
    <freeExtent start='512' end='16022241280'/>
    </device>
 <format type='unknown'/>
  </source>
  <target>
    <path>/dev</path>
    <permissions>
      <mode>0700</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>
 
4. Edit the dsk_pool from "<format type='unknown'/>" to "<format type='dos'/>"
# virsh pool-edit dsk_pool
Pool dsk_pool XML configuration edited.

5. # virsh pool-dumpxml dsk_pool
....
<format type='unknown'/>
....

6. It(format type) is still 'unknown' even after you do pool-destroy and pool-start again.

7. destroy the pool and edit the format type to a correct value:
# virsh pool-destroy dsk_pool
Pool dsk_pool destroyed

Edit the dsk_pool from "<format type='unknown'/>" to "<format type='dos'/>"
# virsh pool-edit dsk_pool
Pool dsk_pool XML configuration edited.

8. # virsh pool-dumpxml dsk_pool
....
<format type='dos'/>
....

Edit works.

9. # virsh pool-start dsk_pool
Pool dsk_pool started

then edit again from 'dos' to 'bsd'.
# virsh pool-edit dsk_pool
Pool dsk_pool XML configuration edited.

10. Destroy and dumpxml check again:
# virsh pool-destroy dsk_pool
Pool dsk_pool destroyed

# virsh pool-dumpxml dsk_pool
....
<format type='dos'/>
....

	
Expected Results:

step 6 :

should be <format type='dos'/>

 

step 10 :

should be <format type='bsd'/>

 

 
Notes:
Comments:

		199605 	[storage] discover potential iscsi storage - BZ#851423 	whuang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

modify your host

# cat /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.1994-05.com.redhat:libvirt

# service iscsid restart
Stopping iscsid:
Starting iscsid:                                    [  OK  ]
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage

bug:

    No bug found

Actions:

#cat iscsi.xml

 <source>
 <host name='10.66.90.100'/>
         <device path='iqn.2001-05.com.equallogic:0-8a0906-6eb1f7d03-30cf49b25f24f94d-libvirt-1-150313'/>
           </source>


do this command 

# /usr/bin/virsh find-storage-pool-sources iscsi iscsi-pool.xml

 <sources>
  <source>
    <host name='10.66.90.100'/>
    <device path='iqn.2001-05.com.equallogic:0-8a0906-12a1f7d03-0daf49b25a84ee02-s3-kyla-131842'/>
  </source>
  <source>
    <host name='10.66.90.100'/>
    <device path='iqn.2001-05.com.equallogic:0-8a0906-9951f7d03-34cf49b25f04f94b-libvirt-2-150313'/>
  </source>
  <source>
    <host name='10.66.90.100'/>
    <device path='iqn.2001-05.com.equallogic:0-8a0906-6eb1f7d03-30cf49b25f24f94d-libvirt-1-150313'/>
  </source>
</sources>

	
Expected Results:

NO segmentation fault

 

 
Notes:
Comments:

		199637 	[Graphical framebuffers]Remove channel content----Bug 851963 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1.Define a guest,add spicevmc channel. 
#virsh dumpxml kvm-win7-i386 
... 
<channel type='spicevmc'> 
<target type='virtio' name='com.redhat.spice.0'/> 
<address type='virtio-serial' controller='0' bus='0' port='1'/> 
</channel> 
... 
2.Edit the guest,remove <target type='virtio' name='com.redhat.spice.0'/> in channel.Then save. 
# virsh edit kvm-win7-i386 
error: internal error Could not format channel target type 
Failed. Try again? [y,n,f,?]: 

3.Type y

4. type n, exit

	
Expected Results:

after step 3, verify can edit xml again.

check libvirt log

#cat /var/log/libvirt/libvirt.log

2012-09-05 00:18:10.201+0000: 31497: error : virDomainChrDefaultTargetType:5094 : XML error: target type must be specified for channel device
2012-09-05 00:18:10.542+0000: 31497: error : virDomainChrDefFormat:12212 : internal error Could not format channel target type

after step 4, verify can exit without error

check guest

#virsh list --all

verify the guest not undefine.
Notes:
Comments:

		199716 	[Memory management]setmem when guest do S3 operations 	zhpeng 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

prepare guest agent ENV, pls install qemu-ga in guest and start the service or run # qemu-ga -d in guest.

Bug 839674  - Revert back to a single seabios binary once s3/s4 configuration is in

From seabios-0.6.1.2-25.el6, no /usr/share/seabios/bios-pm.bin anymore.

Bug 848369 - S3/S4 should be disabled by default

It's fixed on qemu-kvm-0.12.1.2-2.320.el6, so we need to enable it
manually from 320.

#virsh dumpxml qemu-ga
...
 <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <loader>/usr/share/seabios/bios.bin</loader>
    <boot dev='hd'/>
  </os>
<pm>
     <suspend-to-mem enabled='yes'/>
     <suspend-to-disk enabled='yes'/>
   </pm>

...
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/rhel63.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
...

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    memory

bug:

    No bug found

Actions:

S3 ----> setmem ----> wakeup

1. # virsh start rhel63
Domain rhel63 started

2. # virsh dumpxml rhel63
...
 <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>4194304</currentMemory>
...

3.# virsh dompmsuspend rhel63 --target mem
Domain rhel63 successfully suspended

4. # virsh setmem --current rhel63 1000000

5. # virsh dompmwakeup rhel63
Domain rhel63 successfully woken up

6. # virsh dumpxml rhel63
...
  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>1000000</currentMemory>
...

 and pls check the mem in guest: cat /proc/meminfo

setmem ----> S3 -----> wakeup

7. fresh boot the guest
# virsh dumpxml rhel63
...
 <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>4194304</currentMemory>
...

8.# virsh setmem rhel63 --current 1000000

9.# virsh dumpxml rhel63
  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>1000000</currentMemory>
 and pls check this in guest too.

10.# virsh dompmsuspend rhel63 --target mem
Domain rhel63 successfully suspended
#virsh  list
 Id    Name                           State
----------------------------------------------------
 3     rhel63                        pmsuspended

11. # virsh  dompmwakeup rhel63
Domain rhel63 successfully woken up

12. # virsh dumpxml rhel63
  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>1000000</currentMemory>

and pls check this in guest too
	
Expected Results:

All as steps
Notes:
Comments:

		199739 	[Memory management]setmem when guest do S4 operations BZ 872419 878966 	zhpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

prepare guest agent ENV, pls install qemu-ga in guest and start the service or run # qemu-ga -d in guest.

Bug 839674  - Revert back to a single seabios binary once s3/s4 configuration is in

From seabios-0.6.1.2-25.el6, no /usr/share/seabios/bios-pm.bin anymore.

Bug 848369 - S3/S4 should be disabled by default

It's fixed on qemu-kvm-0.12.1.2-2.320.el6, so we need to enable it
manually from 320.

#virsh dumpxml qemu-ga
...
 <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <loader>/usr/share/seabios/bios.bin</loader>
    <boot dev='hd'/>
  </os>
<pm>
     <suspend-to-mem enabled='yes'/>
     <suspend-to-disk enabled='yes'/>
   </pm>

...
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/rhel63.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
...
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    memory
    Regression

bug:

    872420 - From Run 51221

Actions:

S4---->setmem--->wakeup

1. start guest

# virsh dumpxml rhel63...
<memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>4194304</currentMemory>
...

2. # virsh dompmsuspend rhel63 --target disk
Domain qemu-ga successfully suspended


3. # virsh setmem --live rhel63 1000000
error: Requested operation is not valid: domain is not running

# virsh setmem --config rhel63 1000000


4.# virsh start rhel63
Domain rhel63 started
# virsh dumpxml rhel63
...
  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>1000000</currentMemory>
...

and pls check inside the guest

setmem--->S4---->wakeup


5. # virsh setmem --current rhel63 800000

6.
# virsh dumpxml rhel63
...
  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>800000</currentMemory>
...

7.# virsh dompmsuspend rhel63 --target disk
Domain rhel63 successfully suspended

8. # virsh start rhel63


# virsh dumpxml rhel63
...
  <memory unit='KiB'>4194304</memory>
  <currentMemory unit='KiB'>4194304</currentMemory>
...

and pls check in the guest.

 

1.
virsh # start aaa

virsh # setmem aaa 1024000 --current

virsh # dommemstat aaa
actual 1024000
rss 674876
2.
virsh # dompmsuspend aaa --target mem
Domain aaa successfully suspended
virsh # dommemstat aaa
actual 4194304             --------> my maxmem
rss 677068
3.
virsh # dompmwakeup aaa
Domain aaa successfully woken up
virsh # dommemstat aaa
actual 1024000              -------->expect
rss 615340
4.
virsh # dompmsuspend aaa --target disk   
Domain aaa successfully suspended
virsh # dommemstat aaa
actual 4194304             --------> my maxmem
rss 677068
5.
virsh # dompmwakeup aaa
Domain aaa successfully woken up
virsh # dommemstat aaa
actual 4194304              -------->this value should == 1024000
rss 615340

 

 
	
Expected Results:

As steps

Step4 no hang

 

bug 872419 and 878966 are not fixed yet
Notes:
Comments:

		199844 	[virtual networks] openvswitch support 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

ï»¿ Prepare a openvswitch environment.

1.Install openvswitch pkgs , you can find them on breweb

2.#modprobe openvswitch

3. # ovs-vsctl --no-wait init

4.#/etc/init.d/openvswitch start
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Create the Open vSwitch bridge by using the ovs-vsctl utility, and add network device port to the bridge
#ovs-vsctl add-br ovsbr
#ovs-vsctl add-port ovsbr eth0
Then up the bridge, and apply ip address by dhclient.
#ifconfig ovsbr  up
#dhclient ovsbr

 #ovs-vsctl show
72526015-fe77-40ce-95a4-0a9b955926be
    Bridge ovsbr
        Port ovsbr
            Interface ovsbr
                type: internal
        Port "eth0"
            Interface "eth0"

2. Define a guest.
 2.1 Edit the XML file for <interface> section, such as:
    <interface type='bridge'>
      <mac address='52:54:00:eb:79:4f'/>
      <source bridge='ovsbr'/>
      virtualport type='openvswitch'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
Then start the guest, and the guest can get an IP address in the same rang with the host. And the guest's vnet0 interface is attached to the ovsbr bridge.
# ovs-vsctl show
72526015-fe77-40ce-95a4-0a9b955926be
    Bridge ovsbr
        Port "vnet0"
            Interface "vnet0"
        Port ovsbr
            Interface ovsbr
                type: internal
        Port "eth0"
            Interface "eth0"

 2.2 Define a network with eth following XML file:
 #cat ovs-net.xml
    <network>
      <name>ovs-net</name>
      <bridge name="ovsbr" />
      <forward mode="bridge"/>
      <virtualport type='openvswitch'/>
    </network>
# virsh net-define ovs-net.xml 
Network ovs-net defined from ovs-net.xml

# virsh net-start ovs-net
Network ovs-net started

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
ovs-net              active     no  

 then edit the XML file for <interface> section, 
    <interface type='network'>
      <mac address='52:54:00:eb:79:4f'/>
      <source network='ovs-net'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
then start the guest, and the guest can get an IP address in the same rang with the host. And the guest's vnet0 interface is attached to the ovsbr bridge.
# ovs-vsctl show
72526015-fe77-40ce-95a4-0a9b955926be
    Bridge ovsbr
        Port "vnet0"
            Interface "vnet0"
        Port ovsbr
            Interface ovsbr
                type: internal
        Port "eth0"
            Interface "eth0"

	
Expected Results:
Notes:
Comments:

		199993 	[storage]create a pool based on glusterFS 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    rhel6.5
    storage

bug:

    No bug found

Actions:

step:

1. create a pool using glusterFS
	
Expected Results:

verify pool can be defined

======TBD======

refer bug: https://bugzilla.redhat.com/show_bug.cgi?id=849796
Notes:
Comments:

		200107 	[sVirt] libvirt (KVM) NFS Image File Isolation - bug 738882 	gsun 	gsun 	Manual 		--default-- 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt
    rhel6.5

bug:

    No bug found

Actions:

 

	
Expected Results:
Notes:
Comments:

		200239 	[sVirt] libvirt DAC Isolation - bug 822589 	gsun 	gsun 	Manual 		--default-- 	P1 	None 	Edit
Setup:

prepare a running guest

Make sure selinux is enforcing

# setenforce 1

# getenforce

Enforcing

Prepare a user under qemu, kvm group

e.g.

# useradd vdsm -u 36 -g 36 -G qemu -s /sbin/nologin

# id vdsm
uid=36(vdsm) gid=36(kvm) groups=36(kvm),107(qemu)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    No bug found

Actions:

1. start a domain with root

1.1
# vim /etc/libvirt/qemu.conf

security_driver = "selinux"

# service libvirtd restart

1.2 start domain
# virsh start dom_test
Domain dom_test started

1.3 check
check domain xml:
# virsh dumpxml dom_test
...
  <seclabel type='dynamic' model='selinux' relabel='yes'>
    <label>unconfined_u:system_r:svirt_t:s0:c235,c958</label>
    <imagelabel>unconfined_u:object_r:svirt_image_t:s0:c235,c958</imagelabel>
  </seclabel>
...

Only selinux model seclabel, no dac seclabel.

check process and img:
# ps aux|grep qemu
qemu     21023 12.6  0.4 3475808 33816 ?       Sl   11:34   0:00 /usr/libexec/qemu-kvm -name dom_test -S -M rhel6.2.0 -enable-kvm -m 3000 -mem-prealloc -mem-path /var/hugepages/libvirt/qemu -smp 2,sockets=2,cores=1,threads=1 -uuid 0c5659b7-59f8-b5d9-199c-d14255038f91 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/dom_test.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/libvirt/images/dom_test,if=none,id=drive-virtio-disk0,format=raw,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -drive if=none,media=cdrom,id=drive-ide0-0-0,readonly=on,format=raw -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=23,id=hostnet0,vhost=on,vhostfd=24 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=54:52:00:98:92:8d,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

# ll -Z  /var/lib/libvirt/images/dom_test 
-rw-r--r--. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c661,c777 /var/lib/libvirt/images/dom_test

By default they are qemu:qemu, that's expected.

1.4 set static DAC seclabel in domain
# virsh destroy dom_test

# virsh edit dom_test 
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>107:107</label>
  </seclabel>
...

# virsh start dom_test

Domain dom_test started

Check domain xml 

# virsh dumpxml dom_test
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>107:107</label>
    <imagelabel>107:107</imagelabel>
  </seclabel>
...

check process and img:
# ps aux|grep qemu
qemu     3052 40.5 11.0 1477240 426956 ?      Sl   11:24   0:34 /usr/libexec/qemu-kvm -name dom_test -S -M rhel6.2.0 -enable-kvm -m 3000 -mem-prealloc -mem-path /var/hugepages/libvirt/qemu -smp 2,sockets=2,cores=1,threads=1 -uuid 0c5659b7-59f8-b5d9-199c-d14255038f91 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/dom_test.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/libvirt/images/dom_test,if=none,id=drive-virtio-disk0,format=raw,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -drive if=none,media=cdrom,id=drive-ide0-0-0,readonly=on,format=raw -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=23,id=hostnet0,vhost=on,vhostfd=24 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=54:52:00:98:92:8d,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

# ll -Z  /var/lib/libvirt/images/dom_test 
-rw-r--r--. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c661,c777 /var/lib/libvirt/images/dom_test


2. change user/group in qemu.conf
2.1 edit user/group as root in qemu.conf
2.1.1
# vim /etc/libvirt/qemu.conf
...
user = "root"
group = "root"

2.1.2 restart libvirtd
# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

2.1.3 start a domain
# virsh start libvirt_test_api
Domain libvirt_test_api started

2.1.4 check domain and img
# ps aux|grep qemu
root      7619 23.0  0.6 1382768 25556 ?       Sl   15:06   0:00 /usr/libexec/qemu-kvm -name libvirt_test_api -S -M rhel6.3.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -uuid 91739b03-af2d-e8ac-eae4-f1f6ef8d31ff -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/libvirt_test_api.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/libvirt/images/libvirt_test_api,if=none,id=drive-virtio-disk0,format=qed,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=22,id=hostnet0,vhost=on,vhostfd=23 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=54:52:00:45:c3:8a,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

# ll -Z /var/lib/libvirt/images/libvirt_test_api 
-rw-r--r--. root root unconfined_u:object_r:svirt_image_t:s0:c264,c1007 /var/lib/libvirt/images/libvirt_test_api

2.2 edit user/group as vdsm:kvm in qemu.conf

2.2.1
# vim /etc/libvirt/qemu.conf
...
user = "vdsm"
group = "kvm"

2.2.2 restart libvirtd
# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

2.2.3 start a domain
# virsh destroy libvirt_test_api
Domain libvirt_test_api destroyed

# virsh start libvirt_test_api
Domain libvirt_test_api started

2.2.4 check domain and img
# ps aux|grep qemu
vdsm      8201 22.6  0.6 1382768 25556 ?       Sl   15:54   0:00 /usr/libexec/qemu-kvm -name libvirt_test_api -S -M rhel6.3.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -uuid 91739b03-af2d-e8ac-eae4-f1f6ef8d31ff -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/libvirt_test_api.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/libvirt/images/libvirt_test_api,if=none,id=drive-virtio-disk0,format=qed,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=22,id=hostnet0,vhost=on,vhostfd=23 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=54:52:00:45:c3:8a,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

# ll -Z /var/lib/libvirt/images/libvirt_test_api 
-rw-r--r--. vdsm kvm unconfined_u:object_r:svirt_image_t:s0:c249,c291 /var/lib/libvirt/images/libvirt_test_api


2. start a domain with non-root
(use a unprivilige user and prepare a domain)

2.1 start a domain
$ virsh start aaa
Domain aaa started

2.2 check
check with process and img:
# ps aux|grep qemu
wayne    21473 15.2  0.3 1385512 26332 ?       Sl   11:39   0:07 /usr/libexec/qemu-kvm -name aaa -S -M rhel6.3.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -uuid 33cbd2b0-5c72-3df1-509a-ff7dc2a7028e -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/home/wayne/.config/libvirt/qemu/lib/aaa.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/home/wayne/aaa.img,if=none,id=drive-virtio-disk0,format=qed,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev user,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:ac:4f:51,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:1 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

# ll -Z /home/wayne/aaa.img 
-rw-r--r--. wayne wayne unconfined_u:object_r:svirt_image_t:s0:c84,c258 /home/wayne/aaa.img

both process and img with right uid/gid


 
	
Expected Results:
Notes:
Comments:

		200241 	[PCI and USB device assignment] allow to disable usb & vga altogether BZ#818996 	ajia 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.4.0

bug:

    No bug found

Actions:

1. usb hub with controller and model = none
# cat usb-hub.xml
<domain type='kvm'>
  <name>QEMUGuest1</name>
  <uuid>c7a5fdbd-edaf-9455-926a-d65c16db1809</uuid>
  <memory unit='KiB'>219136</memory>
  <currentMemory unit='KiB'>219136</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='i686' machine='pc'>hvm</type>
    <boot dev='hd'/>
  </os>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <controller type='usb' model='none' index='0'/>
    <memballoon model='virtio'/>
    <hub type='usb'>
      <address type='usb' bus='0' port='1'/>
    </hub>
  </devices>
</domain>

# virsh define usb-hub.xml 
error: Failed to define domain from usb-hub.xml
error: unsupported configuration: Can't add USB hub: USB is disabled for this domain

2.  usb controllers with 1 model = none
# cat usb-2.xml
<domain type='kvm'>
  <name>QEMUGuest1</name>
  <uuid>c7a5fdbd-edaf-9455-926a-d65c16db1809</uuid>
  <memory unit='KiB'>219136</memory>
  <currentMemory unit='KiB'>219136</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='i686' machine='pc'>hvm</type>
    <boot dev='hd'/>
  </os>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <controller type='usb' model='none' index='0'/>
    <controller type='usb' index='0'/>
    <memballoon model='virtio'/>
  </devices>
</domain>

# virsh define usb-2.xml 
error: Failed to define domain from usb-2.xml
error: Can't add another USB controller: USB is disabled for this domain

3. usb tablet with controller and model = none
# cat usb-tablet.xml
<domain type='kvm'>
  <name>QEMUGuest1</name>
  <uuid>c7a5fdbd-edaf-9455-926a-d65c16db1809</uuid>
  <memory unit='KiB'>219136</memory>
  <currentMemory unit='KiB'>219136</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='i686' machine='pc'>hvm</type>
    <boot dev='hd'/>
  </os>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <controller type='usb' model='none' index='0'/>
    <input type='tablet' bus='usb'/>
    <memballoon model='virtio'/>
  </devices>
</domain>
<domain type='kvm'>
  <name>QEMUGuest1</name>
  <uuid>c7a5fdbd-edaf-9455-926a-d65c16db1809</uuid>
  <memory unit='KiB'>219136</memory>
  <currentMemory unit='KiB'>219136</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='i686' machine='pc'>hvm</type>
    <boot dev='hd'/>
  </os>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <controller type='usb' model='none' index='0'/>
    <input type='tablet' bus='usb'/>
    <memballoon model='virtio'/>
  </devices>
</domain>

# virsh define usb-tablet.xml 
error: Failed to define domain from usb-tablet.xml
error: unsupported configuration: Can't add USB input device. USB bus is disabled

4. only 1 usb controller with model=none
# cat test.xml
<domain type='kvm'>
  <name>test</name>
  <uuid>73126058-c5a2-846c-428c-5b3ab8f4c2ec</uuid>
  <memory unit='KiB'>8388608</memory>
  <currentMemory unit='KiB'>8388608</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/test1.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </disk>
    <controller type='usb' index='0' model='none'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <memballoon model='none'/>
  </devices>
</domain>

# virsh define test.xml

# virsh start test
Domain test started

# ps aux |grep qemu-kvm
/usr/libexec/qemu-kvm -name test -S -M rhel6.3.0 -enable-kvm -m 8192 -smp 1,sockets=1,cores=1,threads=1 -uuid 73126058-c5a2-846c-428c-5b3ab8f4c2ec -nographic -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/test1.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -drive file=/var/lib/libvirt/images/test1.img,if=none,id=drive-virtio-disk0,format=raw,cache=none -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1

without usb controller and vga

 
	
Expected Results:

For steps 1,  you should get a expected error "Can't add USB hub: USB is disabled for this domain"

For steps 2,  "Can't add another USB controller: USB is disabled for this domain"

For steps 3, "Can't add USB input device. USB bus is disabled"

For steps 4, the guest can be successfully started and can't find usb controller and vga revelvant 

items in qemu-kvm cmdline.

 
Notes:
Comments:

		200624 	[CPU Management] Check the range of cpuset-bug852688,bug867372 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    rhel6.5
    cpu

bug:

    852688
    867372

Actions:

1.# virsh nodeinfo
CPU model:           x86_64
CPU(s):              4
CPU frequency:       1600 MHz
CPU socket(s):       1
Core(s) per socket:  4
Thread(s) per core:  1
NUMA cell(s):        1
Memory size:         7946252 KiB


2.
# virsh edit rhel6.2
......
<vcpu placement='static' cpuset='0-8,^5'>5</vcpu>
......

 
Domain rhel6.2 XML configuration edited.
 

3.# virsh start rhel6.2
Some errors should occur. 

4.# virsh edit rhel6.2
......
<vcpu placement='static' cpuset='0-3,^2'>5</vcpu>
......

Domain rhel6.2 XML configuration edited.



5. # virsh start  rhel6.2
Domain rhel6.2 started


6. #virsh vcpuinfo rhel6.2 

VCPU:           0
CPU:            3
State:          running
CPU time:       5.8s
CPU Affinity:   yyyy

VCPU:           1
CPU:            3
State:          running
CPU time:       0.6s
CPU Affinity:   yyyy

VCPU:           2
CPU:            0
State:          running
CPU time:       0.3s
CPU Affinity:   yyyy

VCPU:           3
CPU:            1
State:          running
CPU time:       0.3s
CPU Affinity:   yyyy

VCPU:           4
CPU:            1
State:          running
CPU time:       0.2s
CPU Affinity:   yyyy

# pidof qemu-kvm
21335

# cat /proc/21335/status |grep Cpus_allowed_list

Cpus_allowed_list:	0-3

 

 

	
Expected Results:

3.Some errors should occur.

6.No vcpu running on cpu 2
Notes:
Comments:

		201790 	[supported hypervisors]Check libvirt can deal ESX guest with distributed virtual switches network.- bug851075 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    supported hypervisors

bug:

    No bug found

Actions:

1.Create a DVS on ESX server.Doc:

https://docspace.corp.redhat.com/docs/DOC-116087 --> 4. How to create VDS?

2.Create a guest with DVS network.

There is a guest on ESXi 5(10.66.6.211),the guest name is RHEL6-DVS

3.Use virsh connect the guest and dump xml.
	
Expected Results:

3.# virsh -c esx://10.66.6.211/?no_verify=1
Enter username for 10.66.6.211 [root]:
Enter root's password for 10.66.6.211:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #  dumpxml RHEL6-DVS
<domain type='vmware'>
  <name>RHEL6-DVS</name>
  <uuid>42399d52-0f05-0b90-6593-2b75cd8c5018</uuid>
  <memory unit='KiB'>2097152</memory>
  <currentMemory unit='KiB'>2097152</currentMemory>
  <vcpu placement='static'>2</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <disk type='file' device='disk'>
      <source file='[datastore1] T3/T3.vmdk'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'/>
    <interface type='bridge'>
      <mac address='00:50:56:b9:28:56'/>
      <source bridge=''/>
      <model type='vmxnet3'/>
    </interface>
    <video>
      <model type='vmvga' vram='8192'/>
    </video>
  </devices>
</domain>

There is no error like this:

virsh # dumpxml RHEL6-DVS
error: internal error Missing essential config entry 'ethernet0.networkName'
Notes:
Comments:

		202519 	[virtual networks] Defining the port masquerading range in XML - bug 851455 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    rhel6.5
    virtual networks

bug:

    No bug found

Actions:

TBD.
	
Expected Results:
Notes:
Comments:

		202520 	[configuration]uri aliases and uri default 	zhpeng 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration

bug:

    No bug found

Actions:

1. modify /etc/libvirt/libvirt.conf

uri_aliases = [
  "zhpeng=qemu+ssh://root@10.66.6.209/system",
]

2. # virsh -c zhpeng
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # uri
qemu+ssh://root@10.66.6.209/system

3. modify /etc/libvirt/libvirt.conf

uri_aliases = [
  "zhpeng=qemu+ssh://root@10.66.6.209/system",
  "pengzhimoutest\.\$=qemu+ssh://root@10.66.7.230/system",
]

4. # virsh -c zhpeng

root@10.66.6.209's password:
error: configuration file syntax error: Malformed 'uri_aliases' config entry 'pengzhimoutest\.\$=qemu+ssh://root@10.66.7.230/system', aliases may only contain 'a-Z, 0-9, _, -'
error: failed to connect to the hypervisor

5. # virsh -c pengzhimoutest\.\$
error: configuration file syntax error: Malformed 'uri_aliases' config entry 'pengzhimoutest\.\$=qemu+ssh://root@10.66.7.230/system', aliases may only contain 'a-Z, 0-9, _, -'
error: failed to connect to the hypervisor

6 modify /etc/libvirt/libvirt.conf

uri_aliases = [
  "zhpeng=qemu+ssh://root@10.66.6.209/system",
  "pengzhimoutest=qemu+ssh://root@10.66.7.230/systemaaaaa",
]

7.# virsh -c zhpeng
root@10.66.6.209's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # uri
qemu+ssh://root@10.66.6.209/system

8.# virsh -c pengzhimoutest
root@10.66.7.230's password:
error: internal error unexpected QEMU URI path '/systemaaaaa', try qemu:///system
error: failed to connect to the hypervisor

9. modify /etc/libvirt/libvirt.conf

uri_aliases = "zhpeng=qemu+ssh://root@10.66.6.209/system"

10. virsh -c zhpeng

error: internal error Expected a list for 'uri_aliases' config parameter
error: failed to connect to the hypervisor

11. change uri_aliases part:

uri_aliases = [
  "hail=qemu+ssh://root@10.66.6.209/system",
  "sleet=qemu+ssh://root@10.66.7.230/system",
]

[root@zhpeng libvirt]# virsh  -c hail
The authenticity of host '10.66.6.209 (10.66.6.209)' can't be established.
RSA key fingerprint is 40:72:80:3f:ad:3c:43:96:de:2f:f1:b5:30:41:54:8b.
Are you sure you want to continue connecting (yes/no)? yes
root@10.66.6.209's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # uri
qemu+ssh://root@10.66.6.209/system

12. modify /etc/libvirt/libvirt.conf

uri_default = "qemu:///session"

13.# virsh
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # uri
qemu:///session

14. modify /etc/libvirt/libvirt.conf

uri_default = "qemu+ssh://10.66.6.209/system"

15.# virsh
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list
root@10.66.6.209's password:
 Id    Name                           State
----------------------------------------------------

virsh # uri
qemu+ssh://10.66.6.209/system

	
Expected Results:

As steps.
Notes:
Comments:

		202526 	[configuration]unix socket modification 	zhpeng 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration

bug:

    No bug found

Actions:

1. mkdir /var/run/libvirt-new/

2. modify /etc/libvirt/libvirtd.conf

unix_sock_group = "kvm"
unix_sock_ro_perms = "0777"
unix_sock_rw_perms = "0770"
unix_sock_dir = "/var/run/libvirt-new"

3. service libvirtd restart

4.# ll /var/run/libvirt-new/libvirt-sock*
srwxrwx---. 1 root kvm 0 Sep 17 14:57 /var/run/libvirt-new/libvirt-sock
srwxrwxrwx. 1 root kvm 0 Sep 17 14:57 /var/run/libvirt-new/libvirt-sock-ro
# virsh -c qemu+unix:///system?socket=/var/run/libvirt-new/libvirt-sock
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #
5. do some normal operation

virsh # list
 Id    Name                           State
----------------------------------------------------

virsh # capabilities
...

	
Expected Results:

As steps
Notes:
Comments:

		202660 	[configuration]host uuid modify BZ 858204 	zhpeng 	None 	Manual 		Function 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration

bug:

    No bug found

Actions:

1. #uuidgen

8052c3d3-937d-4d70-b2a3-9057674a5425

2. change libvirtd.conf:

host_uuid = "8052c3d3-937d-4d70-b2a3-9057674a5425"
restart libvirtd

3.# virsh capabilities
<capabilities>

  <host>
    <uuid>8052c3d3-937d-4d70-b2a3-9057674a5425</uuid>
    <cpu>
....

4.revert what you modified in step2.

Install augeas pkg on brewweb if don't have

# augtool ls /files/etc/libvirt/libvirtd.conf | wc -l

407

# echo host_uuid=\"00000000-0000-0000-0000-000000000000\" >> /etc/libvirt/libvirtd.conf

# augtool ls /files/etc/libvirt/libvirtd.conf | wc -l

408
	
Expected Results:

As steps
Notes:
Comments:

		202666 	[Miscellanea]Guest OS type 	zhpeng 	None 	Manual 		Function 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

1.#virsh capabilities

2. according to step1, host support:

arch:
x86_64
i686

machines:
pc         RHEL 6.4.0 PC (alias of rhel6.4.0)
rhel6.4.0  RHEL 6.4.0 PC (default)
rhel6.3.0  RHEL 6.3.0 PC
rhel6.2.0  RHEL 6.2.0 PC
rhel6.1.0  RHEL 6.1.0 PC
rhel6.0.0  RHEL 6.0.0 PC
rhel5.5.0  RHEL 5.5.0 PC
rhel5.4.4  RHEL 5.4.4 PC
rhel5.4.0  RHEL 5.4.0 PC

Hypervisor:
hvm

3.# virsh dumpxml aaa
...
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
...

4.# replace "arch" "machine" part to test all possibilities.
	
Expected Results:

1.#virsh capabilities
...
  <guest>
    <os_type>hvm</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.4.0</machine>
      <machine canonical='rhel6.4.0'>pc</machine>
      <machine>rhel6.3.0</machine>
      <machine>rhel6.2.0</machine>
      <machine>rhel6.1.0</machine>
      <machine>rhel6.0.0</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <pae/>
      <nonpae/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.4.0</machine>
      <machine canonical='rhel6.4.0'>pc</machine>
      <machine>rhel6.3.0</machine>
      <machine>rhel6.2.0</machine>
      <machine>rhel6.1.0</machine>
      <machine>rhel6.0.0</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>
...
Notes:
Comments:

		202667 	[Miscellanea]Guest OS bios loader 	zhpeng 	None 	Manual 		Function 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

1. define a guest with:

...
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <loader>/usr/share/seabios/bios.bin</loader>
    <boot dev='hd'/>
  </os>
...

2. start guest and do some basic operation in guest.
	
Expected Results:

1. guest can boot up

2. guest can be operated normally.

 

version >= seabios-0.6.1.2-25.el6.x86_64 merge 2 bios binary file to 1.
Notes:
Comments:

		202763 	[Miscellanea] libvirt driver lock is held for too long - bz#856609 	ajia 	ajia 	Manual 		Feature 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea
    rhel6.5

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		202765 	[Miscellanea] Deadlock on libvirt when playing with hotplug and add/remove vm - bz#856950, bz#875710 	ajia 	ajia 	Manual 		Feature 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea
    virtual diks

bug:

    No bug found

Actions:

   1. prepare a guest

2.open the first terminal and run the following cmdline(add your domain name and image name)

# while true; do virsh attach-disk <domain> /var/lib/libvirt/images/<image>.img vda;  virsh detach-disk <domain> vda; sleep 2;done

3. open the second terminal and the following cmdline

Need to prepare guest xml file from a1.xml to a10.xml and guest name is a1..a10.

# while true; do for i in {1..10}; do virsh create /tmp/a$i.xml; done ; for i in {1..10}; do virsh destroy a$i; done; done

Should wait about 2 hours then check

 
	
Expected Results:

You will not see error below in second terminal:

error: Failed to reconnect to the hypervisor 
error: no valid connection 
error: Cannot recv data: Connection reset by peer 

error: Failed to reconnect to the hypervisor 
error: no valid connection
error: internal error client socket is closed

 It should likeï¼

[root@zhpeng ~]#while true; do for i in {1..10}; do virsh create /tmp/a$i.xml; done ; for i in {1..10}; do virsh destroy a$i; done; done


Domain a1 created from a1.xml

Domain a2 created from a2.xml

Domain a3 created from a3.xml

Domain a4 created from a4.xml

Domain a5 created from a5.xml

Domain a6 created from a6.xml
................

Domain a1 destroyed

Domain a2 destroyed

Domain a3 destroyed

Domain a4 destroyed

Domain a5 destroyed

Domain a6 destroyed

.....................

 

And libvirtd still running without error
Notes:
Comments:

		202812 	[cpu management] It is failed to start guest when the number of vcpu is different between <vcpu> and <cputune/> BZ#853930 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu

bug:

    No bug found

Actions:

Steps to Reproduce:

 1.# virsh list --all
 Id    Name                           State
----------------------------------------------------
 18    rhel6.2                        running 

2. # virsh dumpxml rhel6.2
<domain type='kvm' id='27'>

 <vcpu placement='auto'>2</vcpu>
 
...
</domain>

3.# virsh vcpupin rhel6.2 0 0 --config 


4.# virsh destroy rhel6.2
Domain rhel6.2 destroyed

5.# virsh start rhel6.2
Should succeed without error like:
error: Failed to start domain rhel6.2
error: An error occurred, but the cause is unknown


6. # virsh dumpxml rhel6.2
<domain type='kvm' id='30'>
 
 <vcpu placement='auto'>2</vcpu> <cputune> <vcpupin vcpu='0' cpuset='0'/> </cputune> 

	
Expected Results:

Successfully start it.

Notes:
Comments:

		202827 	[Miscellanea]OS boot device and boot rules 	zhpeng 	None 	Manual 		Function 	P3 	None 	Edit
Setup:

download boot.iso and vmlinuz, one for cdrom one for fd.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

 1. define a guest with xml:
...
<os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <boot dev='cdrom'/>
    <boot dev='fd'/>
    <boot dev='hd'/>
    <bootmenu enable='yes'/>
  </os>
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/aaa.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/mnt/boot.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/boot/vmlinuz'/>
      <target dev='fda' bus='fdc'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
...

2. start guest, guest should boot from cdrom

3. modify cdrom part:
...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide' tray='open'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
...
4. start guest. guest should boot from fd, But now direct booting from floppy is no longer supported.
guest will print an error like:

Booting from Floppy...
Direct booting from floppy is no longer supported.
Please use a boot loader program instead.

Remove disk and press any key to reboot . . .

5. now we modify floppy part:
...
    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <target dev='fda' bus='fdc' tray='open'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
...
6. start guest. Now guest will boot from hd.
Check guest OS in hd can boot up.

7. define another guest with xml:
...
  <os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <boot dev='network'/>
    <boot dev='hd'/>
  </os>
...
    <interface type='direct'>
      <mac address='52:54:00:2c:97:02'/>
      <source dev='em1' mode='vepa'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
...

6. start guest, guest will boot up from PXE.

7. modify the new guest:
...
  <os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <kernel>/var/lib/libvirt/boot/vmlinuz</kernel>
    <initrd>/var/lib/libvirt/boot/initrd.img</initrd>
    <boot dev='network'/>
    <boot dev='hd'/>
  </os>
...

8. start guest. guest will boot from kernel directly.
	
Expected Results:

As steps.
Notes:
Comments:

		202828 	[Miscellanea]Guest OS boot menu 	zhpeng 	None 	Manual 		Function 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

1. define a guest with xml:
...
  <os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <boot dev='cdrom'/>
    <boot dev='hd'/>
    <bootmenu enable='yes'/>           -----> open boot menu
  </os>
...
2. start guest, it will show "Press F12 for boot menu."

and press F12, a boot list will show up.

press 1. (1. DVD/CD [ata1-0: QEMU DVD-ROM ATAPI-4 DVD/CD])

Guest will boot up from cdrom.
	
Expected Results:

As steps
Notes:
Comments:

		202869 	[Miscellanea]Guest bios useserial 	zhpeng 	None 	Manual 		Function 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

1. define a guest with xml:
...
  <os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
...

    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
...

and add "console=tty0 console=ttyS0" to guest kernel line.

2. use 2 terminal, one run:

# virsh start guest

quickly shift to the other and run

# virsh console guest

virsh # console aaa
Connected to domain aaa
Escape character is ^]
Initializing cgroup subsys cpuset
Initializing cgroup subsys cpu
Linux version 2.6.32-279.el6.x86_64 (mockbuild@x86-008.build.bos.redhat.com) (gcc version 4.4.6 20120305 (Red Hat 4.4.6-4) (GCC) ) #1 SMP Wed Jun 13 18:24:36 EDT 2012
Command line: ro root=/dev/mapper/VolGroup-lv_root rd_NO_LUKS LANG=en_US.UTF-8 rd_NO_MD rd_LVM_LV=VolGroup/lv_swap SYSFONT=latarcyrheb-sun16 crashkernel=auto rd_LVM_LV=VolGroup/lv_root  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM console=tty0 console=ttyS0
KERNEL supported cpus:
...

3. change guest xml to:
...
  <os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <boot dev='hd'/>
    <bios useserial='yes'/>
  </os>
...

4. repeat step2. This time you will get something more:

virsh # console aaa
Connected to domain aaa
Escape character is ^]
                                                                              

Booting from Hard Disk...

...
	
Expected Results:

As steps
Notes:
Comments:

		203104 	[CPU Management]libvirt should check the range of setting maxvcpus in xml when start guest - bug 855296 	yupzhang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu
    rhel6.5

bug:

    No bug found

Actions:

1.Check the max vcpus number kvm support.
#virsh maxvcpus kvm
160

2. Shutdow a guest.
# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     rhel6.2                         shut off


3.Edit the guest,set the vcpu number more than 160,then save.
# virsh edit rhel6.2
......
<vcpu placement='auto' current='1'>161</vcpu>
......

Domain rhel6.2 XML configuration edited.

4.Start the guest.
#virsh start rhel6.2

	
Expected Results:

4. <The bug is not fixed now.>
Notes:
Comments:

		203105 	[CPU Management]Check vcpuinfo could show correct info after hostplug cpu - bug 855218 	yupzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    No bug found

Actions:

1.# virsh start rhel6.2
Domain rhel6.2 started

2.# virsh vcpupin rhel6.2
VCPU: CPU Affinity
----------------------------------
   0: 0-3

3.# virsh setvcpus rhel6.2 3

4.# virsh vcpupin rhel6.2 1 1

5.# virsh vcpupin rhel6.2
VCPU: CPU Affinity
----------------------------------
   0: 0-3
   1: 1
   2: 0-3

6.# virsh vcpuinfo rhel6.2
VCPU:           0
CPU:            1
State:          running
CPU time:       42.8s
CPU Affinity:   yyyy

VCPU:           1
CPU:            0
State:          running
CPU time:       30.8s
CPU Affinity: -y--

VCPU:           2
CPU:            3
State:          running
CPU Affinity:   yyyy

 

	
Expected Results:

The expected result is the same as the result of command.

Notes:
Comments:

		203108 	[configuration]Set the delay time before rebooting via boot parameter-bug855237 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration
    Regression

bug:

    855237

Actions:

Run a guest without any bootable device or pxe env. so guest can't boot from <boot dev=xxx>

Try several rebootTimeout value: -1, 0, 1, 10000, 65535, >65535, <-1, 1.5, -1.5

when value is Out of range libvirt will report error:
virsh # edit aaa
error: unsupported configuration: invalid value for rebootTimeout, must be in range [-1,65535]
Failed. Try again? [y,n,f,?]:



1. edit guest xml and start
<os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <loader>/usr/share/seabios/bios-pm.bin</loader>
    <boot dev='network'/>
    <boot dev='hd'/>
    <bios useserial='yes' rebootTimeout='$TIME'/>
</os>

2.start guest and check qemu process:
qemu     19789 39.3  0.3 1308304 25984 ?       Sl   10:44   0:05 /usr/libexec/qemu-kvm -name aaa 
-S -M rhel6.4.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 
-uuid f34810e0-3d9c-eadb-262b-4f1159f5ac95 -nodefconfig -nodefaults
 -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/aaa.monitor,server,nowait
 -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown
-boot order=c,menu=on,reboot-timeout=65535 -device
 piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 
-netdev tap,fd=29,id=hostnet0,vhost=on,vhostfd=30
 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:5d:d6:d4,bus=pci.0,addr=0x3
 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 
-device usb-tablet,id=input0 -vnc 127.0.0.1:0
 -vga cirrus -device intel-hda,id=sound0,bus=pci.0,addr=0x4
 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0
 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6

3.
check timeout from 0-65535, guest will reboot after 0~65s if boot from failure.
-1, will not reboot again.
boot screen will show: No bootable device. Retrying in $TIME seconds.


	
Expected Results:

As steps
Notes:
Comments:

		203126 	[Guest resource control] libvirt isn't checking values emulator_period and emulator_quota in XML and in command with --config BZ 854537 854133 	zhpeng 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control
    Regression

bug:

    No bug found

Actions:

1. # virsh start rhel6.2
Domain rhel6.2 started


2.# virsh schedinfo rhel6.2 --set emulator_period=500 --config
Scheduler      : posix
cpu_shares     : 0
vcpu_period    : 0
vcpu_quota     : 0
emulator_period: 500
emulator_quota : 0


3. # virsh schedinfo rhel6.2 --set emulator_quota=500 --config
Scheduler      : posix
cpu_shares     : 0
vcpu_period    : 0
vcpu_quota     : 0
emulator_period: 500
emulator_quota : 500


4. # virsh destroy rhel6.2
Domain rhel6.2 destroyed

5.# virsh start rhel6.2
Domain rhel6.2 started

6.# virsh dumpxml rhel6.2
<domain type='kvm' id='37'>
 .....
  <cputune>
    <emulator_period>500</emulator_period>
    <emulator_quota>500</emulator_quota>
    <vcpupin vcpu='0' cpuset='1'/>
    <vcpupin vcpu='1' cpuset='2'/>
  </cputune>
.....
</domain>

# virsh schedinfo rhel6.2
Scheduler      : posix
cpu_shares     : 1024
vcpu_period    : 100000
vcpu_quota     : -1
emulator_period: 100000
emulator_quota : -1
  

	
Expected Results:

libvirt ignores cpu tuning values set in XML if they are out of range and uses defaults

Refer to the http://libvirt.org/formatdomain.html#elementsCPUTuning
emulator_period - The value should be in range [1000, 1000000]. A period with value 0 means no value.
emulator_quota -The value should be in range [1000, 18446744073709551] or less than 0. 
A quota with value 0 means no value.

Step2,3,6 NEEDUPDATE, BUG 854537 854133 are not fixed yet.

Notes:
Comments:

		204232 	[configuration]keepalive protocol 	zhpeng 	None 	Manual 		Function 	P2 	None 	Edit
Setup:

zhpeng.example.com 10.66.6.209                 --->  runs libvirtd and tcpdump to capture packets

                                                                          #tcpdump -i em1 host 10.66.7.230 in 10.66.6.209

pengzhimoutest.example.com 10.66.7.230    --->  runs virsh client

                                                                          virsh -c qemu+ssh://10.66.6.209/system

                                                                         and keep client in interactive mode.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration

bug:

    No bug found

Actions:

1. modify libvirtd.conf
keepalive_interval = 5
keepalive_count = 5

captured like:
15:04:52.839691 IP zhpeng.example.com.16509 > pengzhimoutest.example.com.52572: Flags [P.], seq 97:125, ack 117, win 114, options [nop,nop,TS val 1901351 ecr 515672], length 28

15:04:52.839981 IP pengzhimoutest.example.com.52572 > zhpeng.example.com.16509: Flags [.], ack 125, win 115, options [nop,nop,TS val 520630 ecr 1901351], length 0

15:04:52.840227 IP pengzhimoutest.example.com.52572 > zhpeng.example.com.16509: Flags [P.], seq 117:145, ack 125, win 115, options [nop,nop,TS val 520630 ecr 1901351], length 28

15:04:52.879466 IP zhpeng.example.com.16509 > pengzhimoutest.example.com.52572: Flags [.], ack 145, win 114, options [nop,nop,TS val 1901391 ecr 520630], length 0

 

15:04:57.845711 IP zhpeng.example.com.16509 > pengzhimoutest.example.com.52572: Flags [P.], seq 125:153, ack 145, win 114, options [nop,nop,TS val 1906357 ecr 520630], length 28

15:04:57.846233 IP pengzhimoutest.example.com.52572 > zhpeng.example.com.16509: Flags [P.], seq 145:173, ack 153, win 115, options [nop,nop,TS val 525636 ecr 1906357], length 28

15:04:57.846260 IP zhpeng.example.com.16509 > pengzhimoutest.example.com.52572: Flags [.], ack 173, win 114, options [nop,nop,TS val 1906357 ecr 525636], length 0

 

15:05:02.851777 IP zhpeng.example.com.16509 > pengzhimoutest.example.com.52572: Flags [P.], seq 153:181, ack 173, win 114, options [nop,nop,TS val 1911363 ecr 525636], length 28

15:05:02.852285 IP pengzhimoutest.example.com.52572 > zhpeng.example.com.16509: Flags [P.], seq 173:201, ack 181, win 115, options [nop,nop,TS val 530643 ecr 1911363], length 28

15:05:02.852313 IP zhpeng.example.com.16509 > pengzhimoutest.example.com.52572: Flags [.], ack 201, win 114, options [nop,nop,TS val 1911363 ecr 530643], length 0

So libvirtd send "hello" packet every 5 seconds(37-72=keepalive_interval)

Server Run:
# iptables -A INPUT -s 10.66.6.209 -j DROP
# iptables -A OUTPUT -s 10.66.6.209 -j DROP

Wait 5*(5 + 1)=30 seconds, clear iptables rules and try to run command in client:

virsh # list
error: Failed to list domains


error: Cannot recv data: : Connection reset by peer

 

virsh # list

error: no valid connection

2.modify libvrtd.conf:
keepalive_interval = -1

This means libvirtd will not send "hello" packet. So tcpdump will get no "hello" packets.
Conncetion will keep and not closed. After a network failure client still alive.
virsh # list
 Id    Name                           State
----------------------------------------------------

3.modify libvirtd.conf:
keepalive_interval = 5
keepalive_count = 0

This means if no full heartbeat ping-pong in 5 seconds, connection will closed.
Specially, any virsh operation is a heartbeat, for example, if you run:virsh# list
every 2 seconds, connection will keep and not closed.

client pls run:

virsh # list   every 2 seconds about 5 times. then wait 5 seconds run virsh #list again:

virsh # list --all

Id    Name                           State

----------------------------------------------------

-     aaa                            shut off

-     rhel63q                        shut off

 

virsh # list

error: no valid connection

 

 4. modify libvirtd.conf:
keepalive_required = 1

 

 

If set to 1, libvirtd will refuse to talk to clients that do not support keepalive protocol.
Our virsh only respond keepalive heartbeat, it's not support keepalive protocol
So if you use keepalive_required = 1, libvirtd force client support keepalive.
So our client will error like:

# virsh -c qemu+ssh://10.66.6.209/system

root@10.66.6.209's password:

error: operation failed: keepalive support is required to connect

error: failed to connect to the hypervisor

 

 

 
	
Expected Results:

As steps
Notes:
Comments:

		204238 	[console and serial devices]Modify target type of channel element from 'virtio' to 'guestfwd' should not cause libvirtd crash -- Bug 856489 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices

bug:

    No bug found

Actions:

1 . prepare a guest with xml
 <channel type='pty'>
      <target type='virtio' name='arbitrary.virtio.serial.port.name'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
 2 . start the guest and edit the xml
  #virsh edit $guest
  change type='virtio' to 'guestfwd' and save
 error: XML error: guestfwd channel does not define a target address
 Failed. Try again? [y,n,f,?]:
 the error msg as expect.
  check libvirtd ,
 

	
Expected Results:

verify :

after step 2 , libvirtd not crash.

#service libvirtd status
 libvirtd (pid  15843) is running...

 
Notes:
Comments:

		204396 	[sVirt] static DAC seclable value check problem - bug 856951 	gsun 	gsun 	Manual 		Negative test 	P1 	None 	Edit
Setup:

Prepare a domain

The bug is fixed,  the value should be valid uid:gid.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    No bug found

Actions:
1. edit domain and adding following xml part:

# virsh edit $domain
...
<seclabel type='static' model='dac' relabel='yes'>
<label>sdfklsdjlfjklsdjkl</label>
</seclabel>
...

2. start domain
# virsh start $domain
# virsh dumpxml $domain
3. start domain with valid user:group in id

3.1
# virsh edit $domain
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>107:107</label>
  </seclabel>
...

3.2 start domain
# virsh start libvirt_test_api


3.3 check xml and img
# virsh dumpxml libvirt_test_api

# ll -Z /var/lib/libvirt/images/libvirt_test_api 

 4. mix with qemu:qemu in qemu.conf and dac seclable as 0:0

4.1 edit qemu.conf and add following (By default it is qemu:qemu, so this step could be skipped. )
user = "qemu"
group = "qemu"

then restart libvirtd. 
# service libvirtd restart

4.2 edit domain and add following xml

# virsh edit $domain
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>0:0</label>
  </seclabel>
...

4.3 start domain
# virsh start libvirt_test_api

5. mix with root:root in qemu.conf and dac seclable as 0:0 
5.1 modify qemu.conf with user/group as root/root.
...
user = "root"
group = "root"

# service libvirtd restart

5.2 start domain
# virsh start libvirt_test_api
Domain libvirt_test_api started

5.3 check domain img and xml
# virsh dumpxml libvirt_test_api

# ll -Z /var/lib/libvirt/images/libvirt_test_api 


# ll -Z /var/lib/libvirt/qemu/libvirt_test_api.monitor 

6. mix with root:root in qemu.conf and dac seclable as 107:107 

Following the setting after step 5
6.1 edit domain with uid:gid as 107:107
 ... 
 <seclabel type='static' model='dac' relabel='yes'>
 <label>107:107</label>
 </seclabel>
 ... 

6.2 start domain
 # virsh start libvirt_test_api



 

 
	
Expected Results:

2.

# virsh start test
error: Failed to start domain test
error: internal error invalid argument: Missing separator ':' in DAC label "sdfklsdjlfjklsdjkl"

# virsh dumpxml $domain

...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>sdfklsdjlfjklsdjkl</label>
  </seclabel>
...

3.2

Domain libvirt_test_api started

 3.3

# virsh dumpxml libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>107:107</label>
    <imagelabel>107:107</imagelabel>
  </seclabel>
...

# ll -Z /var/lib/libvirt/images/libvirt_test_api 
-rw-r--r--. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c199,c861 /var/lib/libvirt/images/libvirt_test_api


4.3
# virsh start libvirt_test_api
error: Failed to start domain libvirt_test_api
error: internal error Process exited while reading console log output: bind(unix:/var/lib/libvirt/qemu/libvirt_test_api.monitor): Permission denied
chardev: opening backend "socket" failed


The domain fail to start, socket is blocked for selinux permission. In theory this could start on non-selinux machine.


5.3 

# virsh dumpxml libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>0:0</label>
    <imagelabel>0:0</imagelabel>
  </seclabel>
...

# ll -Z /var/lib/libvirt/images/libvirt_test_api 
-rw-r--r--. root root unconfined_u:object_r:svirt_image_t:s0:c48,c416 /var/lib/libvirt/images/libvirt_test_api 

# ll -Z /var/lib/libvirt/qemu/libvirt_test_api.monitor 
srwxr-xr-x. root root unconfined_u:object_r:qemu_var_run_t:s0:c48,c416 /var/lib/libvirt/qemu/libvirt_test_api.monitor 

 6.2

# virsh start libvirt_test_api
error: Failed to start domain libvirt_test_api
error: internal error Process exited while reading console log output: bind(unix:/var/lib/libvirt/qemu/libvirt_test_api.monitor): Permission denied
chardev: opening backend "socket" failed

 
Notes:
Comments:

		204915 	[Miscellanea]on_poweroff lifecycle control 	zhpeng 	None 	Manual 		Function 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		204916 	[Miscellanea]on_reboot lifecycle control 	zhpeng 	None 	Manual 		Function 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		204917 	[Miscellanea]on_crash lifecycle control 	zhpeng 	None 	Manual 		Function 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		206121 	[CPU Management] Configure timezone of guest - bug859868 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Have a guest with X windows.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    859868

Actions:

================================================================================
The timezone of guest is America/New_York.

# virsh start rhel6.3
Domain rhel6.3 started


# virsh dumpxml rhel6.3 |grep clock
  <clock offset='utc'/>


guest                                             | host
----------------------------------------------------------------------------------------------------
# date                                            |# TZ=America/New_York date
Fri Sep 28 22:20:18 EDT 2012         |Fri Sep 28 22:20:29 EDT 2012 
----------------------------------------------------------------------------------------------------
# TZ=Asia/Shanghai date                 |# date  
Sat Sep 29 10:21:49 CST 2012         |Sat Sep 29 10:22:11 CST 2012
----------------------------------------------------------------------------------------------------
 
=============================================================================================================
Open system-config-date and turn off the "System clock uses UTC" checkbox in the Timezone tab in guest


# virsh destroy rhel6.3
Domain rhel6.3 destroyed


# virsh edit rhel6.3 ( <clock offset="timezone" timezone="America/New_York"> )
Domain rhel6.3 XML configuration edited.


# virsh start rhel6.3
Domain rhel6.3 started

# virsh dumpxml rhel6.3 |grep clock
  <clock offset='timezone' timezone='America/New_York'/>


guest                                             | host
---------------------------------------------------------------------------------------------------
# date                                            |# TZ=America/New_York date
Fri Sep 28 22:44:06 EDT 2012          |Fri Sep 28 22:44:49 EDT 2012
----------------------------------------------------------------------------------------------------
# TZ=Asia/Shanghai date                 |# date  
Sat Sep 29 10:45:16 CST 2012         |Sat Sep 29 10:45:32 CST 2012
----------------------------------------------------------------------------------------------------

=============================================================================================================
Open system-config-date and set the timezone to Asia/Shanghai.Leave the "System clock uses UTC" off  in guest.

# virsh edit rhel6.3 (<clock offset='localtime'/>)
Domain rhel6.3 XML configuration edited.


# virsh destroy rhel6.3
Domain rhel6.3 destroyed


# virsh start rhel6.3
Domain rhel6.3 started


# virsh dumpxml rhel6.3 |grep clock
  <clock offset='localtime'/>

guest                                              | host
----------------------------------------------------------------------------------------------------
# date                                             |# date
Sat Sep 29 11:09:20 CST 2012         |Sat Sep 29 11:09:41 CST 2012
----------------------------------------------------------------------------------------------------
# TZ=America/New_York date           |# TZ=America/New_York date
Fri Sep 28 23:10:46 EDT 2012          |Fri Sep 28 23:10:10 EDT 2012
----------------------------------------------------------------------------------------------------

=============================================================================================================
Open system-config-date and put the checkbox on  in guest

# virsh edit rhel6.3 (<clock offset='utc'/>)
Domain rhel6.3 XML configuration edited.


# virsh destroy rhel6.3
Domain rhel6.3 destroyed


# virsh start rhel6.3
Domain rhel6.3 started


# virsh dumpxml rhel6.3 |grep clock
  <clock offset='utc'/>

guest                                              | host
------------------------------------------------------------------------------------------------------
# date                                             |# date
Sat Sep 29 11:25:55 CST 2012         |Sat Sep 29 11:25:00 CST 2012
----------------------------------------------------------------------------------------------------
# TZ=America/New_York date           |# TZ=America/New_York date
Fri Sep 28 23:26:53 EDT 2012          |Fri Sep 28 23:27:27 EDT 2012
-----------------------------------------------------------------------------------------------------

	
Expected Results:
Notes:
Comments:

		213360 	[sVirt] security: support for names on DAC labels - 860519 	gsun 	gsun 	Manual 		Function 	P2 	None 	Edit
Setup:

Prepare a domain
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt

bug:

    No bug found

Actions:

 1. edit domain add following xml:
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>qemu:qemu</label>
  </seclabel>
...

2. start domain
# virsh start libvirt_test_api


# virsh dumpxml libvirt_test_api


3. add uid:gid with a plus
# virsh destroy libvirt_test_api
Domain libvirt_test_api destroyed

# virsh edit libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>+107:107</label>
  </seclabel>
...

Domain libvirt_test_api XML configuration edited.

4. start domain and check
# virsh start libvirt_test_api


# virsh dumpxml libvirt_test_api


# ps aux|grep libvirt_test_api


# ll -Z /var/lib/libvirt/images/libvirt-test-api 


5. mix name with id with plus
# virsh destroy libvirt_test_api
Domain libvirt_test_api destroyed

# virsh edit libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>qemu:+107</label>
  </seclabel>
... 

6. start and check
# virsh start libvirt_test_api


# virsh dumpxml libvirt_test_api


# ps aux|grep qemu


# ll -Z /var/lib/libvirt/images/


7. test with non-exist group name or gid
7.1
# virsh destroy libvirt_test_api
Domain libvirt_test_api destroyed

7.2
# virsh edit libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>qemu:+1000</label>
  </seclabel>
... 

7.3
# virsh start libvirt_test_api

7.4
# virsh dumpxml libvirt_test_api


# ll -Z /var/lib/libvirt/images/


# ps aux|grep qemu



7.5
# virsh edit libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>107:vdsm</label>
  </seclabel>
... 

Domain libvirt_test_api XML configuration edited.

7.6
# virsh start libvirt_test_api



8. test with invalid uid or user name
Now try with a noexist uid:
8.1
# virsh destroy libvirt_test_api
Domain libvirt_test_api destroyed

8.2
# virsh edit libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>+37:+1000</label>
  </seclabel>
...
Domain libvirt_test_api XML configuration edited.

8.3
# virsh start libvirt_test_api


8.4
# virsh edit libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>abcd:+1000</label>
  </seclabel>
...
Domain libvirt_test_api XML configuration edited.

8.5
# virsh start libvirt_test_api

 

 

 

 
	
Expected Results:

2. 

# virsh start libvirt_test_api
Domain libvirt_test_api started

# virsh dumpxml libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>qemu:qemu</label>
    <imagelabel>qemu:qemu</imagelabel>
  </seclabel>
... 

 4.

# virsh start libvirt_test_api
Domain libvirt_test_api started

# virsh dumpxml libvirt_test_api
  <seclabel type='static' model='dac' relabel='yes'>
    <label>+107:107</label>
    <imagelabel>+107:107</imagelabel>
  </seclabel>

# ps aux|grep libvirt_test_api
qemu     17621 61.3  0.2 1456976 298024 ?      Sl   11:08   0:15 /usr/libexec/qemu-kvm -name libvirt_test_api -S -M rhel6.4.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -uuid 05867c1a-afeb-300e-e55e-2673391ae080 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/libvirt_test_api.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/libvirt/images/libvirt-test-api,if=none,id=drive-virtio-disk0,format=qcow2 -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=22,id=hostnet0,vhost=on,vhostfd=23 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=54:52:00:45:c3:8a,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -vga cirrus -device intel-hda,id=sound0,bus=pci.0,addr=0x4 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6

# ll -Z /var/lib/libvirt/images/libvirt-test-api 
-rw-r--r--. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c75,c651 /var/lib/libvirt/images/libvirt-test-api 

 6.

# virsh start libvirt_test_api
Domain libvirt_test_api started

 # virsh dumpxml libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>qemu:+107</label>
    <imagelabel>qemu:+107</imagelabel>
  </seclabel>
...

# ps aux|grep qemu
qemu     18100 11.3  0.2 1385196 289660 ?      Sl   11:14   0:15 /usr/libexec/qemu-kvm -name libvirt_test_api -S -M rhel6.4.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -uuid 05867c1a-afeb-300e-e55e-2673391ae080 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/libvirt_test_api.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/libvirt/images/libvirt-test-api,if=none,id=drive-virtio-disk0,format=qcow2 -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=22,id=hostnet0,vhost=on,vhostfd=23 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=54:52:00:45:c3:8a,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -vga cirrus -device intel-hda,id=sound0,bus=pci.0,addr=0x4 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6

# ll -Z /var/lib/libvirt/images/
-rw-r--r--. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c7,c786 libvirt-test-api

 7.

7.3
# virsh start libvirt_test_api
Domain libvirt_test_api started

7.4
# virsh dumpxml libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>qemu:+1000</label>
    <imagelabel>qemu:+1000</imagelabel>
  </seclabel>
...

# ll -Z /var/lib/libvirt/images/
-rw-r--r--. qemu 1000 unconfined_u:object_r:svirt_image_t:s0:c196,c515 libvirt-test-api

# ps aux|grep qemu
qemu     18437  4.7  0.2 1383140 330380 ?      Sl   11:18   0:15 /usr/libexec/qemu-kvm -name libvirt_test_api -S -M rhel6.4.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -uuid 05867c1a-afeb-300e-e55e-2673391ae080 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/libvirt_test_api.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/var/lib/libvirt/images/libvirt-test-api,if=none,id=drive-virtio-disk0,format=qcow2 -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=22,id=hostnet0,vhost=on,vhostfd=23 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=54:52:00:45:c3:8a,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -vga cirrus -device intel-hda,id=sound0,bus=pci.0,addr=0x4 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6

7.6
# virsh start libvirt_test_api
error: Failed to start domain libvirt_test_api
error: internal error invalid argument: Failed to parse group 'vdsm'


8.3
# virsh start libvirt_test_api
error: Failed to start domain libvirt_test_api
error: internal error internal error getpwuid_r failed to retrieve data for uid '37'

8.4
# virsh edit libvirt_test_api
...
  <seclabel type='static' model='dac' relabel='yes'>
    <label>abcd:+1000</label>
  </seclabel>
...
Domain libvirt_test_api XML configuration edited.

8.5
# virsh start libvirt_test_api
error: Failed to start domain libvirt_test_api
error: internal error invalid argument: Failed to parse user 'abcd'



 
Notes:
Comments:

		221606 	[Installation] Creating domain with already used name should always fail - BZ#871452 	ajia 	None 	Manual 		Bug verification 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    installation

bug:

    No bug found

Actions:

1. Create an XML file for domain with specified UUID, that can be defined but not started (due to any reason),

for this case, the following XML file give a wrong label, so guest can be defined but not started.

#cat demo.xml

<domain type='qemu'>
  <name>demo</name>
  <uuid>32fd83a6-5795-1537-bb20-79b71149180c</uuid>
  <memory>32768</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
  </os>
  <features>
    <acpi/><apic/><pae/>
  </features>
  <clock offset="utc"/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <seclabel type='static' model='dac' relabel='yes'>
    <label>system_u:object_r:svirt_image_t:s0:c371,c470</label>
  </seclabel>
</domain>

2. Define the domain

# virsh define demo.xml 
Domain demo defined from test.xml

3. Start the domain, this will fail as expected

4. Create the domain with the XML file
# virsh create test.xml 

5. List the domain

#virsh list --all

	
Expected Results:

2.

# virsh define demo.xml 
Domain demo defined from demo.xml

 3.

# virsh start demo

error: Failed to start domain demo
error: internal error invalid argument: Failed to parse user 'system_u' 

For now bug still not fix, so the actual result is: 

4.

# virsh create demo.xml 

error: Failed to create domain from demo.xml
error: internal error invalid argument: Failed to parse user 'system_u'

5. The domain is undefined.


 

Notes:
This bug move to 6.5.
Comments:

		221849 	[snapshot] poor error message for virsh snapshot-list --roots --current BZ#869100 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

1)
# virsh snapshot-create-as q1 ds1 --disk-only 
Domain snapshot ds1 created

2)
# virsh snapshot-create-as q1 ds2 --disk-only 
Domain snapshot ds2 created


3)
# virsh snapshot-list q1 --roots --current 
error: --roots and --current are mutually exclusive

	
Expected Results:

step 3)

error message should  not be :

error: --roots and --from are mutually exclusive

 
Notes:
Comments:

		222193 	[Virtual Networks] Verify the network XML files when creating transient networks and the dnsmasq persistent host file is not created - bug 869913 	ydu 	ydu 	Manual 		Bug verification 	P1 	None 	Edit
Setup:

Make sure the dnsmasq or radvd packages version is not latest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1# cat multi-dhcp.xml
<network>
  <name>multi-dhcp</name>
  <forward mode='nat'/>
  <bridge name='virbr10' stp='on' delay='0' />
  <ip address='192.168.201.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.201.2' end='192.168.201.2' />
    </dhcp>
  </ip>
  <ip address='192.168.202.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.202.2' end='192.168.202.2' />
    </dhcp>
  </ip>
  <ip family='ipv6' address='2003b8:ac10:fe01::1' prefix='64'>
  </ip>
</network>

2 # virsh net-create multi-dhcp.xml


3 prepare two guests,and the guest should have the network interface like this
    <interface type='network'>
      <mac address='52:54:00:e5:4e3'/>
      <source network='multi-dhcp'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>

4 start the guests,then login the guests , and check the guests network(ip address, ping out)

	
Expected Results:

For now, the bug still not fix, so the actual result is:

2.# virsh net-create multi-dhcp.xml

Network multi-dhcp created from multi-dhcp.xml
# virsh net-list
Name                 State      Autostart     Persistent
--------------------------------------------------
default              active     yes           yes
multi-dhcp           active     no            no

 

The Expected Results shuold be:

2 
error: Failed to create network from muti-dhcp.xml
error: unsupported configuration: Multiple dhcp sections found. dhcp is supported only for a single IPv4 address on each network

 

 
Notes:
Comments:

		222194 	[Virtual Networks] Destroy/Start network after updating dnsmasq or radvd packages - bug 871201 	ydu 	ydu 	Manual 		Function 	P1 	None 	Edit
Setup:

Make sure the dnsmasq or radvd packages version is not latest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1.Define and start a virtual network. 

Or use the 'default' network is also OK.

2. Make sure the network is running.

#virh net-list --all

3. Update dnsmasq or radvd to the latest version.

4. Restart libvirtd

#service libvirtd restart

5. Destroy the network

#virsh net-destroy default

6. Start the network

#virsh net-start default

7. check the dnsmasq ps

#ps aux|grep dnsmasq
	
Expected Results:

2.

# virsh net-list 
Name                 State      Autostart     Persistent
--------------------------------------------------
default              active     no            yes 

4.

# service libvirtd restart

Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

5.

# virsh net-destroy default
Network default destroyed 

For the bug still not fix, the actual result is:

6. 'default' network can't start for the dnsmasq prcess already exist

7. Can find the dnsmasq process.

 

 

Notes:
Comments:

		222196 	[Virtual Networks] Add more than 256 logical networks - bug 869557 &877244 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Try to define and start more than 256 virtual neworks.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    877244 - From Run 54159

Actions:

1. Prepare a template xml file to define networks.
# cat templ.xml 
<network>
  <name>NET-#NIC#</name>
  <forward mode='nat'/>
  <bridge name='virbr-#NIC#' stp='on' delay='0' />
  <ip address='192.168.221.#NIC#' netmask='255.255.255.255'>
  </ip>
</network>
2. Prepare  a script  to define and start 250 networks automatically

# cat vnet.sh 

#!/bin/sh

for i in {1..250}; do

sed "s/#NIC#/$i/g"  templ.xml > net-$i.xml

virsh net-define net-$i.xml

virsh net-start NET-$i

rm -f ï»¿net-$i.xml

sleep 1

done

virsh -r net-list | wc -l

3.  Edit the template xml file and script file to add other 250 networks.

Eg. change the network name, bridge name, ip address.

Note: Run the following step after bug 877244 fixed.

4. After create all networks, restart libvirtd and check time that libvirt will take to finish initialize iptables(

insert rules to iptables).

# service libvirtd restart

#time virsh list

 

	
Expected Results:

All 500 networks can be defined and started successfully.

4. TBD.
Notes:
Comments:

		222198 	[Virtual Networks] Multiple default portgroups definitions - bug 868483 	ydu 	ydu 	Manual 		Bug verification 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Define an virtual Network with two default portgroups(engineering and sales)
#cat multiple-portgroups.xml
<network>
  <name>multi-portgroups</name>
  <forward mode='nat'/>
  <bridge name='virbr2' stp='on' delay='0' />
  <mac address='52:54:00:A5:69:F9'/>
  <ip address='192.168.120.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.120.2' end='192.168.120.254' />
    </dhcp>
  </ip>
  <portgroup name='engineering' default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='test'/>
    </virtualport>
    <bandwidth>
      <inbound average='1000' peak='5000' burst='5120'/>
      <outbound average='1000' peak='5000' burst='5120'/>
    </bandwidth>
  </portgroup>
  <portgroup name='sales'  default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='salestest'/>
    </virtualport>
    <bandwidth>
      <inbound average='500' peak='2000' burst='2560'/>
      <outbound average='128' peak='256' burst='256'/>
    </bandwidth>
  </portgroup>
</network>
# virsh net-define multiple-portgroups.xml

2. Create an virtual Network with two default portgroups(engineering and sales)
Still use the xml file in step 1, running:
# virsh net-create multiple-portgroup.xml 

3. Update the virtual Network
3.1 Define an virtual Network with 1 default portgroup and start it.
# virsh net-dumpxml  multi-portgroups
<network>
  <name>multi-portgroups</name>
  <forward mode='nat'/>
  <bridge name='virbr2' stp='on' delay='0' />
  <mac address='52:54:00:A5:69:F9'/>
  <ip address='192.168.120.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.120.2' end='192.168.120.254' />
    </dhcp>
  </ip>
  <portgroup name='engineering' default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='test'/>
    </virtualport>
    <bandwidth>
      <inbound average='1000' peak='5000' burst='5120'/>
      <outbound average='1000' peak='5000' burst='5120'/>
    </bandwidth>
  </portgroup>
</network>
3.2 Prepare a xml file with a new default portgroup
# cat new-portgroup.xml
 <portgroup name='sales'  default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='salestest'/>
    </virtualport>
    <bandwidth>
      <inbound average='500' peak='2000' burst='2560'/>
      <outbound average='128' peak='256' burst='256'/>
    </bandwidth>
  </portgroup>
3.3 Update the network
 # virsh net-update multi-portgroups add portgroup new-portgroup.xml 

4. Reuse the XML in step 1 to define an virtual Network with only one default portgroups(engineering=default, sales). 

Then try to edit the network to set 'sales' with defualt='yes'.

#virsh net-edit multi-portgroups

	
Expected Results:

1.

# virsh net-define multiple-portgroup.xml 
error: Failed to define network from multiple-portgroup.xml
error: unsupported configuration: network 'multi-portgroups' has multiple default <portgroup> elements (engineering and sales), but only one default is allowed
2.

# virsh net-create multiple-portgroup.xml 
error: Failed to create network from multiple-portgroup.xml
error: unsupported configuration: network 'multi-portgroups' has multiple default <portgroup> elements (engineering and sales), but only one default is allowed
3.

3.3

# virsh net-update multi-portgroups add portgroup portg.xml 

error: Failed to update network multi-portgroups
error: Requested operation is not valid: a different portgroup entry in network 'multi-portgroups' is already set as the default. Only one default is allowed.

4.

# virsh net-edit multi-portgroups
error: unsupported configuration: network 'multi-portgroups' has multiple default <portgroup> elements (engineering and sales), but only one default is allowed
Failed. Try again? [y,n,f,?]:

 
Notes:
Comments:

		222201 	[Virtual Networks] Live add static hosts into a network 	ydu 	ydu 	Manual 		Bug verification 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Start the default network, and check the dnsmasq commandline, both dhcp-hostsfile and addn-hosts files are created
#  ps aux|grep dnsmasq
nobody   23193  0.0  0.0  12884   680 ?        S    17:47   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --local=// --domain-needed --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --listen-address 192.168.122.1 --dhcp-range 192.168.122.2,192.168.122.2 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=1 --dhcp-no-override --dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile --addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts
1.2. update the default network to add a static host
# virsh net-update default add ip-dhcp-host  "<host mac='52:54:00:f9:d4:63' ip='192.168.122.200' />"
Updated network default live state
# virsh net-dumpxml default
<network>
  <name>default</name>
  <uuid>a13a4e7a-31db-b59a-7a35-985bceb84000</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='52:54:00:F4:C7:A5'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.122.2' end='192.168.122.254' />
      <host mac='52:54:00:f9:d4:63' ip='192.168.122.200' />
    </dhcp>
  </ip>
</network>
#cat /var/lib/libvirt/dnsmasq/default.hostsfile
52:54:00:f9:d4:63,192.168.122.200

2. Define and start a network with no dhcp rang and static host
# virsh net-dumpxml default
<network>
  <name>default</name>
  <uuid>a13a4e7a-31db-b59a-7a35-985bceb84000</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='52:54:00:F4:C7:A5'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
  </ip>
</network>
and check the dnsmasq commandline, and there's no dhcp-hostsfile and addn-hosts files
# ps aux|grep dnsmasq
nobody   23935  0.0  0.0  12880   572 ?        S    18:02   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --local=// --domain-needed --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --listen-address 192.168.122.1 --addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts
2.1 update the default network to add a static host
# virsh net-update default add ip-dhcp-host  "<host mac='52:54:00:f9:d4:63' ip='192.168.122.200' />"

# virsh net-dumpxml default
<network>
  <name>default</name>
  <uuid>a13a4e7a-31db-b59a-7a35-985bceb84000</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='52:54:00:F4:C7:A5'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <host mac='52:54:00:f9:d4:62' ip='192.168.122.201' />
    </dhcp>
  </ip>
</network>
and dnsmasq service is restarted:
# ps aux|grep dnsmasq
nobody   24039  0.0  0.0  12884   612 ?        S    18:15   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --local=// --domain-needed --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --listen-address 192.168.122.1 --dhcp-range 192.168.122.1,static --dhcp-no-override --dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile --addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts
2.2 update the default network to delete a static host
# virsh net-update default delete ip-dhcp-host "<host mac='52:54:00:f9:d4:62' ip='192.168.122.201' />"
Updated network default live state
# virsh net-dumpxml default
<network>
  <name>default</name>
  <uuid>a13a4e7a-31db-b59a-7a35-985bceb84000</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='52:54:00:F4:C7:A5'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
  </ip>
</network>
and dnsmasq service is restarted:
# ps aux|grep dnsmasq
nobody   24206  0.0  0.0  12880   576 ?        S    18:19   0:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --local=// --domain-needed --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --listen-address 192.168.122.1 --addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts

 

	
Expected Results:

As steps described.

Notes:
Comments:

		222203 	[Miscellanea]default machine type detection BZ 867764 	zhpeng 	None 	Manual 		Regression 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea
    Regression

bug:

    No bug found

Actions:

For RHEL6.4 pkg, i. e. libvirt-0.10.2-5.el6.x86_64

# virsh capabilities
...
  <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator> <machine>rhel6.4.0</machine> <machine canonical='rhel6.4.0'>pc</machine>
      <machine>rhel6.3.0</machine>
      <machine>rhel6.2.0</machine>
      <machine>rhel6.1.0</machine>
      <machine>rhel6.0.0</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
...

	
Expected Results:

As steps
Notes:
Comments:

		222208 	[Graphical framebuffers] migrate guest with spice (seamless-migration=on) 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

change libvirt log level to 1

#vim /etc/libvirt/libvirtd.conf

un-comment

#log_level = 3   and change level to 1

restart libvirtd
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

step:
 1: prepare two machine for migration 
 2: on source, create a windows guest with spice
 .....
  <graphics type='spice' autoport='yes' listen='0.0.0.0' keymap='en-us'>
      <listen type='address' address='0.0.0.0'/>
    </graphics>
 .....
 3: start guest ,use virt-viewer/remote-viewer to connect guest
  #virsh start $guest
  #virt-viewer $guest
  #remote-viewer spice://$source/?port=5901
 4: migrate guest to target
  #virsh migrate win7_64 qemu+ssh://target.redhat.com/system
 5: check qemu process,make sure seamless-migration=on
  
  6:after migrate finished,virt-viewer still connect the guest and worked well
  7:check libvirt log

	
Expected Results:

verify:

 after step 3 ,virt-viewer/remote-viewer can show the guest w/o error.

 after step 5 ,check qemu cmd.

#ps -ef | grep kvm
   .....
  -spice port=5901,addr=0.0.0.0,disable-ticketing,seamless-migration=on -k en-US -vga qxl -global qxl-vga.vram_size=67108864 -device qxl,id=video1,vram_size=67108864,bus=pci.0,addr=0x7 -device intel-hda,id=sound0,bus=pci.0,addr=0x6
   .....

 after step 7, check libvirt log:

 ...
 2012-10-25 21:42:11.089+0000: 27729: debug : virJSONValueToString:1133 :  result={"execute":"query-spice","id":"libvirt-123"}
 2012-10-25 21:42:11.089+0000: 27729: debug : qemuMonitorJSONCommandWithFd:263 : Send command '{"execute":"query-spice","id":"libvirt-123"}' for write with FD -1
   ......
   2012-10-25 21:42:11.090+0000: 27724: debug : qemuMonitorIOProcess:353 : QEMU_MONITOR_IO_PROCESS: mon=0x7fc6a4007250 buf={"return": {"migrated": false, "enabled": true, "auth": "none", "port": 5901, "host": "0.0.0.0", "channels": [{"port": "54078", "family": "ipv4", "channel-type": 1, "connection-id": 1804289383, "host": "127.0.0.1", "channel-id": 0, "tls": false}, {"port": "54083", "family": "ipv4", "channel-type": 9, "connection-id": 1804289383, "host": "127.0.0.1", "channel-id": 0, "tls": false}, {"port": "54084", "family": "ipv4", "channel-type": 2, "connection-id": 1804289383, "host": "127.0.0.1", "channel-id": 1, "tls": false}, {"port": "54089", "family": "ipv4", "channel-type": 4, "connection-id": 1804289383, "host": "127.0.0.1", "channel-id": 1, "tls": false}, {"port": "54088", "family": "ipv4", "channel-type": 3, "connection-id": 1804289383, "host": "127.0.0.1", "channel-id": 0, "tls": false}, {"port": "54085", "family": "ipv4", "channel-type": 2, "connection-id": 1804289383, "host": "127.0.0.1", "channel-id": 0, "tls": false}, {"port": "54091", "family": "ipv4", "channel-type": 4, "connection-id": 1804289383, "host": len=1023
  .....
 2012-10-25 21:42:20.543+0000: 27724: debug : qemuMonitorIOProcess:353 : QEMU_MONITOR_IO_PROCESS: mon=0x7fc6a4007250 buf={"return": {"migrated": true, "enabled": true, "auth": "none", "port": 5901, "host": "0.0.0.0", "channels": []}, "id": "libvirt-325"}^M
 len=135
2012-10-25 21:42:20.543+0000: 27724: debug : qemuMonitorJSONIOProcessLine:150 : Line [{"return": {"migrated": true, "enabled": true, "auth": "none", "port": 5901, "host": "0.0.0.0", "channels": []}, "id": "libvirt-325"}]
2012-10-25 21:42:20.543+0000: 27724: debug : virJSONValueFromString:975 : string={"return": {"migrated": true, "enabled": true, "auth": "none", "port": 5901, "host": "0.0.0.0", "channels": []}, "id": "libvirt-325"}
 ...

verify point:

 1. After 'query-migrate' returns 'completed', start 'query-spice'.

 2. After 'query-spice' returns 'migrated: true' and QMP emit a event 'SPICE_MIGRATE_COMPLETED', the qemu-kvm process will quit in source host.
Notes:
Comments:

		222222 	[CPU Management]cpu 'host-model' should support features in force or disable mode-bug870484 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

+++ This bug was initially created as a clone of Bug #799354 +++

Would be handy to have <cpu mode='host-model'> support <feature policy='force'/> or policy=disable. That way tools have an option to turn on x2apic for kvm guests which provides a performance boost with no downsides, but doesn't require host CPU support.

--- Additional comment from rjones@redhat.com on 2012-10-25 14:05:16 EDT ---

Also <feature policy='disable'/>.  For one use case, see:
https://bugzilla.redhat.com/show_bug.cgi?id=870071#c2




Waiting for the Bug 870484  to be fixed.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    870484

Actions:

<cpu mode='host-model'> support <feature policy='force'/> or policy=disable. 

	
Expected Results:
Notes:
Comments:

		222228 	[CPU Management]Check CPU topology in capabilities XML when libvirt fails to detect host CPU model-bug866999,bug868972 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Prepare a RHEL6.3 guest and install a newest libvirt in it.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    866999
    868972

Actions:

Login one RHEL6.3 guest ,run the following commands.

# virsh nodeinfo
CPU model:           x86_64
CPU(s):              4
CPU frequency:       3092 MHz
CPU socket(s):       1
Core(s) per socket:  4
Thread(s) per core:  1
NUMA cell(s):        1
Memory size:         1020340 KiB


# virsh capabilities
<capabilities>

  <host>
    <uuid>616e8f71-236c-88e9-748c-72554ad5c790</uuid>
    <cpu>
      <arch>x86_64</arch>
      <topology sockets='1' cores='4' threads='1'/>
    </cpu>
    <power_management>
      <suspend_disk/>
    </power_management>
    <migration_features>
      <live/>
      <uri_transports>
        <uri_transport>tcp</uri_transport>
      </uri_transports>
    </migration_features>
    <topology>
      <cells num='1'>
        <cell id='0'>
          <cpus num='4'>
            <cpu id='0'/>
            <cpu id='1'/>
            <cpu id='2'/>
            <cpu id='3'/>
          </cpus>
        </cell>
      </cells>
    </topology>
    <secmodel>
      <model>selinux</model>
      <doi>0</doi>
    </secmodel>
    <secmodel>
      <model>dac</model>
      <doi>0</doi>
    </secmodel>
  </host>

</capabilities>

	
Expected Results:

Check the topology in virsh capabilities.
Notes:
Comments:

		222241 	[CPU Management] Enable Hyper-V Enlightenment for Windows guests-bug864606 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Bug 864606 - [RFE] Enable Hyper-V Enlightenment for Windows guests.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    864606

Actions:

1.prepare a windows guest 
  

2.add below in guest xml 
  .....

  <features>
    <pae/>
    <acpi/>
    <apic/>
    <hap/>
    <privnet/>
    <hyperv>
      <relaxed state='on' />
    </hyperv>

  </features>

  .....

 3.start guest, and check qemu-kvm process
  #ps -ef | grep qemu
  qemu     29293     1 16 17:05 ?        00:00:00 /usr/libexec/qemu-kvm -name win7_64 -S -M rhel6.4.0 -cpu qemu64,hv_relaxed -enable-kvm -m 2048 -smp 4,sockets=4,cores=1,threads=1 -uuid d551e24f-1368-6ffb-02fd-01eb28301ab5 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/win7_64.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc ........


 4.try this with cpu host model
  add below in guest xml
  ....
  <cpu mode='host-model'>
    <model fallback='allow'/>
  </cpu>
  .... 

  5.repeat step 3
  qemu     29357     1 39 17:08 ?        00:00:04 /usr/libexec/qemu-kvm -name win7_64 -S -M rhel6.4.0 -cpu Penryn,+osxsave,+xsave,+pdcm,+xtpr,+tm2,+est,+vmx,+ds_cpl,+monitor,+dtes64,+pbe,+tm,+ht,+ss,+acpi,+ds,+vme,hv_relaxed -enable-kvm -m 2048 -smp 4,sockets=4,cores=1,threads=1 -uuid d551e24f-1368-6ffb-02fd-01eb28301ab5.......


  6. Change the <relaxed state='on'/> to <relaxed state='off'/>
   ......
   <features>
    <hyperv>
      <relaxed state='off'/>
    </hyperv>
  </features>
  .....

  7. # ps -ef|grep win7  (There isn't  the hv_relaxed  )
qemu     17822     1  8 11:35 ?        00:00:00 /usr/libexec/qemu-kvm -name win7 -S -M rhel6.4.0 -cpu qemu64 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 ........

  8. Check  invalid value
   ......
      <hyperv>
      <relaxed state='yes'/>
    </hyperv>
    ......

  # virsh edit win7
error: XML error: invalid value of state argument for HyperV Enlightenment feature 'relaxed'
Failed. Try again? [y,n,f,?]:

	
Expected Results:
Notes:
Comments:

		222250 	[CPU Management] Check the cpu model on host detected as what model on guest-bug865580 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Waiting for the following bug to be fixed.

Bug 865580 - Intel Xeon-E5450 on host detected as Core 2 Duo on guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    865580

Actions:

When we use "Copy Host Cpu.." in virt-manager, we see kvm32 
automatically selected. The guest will not run using any CPU if manually 
select one.  Intel Xeon E5450 is Harpertown processor family.  
http://ark.intel.com/products/codename/26555/Harpertown

Customer has an application which is having abysmal performance while 
comparing results of the performance test on the guest and host. (Tested 
with equal number of cpu and memory on guest and host). Using sar, I can 
see that cpu is the bottlenect.

I just wanted to confirm whether the CPU difference in Host and Guest 
could be the root cause of abysmal performance or not. If the host CPU 
is Intel Xeon E5450, then the right cpu on guest is Intel Core2 Duo? If 
no, how can we fix it to deliver the expected performance?

	
Expected Results:
Notes:
Comments:

		222359 	[snapshot] create external checkpoint snapshot with xml and virsh cmd BZ# 873285 638512 874171 880521 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    873285 - From Run 51756
    873285 - From Run 54155

Actions:

create external live snapshot with XML 
 
1) perpare a external live snapshot xml 
# cat ex_vm.xml
<domainsnapshot>
  <name>ex_vm1</name>
  <description>! #$%^</description>
  <state>running</state>
  <memory snapshot='external' file='/tmp/ex_vm1'/>
  <creationTime>1272917631</creationTime>
</domainsnapshot>

2) create external live snapshot  

# virsh snapshot-create bug ex_vm.xml 
Domain snapshot ex_vm1 created from 'ex_vm.xml'

# ll -sh /tmp/ex_vm1 
289M -rw-------. 1 root root 289M Nov  6 13:51 /tmp/ex_vm1


3) check xml of new snapshot
# virsh snapshot-dumpxml bug ex_vm1
<domainsnapshot>
  <name>ex_vm1</name>
  <description>! #$%^</description>
  <state>running</state>
  <creationTime>1352181075</creationTime>
  <memory snapshot='external' file='/tmp/ex_vm1'/>
  <disks>
    <disk name='vda' snapshot='external'>
      <driver type='qcow2'/>
      <source file='/var/lib/libvirt/images/bug.ex_vm1'/>
    </disk>
  </disks>


4) snapshot-list  
[root@intel-q9400-4-2 snap]# virsh snapshot-list bug
 Name                 Creation Time             State
------------------------------------------------------------
 ex_vm1               2012-11-06 13:51:15 +0800 running
 internal-s1          2012-11-06 14:10:26 +0800 running
 s2                   2012-11-06 13:45:25 +0800 disk-snapshot
 savevm1              2012-11-06 14:20:18 +0800 running

 

 

5)

create external live snapshot with virsh cmd

# virsh snapshot-create-as  test ex-s3  --diskspec vda --memspec /tmp/ex-s3
Domain snapshot ex-s3 created


# virsh snapshot-list test
 Name                 Creation Time             State
------------------------------------------------------------
 ex-s3                2012-11-14 13:54:40 +0800 running

#virsh snapshot-dumpxml test  ex-s3

<domainsnapshot>
  <name>ex-s3</name>
  <state>running</state>
  <parent>
    <name>ex-s2</name>
  </parent>
  <creationTime>1352872480</creationTime>
  <memory snapshot='external' file='/tmp/ex-s3'/>
  <disks>
    <disk name='vda' snapshot='external'>
      <driver type='qcow2'/>
      <source file='/var/lib/libvirt/images/test.ex-s3'/>
    </disk>
  </disks>
  <domain type='kvm'>
....

 

 

6)start a  domain 

# virsh snapshot-create-as v9 sv1 --disk-only  --memspec /tmp/asdasd

 error: XML error: memory state cannot be saved with offline snapshot

 

 

7) #for i in `seq 1 50` ;do virsh snapshot-create-as v9 v$i --diskspec=hda --memspec=/tmp/v2 ;;done

 libvirtd should not be dead

 

 

8)Bug 879130 - there is not error message when create external checkpoint with --memspec= (NULL)

# virsh snapshot-create-as v9 v2323 --memspec=   

error should be : error: memspec argument must not be empty

 

 

 

 
	
Expected Results:

no error  expect as step

 

check libvirtd.log sure there is not warning message like :

"qemuDomainObjEnterMonitorInternal:1005 : This thread seems to be the async job owner; entering monitor without asking for a nested job is dangerous"

 

 

step 6 :

error message should not be like :

 error: XML error: memory state cannot be saved with offline snapshot


Notes:
Comments:

		222362 	[libvirtd]libvirt calls 'qemu-kvm -help' too often BZ# 863115 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirtd

bug:

    No bug found

Actions:

config /etc/libvirt/libvirtd.conf

log_level = 1

then 
#service restart libvirtd 

1) setup guests 

2.Start two guests.
# virsh list 
 Id    Name                           State
----------------------------------------------------
 1     rhel6.3                        running
 2     rhel6.3-new                    running


3.Check debug log , the qemu-kvm -help only be executed once.

 vim /var/log/libvirt/libvirtd.log

 2012-10-16 08:56:02.469+0000: 20566: debug : virCommandRunAsync:2209 : About to run LC_ALL=C PATH=/sbin:/usr/sbin:/bin:/usr/bin /usr/libexec/qemu-kvm -help

	
Expected Results:

as step
Notes:
Comments:

		222364 	[migration]live migration converge when guest dirties pages too fast --bug 863264 	lsu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Creat the case for tracking a future feature

Bug 863264 - Help live migration converge when guest dirties pages too fast

And this depens on XBZRLE support , which a rhel7? bug   Bug 842857 - Page Delta Compression (xbzrle) for Live Migration Feature Request - support in libvirt.


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		222365 	[remote access] - libvirt auth.conf make virsh cmd Segmentation fault (core dumped) BZ#859320 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access

bug:

    No bug found

Actions:

1. add auth_unix_rw="sasl" in the /etc/libvirt/libvirtd.conf
add sasl user

# saslpasswd2 -a libvirt test
(input your passwd)
# sasldblistusers2 -f /etc/libvirt/passwd.db
test@intel-q9400-4-7.englab.nay.redhat.com: userPassword


2. add auth.conf file in the /etc/libvirtd/
# cat /etc/libvirt/auth.conf

[credentials-sasl]
authname=test
password=redhat123

[auth-libvirt-localhost]
credentials=sasl


3. restart libvirtd
service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:


4. run  virsh cmd

 # virsh list
Segmentation fault (core dumped)

 [root@intel-q9400-4-7 ~]#virsh  capabilities
Segmentation fault (core dumped)

5.  qemu+unix and qemu works well with auth.conf  without sasl username and passwd
# virsh -c qemu+unix:///system list 
 Id    Name                           State
----------------------------------------------------
 2     qq                             running
 4     ga                             running

# virsh -c qemu:///system list 
 Id    Name                           State
----------------------------------------------------
 2     qq                             running
 4     ga                             running



	
Expected Results:

there should not be any core dumped !
Notes:
Comments:

		222376 	[migration]p2p migration with different cache mode -- bug 867412 	lsu 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:

1.Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

 

2.

Dispatch ssh public key of source host to target host.
   Creating your local public key pair
   # ssh-keygen -t rsa
 Copying the public key to remote host
   # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1.

Start a domain with cache='writeback'

...

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='writeback'/>
      <source dev='/var/lib/libvirt/images/test.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>

...

2. Do migration

# virsh migrate --live --p2p guest qemu+ssh://{target ip}/system --unsafe

 3.Test with other cache option

3.1 cache='"default'

3.2 cache="writethrough"

3.3 cache="unsafe"

3.4 cache="none"
	
Expected Results:

2.Shoulde success and No error like this

Timed out during operation: cannot acquire state change lock

3.1 , 3.2 , 3.3 , 3.4 should same to 2

Notes:
Comments:

		222378 	[LXC]check and destroy domain after restart libvirtd -- Bug 867246 ,864336 	lsu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    lxc

bug:

    No bug found

Actions:

1.Prepare a lxc domain

#cat lxc.xml

<domain type='lxc'>
  <name>toy</name>
  <uuid>bb428983-cb9f-4702-0f8d-7d4e143d9aad</uuid>
  <memory unit='KiB'>500000</memory>
  <currentMemory unit='KiB'>500000</currentMemory>
  <vcpu placement='static'>4</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <console type='pty'>
      <target type='lxc' port='0'/>
    </console>
  </devices>
</domain>

 2. virsh -c lxc:// start toy

3. virsh -c lxc:// list

 Id    Name                           State
----------------------------------------------------
 1621  toy                            running

4. service libvirtd restart

5.#cat /var/log/libvirt/libvirtd.log

6..virsh -c lxc:// list

 Id    Name                           State
----------------------------------------------------
 1621  toy                            running

7.virsh -c lxc:// destroy toy

Domain toy destroyed

 
	
Expected Results:

5.Error below should NOT appear

2012-11-08 07:08:46.381+0000: 8443: error : virNetServerAddClient:270 : Too many active clients (1), dropping connection from 127.0.0.1;0

6.Domain should still be alive

7.Should Not hang
Notes:
Comments:

		222416 	[Scalability] Start a nfs pool with 4.5T size - bug 871701 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability
    storage

bug:

    No bug found

Actions:

1. Define a nfs pool with large storage 4.5T
# cat nfs.xml
<pool type='netfs'>
  <name>nfs1</name>
  <source>
    <host name='10.66.90.121'/>
    <dir path='/vol/S3/libvirtmanual'/>
    <format type='auto'/>
  </source>
  <target>
    <path>/tmp/nfs-libvirtmanual</path>
  </target>
</pool>

# virsh pool-define nfs.xml
Pool nfs1 defined from nfs.xml

# virsh pool-list --all
Name                 State      Autostart 
-----------------------------------------
default              active     no                
nfs1                 inactive   no        

2. Build and start the pool
# virsh pool-build nfs1
Pool nfs1 built
     
# virsh pool-start nfs1


	
Expected Results:

Pool should be started without error
Notes:
Need_update because the bug 871701 still not fixed
Comments:

		222472 	[Guest kernel debugging] "virsh dump" support for SR-IOV or path through devices BZ 826325 874418 	zhpeng 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:

Please install

kernel-debuginfo

kernel-debuginfo-common

with respond version in guest,then can found vmlinux in /usr/lib/debug/lib/modules/`uname -r`/

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    Regression

bug:

    874418 - From Run 51770

Actions:

1. Prepare a normal guest(No SR-IOV device passthroug).
a)Dump the core file of the guest with memory-only
# virsh dump guest /tmp/test.core --memory-only

Domain guest dumped to /tmp/test.core
b). Processe the core file with crash(copy the vmlinux from guest).
# crash /tmp/vmlinux /tmp/test.core 
......

      KERNEL: /tmp/vmlinux                      
    DUMPFILE: /tmp/test.core
        CPUS: 1
        DATE: Thu Nov  8 11:25:32 2012
      UPTIME: 00:04:00
LOAD AVERAGE: 0.06, 0.07, 0.02
       TASKS: 94
    NODENAME: dhcp-ydu-vm.nay.com
     RELEASE: 2.6.32-279.el6.x86_64
     VERSION: #1 SMP Wed Jun 13 18:24:36 EDT 2012
     MACHINE: x86_64  (2659 Mhz)
      MEMORY: 1 GB
       PANIC: ""
         PID: 0
     COMMAND: "swapper"
        TASK: ffffffff81a8d020  [THREAD_INFO: ffffffff81a00000]
         CPU: 0
       STATE: TASK_RUNNING (PANIC)



2. Prepare a guest with a SR-IOV VF passthrough
....
   <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x03' slot='0x10' function='0x0'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </hostdev>
....
a). Dump the guest directly, which will get expected error.
# virsh dump mig /tmp/guest.core
error: Failed to core dump domain mig to /tmp/guest.core
error: internal error unable to execute QEMU command 'migrate': An undefined error has ocurred

b). Dump the core file of the guest with memory-only
# virsh dump mig /tmp/guest.core --memory-only

Domain mig dumped to /tmp/guest.core

For bug 874418:
# virsh dump mig /tmp/guest.core
error: Failed to core dump domain mig to /tmp/guest.core error: internal error unable to execute QEMU command 'migrate': An undefined error has ocurred

 c). Check the core file
# file /tmp/guest.core 
/tmp/guest.core: ELF 64-bit LSB core file x86-64, version 1 (SYSV), SVR4-style

d).Processe the core file with crash(copy the vmlinux from guest).
#crash /tmp/vmlinux /tmp/guest-2.core 

      KERNEL: /tmp/vmlinux                      
    DUMPFILE: /tmp/guest-2.core
        CPUS: 1
        DATE: Thu Nov  8 14:19:41 2012
      UPTIME: 00:15:54
LOAD AVERAGE: 0.00, 0.00, 0.00
       TASKS: 157
    NODENAME: dhcp-ydu-vm.nay.com
     RELEASE: 2.6.32-279.el6.x86_64
     VERSION: #1 SMP Wed Jun 13 18:24:36 EDT 2012
     MACHINE: x86_64  (2666 Mhz)
      MEMORY: 1 GB
       PANIC: ""
         PID: 0
     COMMAND: "swapper"
        TASK: ffffffff81a8d020  [THREAD_INFO: ffffffff81a00000]
         CPU: 0
       STATE: TASK_RUNNING (PANIC)

crash> bt
PID: 0      TASK: ffffffff81a8d020  CPU: 0   COMMAND: "swapper"
 #0 [ffffffff81a01ed0] default_idle at ffffffff810149cd
 #1 [ffffffff81a01ef0] cpu_idle at ffffffff81009e06

	
Expected Results:

as steps

BUG 874418 is not fixed, steps with Bold style need update.
Notes:
Comments:

		222473 	[Guest kernel debugging] Coredump filter to exclude KVM guest OS memory out of QEMU process BZ 822641 	zhpeng 	None 	Manual 		Regression 	P2 	None 	Edit
Setup:

libvirt just pass the "dumpCore" to qemu-kvm commandline as "dump-guest-core", you can also check the coredump file for further testing related to qemu-kvm function:

Trigger a core dump

# kill -s SIGSEGV `pidof qemu-kvm`

# ll core.${pid}

# gdb /usr/libexec/qemu-kvm ${coredump-file}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    guest kernel debugging

bug:

    No bug found

Actions:

# virsh dumpxml rhel6
snip..
<memory dumpCore='off' unit='KiB'>1000000</memory>
snip..

# virsh start rhel6

# ps aux|grep qemu-kvm

qemu      7737 31.5  0.3 1627160 30024 ?       Sl   18:26   0:02 /usr/libexec/qemu-kvm -name rhel62 -S -machine rhel6.3.0,**dump-guest-core=off** -enable-kvm -bios /usr/share/seabios/bios.bin -m 977 -smp 2,maxcpus=4,sockets=4,cores=1,threads=1 -uuid e601feea-ebc3-ed65-d4ab-252c3370470b -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/rhel62.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc...

# virsh edit rhel6
set the dumpCore='on': <memory dumpCore='on' unit='KiB'>1000000</memory>

restart the guest, then check the option for qemu-kvm

# ps aux|grep qemu-kvm
qemu      7905 14.0  0.3 1626600 30184 ?       Sl   18:29   0:00 /usr/libexec/qemu-kvm -name rhel62 -S -machine rhel6.3.0,**dump-guest-core=on** -enable-kvm...

set the dumpCore to other values, such as "00","1", "offf", can't update the xml successfully.
# virsh edit rhel6
error: XML error: Bad value 'offf'
Failed. Try again? [y,n,f,?]:

	
Expected Results:

As steps
Notes:
Comments:

		222485 	[Virtual disks] Start a guest with kernel and disk on r/o file system - bug 862756 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Prepare a readonly nfs on host A

# cat /etc/exports

/tmp/nfs *(ro)

# service nfs restart

# iptables -F

# setsebool virt_use_nfs 1

2. Download vmlinuz and initrd.img from

http://download.englab.nay.redhat.com/pub/rhel/released/RHEL-6/6.3/Server/x86_64/os/images/pxeboot/

in dir /tmp/nfs

3. Create a image in /tmp/nfs

# qemu-img create /tmp/nfs/disk.img 1G
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    862756 - From Run 54154

Actions:

1. Mount nfs and check if it is readonly

# mount {host A ip}:/tmp/nfs /testnfs -o vers=3

# cd /testnfs

# touch file

touch: cannot touch `file': Read-only file system

2. Create a guest with kernel and initrd file in nfs

# virsh dumpxml tt

...
  <os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <kernel>/testnfs/vmlinuz</kernel>
    <initrd>/testnfs/initrd.img</initrd>
    <boot dev='hd'/>
  </os>
...

3. Using an exist guest with installed os and attach disk.img to this guest

# virsh attach-disk rhel6u4 /testnfs/disk.img vdb
	
Expected Results:

Step 2

Guest can be started successfully

Step 3

Disk can be attached successfully
Notes:
The bug moving to 6.5 --lsu
Comments:

		222487 	[Virtual Networks] attach/detach of netdevs with matching mac addresss - bug 862515 	ydu 	None 	Manual 		Bug verification 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. persisten attach-interface
# virsh domiflist guest
Interface  Type       Source     Model       MAC
-------------------------------------------------------
vnet0      bridge     virbr0     -           52:54:00:5a:a0:8b

# virsh attach-interface guest bridge virbr0  --mac 52:54:00:5a:a0:8c --persistent
Interface attached successfully

# virsh domiflist guest
Interface  Type       Source     Model       MAC
-------------------------------------------------------
vnet0      bridge     virbr0     -           52:54:00:5a:a0:8b
vnet1      bridge     virbr0     -           52:54:00:5a:a0:8c

# virsh attach-interface guest bridge virbr0  --mac 52:54:00:5a:a0:8c --persistent
Interface attached successfully

# virsh domiflist guest
Interface  Type       Source     Model       MAC
-------------------------------------------------------
vnet0      bridge     virbr0     -           52:54:00:5a:a0:8b
vnet1      bridge     virbr0     -           52:54:00:5a:a0:8c
vnet2      bridge     virbr0     -           52:54:00:5a:a0:8c

2. live attach-interface
# virsh attach-interface guest bridge virbr0  --mac 52:54:00:5a:a0:8c 
Interface attached successfully

# virsh domiflist guest
Interface  Type       Source     Model       MAC
-------------------------------------------------------
vnet0      bridge     virbr0     -           52:54:00:5a:a0:8b
vnet1      bridge     virbr0     -           52:54:00:5a:a0:8c
vnet2      bridge     virbr0     -           52:54:00:5a:a0:8c
vnet3      bridge     virbr0     -           52:54:00:5a:a0:8c

# virsh attach-interface guest bridge virbr0  --mac 52:54:00:5a:a0:8c 
Interface attached successfully
# virsh domiflist guest
Interface  Type       Source     Model       MAC
-------------------------------------------------------
vnet0      bridge     virbr0     -           52:54:00:5a:a0:8b
vnet1      bridge     virbr0     -           52:54:00:5a:a0:8c
vnet2      bridge     virbr0     -           52:54:00:5a:a0:8c
vnet3      bridge     virbr0     -           52:54:00:5a:a0:8c
vnet4      bridge     virbr0     -           52:54:00:5a:a0:8c

3. persistent/live detach-interface
#  virsh detach-interface guest  bridge --mac 52:54:00:5a:a0:8c --persistent
error: Domain has multiple interfaces matching MAC address 52:54:00:5a:a0:8c. You must use detach-device and specify the device pci address to remove it.

#  virsh detach-interface guest  bridge --mac 52:54:00:5a:a0:8c
error: Domain has multiple interfaces matching MAC address 52:54:00:5a:a0:8c. You must use detach-device and specify the device pci address to remove it.

4. use detach-device to remove these interfaces
Prepare a xml file by copying the <interface> specification from the guest's XML config, it looks like:
# cat interface.xml 
 <interface type='bridge'>
      <mac address='52:54:00:5a:a0:8c'/>
      <source bridge='virbr0'/>
      <target dev='vnet3'/>
      <alias name='net3'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </interface>

# virsh detach-device guest interface.xml 
Device detached successfully

# virsh domiflist guest
Interface  Type       Source     Model       MAC
-------------------------------------------------------
vnet0      bridge     virbr0     -           52:54:00:5a:a0:8b
vnet2      bridge     virbr0     -           52:54:00:5a:a0:8c
vnet3      bridge     virbr0     -           52:54:00:5a:a0:8c
vnet4      bridge     virbr0     -           52:54:00:5a:a0:8c

change the PCI address to remove other interfaces(live or persisten)and all these commands works well.

	
Expected Results:
Notes:
Comments:

		222489 	[Virtual Networks] virtualport parameter profileid in a <network> or <portgroup> causes failure to initialize guest interface - bug 864122 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. setup a openvswitch env 
refer to case 

199844 [virtual networks] openvswitch support 

2. create in the test guest, define an interface as:

    <interface type='network'>
      <source network='ovs-test-net'/>
    </interface>

3. define and start a network called 'ovs-test-net' that uses an existing openvswitch bridge :

   <network>
     <name>ovs-test-net</name>
     <forward mode='bridge'/>
     <bridge name='ovsbr'/>
     <virtualport type='openvswitch'>
       <parameters profileid='testprofile'/>
     </virtualport>
   </network>

4. Start the guest.

	
Expected Results:

It can start success with no error.

Notes:
Comments:

		222490 	[PCI and USB device assignment] Start a guest with smartcard and remove usb controller - bug 870003 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Prepare a guest with

   <controller type='ccid' index='0'/>

    <smartcard mode='passthrough' type='spicevmc'>
      <address type='ccid' controller='0' slot='0'/>
    </smartcard>

2. Edit guest to remove usb controller

3. Start guest
	
Expected Results:

TBD
Notes:
Need_update because bug still not fixed and don't know the expect result
Comments:

		222495 	[PCI and USB device assignment] Support USB device with different policy - bug843560 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Prepare a usb disk and do not plug into host
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Prepare a guest with

 <hostdev mode='subsystem' type='usb' managed='yes'>
 <source startupPolicy='requisite'>
 <vendor id='0x0951'/>
 <product id='0x1646'/>
 </source>
 </hostdev>

2. Start guest

3. Change step 1 startupPolicy to 'optional' and start guest

4. Change step 1 startupPolicy to 'mandatory' and start guest

5. Pulg in the usb disk and start guest with startupPolicy='requisite', save guest and then pull out the usb disk, do restore

6. Pulg in the usb disk and start guest with startupPolicy='optional', save guest and then pull out the usb disk, do restore

7. Pulg in the usb disk and start guest with startupPolicy='mandatory', save guest and then pull out the usb disk, do restore

8. Plug in the usb disk and start guest with startupPolicy='requisite', save and restore guest

9. Plug in the usb disk and start guest with startupPolicy='optional', save and restore guest

10. Plug in the usb disk and start guest with startupPolicy='mandatory', save and restore guest

 

 
	
Expected Results:

Step 2

report error

virsh start vr-rhel6u3-x86_64-kvm
error: Failed to start domain vr-rhel6u3-x86_64-kvm
error: internal error Did not find USB device 951:164

Step 3

Guest can be started successfully, check xml

<hostdev mode='subsystem' type='usb' managed='yes'>
      <source startupPolicy='optional' missing='yes'>
        <vendor id='0x0951'/>
        <product id='0x1646'/>
      </source>
      <alias name='hostdev0'/>
    </hostdev>

Step 4

virsh start vr-rhel6u3-x86_64-kvm
error: Failed to start domain vr-rhel6u3-x86_64-kvm
error: internal error Did not find USB device 951:164

Step 5

Guest can be restored successfully

check xml

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source startupPolicy='requisite' missing='yes'>
        <vendor id='0x0951'/>
        <product id='0x1646'/>
        <address bus='2' device='2'/>
      </source>
      <alias name='hostdev0'/>
    </hostdev>

Step 6

Guest can be restored successfully

check xml

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source startupPolicy='optional' missing='yes'>
        <vendor id='0x0951'/>
        <product id='0x1646'/>
        <address bus='2' device='2'/>
      </source>
      <alias name='hostdev0'/>
    </hostdev>

Step 7

Report error

error: Failed to restore domain from /tmp/vr-rhel6u3-x86_64-kvm.save
error: internal error Did not find USB device 951:1646

Step 8,9,10

All restore will succeed
Notes:
Comments:

		222526 	[Managed save] managed save the guest in pmsuspended status and resume it 	dyuan 	None 	Manual (Autoproposed) 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    managed save

bug:

    No bug found

Actions:

1. pmsuspend the guest to memory

# virsh dompmsuspend --target mem rhel63
Domain rhel63 successfully suspended

# virsh list
 Id    Name                           State
----------------------------------------------------
 1     rhel63                         pmsuspended
 1     rhel62                         running

2. managed save the guest

# virsh managedsave rhel63

Domain rhel63 state saved by libvirt

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 1     rhel62                         running
 -     qed                            shut off
 -     rhel63                         shut off

3. start the guest

====expected result for dompmwakeup
# virsh dompmwakeup rhel63
error: Domain rhel63 could not be woken up
error: Requested operation is not valid: domain is not running

# virsh start rhel63
Domain rhel63 started

====the guest status become to "paused"
# virsh list --all
 Id    Name                           State
----------------------------------------------------
 1     rhel62                         running
 2     rhel63                         paused
 -     qed                            shut off

4. resume the guest

====expected result for dompmwakeup
# virsh dompmwakeup rhel63
Domain rhel63 successfully woken up
# virsh list
 Id    Name                           State
----------------------------------------------------
 1     rhel62                         running
 2     rhel63                         paused


# virsh resume rhel63
Domain rhel63 resumed

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 1     rhel62                         running
 2     rhel63                         running
 
Actual results:
The guest can't be resumed to the original status when it's pmsuspended.
	
Expected Results:
Notes:
Comments:

		222632 	[Miscellanea] booting delay time BZ 855237 	zhpeng 	None 	Manual 		Regression 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    miscellanea

bug:

    No bug found

Actions:

Run a guest without any bootable device or pxe env.

try several rebootTimeout value: 
$TIME:  -1, 0, 1, 10000, 65535, >65535, <-1, 1.5, -1.5


<os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <loader>/usr/share/seabios/bios.bin</loader>
    <boot dev='network'/>
    <boot dev='hd'/>
    <bios useserial='yes' rebootTimeout='$TIME'/>
</os>

# ps aux|grep $guest
make sure qemu process include this parameter and value
  -boot reboot-timeout=$TIME

 0-65535, guest will reboot after 0~65s if boot from failure.
-1, will not reboot again.

when value is Out of range libvirt will report error:
virsh # edit aaa
error: unsupported configuration: invalid value for rebootTimeout, must be in range [-1,65535]
Failed. Try again? [y,n,f,?]:

	
Expected Results:

As subject.
Notes:
Comments:

		224702 	[Virtual Networks] libvirt iptable rules integration with firewalld - bug 622649 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

 

 

	
Expected Results:
Notes:
Comments:

		224808 	[PCI and USB device assignment] support for USB 2.0 (EHCI) - bug 715590 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

plugin 3 usb flash disk in host and get the bus and device number

# lsusb

Bus 002 Device 004: ID 0951:1646 Kingston Technology
Bus 006 Device 005: ID 058f:6387 Alcor Micro Corp. Flash Drive
Bus 006 Device 006: ID 10d6:1101 Actions Semiconductor Co., Ltd D-Wave 2GB MP4 Player / AK1025 MP3/MP4 Player

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. virsh dumpxml guest
<domain type='kvm' id='13'>
  <name>guest</name>
  <uuid>c631184e-3027-0edd-48be-8798c9c64682</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/test.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='usb' index='0' model='piix3-uhci'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
    </controller>
    <controller type='usb' index='1' model='ich9-ehci1'>
      <alias name='usb1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <controller type='usb' index='1' model='ich9-uhci1'>
      <alias name='usb1'/>
      <master startport='0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </controller>
    <controller type='usb' index='1' model='ich9-uhci2'>
      <alias name='usb1'/>
      <master startport='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </controller>
    <controller type='usb' index='1' model='ich9-uhci3'>
      <alias name='usb1'/>
      <master startport='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>
    </controller>
    <controller type='usb' index='2' model='ich9-ehci1'>
      <alias name='usb2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0a' function='0x0'/>
    </controller>
    <controller type='usb' index='2' model='ich9-uhci1'>
      <alias name='usb2'/>
      <master startport='0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0b' function='0x0'/>
    </controller>
    <controller type='usb' index='2' model='ich9-uhci2'>
      <alias name='usb2'/>
      <master startport='2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0c' function='0x0'/>
    </controller>
    <controller type='usb' index='2' model='ich9-uhci3'>
      <alias name='usb2'/>
      <master startport='4'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0d' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:31:14:4b'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/6'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/6'>
      <source path='/dev/pts/6'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='tablet' bus='usb'>
      <alias name='input0'/>
      <address type='usb' bus='0' port='2'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5900' autoport='yes'/>
    <sound model='ich6'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <hostdev mode='subsystem' type='usb' managed='no'>
      <source>
        <address bus='2' device='4'/>
      </source>
      <alias name='hostdev0'/>
      <address type='usb' bus='2' port='1'/>
    </hostdev>
    <hostdev mode='subsystem' type='usb' managed='no'>
      <source>
        <address bus='6' device='5'/>
      </source>
      <alias name='hostdev1'/>
      <address type='usb' bus='2' port='2'/>
    </hostdev>
    <hostdev mode='subsystem' type='usb' managed='no'>
      <source>
        <address bus='6' device='6'/>
      </source>
      <alias name='hostdev2'/>
      <address type='usb' bus='2' port='3'/>
    </hostdev>
    <hub type='usb'>
      <alias name='hub0'/>
      <address type='usb' bus='1' port='1'/>
    </hub>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux' relabel='yes'>
    <label>system_u:system_r:svirt_t:s0:c266,c967</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c266,c967</imagelabel>
  </seclabel>
</domain>

# virsh start guest
# ps aux |grep guest 

2. Login guest and check all usb device can be find in the guest

3. Do hotunplug/hotplug for 1 usb
# cat usb.xml

    <hostdev mode='subsystem' type='usb' managed='no'>
      <source>
        <address bus='2' device='4'/>
      </source>
      <alias name='hostdev0'/>
      <address type='usb' bus='2' port='1'/>
    </hostdev>


# virsh detach-device guest usb.xml

# virsh attach-device guest usb.xml

	
Expected Results:

Step 1
-device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 
-device ich9-usb-ehci1,id=usb1,bus=pci.0,addr=0x6 
-device ich9-usb-uhci1,masterbus=usb1.0,firstport=0,bus=pci.0,addr=0x7 
-device ich9-usb-uhci2,masterbus=usb1.0,firstport=2,bus=pci.0,addr=0x8 
-device ich9-usb-uhci3,masterbus=usb1.0,firstport=4,bus=pci.0,addr=0x9 
-device ich9-usb-ehci1,id=usb2,bus=pci.0,addr=0xa 
-device ich9-usb-uhci1,masterbus=usb2.0,firstport=0,bus=pci.0,addr=0xb 
-device ich9-usb-uhci2,masterbus=usb2.0,firstport=2,bus=pci.0,addr=0xc 
-device ich9-usb-uhci3,masterbus=usb2.0,firstport=4,bus=pci.0,addr=0xd 
-device usb-hub,id=hub0,bus=usb1.0,port=1 
-device usb-tablet,id=input0,bus=usb.0,port=2 
-device usb-host,hostbus=2,hostaddr=4,id=hostdev0,bus=usb2.0,port=1 
-device usb-host,hostbus=6,hostaddr=5,id=hostdev1,bus=usb2.0,port=2 
-device usb-host,hostbus=6,hostaddr=6,id=hostdev2,bus=usb2.0,port=3

Step 3
Detach should succeed and check the guest, corresponding usb device will be remove from guest
Attach should succeed, you can find usb again in guest

Notes:
Comments:

		225412 	[CPU Management] Hotplug vcpus when guest do S3 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		225413 	[CPU Management] Hotplug vcpus when guest do S3 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		225418 	[CPU Management]Hotplug vcpus when guest do S3 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1.Prepare the following guest.

#virsh dumpxml rhel6
...

<vcpu placement='static' current='1'>4</vcpu>
 <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <loader>/usr/share/seabios/bios.bin</loader>
    <boot dev='hd'/>
  </os>
<pm>
     <suspend-to-mem enabled='yes'/>
     <suspend-to-disk enabled='yes'/>
   </pm>

...
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/rhel6.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
...

2. Install  qemu-guest-agent  in guest  and start the service or run # qemu-ga -d in guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    No bug found

Actions:

# virsh start rhel6
Domain rhel6 started

# virsh dumpxml rhel6|grep vcpu
  <vcpu placement='static' current='1'>4</vcpu>

# virsh setvcpus rhel6 2

# virsh vcpucount rhel6
maximum      config         4
maximum      live             4
current         config          1
current         live              2

# virsh dompmsuspend rhel6 --target mem
Domain rhel6 successfully suspended

# virsh vcpucount rhel6
maximum      config         4
maximum      live             4
current         config          1
current         live              2

# virsh setvcpus rhel6 3

# virsh vcpucount rhel6
maximum      config         4
maximum      live             4
current         config          1
current         live             3

# virsh dompmwakeup rhel6
Domain rhel6 successfully woken up

# virsh vcpucount rhel6
maximum      config         4
maximum      live             4
current         config          1
current         live              3
	
Expected Results:
Notes:
Comments:

		225441 	[CPU Management]Hotplug vcpus when guest do S4 -bug872419 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1.Prepare the following guest.

#virsh dumpxml rhel6
...

<vcpu placement='static' current='1'>4</vcpu>
 <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <loader>/usr/share/seabios/bios.bin</loader>
    <boot dev='hd'/>
  </os>
<pm>
     <suspend-to-mem enabled='yes'/>
     <suspend-to-disk enabled='yes'/>
   </pm>

...
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/rhel6.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
...

2. Install  qemu-guest-agent  in guest  and start the service or run # qemu-ga -d in guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    872419

Actions:

# virsh start rhel6
Domain rhel6 started

# virsh vcpucount rhel6
maximum      config         4
maximum      live           4
current      config         1
current      live           1

# virsh setvcpus rhel6 2

# virsh vcpucount rhel6
maximum      config         4
maximum      live           4
current      config         1
current      live           2

# virsh dompmsuspend rhel6 --target disk
Domain rhel6 successfully suspended


# virsh setvcpus rhel6 3
error: Requested operation is not valid: domain is not running

# virsh start rhel6
Domain rhel6 started

# virsh vcpucount rhel6
maximum      config         4
maximum      live           4
current      config         1
current      live           1
	
Expected Results:
Notes:
The bug 872419 is not fixed yet - bili
Comments:

		226898 	[snapshot] libvirt should allow disk-only (external) snapshots of offline VMs BZ#876816 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

# virsh list  --all
 Id    Name                           State
----------------------------------------------------
-     b1                          shut off

 

# virsh snapshot-create-as b1 --disk-only

 

 

 
	
Expected Results:

1) do not show error like :

error: unsupported configuration: disk snapshots of inactive domains not implemented yet

Notes:
this case need
https://bugzilla.redhat.com/show_bug.cgi?id=876816 has been verified
Comments:

    #1 whuang@redhat.com 2012-12-03 17:59:30
    merge a into the disk-only snapshot create case

		226899 	[snapshot] [RFE] Add support for reverting and deleting of external checkpoints BZ#873285 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:

this case need https://bugzilla.redhat.com/show_bug.cgi?id=873285 has been verified 
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

1) create external checkpoint snapshot 

# virsh snapshot-create-as  test ex-s3  --diskspec vda --memspec /tmp/ex-s3
Domain snapshot ex-s3 created


# virsh snapshot-list test
 Name                 Creation Time             State
------------------------------------------------------------
 ex-s3                2012-11-14 13:54:40 +0800 running


2) do some change in the guest 


3) try to revert to external checkpoint snapshot 

#virsh snapshot-revert test ex-s3 




4) try to delete external checkpoint snapshot

#virsh snapshot-delete test ex-s3



	
Expected Results:

3)

Domain snapshot es1 created

 

4)

Domain snapshot es1 deleted

Notes:
Comments:

		226902 	[snapshot] - virsh should make it easier to filter snapshots by type BZ#876817 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

1) start qcow2 domain then 
 
[root@intel-q9400-4-7 ~]# virsh snapshot-create-as bug active-internal
Domain snapshot active-internal created
[root@intel-q9400-4-7 ~]# virsh snapshot-create-as bug active-external --memspec /tmp/bug
Domain snapshot active-external created
[root@intel-q9400-4-7 ~]# virsh snapshot-create-as bug active-disk-only --disk-only
Domain snapshot active-disk-only created

2) destroy domain then 
[root@intel-q9400-4-7 ~]# virsh snapshot-create-as bug inactive-disk-only --disk-only
Domain snapshot inactive-disk-only created
[root@intel-q9400-4-7 ~]# virsh snapshot-create-as bug inactive-internal
Domain snapshot inactive-internal created

3) check snapshot-list 

# virsh snapshot-list bug
 Name                 Creation Time             State
------------------------------------------------------------
 active-disk-only     2012-11-27 17:24:11 +0800 disk-snapshot
 active-external      2012-11-27 17:23:59 +0800 running
 active-internal      2012-11-27 17:22:26 +0800 running
 inactive-disk-only   2012-11-27 17:24:57 +0800 shutoff
 inactive-internal    2012-11-27 17:25:06 +0800 shutoff

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --active 
 Name                 Creation Time             State
------------------------------------------------------------
 active-external      2012-11-27 17:23:59 +0800 running
 active-internal      2012-11-27 17:22:26 +0800 running

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --inactive 
 Name                 Creation Time             State
------------------------------------------------------------
 inactive-disk-only   2012-11-27 17:24:57 +0800 shutoff
 inactive-internal    2012-11-27 17:25:06 +0800 shutoff

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --external
 Name                 Creation Time             State
------------------------------------------------------------
 active-disk-only     2012-11-27 17:24:11 +0800 disk-snapshot
 active-external      2012-11-27 17:23:59 +0800 running
 inactive-disk-only   2012-11-27 17:24:57 +0800 shutoff

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --internal
 Name                 Creation Time             State
------------------------------------------------------------
 active-internal      2012-11-27 17:22:26 +0800 running
 inactive-internal    2012-11-27 17:25:06 +0800 shutoff

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --disk-only
 Name                 Creation Time             State
------------------------------------------------------------
 active-disk-only     2012-11-27 17:24:11 +0800 disk-snapshot
[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --active --inactive 
 Name                 Creation Time             State
------------------------------------------------------------
 active-external      2012-11-27 17:23:59 +0800 running
 active-internal      2012-11-27 17:22:26 +0800 running
 inactive-disk-only   2012-11-27 17:24:57 +0800 shutoff
 inactive-internal    2012-11-27 17:25:06 +0800 shutoff

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --active --disk-only
 Name                 Creation Time             State
------------------------------------------------------------
 active-disk-only     2012-11-27 17:24:11 +0800 disk-snapshot
 active-external      2012-11-27 17:23:59 +0800 running
 active-internal      2012-11-27 17:22:26 +0800 running

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --active --internal
 Name                 Creation Time             State
------------------------------------------------------------
 active-internal      2012-11-27 17:22:26 +0800 running

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --active --external
 Name                 Creation Time             State
------------------------------------------------------------
 active-external      2012-11-27 17:23:59 +0800 running

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --inactive --disk-only
 Name                 Creation Time             State
------------------------------------------------------------
 active-disk-only     2012-11-27 17:24:11 +0800 disk-snapshot
 inactive-disk-only   2012-11-27 17:24:57 +0800 shutoff
 inactive-internal    2012-11-27 17:25:06 +0800 shutoff

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --inactive --internal
 Name                 Creation Time             State
------------------------------------------------------------
 inactive-internal    2012-11-27 17:25:06 +0800 shutoff

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --inactive --external
 Name                 Creation Time             State
------------------------------------------------------------
 inactive-disk-only   2012-11-27 17:24:57 +0800 shutoff


 [root@intel-q9400-4-7 ~]# virsh snapshot-list bug --disk-only --internal
 Name                 Creation Time             State
------------------------------------------------------------

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --disk-only --external
 Name                 Creation Time             State
------------------------------------------------------------
 active-disk-only     2012-11-27 17:24:11 +0800 disk-snapshot

[root@intel-q9400-4-7 ~]# virsh snapshot-list bug --internal --external
 Name                 Creation Time             State
------------------------------------------------------------
 active-disk-only     2012-11-27 17:24:11 +0800 disk-snapshot
 active-external      2012-11-27 17:23:59 +0800 running
 active-internal      2012-11-27 17:22:26 +0800 running
 inactive-disk-only   2012-11-27 17:24:57 +0800 shutoff
 inactive-internal    2012-11-27 17:25:06 +0800 shutoff

	
Expected Results:

as steps
Notes:
Comments:

		229395 	[configuration] host pm tag discover 	zhpeng 	zhpeng 	Manual 		Function 	P2 	None 	Edit
Setup:

# pm-is-supported --help
pm-is-supported [--suspend | --hibernate | --suspend-hybrid ]

# man pm-is-supported

....

RETURN VALUE
       The result of the test for a certain powermanagement state is defined by the following exit codes.

       Code   Diagnostic
       0      State available.
       1      State NOT available.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration

bug:

    No bug found

Actions:

# pm-is-supported --suspend
# echo $?
0
# pm-is-supported --hibernate
# echo $?
0
# pm-is-supported --suspend-hybrid
# echo $?
1

 

virsh # capabilities
...
    <power_management>
      <suspend_mem/>
      <suspend_disk/>
    </power_management>
...

	
Expected Results:

as steps.

and if host support suspend_hybrid, capabilities is:

virsh # capabilities
...
    <power_management>
      <suspend_mem/>
      <suspend_disk/>
      <suspend_hybrid/>
    </power_management>
...

Notes:
Comments:

		229440 	[LXC] libvirt_lxc segfaults when staring lxc guest - BZ#874549,#880064 	ajia 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

# mkdir -p /tmp/instance-00000069/rootfs

# febootstrap --group-install="base" rawhide /tmp/instance-00000069/rootfs/

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    rhel6.4

bug:

    No bug found

Actions:

1. the lxc guest XML configuration.

# cat instance-00000069

<domain type='lxc'>
  <name>instance-00000069</name>
  <uuid>5abb4ca2-9e9b-4b33-b489-b09d301b1e8f</uuid>
  <memory unit='KiB'>524288</memory>
  <currentMemory unit='KiB'>524288</currentMemory>
  <vcpu placement='static'>2</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/sbin/init</init>
    <cmdline>console=ttyS0</cmdline>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <filesystem type='mount' accessmode='passthrough'>
      <source dir='/tmp/instance-00000069/rootfs/'/>
      <target dir='/'/>
    </filesystem>
    <interface type='bridge'>
      <mac address='fa:16:3e:24:b3:65'/>
      <source bridge='br100'/>
      <filterref filter='nova-instance-instance-00000069-fa163e24b365'>
        <parameter name='DHCPSERVER' value='10.48.253.1'/>
        <parameter name='IP' value='10.48.253.2'/>
        <parameter name='PROJMASK' value='255.255.255.0'/>
        <parameter name='PROJNET' value='10.48.253.0'/>
      </filterref>
    </interface>
    <console type='pty'>
      <target type='lxc' port='0'/>
    </console>
  </devices>
</domain>

define domain but not start it

prepare a xml to /var/run/libvirt/lxc/
# virsh -c lxc:/// dumpxml instance-00000069 >/var/run/libvirt/lxc/instance-00000069.xml

2. create the lxc guest

# /usr/libexec/libvirt_lxc --name instance-00000069 --console 23 --handshake 26 --background --veth veth1

3. create with valgrind

# valgrind /usr/libexec/libvirt_lxc --name instance-00000069 --console 23 --handshake 26 --background --veth veth1

	
Expected Results:

After step2, you shouldn't see "Segmentation fault (core dumped)" error, but the guest

maybe not been started successfully if you haven't a rootfs configuration as follows:

 /tmp/instance-00000069/rootfs/

3.

==22764== HEAP SUMMARY:
==22764==     in use at exit: 78,779 bytes in 99 blocks
==22764==   total heap usage: 2,061 allocs, 1,962 frees, 392,095 bytes allocated
==22764==
==22764== LEAK SUMMARY:
==22764==    definitely lost: 0 bytes in 0 blocks
==22764==    indirectly lost: 0 bytes in 0 blocks
==22764==      possibly lost: 349 bytes in 18 blocks
==22764==    still reachable: 78,430 bytes in 81 blocks
==22764==         suppressed: 0 bytes in 0 blocks
==22764== Rerun with --leak-check=full to see details of leaked memory
==22764==
==22764== For counts of detected and suppressed errors, rerun with: -v
==22764== Use --track-origins=yes to see where uninitialised values come from
==22764== ERROR SUMMARY: 90 errors from 10 contexts (suppressed: 8 from 6


No signal 11 (SIGSEGV) in output

 
Notes:
Comments:

		229441 	[PCI and USB device assignment] Support ich9-uhci1/2/3 model as usb controller model - BZ#877330 	ajia 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. the guest XML configuration as follows:

# cat test.xml
<domain type='kvm' id='20'>
  <name>test</name>
  <memory unit='KiB'>524288</memory>
  <currentMemory unit='KiB'>524288</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/test.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='usb' index='0' model='ich9-uhci1'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </controller>
    <hub type='usb'>
       <address type='usb' bus='0' port='1'/>
    </hub>
    <memballoon model='none'>
      <alias name='balloon0'/>
    </memballoon>
  </devices>
</domain>


2.  Create a image file according to above XML configuration

# qemu-img create /var/lib/libvirt/images/test.img 1M

3. Create the guest from above XML file

# virsh create test.xml

	
Expected Results:

After step3, you shouldn't meet the following error and guest can be successfully created.

# virsh create test.xml

Actual result
report error
error: Failed to create domain from test.xml
error: internal error process exited while connecting to monitor: qemu-kvm: -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,addr=0x2: Parameter 'masterbus' expects an USB masterbus
qemu-kvm: -device ich9-usb-uhci1,masterbus=usb.0,firstport=0,bus=pci.0,addr=0x2: Device 'ich9-usb-uhci1' could not be initialized

 

Notes:
Comments:

		229444 	[Memory management] change memory/currentMemory by virsh then do operations BZ 873134 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    memory management
    Regression

bug:

    No bug found

Actions:

1.
# virsh edit rhel6
.....
  <memory unit='KiB'>2000000</memory>
  <currentMemory unit='KiB'>2000000</currentMemory>
.....

Domain rhel6 XML configuration edited.
2.
# virsh start rhel6
Domain rhel6 started
3.
# virsh managedsave rhel6 

Domain rhel6 state saved by libvirt
4.
# virsh start rhel6
Domain rhel6 started
5.
# virsh dumpxml rhel6
<domain type='kvm' id='36'>
.....
  <memory unit='KiB'>2000896</memory>
  <currentMemory unit='KiB'>2000000</currentMemory>
.....
</domain>

	
Expected Results:

As steps
Notes:
Comments:

		229451 	[Virtio disks] Hotplug/unplug virtio-scsi CD-ROM 	ydu 	ydu 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Fresh install the guest(should be 6.3 or later) then edit xml with scsi controller

<controller type="scsi" index="0" model="virtio-scsi">

2. Make sure there's DVD in the host's CDROM, and start the guest.

3. Hotplug cdrom with xm

#cat cdrom.xml

<disk type='block' device='lun'>

<driver name='qemu' type='raw'/>

<source dev='/dev/cdrw'/>

<target dev='sdc' bus='scsi'/> 

</disk>

# virsh attach-device guest cdrom.xml

4. Log in the guest, and find which is virtio-scsi host

# cat /sys/class/scsi_host/host[N]/proc_name (which named virtio_scsi)

scan this host

# echo "- - -" > /sys/class/scsi_host/host[N]/scan (there is a blank between "-")

find the scsi disk

# scsi-rescan(need install sg3_utils package)

mount /dev/cdrom and check

umount /dev/cdrom after check

5. On the host, hotunplug the CDROM device by:

# virsh detach-disk guest sdc

Disk detached successfully

6. Check again in the guest.
	
Expected Results:

4. in guest,

# scsi-rescan 
Host adapter 0 (ata_piix) found.
Host adapter 1 (ata_piix) found.
Host adapter 2 (virtio_scsi) found.
Scanning SCSI subsystem for new devices
Scanning host 0 for  SCSI target IDs  0 1 2 3 4 5 6 7, all LUNs
Scanning host 1 for  SCSI target IDs  0 1 2 3 4 5 6 7, all LUNs
Scanning host 2 for  SCSI target IDs  0 1 2 3 4 5 6 7, all LUNs
Scanning for device 2 0 0 2 ...
OLD: Host: scsi2 Channel: 00 Id: 01 Lun: 02
      Vendor: PLDS     Model: DVD+-RW DH-16AAS Rev: JD12
      Type:   CD-ROM                           ANSI SCSI revision: 05
0 new device(s) found.               
0 device(s) removed.

Mount succeed and you can check iso content in mount dir
     

6. should NO CDROM detected.

Notes:
Comments:

		229462 	[remote access]Permission denied error printed in wrong place when input invalid password - bug 874897 	bili 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.5.0
    remote access

bug:

    No bug found

Actions:

1. On one host A, stop libvirtd service and start it by a non-root user
#service libvirtd stop
#su tester
$libvirtd -d

2. On another host B, try to connect that libvirtd
#virsh -c qemu+ssh://tester@${hostA_ip}/session
or
#virsh -c qemu+ssh://tester@${hostA_ip}/system
or 
#virsh -c qemu+ssh://tester@${hostA_ip}/aaaaaa


3.Try to input inavalid password for tester when issue step 2.

 
	
Expected Results:

Step2:

error: End of file while reading data: : Input/output error
error: failed to connect to the hypervisor

and no such error messages:

error: End of file while reading data: 2012-08-06 02:41:17.381+0000: 7888: info : libvirt version: 0.10.0, package: 0rc0.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2012-08-02-03:44:27, x86-007.build.bos.redhat.com)
2012-08-06 02:41:17.381+0000: 7888: warning : virFileClose:65 : Tried to close invalid fd 7: Input/output error
error: failed to connect to the hypervisor

Step 3:
Should like:
guest@10.66.6.38's password: 
Permission denied, please try again.
guest@10.66.6.38's password: 
error: End of file while reading data: : Input/output error
error: failed to connect to the hypervisor 

Notes:
Comments:

		229837 	[snapshot] - create external checkpoint snapshot will change the guest pmsuspended state and guest hang forever BZ#876829 885963 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

make sure 

bug 876829  is fixed 

Bug 885963 is not fixed yet
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

1.)
add these in the guest xml then restart the guest
...
<pm>
<suspend-to-mem enabled='yes'/>
<suspend-to-disk enabled='yes'/>
</pm>
...

<channel type='unix'>
<source mode='bind' path='/var/lib/libvirt/qemu/demo2.agent'/>
<target type='virtio' name='org.qemu.guest_agent.0'/>
<address type='virtio-serial' controller='0' bus='0' port='1'/>
</channel>
...

2). install the qemu-guest-agent in the guest then
#service qemu-ga start


3). in the host

 #s3 the guest
[root@intel-q9400-4-2 ~]# virsh dompmsuspend test --target mem
Domain test successfully suspended

#check guest state
[root@intel-q9400-4-2 ~]# virsh list
Id Name State
----------------------------------------------------
8 test pmsuspended

4) 
# do external checkpoint snapshot
[root@intel-q9400-4-2 ~]# virsh snapshot-create-as test ex-s4 --diskspec
vda --memspec /tmp/ex-s4
Domain snapshot ex-s4 created

#guest state is changed to running
[root@intel-q9400-4-2 ~]# virsh list
Id Name State
----------------------------------------------------
8 test running

and guest will hang forever with running state  if there is a bug 

For bug 885963:
1.start a guest
# virsh list
Id Name State
----------------------------------------------------
14 q64 running

2. try to create external system checkpointing but fail
# virsh snapshot-create-as q64 --memspec /tmp/q641
error: operation failed: Failed to take snapshot: unknown command:
'snapshot_blkdev'

[root@5-12-r6 rpms]# echo $?
1

3. guest mem still copy to a file
# ll /tmp/q641 --> not exist
This file should removed by libvirt

4. guest still running
# virsh list
Id Name State
----------------------------------------------------
14 q64 running

	
Expected Results:

step 4)

after create snapshot  the domain should not be hang !

 

For bug 885963 Step 3 the file should not exist
Notes:
pls check this case after bug 876829 and 885963 are fixed

whuang 20130109
Comments:

		229838 	[snapshot] - virsh snapshot-edit prints garbage with wrong parameters BZ#877303 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot

bug:

    No bug found

Actions:

1)  
there is not a domain named asdf
# virsh snapshot-edit asdf sadf
error: failed to get domain 'asdf'
error: Domain not found: no domain with matching name 'asdf'

2) 
there is a domain named redhat 
# virsh snapshot-edit redhat  sdf
error: Domain snapshot not found: no domain snapshot with matching name 'sdf'

	
Expected Results:

 error as the step :
Notes:
Comments:

		229841 	[Graphical framebuffers] add possibility to autoconfigure and set tls-port only BZ#875729 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

add this in a inactive domain then start domain

 

...

 <graphics type='spice' tlsPort="5901" autoport='yes'/>

....

 

after domain start  dumpxml it there is no tlsport

#virsh dumpxml domain

...

 <graphics type='spice' port='5900' autoport='yes' listen='127.0.0.1'>
      <listen type='address' address='127.0.0.1'/>
    </graphics>
...

#virsh dumpxml domain

 

there is a bug : autoport will be ignored by autoport 
	
Expected Results:

There is one existed bug 875729 now, after the bug is fixed,

"autoport='yes'" shouldn't be displayed in the dumpxml after the domain is started.

 
Notes:
Comments:

		229898 	[Virtio disks] Attach disks with wrong optional arguments - bug 872498 	weizhan 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Create a raw image file.

# qemu-img create -f raw /var/lib/libvirt/images/rawdisk.img 1M

2. Define and start a domain.
  
# virsh list
 Id    Name                           State
----------------------------------------------------
 2     guest                            running

3. Attach the disk to guest with wrong optional argument ,assigned the disk's subdriver "qcow2"
# virsh attach-disk guest /var/lib/libvirt/images/rawdisk.img vdb --driver qemu --subdriver qcow2 --type disk --sourcetype file --mode shareable 

4 create a qcow2 img file 
# qemu-img create -f qcow2 /var/lib/libvirt/images/qcow2disk.img 1M

5 Attach qcow2 disk with --subdriver raw, it does not report error
# virsh attach-disk sec /var/lib/libvirt/images/qcow2disk.img vdb --driver qemu --subdriver raw --type disk --sourcetype file --mode shareable 


	
Expected Results:

Step 3
error: Failed to attach disk
error: TBD, should report like subdriver type mismatch

Step 5

error: Failed to attach disk
error: TBD, should report like subdriver type mismatch

 

 

Notes:
need update because bug still assign and not sure the exact the error messages should be
Comments:

		229901 	[console and serial devices]hot plug/unplug chardev 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices

bug:

    No bug found

Actions:

====TBD=====

refer bug:

https://bugzilla.redhat.com/show_bug.cgi?id=878109

 

libvirt should able to hot plug/unplug chardev
	
Expected Results:
Notes:
Comments:

		229902 	[Graphical framebuffers] when tlsPort is set and port is omitted, port should not set to 0 -- bug: 875730 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

step:

1: config qemu.conf to support tls

  1.1 uncomment the following settings in qemu.conf
    spice_listen = "0.0.0.0"
    spice_tls = 1
    spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"
  1.2 erform the following script, to generate the cert files for ssl , and then copy *.pem file info /etc/pkil/libvirt-spice directory

#!/bin/bash

SERVER_KEY=server-key.pem

# creating a key for our ca
if [ ! -e ca-key.pem ]; then
    openssl genrsa -des3 -out ca-key.pem 1024
fi
# creating a ca
if [ ! -e ca-cert.pem ]; then
    openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem  -subj "/C=IL/L=Raanana/O=Red Hat/CN=my CA"
fi
# create server key
if [ ! -e $SERVER_KEY ]; then
    openssl genrsa -out $SERVER_KEY 1024
fi
# create a certificate signing request (csr)
if [ ! -e server-key.csr ]; then
    openssl req -new -key $SERVER_KEY -out server-key.csr -subj "/C=IL/L=Raanana/O=Red Hat/CN=my server"
fi
# signing our server certificate with this ca
if [ ! -e server-cert.pem ]; then
    openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem
fi

# now create a key that doesn't require a passphrase
openssl rsa -in $SERVER_KEY -out $SERVER_KEY.insecure
mv $SERVER_KEY $SERVER_KEY.secure
mv $SERVER_KEY.insecure $SERVER_KEY

# show the results (no other effect)
openssl rsa -noout -text -in $SERVER_KEY
openssl rsa -noout -text -in ca-key.pem
openssl req -noout -text -in server-key.csr
openssl x509 -noout -text -in server-cert.pem
openssl x509 -noout -text -in ca-cert.pem

# copy *.pem file to /etc/pki/libvirt-spice
if [[ -d "/etc/pki/libvirt-spice" ]]
then
    cp ./*.pem /etc/pki/libvirt-spice
else
    mkdir /etc/pki/libvirt-spice
        cp ./*.pem /etc/pki/libvirt-spice
fi

# echo --host-subject
echo "your --host-subject is" \" `openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "` \"

2:

echo '<?xml version="1.0"?>
<domain type="kvm">
  <name>tls-only</name>
  <memory>32768</memory>
  <os>
    <type arch="x86_64" machine="pc">hvm</type>
  </os>
  <devices>
    <graphics type="spice" tlsPort="5800" passwd="123" >
      <listen type="address" address="::" />
    </graphics>
    <video>
      <model type="qxl" vram="32768" heads="1"/>
    </video>
  </devices>
</domain>' | virsh domxml-to-native qemu-argv /dev/stdin

 
	
Expected Results:

after step 2:

below is wrong output, wait bug 875730 fixed.

LC_ALL=C PATH=/sbin:/usr/sbin:/bin:/usr/bin QEMU_AUDIO_DRV=spice /usr/libexec/qemu-kvm -name tls-only -S -M pc -enable-kvm -m 32 -smp 1,sockets=1,cores=1,threads=1 -uuid 74377671-f095-aa5b-031b-2c5037b81977 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/tls-only.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -no-acpi -device piix3-usb-uhci,id=usb -spice port=0,tls-port=5800,addr=::,x509-dir=/etc/pki/libvirt-spice,seamless-migration=on -vga qxl -global qxl-vga.vram_size=33554432 -device virtio-balloon-pci,id=balloon0
Notes:
Comments:

		229944 	[Virtual disks] open UNMAP and WRITE SAME commands to users without CAP_SYS_RAWIO - bug 878578 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Install lsscsi first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Virtual disks

bug:

    No bug found

Actions:

1. 
# modprobe scsi_debug dev_size_mb=1024 lbpws=1
# lsscsi |grep scsi_debug
[19:0:0:0]   disk    Linux    scsi_debug       0004  /dev/sdj

# cat /sys/block/sdj/device/scsi_disk/21\:0\:0\:0/provisioning_mode 
writesame_16

# cat /sys/bus/pseudo/drivers/scsi_debug/map 

2. Start guest with
...
    <disk type='block' device='lun'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/sdj'/>
      <target dev='sdj' bus='scsi'/>
    </disk>
...

3. Login guest and do
# dd if=/dev/zero of=/dev/sda bs=1M
dd: writing `/dev/sda': No space left on device
1025+0 records in
1024+0 records out
1073741824 bytes (1.1 GB) copied, 8.96604 s, 120 MB/s

In host, check 
# cat /sys/bus/pseudo/drivers/scsi_debug/map 
1-2097151

In guest, continue to do
# parted /dev/sda mklabel msdos
Information: You may need to update /etc/fstab.

# parted /dev/sda mkpart primary ext3 2048s 100%
Information: You may need to update /etc/fstab.

# mkfs.ext4 /dev/sda1

# mkdir /mnt/test
# mount /dev/sda1 /mnt/test/
# fstrim /mnt/test/

In host, check again
# cat /sys/bus/pseudo/drivers/scsi_debug/map 

4. Destroy guest, and remove module
# modprobe -r scsi_debug

5. # modprobe scsi_debug dev_size_mb=1024 lbpu=1 

lsscsi |grep scsi_debug
[20:0:0:0]   disk    Linux    scsi_debug       0004  /dev/sdj

# cat /sys/block/sdj/device/scsi_disk/20\:0\:0\:0/provisioning_mode 
unmap

# cat /sys/bus/pseudo/drivers/scsi_debug/map 

6. Do step2-3 again and check result



	
Expected Results:

Step 1

check

# cat /sys/bus/pseudo/drivers/scsi_debug/map

return none


Step 3

last check of

# cat /sys/bus/pseudo/drivers/scsi_debug/map

1-2600,2609-2616,2625-2672,2697-2704,2825-35600,264193-264208,526337-559104,788481-788496,1312769-1312784,1837057-1837072,2097145-2097151

 

Step 5

check

# cat /sys/bus/pseudo/drivers/scsi_debug/map

return none

 

Step 6

same result as in step 3
Notes:
needupdate because not sure the fixed method and how to change for domain xml or libvirt config file
Comments:

		232404 	[CPU Management] When CPUs in host aren't equal ,libvirt should report error-bug873926 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

There is one test machine which has 4 cpus,but cpu0 doesn't have the monitor flag,and the other cpus have it.
When start one guest with sandybridge cpu model .it will success when libvirtd' worker threads on cpu1-3 ,fail on cpu0.

You can get the machine from Hongming's seat.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    873926

Actions:

1.
# for i in {1..5};do virsh capabilities|grep monitor|wc -l;done
1
1
0
0
0

2.
# virsh capabilities  (There isn't the "monitor" feature ,libvirtd is running on cpu0, but host cpu 1-3 include it)
<capabilities>

<host>
<uuid>5bc55087-5efb-5107-6702-5133293b515f</uuid>
<cpu>
<arch>x86_64</arch>
<model>SandyBridge</model>
<vendor>Intel</vendor>
<topology sockets='1' cores='4' threads='1'/>
<feature name='osxsave'/>
<feature name='pdcm'/>
<feature name='xtpr'/>
<feature name='tm2'/>
<feature name='est'/>
<feature name='smx'/>
<feature name='vmx'/>
<feature name='ds_cpl'/>
<feature name='dtes64'/>
<feature name='pbe'/>
<feature name='tm'/>
<feature name='ht'/>
<feature name='ss'/>
<feature name='acpi'/>
<feature name='ds'/>
<feature name='vme'/>
</cpu>
......
<capabilities>

3.
# virsh start rhel6.3-new
error: Failed to start domain rhel6.3-new
error: unsupported configuration: guest and host CPU are not compatible:
Host CPU does not provide required features: monitor

4.
# virsh dumpxml rhel6.3-new  (There is the "monitor" feature)
<domain type='kvm'>
   ......
<cpu mode='custom' match='exact'>
<model fallback='allow'>SandyBridge</model>
<vendor>Intel</vendor>
<feature policy='require' name='tm2'/>
<feature policy='require' name='est'/>
<feature policy='require' name='monitor'/>
......
</cpu>
.....
</domain> 

	
Expected Results:
Notes:
Comments:

		232425 	[CPU Management] Check CPU affinity setting on big machine using virDomainGetVcpuPinInfo-bug876415 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Prepare one ibm x3850x5 box with 32 cpus from virtlab.

Bug 876415 This bug only could be reproduced on machine with a big number of cpus(32 in this case)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    No bug found

Actions:

# lscpu
Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                32
On-line CPU(s) list:   0-31
Thread(s) per core:    2
Core(s) per socket:    8
CPU socket(s):         2
NUMA node(s):          2
Vendor ID:             GenuineIntel
CPU family:            6
Model:                 47
Stepping:              2
CPU MHz:               1064.000
BogoMIPS:              3989.86
Virtualization:        VT-x
L1d cache:             32K
L1i cache:             32K
L2 cache:              256K
L3 cache:              18432K
NUMA node0 CPU(s):     0-7,16-23
NUMA node1 CPU(s):     8-15,24-31

# virsh list
 Id    Name                           State
----------------------------------------------------
 1     libvirt_test_api               running

# virsh vcpupin libvirt_test_api 0 2-8

# virsh vcpuinfo libvirt_test_api
VCPU:           0
CPU:            8
State:          running
CPU time:       8.6s
CPU Affinity:   --yyyyyyy-----------------------

VCPU:           1
CPU:            14
State:          running
CPU time:       2.0s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy

VCPU:           2
CPU:            4
State:          running
CPU time:       2.4s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy

VCPU:           3
CPU:            29
State:          running
CPU time:       2.2s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy

VCPU:           4
CPU:            1
State:          running
CPU time:       2.9s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy

VCPU:           5
CPU:            5
State:          running
CPU time:       2.2s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy

VCPU:           6
CPU:            15
State:          running
CPU time:       2.1s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy

VCPU:           7
CPU:            23
State:          running
CPU time:       1.9s
CPU Affinity:   yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy

# python
Python 2.6.6 (r266:84292, Aug 28 2012, 10:55:56) 
[GCC 4.4.6 20120305 (Red Hat 4.4.6-4)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> import libvirt >>> con = libvirt.open(None) >>> dom = con.lookupByName('libvirt_test_api') >>> dom.vcpus()
([(0, 1, 8570000000L, 8), (1, 1, 1990000000L, 14), (2, 1, 2370000000L, 4), (3, 1, 2160000000L, 29), (4, 1, 2960000000L, 8), (5, 1, 2240000000L, 5), (6, 1, 2150000000L, 15), (7, 1, 1940000000L, 23)], [(False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True)])
>>> dom.vcpuPinInfo(1)
[(False, False, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True), (True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True)]

	
Expected Results:
Notes:
Comments:

		233016 	[snapshot] it should give better error message when fail to create external system check point snapshot BZ#884926 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1. create a guest then PCI passthrough a NIC to the guest

 #virsh dumpxml w
...
<hostdev mode='subsystem' type='pci' managed='yes'>
<source>
<address domain='0x0000' bus='0x03' slot='0x10' function='0x4'/>
</source>
<alias name='hostdev0'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
</hostdev>
...


2. create external system check point snapshot

 [root@test ~]# virsh snapshot-create-as w external-checkpoint --memspec
/tmp/whuang
error: internal error unable to execute QEMU command 'migrate': An
undefined error has ocurred

[root@test ~]# tail /var/log/libvirt/libvirtd.log

 2012-12-07 02:42:19.908+0000: 16124: error :
qemuMonitorJSONCheckError:353 : internal error unable to execute QEMU
command 'migrate': An undefined error has ocurred

	
Expected Results:

step2 )

the error shoud  not be like :

error: internal error unable to execute QEMU command 'migrate': An undefined error has ocurred

 

need more clear error message

 
Notes:
Pls set this case to proposed after bug 884926 is verified
Comments:

		233017 	[snapshot] - virsh snapshot-delete should add a check point of comparing disk before delete the snapshot.xml BZ#880565 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

bug https://bugzilla.redhat.com/show_bug.cgi?id=880565  need be fixed 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1. Prepare guest with a qcow2 type disk, check disk info:
# qemu-img info /mnt/nfs/rhel62.qcow2
image: /mnt/nfs/rhel62.qcow2
file format: qcow2
virtual size: 8.0G (8589934592 bytes)
disk size: 3.2G
cluster_size: 65536

2. Create a basic snapshot when the guest is shut off, and check snapshot status:
# virsh snapshot-create-as qcow2 snap1
Domain snapshot snap1 created

# virsh snapshot-list qcow2
 Name                 Creation Time             State
------------------------------------------------------------
 snap1                2012-11-27 17:41:02 +0800 shutoff

# qemu-img info /mnt/nfs/rhel62.qcow2
image: /mnt/nfs/rhel62.qcow2
file format: qcow2
virtual size: 8.0G (8589934592 bytes)
disk size: 3.2G
cluster_size: 65536
Snapshot list:
ID        TAG                 VM SIZE                DATE       VM CLOCK
1         snap1                     0 2012-11-27 17:41:02   00:00:00.000

3. # virsh start qcow2
Domain qcow2 started

4. Create a disk snapshot for the guest:
# virsh snapshot-create-as qcow2 snap2 --disk-only
Domain snapshot snap2 created

5. # virsh snapshot-list qcow2
 Name                 Creation Time             State
------------------------------------------------------------
 snap1                2012-11-27 17:41:02 +0800 shutoff
 snap2                2012-11-27 17:43:12 +0800 disk-snapshot

6. # virsh dumpxml qcow2
....
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/mnt/nfs/rhel62.snap2'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
....

7. Keep guest running, and delete snap1:
# virsh snapshot-delete qcow2 snap1
Domain snapshot snap1 deleted

# virsh snapshot-list qcow2
 Name                 Creation Time             State
------------------------------------------------------------
 snap2                2012-11-27 17:43:12 +0800 disk-snapshot

8. But then check qemu-img info, snap1 still in the disk:
# qemu-img info /mnt/nfs/rhel62.qcow2
image: /mnt/nfs/rhel62.qcow2
file format: qcow2
virtual size: 8.0G (8589934592 bytes)
disk size: 3.2G
cluster_size: 65536
Snapshot list:
ID        TAG                 VM SIZE                DATE       VM CLOCK
1         snap1                     0 2012-11-27 17:41:02   00:00:00.000



	
Expected Results:

as steps :

in the step 8 :

there should not be  a internal snapshot with this image

 
Notes:
Comments:

		233021 	[snapshot] - when guest use 2 same disk, create/delete snapshot will get unexpect result BZ#879087 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

need https://bugzilla.redhat.com/show_bug.cgi?id=879087  fixed first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1.define a guest with 2 same disk.
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/virt/qcow.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/virt/qcow.img'/>
      <target dev='vdb' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
...

2. when guest is shutdown, take snapshot will create twice in image, but when guest is running, snapshot will create once.
# virsh snapshot-list qcow
 Name                 Creation Time             State
------------------------------------------------------------
 running1             2012-11-22 10:16:35 +0800 running
 shutdown1            2012-11-22 10:16:59 +0800 shutoff
 shutdown2            2012-11-22 10:17:33 +0800 shutoff
 shutdown3            2012-11-22 10:17:34 +0800 shutoff


# qemu-img snapshot -l /virt/qcow.img
Snapshot list:
ID        TAG                 VM SIZE                DATE       VM CLOCK
1         running1                  0 2012-11-22 10:16:35   00:00:06.434
2         shutdown1                 0 2012-11-22 10:17:00   00:00:00.000
3         shutdown1                 0 2012-11-22 10:17:00   00:00:00.000
4         shutdown2                 0 2012-11-22 10:17:33   00:00:00.000
5         shutdown2                 0 2012-11-22 10:17:33   00:00:00.000
6         shutdown3                 0 2012-11-22 10:17:34   00:00:00.000
7         shutdown3                 0 2012-11-22 10:17:35   00:00:00.000

3. when guest is shutdown,i.e. delete shutdown3 will remove all the shutdown3 in images
but when guest is running, delete shutdown2 will only remove 1 shutdown2 in images:

# virsh destroy qcow # virsh snapshot-delete qcow shutdown3 # qemu-img snapshot -l /virt/qcow.img
Snapshot list:
ID        TAG                 VM SIZE                DATE       VM CLOCK
1         running1                  0 2012-11-22 10:16:35   00:00:06.434
2         shutdown1                 0 2012-11-22 10:17:00   00:00:00.000
3         shutdown1                 0 2012-11-22 10:17:00   00:00:00.000
4         shutdown2                 0 2012-11-22 10:17:33   00:00:00.000
5         shutdown2                 0 2012-11-22 10:17:33   00:00:00.000

# virsh start qcow # virsh snapshot-delete qcow shutdown2 # qemu-img snapshot -l /virt/qcow.img
Snapshot list:
ID        TAG                 VM SIZE                DATE       VM CLOCK
1         running1                  0 2012-11-22 10:16:35   00:00:06.434
2         shutdown1                 0 2012-11-22 10:17:00   00:00:00.000
3         shutdown1                 0 2012-11-22 10:17:00   00:00:00.000
5         shutdown2                 0 2012-11-22 10:17:33   00:00:00.000

	
Expected Results:
Notes:
Comments:

		233028 	[performance] High disk usage when both libvirt and virt-manager are opened - bug 886933 	gsun 	gsun 	Manual 		Regression 	P3 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

1. set log level to 1
log_outputs="1:file:/tmp/libvirtd.log"
log_filters="1:libvirt"

then restart libvirtd

2. prepare two guests paused at grub
# virsh list
 Id    Name                           State
----------------------------------------------------
 17    T1                             paused
 18    libvirt_test_api               paused

3. prepare a script
# vim getinfo.sh
#!/bin/sh

while true; do
  count=$(grep virDomainGetInfo -n /tmp/libvirtd.log |wc -l)
  if [ "$count" == "2000" ]; then
    exit
  fi
done

# time ./getinfo.sh

real	16m38.126s
user	10m28.336s
sys	8m12.531s

2000 calls to virDomainGetInfo() of 2 guests take 16 *minutes* 35 secs.

4. calculate disk write traffic
# dstat -d 1 60

manual calculate shows write traffic is 6k (relatively low, not bigger than 300k) per second.

 
	
Expected Results:
Notes:
Comments:

		233030 	[Virtual Networks] libvirt-launched dnsmasq listens on localhost when it shouldn't - Bug 886821 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Start a libvirt network(such as 'default')

# virsh net-list --all
Name                 State      Autostart     Persistent
--------------------------------------------------
default              active     yes           yes

2. Check the dnsmasq process which the network started

# ps -ef |grep dnsmasq
nobody    7100     1  0 14:23 ?        00:00:00 /usr/sbin/dnsmasq --strict-order --local=// --domain-needed --pid-file=/var/run/libvirt/network/default.pid --conf-file= --bind-dynamic --interface virbr0 --dhcp-range 192.168.122.2,192.168.122.254 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=253 --dhcp-no-override --dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile --addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts
root      7135  5306  0 14:24 pts/0    00:00:00 grep dnsmasq

 

3. Define a start another libvirt network

# virsh net-list
Name                 State      Autostart     Persistent
--------------------------------------------------
default              active     yes           yes
net1                 active     no            yes

4. Check the dnsmasq processes again
	
Expected Results:

2.  After bug 24953 fix, the dnsmasq process should not listen on localhost any more.

# lsof -p 7100 -n -P
COMMAND  PID   USER   FD   TYPE             DEVICE SIZE/OFF    NODE NAME
dnsmasq 7100 nobody  cwd    DIR                8,1     4096       2 /
dnsmasq 7100 nobody  rtd    DIR                8,1     4096       2 /
dnsmasq 7100 nobody  txt    REG                8,1   185920  657580 /usr/sbin/dnsmasq
dnsmasq 7100 nobody  mem    REG                8,1   156872  931139 /lib64/ld-2.12.so
dnsmasq 7100 nobody  mem    REG                8,1  1922152  931141 /lib64/libc-2.12.so
dnsmasq 7100 nobody  mem    REG                8,1   145720  931147 /lib64/libpthread-2.12.so
dnsmasq 7100 nobody  mem    REG                8,1    47064  920631 /lib64/librt-2.12.so
dnsmasq 7100 nobody  mem    REG                8,1   268232  931159 /lib64/libdbus-1.so.3.4.0
dnsmasq 7100 nobody  mem    REG                8,1    65928  920434 /lib64/libnss_files-2.12.so
dnsmasq 7100 nobody    0u   CHR                1,3      0t0    3698 /dev/null
dnsmasq 7100 nobody    1u   CHR                1,3      0t0    3698 /dev/null
dnsmasq 7100 nobody    2u   CHR                1,3      0t0    3698 /dev/null
dnsmasq 7100 nobody    3u  sock                0,6      0t0  146618 can't identify protocol
dnsmasq 7100 nobody    4u   REG                8,1        0 1187408 /var/lib/libvirt/dnsmasq/default.leases
dnsmasq 7100 nobody    5u  IPv4             146619      0t0     UDP *:67 
dnsmasq 7100 nobody 6u IPv6 146626 0t0 TCP [::1]:53 (LISTEN)
dnsmasq 7100 nobody    7u  IPv6             146627      0t0     UDP [::1]:53 
dnsmasq 7100 nobody    8u  IPv4             146628      0t0     TCP 192.168.122.1:53 (LISTEN)
dnsmasq 7100 nobody    9u  IPv4             146629      0t0     UDP 192.168.122.1:53 
dnsmasq 7100 nobody 10u IPv4 146630 0t0 TCP 127.0.0.1:53 (LISTEN)
dnsmasq 7100 nobody   11u  IPv4             146631      0t0     UDP 127.0.0.1:53 
dnsmasq 7100 nobody   12r  FIFO                0,8      0t0  146636 pipe
dnsmasq 7100 nobody   13w  FIFO                0,8      0t0  146636 pipe
dnsmasq 7100 nobody   14u  unix 0xffff880121f293c0      0t0  146638 socket

3. The second virtual network can start well, both dnsmasq processed should start and no listen on local

    And , restart libvirtd should no such error message:

# tail -f /var/log/libvirt/libvirtd.logï»¿

2012-12-17 06:40:30.231+0000: 7650: error : virCommandWait:2345 : internal error Child process (/usr/sbin/dnsmasq --strict-order --local=// --domain-needed --pid-file=/var/run/libvirt/network/net1.pid --conf-file= --bind-dynamic --interface virbr1 --dhcp-range 192.168.123.2,192.168.123.254 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/net1.leases --dhcp-lease-max=253 --dhcp-no-override --dhcp-hostsfile=/var/lib/libvirt/dnsmasq/net1.hostsfile --addn-hosts=/var/lib/libvirt/dnsmasq/net1.addnhosts) unexpected exit status 2: 

dnsmasq: failed to bind listening socket for ::1: Address already in use

Notes:
Comments:

		233034 	[Virtual Networks] virDomainUpdateDeviceFlags fails when interface type is 'network' - bug 881480&885838 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1.Run the following shell script to test  virDomainUpdateDeviceFlags function

#cat libvirt_updateDevice.sh 
=====================================================================

 #!/bin/bash

IMG="/var/lib/libvirt/images/foobar.img"
brctl addbr foo
brctl addbr bar

cat > foo.xml << EOF
<network>
        <name>foo</name>
        <forward mode="bridge"/>
        <bridge name="foo"/>
      </network>
EOF
virsh net-create foo.xml
cat > bar.xml << EOF
<network>
        <name>bar</name>
        <forward mode="bridge"/>
        <bridge name="bar"/>
      </network>
EOF
virsh net-create bar.xml

if [ ! -f "$IMG" ];
then
dd if=/dev/zero bs=1M count=512 of="$IMG"
fi
chown qemu:qemu "$IMG"

cat > vm.xml << EOF
<domain type='kvm' id='17'>
  <name>foobar</name>
  <uuid>d27b0bd4-9479-fd90-14e9-1097fa0cf116</uuid>
  <memory unit='KiB'>2097152</memory>
  <currentMemory unit='KiB'>2097152</currentMemory>
  <vcpu placement='static'>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
    <bootmenu enable='no'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
   <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/foobar.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <controller type='usb' index='0'>
      <alias name='usb0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x2'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:a2:b4:50'/>
      <source network='foo'/>
      <target dev='vnet4'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/3'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/3'>
      <source path='/dev/pts/3'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='tablet' bus='usb'>
      <alias name='input0'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5901' autoport='yes' listen='127.0.0.1'>
      <listen type='address' address='127.0.0.1'/>
    </graphics>
    <video>
      <model type='qxl' vram='65536' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux' relabel='yes'>
    <label>system_u:system_r:svirt_t:s0:c673,c892</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c673,c892</imagelabel>
  </seclabel>
</domain>
EOF
virsh create vm.xml
echo "This should show vnetX in foo bridge"
brctl show
python - << EOF
import libvirt
# change to vdsm.libvirtconn.get if sasl is set for your libvirt
conn = libvirt.open(None)
dom = conn.lookupByName('foobar')
print dom.updateDeviceFlags('''<interface type='network'><source network='bar'/><mac address='52:54:00:a2:b4:50'/><model type='virtio'/><link state='down'/></interface>''', libvirt.VIR_DOMAIN_AFFECT_LIVE)
EOF
echo "This should show vnetX in bar bridge"
brctl show

echo "Cleaning up"
virsh destroy foobar
virsh net-destroy foo
virsh net-destroy bar

brctl delbr foo
brctl delbr bar

#=====================================================

	
Expected Results:

No error.
Notes:
Comments:

		233035 	[Virtual Networks] Guest can use inactive macvtap-passthrough network - bug 880483 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. On a SRIOV machine, create several VFs.

2. Define a network 
#cat vf-passthrough.xml
<network>
  <name>vf-passthrough</name>
  <forward mode='passthrough'>
    <interface dev='eth3'/>
    <interface dev='eth5'/>
   </forward>
</network>


# virsh net-define vf-passthrough.xml
Network vf-passthrough defined from vf-passthrough.xml

# virsh net-list --all
Name                 State      Autostart     Persistent
--------------------------------------------------
default              active     yes           yes
vf-passthrough       inactive   no            yes

# virsh net-dumpxml vf-passthrough
<network>
  <name>vf-passthrough</name>
  <uuid>841442fe-4974-30f4-655b-c879864e8f9d</uuid>
  <forward dev='eth3' mode='passthrough'>
    <interface dev='eth3'/>
    <interface dev='eth5'/>
  </forward>
</network>

3. Define a guest which using the network
----
    <interface type='network'>
      <mac address='52:54:00:0b:ab:52'/>
      <source network='vf-passthrough'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </interface>
---

4. Start the guest and check the network
# virsh start mig
Domain mig started
# virsh net-list --all
Name                 State      Autostart     Persistent
--------------------------------------------------
default              active     yes           yes
vf-passthrough       inactive   no            yes

# virsh net-dumpxml vf-passthrough
<network connections='1'>
  <name>vf-passthrough</name>
  <uuid>841442fe-4974-30f4-655b-c879864e8f9d</uuid>
  <forward dev='eth3' mode='passthrough'>
    <interface dev='eth3' connections='1'/>
    <interface dev='eth5'/>
  </forward>
</network>

	
Expected Results:

After bug 880483 fix, guest can not use the inactive network.
Notes:
https://bugzilla.redhat.com/show_bug.cgi?id=880483
bug is not fixed so set it's NEEDUPDATE
Comments:

		233195 	[storage]Detection of QED file format in libvirt -- Bug : 886467 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

1.Use virsh vol-create-as to create volume with qed format.
# virsh vol-create-as test qed2.img 2G --format=qed --allocation=1G
Vol qed2.img created

# virsh vol-list test --details
Name      Path                                   Type    Capacity  Allocation
-----------------------------------------------------------------------------
qed2.img  /var/lib/libvirt/images/test/qed2.img  file    2.00 GiB  260.00 KiB

# qemu-img info /var/lib/libvirt/images/test/qed2.img
image: /var/lib/libvirt/images/test/qed2.img
file format: qed
virtual size: 2.0G (2147483648 bytes)
disk size: 260K
cluster_size: 65536

# virsh pool-refresh test
Pool test refreshed

# virsh vol-list test --details
Name      Path                                   Type    Capacity  Allocation
-----------------------------------------------------------------------------
qed2.img  /var/lib/libvirt/images/test/qed2.img  file  320.00 KiB  260.00 KiB

need_update : refer bug:https://bugzilla.redhat.com/show_bug.cgi?id=886467

	
Expected Results:
Notes:
Comments:

		233196 	[storage]Libvirt should not keep waiting if cannot start an nfs pool.--Bug : 881574 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

1. Have an NFS server.
2. Create an NFS pool xml file, like following.
<pool type='netfs'>
  <name>NFS</name>
  <uuid>fa6a9eb4-c76a-e7cb-99ef-4b5a876cfa41</uuid>
  <capacity unit='bytes'>69082284032</capacity>
  <allocation unit='bytes'>24137170944</allocation>
  <available unit='bytes'>44945113088</available>
  <source>
    <host name='10.66.5.87'/>
    <dir path='/NFS'/>
    <format type='auto'/>
  </source>
  <target>
    <path>/var/lib/libvirt/images/NFS</path>
    <permissions>
      <mode>0755</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>
3. virsh pool-define $xmlfile
4. ln -s /etc/libvirt/storage/autostart/NFS.xml /etc/libvirt/storage/NFS.xml
5. Stop the NFS server.
6. Service libvirtd restart
7. virsh -c qemu:///system

need update ,refer bug:
https://bugzilla.redhat.com/show_bug.cgi?id=881574

	
Expected Results:
Notes:
Comments:

		233198 	[storage] use vol-clone to clone LVM volumes -- Bug: 879780 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

 1: create a lvm pool
 # virsh pool-dumpxml lvm
<pool type='logical'>
  <name>lvm</name>
  <uuid>a2dc4aad-ff27-41d5-f598-612dbfe91620</uuid>
  <capacity unit='bytes'>20967325696</capacity>
  <allocation unit='bytes'>11811160064</allocation>
  <available unit='bytes'>9156165632</available>
  <source>
    <device path='/dev/sda2'/>
    <name>vg_virt</name>
    <format type='lvm2'/>
  </source>
  <target>
    <path>/dev/vg_virt</path>
    <permissions>
      <mode>0755</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>
  2: clone lvm vol
  # virsh vol-clone --pool lvm vgtest1 vgtest3
Vol vgtest3 cloned from vgtest1
  
  # virsh vol-list lvm

	
Expected Results:

after step 2 verify

Name                 Path                                    
-----------------------------------------             
vgtest1              /dev/vg_virt/vgtest1                    
vgtest2              /dev/vg_virt/vgtest2                    
vgtest3              /dev/vg_virt/vgtest3 

 vgtest3 created w/o error
Notes:
Comments:

		233199 	[storage]NULL pointer usage when starting guest with broken image chain -- Bug:878862 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

step:
 1: create image with backing file
 #qemu-img create -f qcow2 base.img 100M
 #qemu-img create -f qcow2 -b base.img leaf.img 
  check leaf.img
 #qemu-img info leaf.img
image: leaf.img
file format: qcow2
virtual size: 256K (262144 bytes)
disk size: 136K
cluster_size: 65536
backing file: base.img

  2: remove base.img

  3: add this img to a guest
......
 <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/leaf.img'/>
      <target dev='vdb' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
.....
  4: start guest

	
Expected Results:

sfter step 4 verify:
when start guest ,will get error:
error: Failed to start domain rhel6.4
error: internal error Process exited while reading console log output: char device redirected to /dev/pts/2
qemu-kvm: -drive file=/var/lib/libvirt/images/leaf.img,if=none,id=drive-virtio-disk1,format=qcow2,cache=none: could not open disk image /var/lib/libvirt/images/leaf.img: No such file or directory

check libvirt log:
2012-11-30 20:11:07.309+0000: 960: error : absolutePathFromBaseFile:560 : Can't canonicalize path 'base.img': No such file or directory
2012-11-30 20:11:07.309+0000: 960: warning : virStorageFileGetMetadataFromBuf:736 : Backing file 'base.img' of image '/var/lib/libvirt/images/leaf.img' is missing.
2012-11-30 20:11:07.606+0000: 960: error : qemuProcessReadLogOutput:1401 : internal error Process exited while reading console log output: char device redirected to /dev/pts/2
qemu-kvm: -drive file=/var/lib/libvirt/images/leaf.img,if=none,id=drive-virtio-disk1,format=qcow2,cache=none: could not open disk image /var/lib/libvirt/images/leaf.img: No such file or directory

Notes:
Comments:

		233575 	[Virtual Networks] libvirt needs to use new dnsmasq option to avoid open DNS proxy and listen on localhost- bug 882265 	ydu 	ydu 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

0. Start the libvirt default network
# virsh net-list
Name                 State      Autostart     Persistent
--------------------------------------------------
default              active     no            yes


# virsh net-dumpxml default
<network>
  <name>default</name>
  <uuid>9c60a9d2-d0ac-492c-a58b-3b8375ab62af</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <mac address='52:54:00:21:2E:F7'/>
  <ip address='192.168.122.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.122.2' end='192.168.122.254' />
    </dhcp>
  </ip>
</network>


check dnsmasq do not contain the bind-dynamic and has except-interface lo 

2)
#ps -ef |grep dnsmasq 
 /usr/sbin/dnsmasq --strict-order --local=// --domain-needed --pid-file=/var/run/libvirt/network/default.pid --conf-file= --except-interface lo --bind-interfaces --listen-address 192.168.122.1 --dhcp-range 192.168.122.2,192.168.122.254 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=253 --dhcp-no-override --dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile --addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts

 

3. check if dnsmasq listens on localhost

# lsof -p 8382 -n -P
COMMAND  PID   USER   FD   TYPE             DEVICE SIZE/OFF    NODE NAME
dnsmasq 8382 nobody  cwd    DIR                8,1     4096       2 /
dnsmasq 8382 nobody  rtd    DIR                8,1     4096       2 /
dnsmasq 8382 nobody  txt    REG                8,1   180176  667765 /usr/sbin/dnsmasq
dnsmasq 8382 nobody  mem    REG                8,1   156872  931139 /lib64/ld-2.12.so
dnsmasq 8382 nobody  mem    REG                8,1  1922152  931141 /lib64/libc-2.12.so
dnsmasq 8382 nobody  mem    REG                8,1   145720  931147 /lib64/libpthread-2.12.so
dnsmasq 8382 nobody  mem    REG                8,1    47064  920631 /lib64/librt-2.12.so
dnsmasq 8382 nobody  mem    REG                8,1   268232  931159 /lib64/libdbus-1.so.3.4.0
dnsmasq 8382 nobody  mem    REG                8,1    65928  920434 /lib64/libnss_files-2.12.so
dnsmasq 8382 nobody    0u   CHR                1,3      0t0    3698 /dev/null
dnsmasq 8382 nobody    1u   CHR                1,3      0t0    3698 /dev/null
dnsmasq 8382 nobody    2u   CHR                1,3      0t0    3698 /dev/null
dnsmasq 8382 nobody    3u  sock                0,6      0t0  134665 can't identify protocol
dnsmasq 8382 nobody    4u   REG                8,1        0 1187408 /var/lib/libvirt/dnsmasq/default.leases
dnsmasq 8382 nobody    5u  IPv4             134666      0t0     UDP *:67 
dnsmasq 8382 nobody    6u  IPv4             134675      0t0     TCP 192.168.122.1:53 (LISTEN)
dnsmasq 8382 nobody    7u  IPv4             134676      0t0     UDP 192.168.122.1:53 
dnsmasq 8382 nobody    8r  FIFO                0,8      0t0  134681 pipe
dnsmasq 8382 nobody    9w  FIFO                0,8      0t0  134681 pipe
dnsmasq 8382 nobody   10u  unix 0xffff8801203aa0c0      0t0  134683 socket

It does not listen "127.0.0.1:53" and "[::1]:53 ".

	
Expected Results:



2)
check dnsmasq do not contain the bind-dynamic and has except-interface lo 

3)
It does not listen "127.0.0.1:53" and "[::1]:53 ".

Notes:
Comments:

		233579 	[Virtual Networks] libvirt RPM should depend on dbus (but not avahi-daemond) - bug 830201 	ydu 	ydu 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

 

1.Install libvirt on a host without dbus installed.

# rpm -ivh libvirt-0.10.2-11.el6.x86_64.rpm libvirt-client-0.10.2-11.el6.x86_64.rpm libvirt-devel-0.10.2-11.el6.x86_64.rpm libvirt-python-0.10.2-11.el6.x86_64.rpm
error: Failed dependencies:
	dbus is needed by libvirt-0.10.2-11.el6.x86_64
	polkit >= 0.93 is needed by libvirt-0.10.2-11.el6.x86_64
	pm-utils is needed by libvirt-client-0.10.2-11.el6.x86_64

# yum install dbus polkit pm-utils

# rpm -ivh libvirt-0.10.2-11.el6.x86_64.rpm libvirt-client-0.10.2-11.el6.x86_64.rpm libvirt-devel-0.10.2-11.el6.x86_64.rpm libvirt-python-0.10.2-11.el6.x86_64.rpm
Preparing...                ########################################### [100%]
   1:libvirt-client         ########################################### [ 25%]
   2:libvirt                ########################################### [ 50%]
   3:libvirt-devel          ########################################### [ 75%]
   4:libvirt-python         ########################################### [100%]

# service libvirtd start
Starting libvirtd daemon:                                  [  OK  ]

 

2. Check the libvirt dependency

# rpm -qR libvirt |grep dbus

dbus  

 

	
Expected Results:
Notes:
Comments:

		233720 	[Log and debugging] Libvirt won't parse dnsmasq capabilities when debug logs are enabled - bug 885727 	bili 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Log and debugging

bug:

    No bug found

Actions:

1. enable debug logs (log_level=1)
2. restart libvirtd
3. virsh net-destroy default && virsh net-start default
4. ps axf | grep dnsmasq

	
Expected Results:

Should get like:
 7478 ?        S      0:00 /usr/sbin/dnsmasq --strict-order --local=// --domain-needed --pid-file=/var/run/libvirt/network/default.pid --conf-file= --bind-dynamic --interface virbr0 --dhcp-range 192.168.122.2,192.168.122.253 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=252 --dhcp-no-override --dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile --addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts

Should not like:

32677 ?        S      0:00 /usr/sbin/dnsmasq --strict-order --local=// --domain-needed --pid-file=/var/run/libvirt/network/default.pid --conf-file= --bind-interfaces --except-interface lo --listen-address 192.168.122.1 --dhcp-range 192.168.122.2,192.168.122.253 --dhcp-leasefile=/var/lib/libvirt/dnsmasq/default.leases --dhcp-lease-max=252 --dhcp-no-override --dhcp-hostsfile=/var/lib/libvirt/dnsmasq/default.hostsfile --addn-hosts=/var/lib/libvirt/dnsmasq/default.addnhosts

 

Notes:
Comments:

		233723 	[Libvirt domain event handler]Track memory balloon event-bug884650 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt domain event handler

bug:

    884650

Actions:

Prepare one guest with memory balloon device

1.# virsh start rhel6
Domain rhel6 started

# virsh dumpxml rhel6
<domain type='kvm' id='12'>
  <name>rhel6</name>
   ......
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
  ......

  <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
 </domain>

 

2.#  virsh setmem --live rhel6 800M

 

3. In another terminal .check  python-event info

# python /usr/share/doc/libvirt-python-0.10.2/events-python/event-test.py
Using uri:qemu:///system
myDomainEventBalloonChangeCallback: Domain rhel6(12) 1047552
myDomainEventBalloonChangeCallback: Domain rhel6(12) 819200

4.# ps -ef|grep rhel6
qemu     20090     1  1 16:23 ?        00:00:21 /usr/libexec/qemu-kvm -name rhel6 -S -M rhel6.3.0 -enable-kvm -m 1024 -smp 4,sockets=2,cores=1,threads=2 -uuid 1db7cfab-267e-e959-becf-b

5.# kill -19 20090

6.Dump domain's  xml  . it can return result immediately.

# virsh dumpxml rhel6
<domain type='kvm' id='12'>
  <name>rhel6</name>
  <uuid>1db7cfab-267e-e959-becf-ba85453c72e3</uuid>
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>819200</currentMemory>
 ......
</domain>

	
Expected Results:
Notes:
Comments:

		233936 	[Virtual disks] change-media on virtio scsi cdrom - bug 886456 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

# setenforce 1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

1.create two iso via mkiso
# mkisofs -o /var/lib/libvirt/images/bb.iso /tmp

# ll -Z /var/lib/libvirt/images/bb.iso
-rw-r--r--. qemu qemu system_u:object_r:virt_content_t:s0
/var/lib/libvirt/images/bb.iso


2.setup a domain with empty scsi cdrom
...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='sdc' bus='scsi' tray='open'/>
      <readonly/>
      <alias name='scsi0-0-1-0'/>
      <address type='drive' controller='0' bus='0' target='1' unit='0'/>
    </disk>
    <controller type='scsi' index='0' model='virtio-scsi'>
      <alias name='scsi0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </controller>
...

3. insert cdrom
# virsh change-media libvirt_test_api sdc /var/lib/libvirt/images/bb.iso
--insert 

 # virsh change-media libvirt_test_api sdc /var/lib/libvirt/images/bb.iso
--insert --force 

4. edit domain xml
# mkisofs -o /tmp/aaa.iso /tmp

# ll -Z /tmp/aaa.iso
-rw-r--r--. root root unconfined_u:object_r:user_tmp_t:s0 /tmp/aaa.iso

# virsh edit libvirt_test_api
...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/tmp/aaa.iso'/>
      <target dev='sdc' bus='scsi'/>
      <readonly/>
      <address type='drive' controller='0' bus='0' target='1' unit='0'/>
    </disk>
...

# virsh start libvirt_test_api
Domain libvirt_test_api started

# ll -Z /tmp/aaa.iso
-rw-r--r--. qemu qemu system_u:object_r:virt_content_t:s0 /tmp/aaa.iso


5. change-media with update

# virsh change-media libvirt_test_api sdc /var/lib/libvirt/images/bb.iso
--update --force 

6. eject
# virsh change-media libvirt_test_api sdc --eject
succeeded to complete action eject on media

 

 

	
Expected Results:

Step 3

should succeed without error

Step 5

should succeed without error

Step 6

should succeed without error

 
Notes:
bug still not fixed
Comments:

		233937 	[Remote access] connecting with the libssh2 connection driver - bug 881663 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1: machine rhel6-host has "ChallengeResponseAuthentication no" in /etc/ssh/sshd_config  (should be remote host)

2: virsh -c qemu+libssh2://root@rhel6-host/system 

	
Expected Results:

Step 2:

$ virsh -c qemu+libssh2://root@localhost/system
Accept SSH host key with hash '...' for host 'localhost:22' (y/n)?: y
Password: 
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #

Notes:
bug still not fixed
Comments:

		234077 	[Snapshot] The guest will be destroyed when Fail to create snapshot for running guest BZ 885937 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Bug 885937 - The guest will be destroyed when Fail to create snapshot for running guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snaphost
    Regression

bug:

    No bug found

Actions:

For Bug 885937

80% reproduceable

1. Mount an NFS to local directory
# mount nfs-sever-ip:/vol/S3/libvirtmanual /mnt -o vers=3

2. Create a guest which the image file testkkk.img is on the NFS storage and format is qcow2.
# qemu-img info /mnt/testkkk.img
image: /mnt/testkkk.img
file format: qcow2
virtual size: 7.8G (8388608000 bytes)
disk size: 4.0G
cluster_size: 65536

3. create snapshot for the guest while the guest start completely
# virsh list
Id Name State
----------------------------------------------------
9 testkkk running

# virsh snapshot-create-as testkkk snap2
error: Unable to read from monitor: Connection reset by peer
: Connection reset by peer
# virsh list
Id Name State
----------------------------------------------------
9 testkkk running           ---> guest should running.

 libvirtd.log NOT show this:

2012-12-11 03:48:41.754+0000: 24049: error : qemuMonitorIORead:513 : Unable to read from monitor: Connection reset by peer 2012-12-11 03:48:41.754+0000: 24049: debug : qemuMonitorIO:646 : Error on monitor Unable to read from monitor: Connection reset by peer
	
Expected Results:

as subject
Notes:
Comments:

		234078 	[migration]RFE: add optional [--clienturi] [<clienturi>] option to the migrate command BZ 883936 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Bug 883936 - RFE: add optional [--clienturi] [<clienturi>] option to the migrate command NEW FEATURE keep needupdate ---> zhpeng
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    migration

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		234079 	[migration] [FEAT RHEL6.5]: Improve the manual about "migrateuri" option of "virsh" command. BZ 878765 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
 

Bug 878765 - [FEAT RHEL6.5]: Improve the manual about "migrateuri" option of "virsh" command. NEW FEATURE keep needupdate --> zhpeng
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		236323 	[Remote access] virt-viewer to connect guest with parameter "--attach" works well BZ#891837 	xuzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access

bug:

    No bug found

Actions:

1.Use virt-viewer --attach to connect a guest.
# virt-viewer --attach -c qemu+ssh://root@localhost/system test
connect : Connection refused
connect : Connection refused

(virt-viewer:3179): atk-bridge-WARNING **: AT_SPI_REGISTRY was not started at session startup.

(virt-viewer:3179): atk-bridge-WARNING **: IOR not set.

(virt-viewer:3179): atk-bridge-WARNING **: Could not locate registry
root@localhost's password: 
root@localhost's password: 

2.Close virt-viewer,check the status of libvirtd.
# service libvirtd status

3.Check libvirtd.log,

	
Expected Results:

Since there is one existing bug 891837, so the actual resut is the following one now:

2. libvirtd dead but pid file exists

3. there is error:

2013-01-04 05:58:08.213+0000: 6598: error : virNetSocketRecvFD:1411 : Failed to recv file descriptor: Permission denied
Caught Segmentation violation dumping internal log buffer:

 

After this bug fixed, the following result is expected:

libvirtd is still running in step2, and there is no error in step3.

Notes:
Comments:

		236422 	[snapshot] snapshot --redefine disk snapshot may cause libvirtd crash BZ#889407 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1:prepare a shutdown guest

 2:create snapshot with:
# virsh snapshot-create rhel6.3 rhel-sn.xml --reuse-external --quiesce --disk-only
# cat rhel-sn.xml
<domainsnapshot>
  <name>snapshot-rhel6.3</name>
    <state>shutoff</state>
    <creationTime>1356059598</creationTime>
    <memory snapshot='no'/>
       <disks>
        <disk name='hda' snapshot='external'>
        <driver type='qcow2'/>
        <source file='/var/lib/libvirt/images/rhel6.3.img'/>
        </disk>
       </disks>
</domainsnapshot>

3. redefine snapshot
# virsh snapshot-create rhel6.3 rhel-sn.xml --redefine
Domain snapshot snapshot-rhel6.3 created from 'rhel-sn.xml'

	
Expected Results:

step 3 :

libvirtd not crash

Notes:
Comments:

		236423 	- EMBARGOED CVE-2013-0170 libvirt: use-after-free in virNetMessageFree() BZ#891837 	whuang 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

start a guest 
1.Use virt-viewer --attach to connect a guest.
# virt-viewer --attach -c qemu+ssh://root@localhost/system test

root@localhost's password: 
root@localhost's password: 

2.Close virt-viewer,check the status of libvirtd.
# service libvirtd status


	
Expected Results:

1. get connect with domain

 

2, libvirtd is still running
Notes:
Comments:

		236470 	[SR-IOV] Attach pci device with interface section - bug 889319 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Do  124766: [SR-IOV] Prepare: Enable VT-D first

2. For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

modprobe -r kvm_intel

modprobe -r kvm

modprobe kvm allow_unsafe_assigned_interrupts=1

modprobe kvm_intel

3. Generate VFs

# modprobe -r igb

# modprobe igb max_vfs=2

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. # lspci |grep Ethernet
00:19.0 Ethernet controller: Intel Corporation 82579LM Gigabit Network Connection (rev 05)
01:00.0 Ethernet controller: Intel Corporation 82574L Gigabit Network Connection
09:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
09:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
0a:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
0a:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
0a:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
0a:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
.......

Add the following xml to guest
    <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:ab:8d:40'/>
      <source>
        <address domain='0x0000' bus='0x0a' slot='0x10' function='0x0'/>
      </source>
    </interface>

2. # virsh start rhel63 

3. # virsh dumpxml rhel63 

4. Login guest , Check the Virtual Function using lspci

5. # virsh detach-interface rhel63 --type hostdev --mac 52:54:00:60:6e:2d

6. # virsh dumpxml rhel63

7. Login guest  and check

	
Expected Results:

Step 2

Domain rhel63 started

Step 3

<domain type='kvm' id='4'>
  <name>rhel63</name>
  ......
  <devices>
  ......
    <interface type='network'>
      <mac address='52:54:00:d7:f0:f2'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <model type='rtl8139'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:60:6e:2d'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x0a' slot='0x10' function='0x0'/>
      </source>
      <alias name='hostdev0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>
 ......
  </devices>
 ......
</domain>

 Step 4

    Can find extra network interface and it works well

Step 5

    Interface detached successfully

Step 6

    Can not find

 <interface type='hostdev' managed='yes'>
      <mac address='52:54:00:60:6e:2d'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x0a' slot='0x10' function='0x0'/>
      </source>
      <alias name='hostdev0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>

 Step 7

     Have no extra network interface int it.
Notes:
Comments:

		236474 	[SR-IOV] Attach pci device with interface section without up PF - bug 893738 	weizhan 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

1. Do  124766: [SR-IOV] Prepare: Enable VT-D first

2. For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

modprobe -r kvm_intel

modprobe -r kvm

modprobe kvm allow_unsafe_assigned_interrupts=1

modprobe kvm_intel

3. If down all the PF

4. Generate VFs

# modprobe -r igb

# modprobe igb max_vfs=7
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

# ip link

4: eth4: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 90:e2:ba:02:22:00 brd ff:ff:ff:ff:ff:ff
    vf 0 MAC 6e:2e:e4:9f:46:6e
    vf 1 MAC 46:13:65:5f:23:2b
    vf 2 MAC 9a:ce:da:20:98:63
    vf 3 MAC f6:fb:38:47:da:64
    vf 4 MAC 0e:b8:94:02:78:d2
    vf 5 MAC ba:a0:19:7a:cf:0a
    vf 6 MAC 76:c4:20:37:6b:e1
5: eth3: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 90:e2:ba:02:22:01 brd ff:ff:ff:ff:ff:ff
    vf 0 MAC 0e:3e:9d:bb:b5:59
    vf 1 MAC ba:63:5f:a9:59:b1
    vf 2 MAC 72:19:ea:9d:1a:42
    vf 3 MAC 02:d2:7f:d3:fb:0e
    vf 4 MAC 2e:49:d4:6b:98:a9
    vf 5 MAC 86:cc:fe:a8:f1:7c
    vf 6 MAC 9a:1a:66:1c:d2:89

Then put the following in /tmp/hostdev.xml:

<interface type='hostdev' managed='yes'>
      <mac address='52:54:00:3b:3e:02'/>
      <source>
        <address type='pci' domain='0x0000' bus='0x03' slot='0x10' function='0x1'/>
      </source>
      <model type='virtio'/>
      <alias name='hostdev0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
</interface>

2. Attach device

# virsh attach-device RHEL6 /tmp/hostdevl.xml

 

 
	
Expected Results:

libvirt should ifup the PF
Notes:
bug still assign
Comments:

		236561 	[Guest resource control] Cgroups memory limit are causing the virt to be terminated unexpectedly - bug 891653 	bili 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

 1. # virsh dumpxml guest

...
<memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>1048576</currentMemory>
...
      <model type='cirrus' vram='9216' heads='1'/>
...

2. # virsh start guest

3. # cat /cgroup/memory/libvirt/qemu/guest/memory.limit_in_bytes 
1868038144

 
	
Expected Results:

step 3: should not get value:

1314557952
Notes:
Comments:

		236843 	[storage]create/start a 'fs' pool should raise error when source device path does not exist -- Bug : 889099 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

1. # cat fs-pool.xml
<pool type='fs'>
  <name>mypool</name>
  <source>
    <device path='/dev/sda11'/>           ---------->   which does not exist actually
    <format type='auto'/>
  </source>
  <target>
    <path>/var/lib/libvirt/images/mypool</path>
  </target>
</pool>

2. # virsh pool-define fs-pool.xml
Pool mypool defined from fs-pool.xml

3. # virsh pool-build mypool
Pool mypool built

4. # virsh pool-start mypool
Pool mypool started

5. # mount
/dev/sda1 on / type ext4 (rw)
proc on /proc type proc (rw)
sysfs on /sys type sysfs (rw)
devpts on /dev/pts type devpts (rw,gid=5,mode=620)
tmpfs on /dev/shm type tmpfs (rw,rootcontext="system_u:object_r:tmpfs_t:s0")
none on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)
sunrpc on /var/lib/nfs/rpc_pipefs type rpc_pipefs (rw)

There is no /dev/sda11 mounted to /var/lib/libvirt/images/mypool.

6. # virsh pool-list --all
Name                 State      Autostart
-----------------------------------------
mypool               active     no

7. # virsh pool-dumpxml mypool
<pool type='fs'>
  <name>mypool</name>
  <uuid>93d92e37-462e-eabb-3713-654a4b33ed50</uuid>
  <capacity unit='bytes'>105689415680</capacity>
  <allocation unit='bytes'>2975559680</allocation>
  <available unit='bytes'>102713856000</available>
  <source>
    <device path='/dev/sda11'/>
    <format type='auto'/>
  </source>
  <target>
    <path>/var/lib/libvirt/images/mypool</path>
    <permissions>
      <mode>0755</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

	
Expected Results:

Should get error in step 2 or step 4.

refer bug: https://bugzilla.redhat.com/show_bug.cgi?id=889099

Notes:
Comments:

		236844 	[storage]block-copy full support -- Bug: 888426,856247 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

# vgcreate vgtest1 /dev/mapper/1IET_00010001
# lvcreate -n lvbaseimg1 -L 1G vgtest1
# lvcreate -n lvbaseimg2 -L 1G vgtest1
# lvcreate -n lvleafimg1 -L 1G vgtest1
# lvcreate -n lvleafimg2 -L 1G vgtest1
# qemu-img create -f qcow2 -F raw -b /dev/vg_virt/lvbaseimg1 /dev/vg_virt/lvleafimg1
# qemu-img create -f qcow2 -F raw -b /dev/vg_virt/lvbaseimg2 /dev/vg_virt/lvleafimg2
# cat > vm1.xml <<EOF
<domain type="kvm">
  <name>vm1</name>
  <memory>131072</memory>
  <devices>
    <disk device="disk" type="block">
      <source dev="/dev/vg_virt/lvleafimg1"/>
      <target bus="ide" dev="hda"/>
      <driver name="qemu" type="qcow2"/>
    </disk>
  </devices>
  <os>
    <type arch="x86_64" machine="pc">hvm</type>
  </os>
</domain>
EOF

# for i in /dev/vg_virt/lv*; do chgrp -v qemu $(readlink -f $i); done

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

S1: 
step 1: #virsh create vm1.xml

# virsh blockcopy --shallow --reuse-external --wait vm1 /dev/vg_virt/lvleafimg1 /dev/vg_virt/lvleafimg2

Now in mirroring phase

step 2:#for i in /dev/vg_virt/lv*; do ls -Z $(readlink -f $i); done



make sure blockjob in phase two
#virsh blockjob --info vm1 /dev/vg_virt/lvleafimg1
Block Copy: [100 %]

step 3:# virsh blockjob --pivot vm1 /dev/vg_virt/lvleafimg1


step 4 : #virsh dumpxml vm1


S2:
step 1 : #virsh create vm1.xml

# virsh blockcopy --shallow --reuse-external --wait vm1 /dev/vg_virt/lvleafimg1 /dev/vg_virt/lvleafimg2

Now in mirroring phase

step 2 : #for i in /dev/vg_virt/lv*; do ls -Z $(readlink -f $i); done


step 3 : 
# lvchange -an /dev/vg_virt/lvbaseimg2
# lvchange -ay /dev/vg_virt/lvbaseimg2

step 4:
# for i in /dev/vg_virt/lv*; do ls -Z $(readlink -f $i); done
brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-0
brw-rw----. root disk system_u:object_r:fixed_disk_device_t:s0 /dev/dm-1
brw-rw----. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c256,c359 /dev/dm-2
brw-rw----. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c256,c359 /dev/dm-3

step 5:
# virsh blockjob --pivot vm1 /dev/vg_virt/lvleafimg1



step 6:
#virsh dumpxml vm1

	
Expected Results:

S1 :

 after step 2 verify :

brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-0
brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-1
brw-rw----. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c256,c359 /dev/dm-2
brw-rw----. root root system_u:object_r:fixed_disk_device_t:s0 /dev/dm-3

 after step 3 verify:

no error display
# for i in /dev/vg_virt/lv*; do ls -Z $(readlink -f $i); done
brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-0
brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-1
brw-rw----. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c538,c961 /dev/dm-2
brw-rw----. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c538,c961 /dev/dm-3

 after step 4 verify:

.....
<disk type='block' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source dev='/dev/vg_virt/lvleafimg2'/>
      <target dev='hda' bus='ide'/>
.....

 S2:

after step 2 verify :

brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-0
brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-1
brw-rw----. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c256,c359 /dev/dm-2
brw-rw----. root root system_u:object_r:fixed_disk_device_t:s0 /dev/dm-3

after step 5 verify:

no error display
# for i in /dev/vg_virt/lv*; do ls -Z $(readlink -f $i); done
brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-0
brw-rw----. qemu qemu system_u:object_r:virt_content_t:s0 /dev/dm-1
brw-rw----. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c256,c359 /dev/dm-2
brw-rw----. qemu qemu unconfined_u:object_r:svirt_image_t:s0:c256,c359 /dev/dm-3

 after step 6 verify:

.....
<disk type='block' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source dev='/dev/vg_virt/lvleafimg2'/>
      <target dev='hda' bus='ide'/>
.....

 
Notes:
Comments:

		236845 	[console and serial devices] Provide XML mode to autoallocate unix socket path for char device -- Bug: 888325 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial device

bug:

    No bug found

Actions:

There could be one way to go with largest degree of backward compatibility - mimick autoport='bool' parameter of graphics element:
  * add autopath='bool' parameter for source elements that may 
    contain path='PATH' parameter
  * if autopath is set to true/1, path='' parameter is ignored when domain
    is down; path is created automatically on domain start/create
  * if autopath is set to false, keep current behaviour
  * for existing elements with path='' set, set autopath='no'
  * for new elements that should contain <source path='...'/> element
    currently but do not, auto-add <source autopath='true'/>

If you want to configure the qemu guest agent, a user/app has to add an XML block to the guest like:

    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/GUEST.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
    </channel>

Having to hardcode a path like that sucks, because:

1) /var/lib/libvirt/qemu is not guarantee'd to be a correctly permissioned place to drop things, only libvirt knows for sure the best spot
2) All apps would have to duplicate some logic to generate a socket name that doesn't conflict with other VMs, and the app may not even be in a position to guarantee it doesn't conflict with the local FS.

It would be helpful if we could do something like

  <source mode='bind' autopath='on'/>

And libvirt would just fill in a path for us. Would also be a useful mode for VNC unix sockets as well.

 

	
Expected Results:

refer bug:

https://bugzilla.redhat.com/show_bug.cgi?id=888325

https://bugzilla.redhat.com/show_bug.cgi?id=887415
Notes:
Comments:

		236847 	[storage]LVM volume created with allocation=0 -- Bug: 888118 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

# virsh vol-create-as --pool lvm sparsetest --capacity 16M --allocation 0
error: Failed to create vol sparsetest
error: internal error Child process (/sbin/lvchange -aln vgtest/sparsetest) unexpected exit status 5:   One or more specified logical volume(s) not found.

	
Expected Results:

refer bug : https://bugzilla.redhat.com/show_bug.cgi?id=888118
Notes:
Comments:

		236848 	[Graphical framebuffer]gain ability to limit maximum number of heads of qxl device -- bug:888762 	zpeng 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

refer bug:

https://bugzilla.redhat.com/show_bug.cgi?id=888762
	
Expected Results:
Notes:
Comments:

		236850 	[Migration] Bug 890478 - [FJ6.4 Bug]: [REG]Migration fails at high load in system activity. 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Bug 890478 - [FJ6.4 Bug]: [REG]Migration fails at high load in system activity.  steps NEEDUPDATE
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Bug 890478 It's closed as CLOSED WORKSFORME
Comments:

		236851 	[Migration] when do block_stream the target snapshot file should be locked BZ 888645 	zhpeng 	None 	Manual 		Regression 	P1 	None 	Edit
Setup:

Bug 888645 - when do block_stream the target snapshot file should be locked is ASSIGNED keep case NEEDUPDATE -- zhpeng
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		236852 	[smbios]Verify SMBIOS values in domain xml and doc --Bug 890494 	lsu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Once Bug 890494 - Some issues about verifying SMBIOS values in domain xml and doc fixed

Please update the case
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    .

bug:

    No bug found

Actions:

1.prepare a healthy guest

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     test                           shut off
2..

Add

 <sysinfo type='smbios'>
    <bios>
      <entry name='date'>LENOVO</entry>
      <entry name='release'>LENOVO</entry>
    </bios>
 </sysinfo>

to the guest's xml

Login the guest
#dmidecode
 dmidecode 2.11
SMBIOS 2.4 present.
10 structures occupying 306 bytes.
Table at 0x3FFFFEC0.

Handle 0x0000, DMI type 0, 24 bytes
BIOS Information
    Vendor: Seabios
    Version: 0.5.1
    Release Date: 01/01/2007
    Address: 0xE8000
    Runtime Size: 96 kB
    ROM Size: 64 kB
    Characteristics:
        BIOS characteristics not supported
        Targeted content distribution is supported
    BIOS Revision: 1.0

 The data should be show correctly

3.

add

  <system>
      <entry name='uuid'>1db7cfab-267e-e959-becf-ba8545-3c72e3</entry>
    </system>
  </sysinfo>
To the guest , make sure the uuid is different from the guest's uuid
After save , should report an error

error: internal error UUID mismatch between <uuid> and <sysinfo>

 

 

 
	
Expected Results:
Notes:
Comments:

		236958 	[Host network interface management]Set "DELAY=0" by virsh iface-bridge command - bug 892403 	ydu 	None 	Manual 		Function 	P1 	None 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

1. Prepare a healthy host, then create a bridge vir virsh command, and set delay=0 for it.

# virsh iface-bridge eth0 br0 --no-stp --delay 0
   

2. Check the  network configuration file

# cat -n /etc/sysconfig/network-scripts/ifcfg-br0
      1  DEVICE=br0
      2  ONBOOT=yes
      3  TYPE=Bridge
      4  BOOTPROTO=dhcp
      5  IPADDR=*.*.*.*
      6  NETMASK=255.255.255.0
      7  STP=off3

3. Repeat step 1 to create a bridge and set delay=$x($x != 0) for it, then check it again

    # virsh iface-bridge eth0 br0--no-stp --delay 5
    # cat -n /etc/sysconfig/network-scripts/ifcfg-br0
      1  DEVICE=br0
      2  ONBOOT=yes
      3  TYPE=Bridge
      4  BOOTPROTO=dhcp
      5  IPADDR=*.*.*.*
      6  NETMASK=255.255.255.0
      7  STP=off
 8 DELAY=5

	
Expected Results:

2. After bug fix, the  "DELAY" parameter should set successfully.

-------

DELAY=0

 -------

3. "DELAY" parameter should set successfully.
Notes:
Comments:

		237060 	[CPU Management] Check wrong cpu topology for AMD Bulldozer 62XX familly-bug888503 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Apply the AMD Bulldozer 62XX machine from beaker.

Waiting for the following bug to be fixed. 

Bug 888503 - libvirt: wrong cpu topology - AMD Bulldozer 62XX familly
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    888503

Actions:


1. Add the following xml to guest.
<topology totalsockets='2' sockets='1' cores='8' threads='2'/>
...
<cells num='4'>


2.
Just to clarify our needs, vdsm gets the total sockets and total cores (without threads) from libvirt. To this system for example, we are looking for a way to have 2 sockets and 16 cores total from libvirt.

With that, we would report in vdsm the field 'report_host_threads_as_cores' as:

if enabled:
================
(cores = 8) * (threads = 2) * (new_libvirt_field_total_sockets = 2) = 32 total cores

if disabled:
===================
(cores = 8) * (new_libvirt_field_total_sockets = 2) = 16 total cores

we have others system resources sharing:
/proc/cpuinfo we have the split:

<snip>
  cpu cores	: 8   (number of cores per CPU package)
  siblings	: 16  (HT per CPU package) * (number of cores per CPU package)
</snip>

Socket:
=====================
# cat /proc/cpuinfo | grep "physical id" | sort | uniq | wc -l
2

also from lscpu:
=========================
<snip>
  Thread(s) per core:    2   (core + thread)
  Core(s) per socket:    8
  CPU socket(s):         2
  On-line CPU(s) list:   0-31

  NUMA node(s):          4 
  NUMA node0 CPU(s):     0-7
  NUMA node1 CPU(s):     8-15
  NUMA node2 CPU(s):     16-23
  NUMA node3 CPU(s):     24-31
</snip>

 

	
Expected Results:
Notes:
Comments:

		237061 	[LXC]Checking a cloned lxc container -Bug888562 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Waiting for the following bug to be fixed.

Bug 888562 - Cloning lxc container creates unusable container.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC

bug:

    888562

Actions:

1. Create an lxc container with a network device
2. While that container is up, clone it
3. Run the clone

	
Expected Results:
Notes:
Comments:

		237062 	[LXC]Checking lxc containers's networking when selinux is enabled-bug888552 	honzhang 	None 	Manual 		Function 	P1 	None 	Edit
Setup:

Waiting for the following bug to be fixed.

Bug 888552 - lxc containers don't support networking when selinux is enabled
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC

bug:

    888552

Actions:

1. Configure a selinux container
2. Run the container
3. In the container, ifup the network device

	
Expected Results:
Notes:
Comments:

		237406 	[Scalability] Migrate 514 guests through TCP connection --tunnelled with ipv6 - bug 807907,807910 - large machine 	honzhang 	honzhang 	Manual 		Stress 	P1 	None 	Edit
Setup:

Prepare

1.Following case:

 [Remote access] Connect to the hypervisor on host using a plain TCP connection without SASL via ipv4

<https://tcms.engineering.redhat.com/case/177450/?from_plan=5066>

 

Run the case on both source host and target host to enable tcp listen for libvirtd.

 

2. Define 514 guests using following scripts

#wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/hongming/clone_vm.tar

 

3. A pair of large machines which have more than 512G mem for each

 

 

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. Start 514 domain on source host with diferent os and configuration including

- win7/SP1(32/64) 1GB mem / 1 vCPU
- win2008 (32/64) 2 GB mem / 2 vCPU
- win2008r2 (64) 2 GB mem / 2 vCPU
- win2003 (32/64) 1GB mem / 1 vCPU
- winxp 1GB mem / 1 vCPU
- rhel6.3(32/64) 512 MB mem / 1 vCPU
- rhel5.8 (32/64) 512 MB mem / 1 vCPU
- rhel4.9 (32/64) 512 MB mem / 1 vCPU
- rhel3.9 (32) 512 MB mem / 1 vCPU

# for i in {1..514};do virsh start  guest$i;done

2. Start migration

#for i in {1..514};do time virsh migrate --live guest$i qemu+tcp://[target host ipv4 address]/system --verbose --p2p --tunnelled;done

Note Pay attention to the performance of migration.

Bug 807910 - Tunnelled migration speed is much slower than without it

 

 


 

 
	
Expected Results:

2. Migration should be fininshed successfully.

Output

.....

Migration: [100 %]

real    0m4.656s
user    0m0.011s
sys    0m0.009s
Migration: [100 %]

real    0m5.483s
user    0m0.010s
sys    0m0.009s
Migration: [100 %]

real    0m7.480s
user    0m0.008s
sys    0m0.011s
Migration: [100 %]

real    0m5.448s
user    0m0.011s
sys    0m0.008s

 

No the following error occurs

Migration: [ 43 %]error: internal error received hangup / error event on socket

 

The migration speed should be the similar with non-tunnelled migration
Notes:
Comments:

		176742 	[Guest resource control] Persistently hot-plug a qcow2 disk with a backing file pointing at a host device 	ajia 	ajia 	Manual 		Feature 	P2 	10 	Edit
Setup:

1. make sure you have a running guest

2. create a qcow2 image under some directory (such as : qemu-img create -f qcow2 /var/lib/libvirt/images/test.test 5M -b /dev/sdbï»¿ï»¿  )
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. # virsh list

 Id Name                 State
----------------------------------
  3 test           running

 

2. attach the qcow2 image persistently to guest

# attach-disk kvm1 /var/lib/libvirt/images/test.test  vdc --subdriver qcow2 --persistent

3. check the qcow2 images is attached persistently

# virsh destroy <Guestname>

# virsh dumpxml <Geustname> |grep disk -A 3  

# virsh start <Guestname>

# virsh dumpxml <Guestname>|grep disk -A 3

4. login to the restarted guest , and check if the disk is really there in the guest

[in guest]

# fdisk -l
	
Expected Results:

2. make sure after performing attach-disk --persistent command , you could get successful return message .

3. make sure after destroying the guest , you could still get the attached disk xml description in guest xml

4. after restarting the guest you could see the attached disk with fdisk -l in guest

 

Note : currently --persistent is not available for attach-device and attach-disk commands . So you can ignore this case temporarily
Notes:
Comments:

		176756 	[configuration] domain resume when nfs with soft option is not available permanently/temporarily - bug 707202 	gren 	None 	Manual 		Regression 	P1 	10 	Edit
Setup:

1, setup a NFS server(eg: ip 10.66.5.5) and a shared folder.

# cat /etc/exports
/var/lib/libvirt/images        *(async,rw,all_squash,anonuid=36,anongid=36)

# service nfs restart

# iptables -F

2, On the test machine, mount the shared folder with option "soft"   10.66.5.5:/var/lib/libvirt/images/ on /mnt/nfsdir type nfs (rw,soft,vers=4,addr=10.66.5.5,clientaddr=<IP>)   NOT -o vers=3

# mount -o soft 10.66.5.5:/var/lib/libvirt/images /mnt

# setsebool virt_use_nfs=1

Note: <If neither soft/hard option  is  specified  (or if the hard option is specified), NFS requests are retried indefinitely.  If the soft option is specified, then the NFS client  fails an NFS  request  after  retrans retransmissions have been sent, causing the NFS client to return an error to the calling application.>

<If the option timeo is not specified, requests are retried every 60 seconds for NFS over TCP, If the retrans option is not specified, the NFS client tries each  request 3 times.>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    QE consumption
    configuration

bug:

    No bug found

Actions:

1, On the test machine, install a guest with its disk image placed into the /mnt dir

# virsh list --all

 Id Name                 State
----------------------------------
  5 rhel6                running

# virsh dumpxml rhel6   // Make sure guest img file is in NFS

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='threads'/>
      <source file='/mnt/rhel6.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
    </disk>

...

2, use iptables to drop the packet into the NFS server

 # iptables -A OUTPUT -d 10.66.5.5 -p tcp --dport 2049 -j DROP

3. # virsh list --all

4.

4.1 conduct some operation in guest

4.2 # virsh save rhel6 /tmp/rhel6.save

After 4 mins of step 2, you can find the following msg in /var/log/messages:

Aug 12 16:36:33 RHEL62-0623N0 kernel: nfs: server 10.66.5.5 not responding, timed out

5, The guest will not be unlocked until execute the following cmd to delete the rule:

# iptables -D OUTPUT -d 10.66.5.5 -p tcp --dport 2049 -j DROP

 

(For step 2, or add the iptable rule during the guest booting. After 4 mins, kernel will timeout after 3 retries every 60s and return error to userspace, you can get the timeout related info in /var/log/messages. After delete the iptable rule for nfs, the guest will continue to boot up and you can conduct the operation in guest.)
	
Expected Results:

step 3: The libvirtd will not hang

 Id Name                 State
----------------------------------
  5 rhel6                running

 

step 4:

4.1 the operation in guest will pause, such as scp guest big file to other host

4.2 save succeed, NO message like:   if you save failed pls chanage another NFS server check it again

error: Failed to save domain fff to /tmp/fff.save
error: Timed out during operation: cannot acquire state change lock

 

step 5:

the guest will be unlocked for the operation above.

1. The file can be sent successfully.

2. # virsh save rhel6 /tmp/rhel6.save

Domain rhel6 saved to /tmp/rhel6.save

 
Notes:
Comments:

		176989 	[Guest resource control] Support disk I/O bandwidth limitation / priority control - bug 515694 	nzhang 	yoyzhang 	Manual 		Regression 	P2 	10 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:

TBD as the RFE bug is still in assigned status
	
Expected Results:
Notes:
Comments:

		177391 	[NUMA] memory tuning for node out of range 	mzhan 	gren 	Manual 		Negative test 	P1 	10 	Edit
Setup:

1 on a NUMA machine with two or more nodes, install a guest.

# numactl --show
policy: default
preferred node: current
physcpubind: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
cpubind: 0 1
nodebind: 0 1
membind: 0 1

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NUMA

bug:

    No bug found

Actions:

1 edit guest xml description and add following xml between <domain> element

# virsh edit <guest>

  <numatune>
    <memory mode='interleave' nodeset='2-3'/> // 2-3 is out of range, adjust here if you have more than 2 nodes
  </numatune>

 2. Start the guest and check

# virsh start <guest>

3. Check /var/log/libvirt/qemu/$guest.log 

4. redo strep 1-3 with mode='preferred'

5. Destroy the guest.

#virsh destroy <guest>

6. Changed the guest.xml as

#virsh edit <guest>

  <numatune>
    <memory mode='strict' nodeset='2-3'/> // 2-3 is out of range
  </numatune>

 7. Start the guest and check

# virsh start <guest>

8. Check the libvirtd.log
	
Expected Results:

2. Domain <guest> started

3.#vim /var/log/libvirt/qemu/<guest>.log

2012-05-29 07:40:43.416+0000: 36083: warning : qemuProcessInitNumaMemoryPolicy:1623 : nodeset is out of range, there is only 2 NUMA nodes on host
set_mempolicy: Invalid argument

 

7. # virsh start vcpu3

error: Failed to start domain vcpu3
error: Unable to set cpuset.mems for domain vcpu3: Invalid argument

8. Check libvirtd.log found the following error:

# vim /var/log/libvirt/libvirtd.log

2012-05-29 09:55:23.508+0000: 29969: error : qemuSetupCgroup:418 : Unable to set cpuset.mems for domain vcpu3: Invalid argument
2012-05-29 09:55:23.703+0000: 29969: error : qemuRemoveCgroup:592 : internal error Unable to find cgroup for vcpu3
2012-05-29 09:55:23.703+0000: 29969: warning : qemuProcessStop:3897 : Failed to remove cgroup for vcpu3

 

Notes:
Comments:

		176758 	[configuration] libvirt fails to save VM into NFS with RHEVM - bug 707257 	nzhang 	None 	Manual 		Function 	P1 	20 	Edit
Setup:

1. Prepare 2 machines, one is for RHEL/VDSM, another one is for RHEVM.

2. Follow the instructions to config RHEVM.

http://cleo.tlv.redhat.com/qumrawiki/Integration/RHEVM_RPM_HOWTO

https://docspace.corp.redhat.com/docs/DOC-68146

3. Configure a NFS server with the options below.

# vi /etc/exports

/export    *(rw,sync)

# service nfs restart

# chown 36:36 /export

# ls -ld /export

drwxr-xr-x.   3 vdsm kvm   4096 Jul 20 04:43 /export

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    configuration

bug:

    837485 - From Run 42159
    851397 - From Run 44420

Actions:

1. Suspending the VM by clicking suspend button in RHEVM.

2. See log in RHEL/VDSM.

# cat /var/log/vdsm/vdsm.log
	
Expected Results:

To confirm the following messages can't be shown in vdsm.log.

 

Thread-19995::ERROR::2011-07-20 05:44:10,548::vm::230::vm.Vm::(run) vmId=`fe4cb182-be6c-4456-b754-15e36694d265`::Traceback (most recent call last):
File "/usr/share/vdsm/vm.py", line 222, in run
  self._startUnderlyingMigration()
File "/usr/share/vdsm/libvirtvm.py", line 261, in _startUnderlyingMigration
  self._vm._dom.save(fname)
File "/usr/share/vdsm/libvirtvm.py", line 311, in f
  ret = attr(*args, **kwargs)
File "/usr/share/vdsm/libvirtconnection.py", line 63, in wrapper
  ret = f(*args, **kwargs)
File "/usr/lib64/python2.6/site-packages/libvirt.py", line 664, in save
  if ret == -1: raise libvirtError ('virDomainSave() failed', dom=self)
libvirtError: Error from child process creating '/rhev/data-center/6c04d207-8dba-41a7-b722-236e8c133b45/26df4b73-8189-4579-bf46-b28f3f4194f4/images/0b5b14d5-c917-4401-b256-59a233da6166/f31d7136-ef87-4e5b-9991-7b58a4dd142a': Permission denied
Notes:
Comments:

		176969 	[Guest resource control] blkiotune set weight on total and virtio device together will not cause libvirtd hang BZ#770520 	zhpeng 	None 	Manual 		Regression 	P2 	20 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. start a guest with virtio disk
# virsh dumpxml guest
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/kvm-rhel6u2-x86_64-new.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05'
function='0x0'/>
    </disk>

2. Do command
# virsh blkiotune guest --device-weights /dev/sda,100 --weight 500

	
Expected Results:

Step 2 will not hang
Notes:
Comments:

		176990 	[Guest resource control] Support network I/O bandwidth controls 	nzhang 	yoyzhang 	Manual 		Feature 	P2 	20 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177385 	[NPIV] RFE: Automatically generate unique WWN -bug 557935 	nzhang 	None 	Manual 		Function 	P1 	20 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NPIV

bug:

    No bug found

Actions:

1.create a vHBA by an XML file without wwnn
specified.
# cat vHBA.xml
<device>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
     </capability>
  </capability>
</device>

# service libvirtd restart 

2 # virsh nodedev-create vHBA.xml 
And create it again

3 # virsh nodedev-create vHBA.xml  

4. Check the WWN value of the created vHBAs  
# virsh nodedev-dumpxml scsi_host9

5 # virsh nodedev-dumpxml scsi_host10 

	
Expected Results:

verify:

after step1:

the libvirtd start successful

Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

 step 2:

Node device scsi_host9 created from vHBA.xml

step 3:

Node device scsi_host10 created from vHBA.xml

 step 4:

<device>
  <name>scsi_host9</name>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <host>9</host>
    <capability type='fc_host'>
      <wwnn>5001a4af1ecab19a</wwnn>
      <wwpn>5001a4a6c8b11b08</wwpn>
      <fabric_wwn>2001000dec9877c1</fabric_wwn>
    </capability>
  </capability>
</device>
step 5:

<device>
  <name>scsi_host10</name>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <host>10</host>
    <capability type='fc_host'>
      <wwnn>5001a4a0ec67fd79</wwnn>
      <wwpn>5001a4aa8d7f64f8</wwpn>
      <fabric_wwn>2001000dec9877c1</fabric_wwn>
    </capability>
  </capability>
</device>

 From the out put XML file, we can get the WWN value for the vHBA.
 The the first nibble is hex 5, and since the hypervisor type is QEMU of my host, 
  the 3-bytes vendor ID is 001a4a, and the left 36 bits are auto-generated uniquely.

 
Notes:
Comments:

		176759 	[configuration] libvirtd fails to start with rhevm's cert configured - bug 723447 	nzhang 	None 	Manual 		Function 	P1 	30 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    configuration
    Regression

bug:

    837485 - From Run 42159
    851397 - From Run 44420

Actions:

1. Refer to the following link to setup RHEVM.

https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/Install%20Linux%20version%20of%20RHEVM

2. Restart vdsmd service, and the 2 configs will be set in /etc/libvirt/libvirtd.conf

cert_file="/etc/pki/vdsm/certs/vdsmcert.pem" # by vdsm
key_file="/etc/pki/vdsm/keys/vdsmkey.pem" # by vdsm

3. Connect to RHEVM, and run a VM on start.

4. Check log in /var/log/vdsm/vdsm.log

 

 
	
Expected Results:

Should NOT be gotten any errors like following:

11:08:43.672: 27874: error : virNetTLSContextSanityCheckCert:198 : Unable to
query certificate /etc/pki/vdsm/certs/vdsmcert.pem basic constraints The
requested data were not available.

 
Notes:
Comments:

		176978 	[Guest resource control] Guest can be started with <iotune> setting in xml BZ#768870 	zhpeng 	None 	Manual 		Regression 	P2 	30 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. define a guest like:
.......

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/kvm-rhel6u2-x86_64-new.img'/>
      <target dev='hda' bus='ide'/>
 <iotune>
 <total_bytes_sec>10000000</total_bytes_sec>
 <read_iops_sec>400000</read_iops_sec>
 <write_iops_sec>100000</write_iops_sec>
 </iotune>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

.......
2. virsh dumpxml guest
3. start guest

	
Expected Results:

2. Not like:

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/kvm2.img'/>
      <target dev='hda' bus='ide'/>
      <iotune>
        <total_bytes_sec>10000000</total_bytes_sec>
        <read_iops_sec>400000</read_iops_sec>        <write_iops_sec>100000</write_iops_sec>      </iotune>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

3. Not like:

# virsh start kvm-rhel6u2-x86_64-new
error: Failed to start domain kvm-rhel6u2-x86_64-new
error: internal error Process exited while reading console log output:
qemu-kvm: -drive
file=/var/lib/libvirt/images/kvm-rhel6u2-x86_64-new.img,if=none,id=drive-ide0-0-0,format=raw,cache=none,bps=10000000,iops_rd=400000,iops_wr=100000:
Invalid parameter 'bps'
char device redirected to /dev/pts/1
qemu-kvm: -device
ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1:
Property 'ide-drive.drive' can't find value 'drive-ide0-0-0'

 Now , qemu doesn't support this ,so this result is expected:

error: Failed to start domain v1-clone
error: unsupported configuration: block I/O throttling not supported with this
QEMU binary

 
Notes:
Comments:

		176991 	[Guest resource control] Use cgroups to enforce host RAM limit according to current memory balloon allocation - bug 515692 	nzhang 	yoyzhang 	Manual 		Regression 	P2 	30 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:

TBD as the RFE bug is still in assigned status
	
Expected Results:
Notes:
Comments:

		176760 	[configuration] run libvirtd with all vdsm related configure 	dyuan 	None 	Manual 		Feature 	P1 	40 	Edit
Setup:

1) Perpare a rhevm  and a clean rhel6.3 host

 

2) add host to rhevm 

step :

https://docspace.corp.redhat.com/docs/DOC-101056
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    configuration

bug:

    837485 - From Run 42159
    842208 - From Run 42935
    842208 - From Run 42300
    851397 - From Run 44420

Actions:

1.

clean your libvirt config file  remove all settting  without '#' in the front of the line .

3 files  

/etc/libvirt/libvirtd.conf 

/etc/libvirt/qemu.conf

/etc/sysconfig/libvirtd

then restart vdsmd

 

#service vdsmd restart

 

2.

# echo redhat | saslpasswd2 -p -a libvirt test

# virsh list --all
	
Expected Results:

1:

vdsmd start successed

2:

# virsh list --all

Please enter your authentication name: test
Please enter your password:

 Id Name                 State
----------------------------------
3 RHEL6                running
Notes:
Comments:

		176962 	[Guest resource control] Hot-plug a qcow2 disk with a backing file pointing at a host device, and restarting libvirtd service 	ajia 	None 	Auto 		Feature 	P2 	40 	Edit
Setup:

1. make sure you have a running guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. create the qcow2 image pointing to the host device

# qemu-img create -f qcow2 /var/lib/libvirt/images/backup.qcow2 10M -b /dev/sdb

2. create a backup.xml

# cat backup.xml

    <disk type='file' device='disk'>
              <driver name='qemu' type='qcow2'/>
                <source file='/var/lib/libvirt/images/demo.qcow2'/>
                      <target dev='vdb' bus='virtio'/>
                          </disk>


3. start the guest

# virsh start <guestname>

4. virsh attach-device <guestname> backup.xml

Device attached successfully

5. confirm the qcow2 image is attached successfully

# virsh dumpxml <guestname> |grep disk -A 3

[on guest]

# fdisk -l

6. try to restart the libvirtd on the host 

# service libvirtd restart

7. check the guest status , and whether the image is still attached

# virsh list --all

# virsh dumpxml <guestname> |grep disk -A 3

[on guest]

# fdisk -l

# dd if=/dev/vdb of=/dev/null 512+0 records in
512+0 records out
262144 bytes (262 kB) copied, 0.00112692 s, 233 MB/s

	
Expected Results:

4. make sure you could attach the qcow2 image successfully into guest

5. you could see the xml are record correctly for the new attached qcow2 image, and in the guest , you could see the qcow2 image is attached

6. after restarting libvirtd the qcow2 image is still attached, and dd of result is NOT 0 .

 
Notes:
Comments:

		176762 	[configuration] save/restore the guest to pre-created file on root_squashing export nfs with dynamic_ownership=1 - bug 810241 	gsun 	None 	Manual 		Function 	P1 	50 	Edit
Setup:

# vim /etc/yum.repos.d/rhel6.repo
[brewroot_rhel6]
name = brewroot_rhel6
baseurl = http://porkchop.devel.redhat.com/brewroot/repos/RHEL-6.2-build/latest/$basearch
enabled = 1
gpgcheck = 0

# yum install -y vdsm

(if so, modify /etc/libvirt/libvirtd.conf /etc/libvirt/qemu.conf to comment out vdsm config)

Or add user vdsm:
# useradd vdsm -g qemu -u 36


Bug 534010 - Qemu security driver 'restore' will chown files to root, even if they were not originally owned by root


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt
    configuration

bug:

    No bug found

Actions:

1. set the dynamic_ownership = 1 in /etc/libvirt/qemu.conf
dynamic_ownership = 1
also edit the user and group as qemu
user = qemu
group = qemu

2. setup a nfs server
# cat /etc/exports
/var/lib/libvirt/images      *(rw,root_squash)

# service nfs restart

3. created a file with ownership 36:36 in nfs export dir
# touch /var/lib/libvirt/images/save
# chown 36:36 /var/lib/libvirt/images/save
# chmod 664 /var/lib/libvirt/images/save

also create a file with qemu:qemu
# touch /var/lib/libvirt/images/save1
# chown qemu:qemu /var/lib/libvirt/images/save1
# chmod 664 /var/lib/libvirt/images/save1



4. mount the nfs export on the local host
# mount -o vers=3 $nfs_server_ip:/var/lib/libvirt/images /mnt

5. # ll /mnt/save
-rw-rw-r--. 1 vdsm kvm 0 Jul 20  2011 /mnt/save

6. save a running domain to the pre-created file
(conduct some operations in guest, such as: ls, ps aux...)
# virsh save rhel6 /mnt/save 



7. save the domain to a pre-exist file with permission as qemu:qemu on nfs
# virsh save rhel6 /mnt/save1

# ll -Z /mnt/save1


8. # virsh restore /mnt/save1

9. # ll -Z /mnt/save1

	
Expected Results:

6.

# virsh save dom /mnt/save
error: Failed to save domain dom to /mnt/save
error: Error from child process creating '/mnt/save': Permission denied

Fail to save.

 

7.


Domain rhel6 saved to /mnt/bba

-rw-rw-r--. qemu qemu system_u:object_r:nfs_t:s0       /mnt/bba

the ownship of save target file not change

 8.

Domain restored from /mnt/save1

 9.

-rw-rw-r--. qemu qemu system_u:object_r:nfs_t:s0       /mnt/bba

the ownship of save target file not change
Notes:
Comments:

		176979 	[Guest resource control] Hot-plug a qcow2 disk with a backing file pointing at a host device, and shutdowning guest 	ajia 	None 	Auto 		Feature 	P2 	50 	Edit
Setup:

1. make sure you have a running guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. create the qcow2 image pointing to the host device

# qemu-img create -f qcow2 /var/lib/libvirt/images/backup.qcow2 10M -b /dev/sdb

2. create a backup.xml

# cat backup.xml

    <disk type='file' device='disk'>
          <source file='/var/lib/libvirt/images/backup.qcow2'/>
                <target dev='vda' bus='virtio'/>
                    </disk>

3. start the guest

# virsh start <guestname>

4. virsh attach-device <guestname> backup.xml

Device attached successfully

5. confirm the qcow2 image is attached successfully

# virsh dumpxml <guestname> |grep disk -A 3

[on guest]

# fdisk -l

6. try to shut down the guest from the host

# virsh shutdown <guestname>
	
Expected Results:

4. make sure you could attach the qcow2 image successfully into guest

5. you could see the xml are record correctly for the new attached qcow2 image, and in the guest , you could see the qcow2 image is attached

6.you could shut down the guest on the host successfully
Notes:
Comments:

		177765 	[Virtio] EMBARGOED libvirt: support SG_IO for virtio-blk opt-in (CVE-2011-4127 mitigation) [rhel-6.3] - bug 756678 	nzhang 	None 	Manual 		Function 	P2 	50 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

NB: qemu >= 0.11


Case 1: Add a disk for new type 'lun' in guest.

<disk type='block' device='lun'>
    <target dev='xxx' bus='virtio'/>
    ...
</disk>


Case 2: Add a disk for type 'block' in guest.

<disk type='block' device='disk'>
    <target dev='xxx' bus='virtio'/>
    ...
</disk>

 

 

	
Expected Results:

Case 1:

    # ps -ef|grep qemu-kvm

      -device virtio-blk-pci,scsi=on,...

 Case 2:

    # ps -ef|grep qemu-kvm

      -device virtio-blk-pci,scsi=off,...

 
Notes:
Comments:

		176764 	[configuration] suspend-->stop VM to nfs with dynamic_ownership=0 with RHEVM - bug 716478 	dyuan 	None 	Manual 		Feature 	P1 	60 	Edit
Setup:

1. Prepare 2 machines, one is for RHEL/VDSM, another one is for RHEVM.

2. Follow the instructions to config RHEVM.

http://cleo.tlv.redhat.com/qumrawiki/Integration/RHEVM_RPM_HOWTO

https://docspace.corp.redhat.com/docs/DOC-68146

3. Configure a NFS server with the options below.

# vi /etc/exports

/export    *(rw,sync)

# service nfs restart

# chown 36:36 /export

# ls -ld /export

drwxr-xr-x.   3 vdsm kvm   4096 Jul 20 04:43 /export
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    configuration

bug:

    837485 - From Run 42159
    851397 - From Run 44420

Actions:

NOTEï¼
VMâs graphic need be spice 


1. Have a diskless VM on nfs in 3.0 RHEVM datacenter
2. Suspend the VM
3. Stop the VM

	
Expected Results:

step 3:

start guest success 

VM is stopped.

Notes:
Comments:

		176980 	[Guest resource control] Hot-plug a qcow2 disk with a backing file pointing at a host partition, and restarting libvirtd service 	ajia 	None 	Auto 		Feature 	P2 	60 	Edit
Setup:

1. make sure you have a running guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. create the qcow2 image pointing to the host device

# qemu-img create -f qcow2 /var/lib/libvirt/images/backup.qcow2 10M -b /dev/sdb1

2. create a backup.xml

# cat backup.xml

    <disk type='file' device='disk'>
          <source file='/var/lib/libvirt/images/backup.qcow2'/>
                <target dev='vda' bus='virtio'/>
                    </disk>

3. start the guest

# virsh start <guestname>

4. virsh attach-device <guestname> backup.xml

Device attached successfully

5. confirm the qcow2 image is attached successfully

# virsh dumpxml <guestname> |grep disk -A 3

[on guest]

# fdisk -l

6. try to restart the libvirtd on the host 

# service libvirtd restart

7. check the guest status , and whether the image is still attached

# virsh list --all

# virsh dumpxml <guestname> |grep disk -A 3

[on guest]

# fdisk -l

# dd if=/dev/vda of=/dev/null

512+0 records in
512+0 records out
262144 bytes (262 kB) copied, 0.00112692 s, 233 MB/s
	
Expected Results:

4. make sure you could attach the qcow2 image successfully into guest

5. you could see the xml are record correctly for the new attached qcow2 image, and in the guest , you could see the qcow2 image is attached

6. after restarting libvirtd the qcow2 image is still attached, and dd of result is NOT 0 .
Notes:
Comments:

		177855 	[Watchdog device] Watchdog device "ib700" - dump(bug667090) 	ydu 	None 	Auto 		Feature 	P3 	60 	Edit
Setup:

1. Prepare a guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh-rail
    watchdog
    watchdog device

bug:

    No bug found

Actions:

1. Edit the config file /etc/libvirt/qemu.conf and uncomment the following line:

auto_dump_path = "/var/lib/libvirt/qemu/dump"


2. Add a watchdog device through edit the guest's XML file, and start the guest.

<watchdog model='ib700' action='dump'/>

3. watch dog ib700 kernel mod is ib700wdt. and you must load  the module by hand. 

In guest(for rhel6 guest),add "modprobe ib700wdt" to /etc/rc.d/rc.sysinit and reboot.

4.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

5. In guest, check the watchdog dirvier is installed:

# dmesg | grep ib700

# /sbin/lsmod | grep ib700

6. In guest, install watchdog software.

# yum install watchdog

7. In guest, adjust settings in /etc/watchdog.conf:
interval        = 100
watchdog-device = /dev/watchdog

** should be larger than 60.

8. In guest, start watchdog process.
# watchdog -f

9. Wait for some minutes, check the status of guest and if there's a dump file for the guest.
	
Expected Results:

4. there are some values about watchdog device, like:

  -device ib700

  -watchdog-action pause               *QEMU has no support for a 'dump' action*
Notes:
Comments:

		176765 	[configuration]Install a new guest with spice via Runonce in rhev-m 	whuang 	None 	Manual 		Regression 	P1 	70 	Edit
Setup:

need a  rhevM + rhel with vdsm to test this case 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    configuration

bug:

    837485 - From Run 42159
    851397 - From Run 44420

Actions:

1.add new vm with spice video  in the rhev-m
2.installed it from PXE via RunOnce
3.open spice . complete installation   

	
Expected Results:

everything (guest and spice )works well ! 

no error no hung ! 
Notes:
Comments:

		176981 	[guest resource control] Hot-plug a qcow2 disk with a backing file pointing at a host partition, and shutdowning guest 	ajia 	None 	Auto 		Feature 	P2 	70 	Edit
Setup:

1. make sure you have a running guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. create the qcow2 image pointing to the host device

# qemu-img create -f qcow2 /var/lib/libvirt/images/backup.qcow2 10M -b /dev/sdb1

2. create a backup.xml

# cat backup.xml

    <disk type='file' device='disk'>
          <source file='/var/lib/libvirt/images/backup.qcow2'/>
                <target dev='vda' bus='virtio'/>
                    </disk>

3. start the guest

# virsh start <guestname>

4. virsh attach-device <guestname> backup.xml

Device attached successfully

5. confirm the qcow2 image is attached successfully

# virsh dumpxml <guestname> |grep disk -A 3

[on guest]

# fdisk -l

6. try to shut down the guest from the host

# virsh shutdown <guestname>
	
Expected Results:

4. make sure you could attach the qcow2 image successfully into guest

5. you could see the xml are record correctly for the new attached qcow2 image, and in the guest , you could see the qcow2 image is attached

6.you could shut down the guest on the host successfully
Notes:
Comments:

		176984 	[guest resource control] Persistently hot-plug a qcow2 disk with a backing file pointing at a host partition 	ajia 	None 	Manual 		Feature 	P2 	80 	Edit
Setup:

1. make sure you have a running guest

2. create a qcow2 image  under some directory (such as : qemu-img create -f qcow2 /var/lib/libvirt/images/test.test 5M -b /dev/sdbï»¿ï»¿1  )
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. # virsh list

 Id Name                 State
----------------------------------
  3 test           running

 

2. attach the qcow2 image persistently to guest

# attach-disk kvm1 /var/lib/libvirt/images/test.test vdb --persistent --subdriver qcow2

3. check the qcow2 images is attached persistently

# virsh destroy <Guestname>

# virsh dumpxml <Geustname> |grep disk -A 3  

# virsh start <Guestname>

# virsh dumpxml <Guestname>|grep disk -A 3

4. login to the restarted guest , and check if the disk is really there in the guest

[in guest]

# fdisk -l
	
Expected Results:

2. make sure after performing attach-disk --persistent command , you could get successful return message .

3. make sure after destroying the guest , you could still get the attached disk xml description in guest xml

4. after restarting the guest you could see the attached disk with fdisk -l in guest

 

Note : currently --persistent is not available for attach-device and attach-disk commands . So you can ignore this case temporarily
Notes:
Comments:

		177752 	[VirtFS]support readonly 	zpeng 	None 	Manual 		Function 	P1 	80 	Edit
Setup:

Make sure the guest kernel support p9fs.

How to check if your current kernel supports 9pfs?

# grep 9P /boot/config-$(uname -r)
CONFIG_NET_9P=m
CONFIG_NET_9P_VIRTIO=m
CONFIG_NET_9P_RDMA=m
# CONFIG_NET_9P_DEBUG is not set
CONFIG_9P_FS=m
CONFIG_9P_FSCACHE=y

If the option value is "y", that indicates p9fs driver is built into the kernel ("=y").
If the option value is "m", that indicates p9fs driver will be built as a module ("=m"), user can load it manually via "modprobe" command.
If the option value is "n" or no the config line, that indicates p9fs driver is not selected. User need re-compile kernel.

 

Acctually F14 release kernel has supported p9fs. So we can use FC14 or later to test this case.

 

NOTE:

Not all the qemu emulator is support virtFS feature, so before run this case, make sure your qemu emulator suport virtFS.

Check if your qemu support p9fs:
# [qemu-kvm | qemu | qemu-system-x86_64] --help
...
File system options:
-fsdev local,id=id,path=path,security_model=[mapped|passthrough|none]
Virtual File system pass-through options:
-virtfs local,path=path,mount_tag=tag,security_model=[mapped|passthrough|none]

...

# [qemu-kvm | qemu | qemu-system-x86_64] -device ?
...
name "virtio-9p-pci", bus PCI
...

If any of the above two options is missing, that indicates your qemu does NOT support 9pfs.

The upstream qemu does support p9fs, so you need compile the latest qemu emulator.

Get and build latest qemu binary.

1). Get the latest git repository from http://git.qemu.org/.
# git clone git://git.qemu.org/qemu.git

2). cd to the downloaded qemu source directory

3). Configure QEMU for the desired target.
# ./configure '--target-list=x86_64-softmmu' '--enable-debug' '--enable-kvm' '--enable-attr' '--prefix=/usr'

NOTE:
Make sure install zlib-devel, libattr, libattr-devel, glib2-devel, glib2, glibc-devel, glibc, glibc-common & glibc-headers.

4). Compile QEMU
# make
# make install
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    VirtFS

bug:

    No bug found

Actions:

1. Create a guest with p9fs supported.

2. Shut down the guest.

3. Add the following lines to the guest xml.

   <filesystem type='mount' accessmode='passthrough'>
      <source dir='/tmp'/>
      <target dir='test_mount'/>
      <readonly/>
    </filesystem>
If get error like: your qemu not support readonly, you can use qemu-stable-1.0 in upstream, and that version is proved to support well.

NOTE:
"/tmp" is what host expose to guest
"test_mount" is not actually a directory, it is merely a arbitrary string tag that is exported to the guest as a hint for where to mount it.

Example:

#virsh dumpxml fedora

<domain type='kvm'>
  <name>fedora</name>
  <uuid>0bed1aba-3e13-f0bf-9a21-4c6cc49d40d4</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='pc-0.14'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fedora.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <filesystem type='mount' accessmode='passthrough'>
      <source dir='/tmp'/>
      <target dir='test_mount'/>
      <readonly/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </filesystem>
    <interface type='network'>
      <mac address='52:54:00:39:e7:6a'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
</domain>


4. Make sure user=root and group=root in /etc/libvirt/qemu.conf, then start the domain.

# ps -ef|grep qemu

 

5. In guest, check the virtFS functionality.

# lsmod|grep  9p

# mount -t 9p -o trans=virtio test_mount /tmp/shared/

# cd /tmp/shared/

# ls

Try to write and read some data in this folder.
	
Expected Results:

4. Output:
qemu     11685     1 56 05:48 ?        00:00:35 /usr/bin/qemu-kvm -S -M pc-0.14 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name fedora -uuid 0bed1aba-3e13-f0bf-9a21-4c6cc49d40d4 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/fedora.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -drive file=/var/lib/libvirt/images/fedora.img,if=none,id=drive-virtio-disk0,format=raw -device virtio-blk-pci,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -fsdev local,security_model=passthrough,id=fsdev-fs0,path=/tmp -device virtio-9p-pci,id=fs0,fsdev=fsdev-fs0,mount_tag=test_mount,bus=pci.0,addr=0x7 -netdev tap,fd=24,id=hostnet0,vhost=on,vhostfd=25 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:39:e7:6a,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -device usb-tablet,id=input0 -vnc 127.0.0.1:0 -vga cirrus -device intel-hda,id=sound0,bus=pci.0,addr=0x4 -device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6

NOTE:

passthrough is the default security model for virtFS.

5. Output:

# lsmod|grep  9p
9p                     28949  0
fscache                44467  1 9p
9pnet_virtio            3717  0
virtio                  4242  1 9pnet_virtio
virtio_ring             6110  1 9pnet_virtio
9pnet                  40618  2 9p,9pnet_virtio

After mount successfully, find ls output is listing context from your host dir - /tmp

read data should be successful.

write date should be failed.

mkdir : cannot create directory 'test':Operation not permitted

the filesystem is readonly.

Note: Currently, you could modify existing files but may fail to touch/mkdir new file/folder in the guest. To workaround this problem, change selinux to permissive in guest. A patch is on the external list which allows SElinux to identify 9p(VirtFS) as a filesystem.
Notes:
Comments:

		176761 	[configuration] save/restore the guest to pre-created file on root_squashing export nfs with dynamic_ownership=0 	dyuan 	None 	Auto 		Feature 	P1 	90 	Edit
Setup:

# vim /etc/yum.repos.d/rhel6.repo
[brewroot_rhel6]
name = brewroot_rhel6
baseurl = http://porkchop.devel.redhat.com/brewroot/repos/RHEL-6.2-build/latest/$basearch
enabled = 1
gpgcheck = 0

# yum install -y vdsm

Bug 534010 - Qemu security driver 'restore' will chown files to root, even if they were not originally owned by root
https://bugzilla.redhat.com/show_bug.cgi?id=534010


# setsebool virt_use_nfs 1

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    configuration

bug:

    842208 - From Run 42300

Actions:

1. set the dynamic_ownership = 0 in /etc/libvirt/qemu.conf
dynamic_ownership = 0

# service libvirtd restart

2. setup a nfs server
# cat /etc/exports
/var/lib/libvirt/images      *(rw,root_squash)

# service nfs restart

3. created a file with ownership 36:36 in nfs export dir, make sure it have group write permission
# touch /var/lib/libvirt/images/save
# chown 36:36 /var/lib/libvirt/images/save
# chmod 664 /var/lib/libvirt/images/save

4. mount the nfs export on the local host
# mount -o vers=3 $nfs_server_ip:/var/lib/libvirt/images /mnt

5. # ll /mnt/save
-rw-r--r--. 1 vdsm kvm 0 Jul 20  2011 /mnt/save

6. save a running domain to the pre-created file
(conduct some operations in guest, such as: ls, ps aux...)
# virsh save rhel6 /mnt/save 
Domain rhel6 saved to /mnt/save

7. # ll /mnt/save


8. # virsh restore /mnt/save

9. # ll /mnt/save

 

	
Expected Results:

step 7:

-rw-r--r--. 1 vdsm kvm 41952869 Jan 18 02:54 /mnt/save

the ownship of save target file should be no change.

step 8:

The guest restore successfully but not boot normally.

you should see the result of ls, ps aux in the guest.

Domain restored from /mnt/save

step 9:

-rw-r--r--. 1 vdsm kvm 0 Jul 20  2011 /mnt/save

the ownship of the file should be no change.


Notes:
Comments:

		176987 	[guest resource control] Set the outside range of 0-262144 value for cpu_shares of the guest 	ajia 	None 	Auto 		Feature 	P2 	90 	Edit
Setup:

1. make sure you have a running guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. # virsh list --all
 Id Name                 State
----------------------------------
  2 test    running

2. get current guest schedule info 

# virsh schedinfo <guestname>

3. set guest cpu_share to a value in the range of 0-262144 (such as 100)

# virsh schedinfo <guestname> --set cpu_shares=100

Scheduler      : posix
cpu_shares     : 100

4. look up the guest cgroup info 

# cat /cgroup/cpu/libvirt/qemu/<guestname>/cpu.share

100

Note: On the Linux kernel, the values 0 and 1 are automatically converted to a minimal value of 2.

5. set guest cpu_share to a negtive number (such as -1)

# virsh schedinfo <guestname> --set cpu_shares=-1

Scheduler      : posix
cpu_shares     : 262144

6. look up the guest cgroup info 

# cat /cgroup/cpu/libvirt/qemu/<guestname>/cpu.share

262144

7. set guest cpu_share to a number larger than 262144 (such as 262166)

# virsh schedinfo <guestname> --set cpu_shares=262166

Scheduler      : posix
cpu_shares     : 262144

8.look up the guest cgroup info 

# cat /cgroup/cpu/libvirt/qemu/<guestname>/cpu.share

262144
	
Expected Results:

make sure you can get the same result as in the steps
Notes:
Comments:

		176763 	[configuration] Starting libvirtd with libvirtd.conf containing an unterminated string - bug 728654 	dyuan 	dyuan 	Manual 		Regression 	P1 	100 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration
    Regression

bug:

    No bug found

Actions:

1. Adding the following to libvirtd.conf

log_outputs="1:file:/var/log/libvirt/libvirt_debug.log

2. # service libvirtd restart

3. # libvirtd 

	
Expected Results:

step 2:

Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon: 05:10:16.742: 11484: info : libvirt version: 0.9.4,
package: 5.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>,
2011-08-22-12:26:05, x86-009.build.bos.redhat.com)
05:10:16.742: 11484: error : main:1394 : Can't load config file
'/etc/libvirt/libvirtd.conf' 


step 3:

05:10:35.896: 11488: info : libvirt version: 0.9.4, package: 5.el6 (Red Hat,
Inc. <http://bugzilla.redhat.com/bugzilla>, 2011-08-22-12:26:05,
x86-009.build.bos.redhat.com)
05:10:35.896: 11488: error : main:1394 : Can't load config file
'/etc/libvirt/libvirtd.conf'

 

Notes:
Comments:

		176961 	[Guest resource control] a guest qcow2 disk with a backing file pointing at a host device, containing a qcow2 image, pointing at another raw host device 	xhu 	None 	Auto 		--default-- 	P2 	100 	Edit
Setup:

Firstly, you must run "56554 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. install a guest using virsh, virt-install or virt-manager

2. get disk partition
# df -h

3. create a qcow2 format image and let it point at /dev/sda2 , make sure "/dev/sda2" format is qcow2 .
   Note: (If "/dev/sda2" format is raw ,please conver it.
       # qemu-img convert -O qcow2 /dev/sda2 /dev/sda  )

# qemu-img create -f qcow2  -b /dev/sda5 -o backing_fmt=qcow2  demo.qcow2 1G


4. check whether demo.qcow2 disk pointing at /dev/sda2 and disk format is qcow2
# qemu-img info /var/lib/libvirt/images/demo.qcow2

5. create a qcow2 format image with backing_fmt=raw and let it point at
/dev/sda3

# qemu-img create -f qcow2 -b /dev/sda6 -o backing_fmt=raw /dev/sda5

6. check whether /dev/sda2 partition pointing at /dev/sda3 and disk format is qcow2
# qemu-img info /dev/sda5

7. using virsh edit to editor the following xml block to shutoff state guest
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/demo.qcow2'/>
      <target dev='hdb' bus='ide'/>
    </disk>

8. check whether the above xml is successfully added
# virsh dumpxml rhel6

9. start the guest
# virsh start rhel6

10. get /dev/sda5 and /dev/sda6 partition major:minor number
# ls -la /dev/sda5

# ls -la /dev/sda6

11. check whether the above major:minor(8:2 and 8:3) number exists in
devices.list
# egrep '8:5|8:6' /cgroup/devices/libvirt/qemu/rhel6/devices.list

	
Expected Results:

2 # df -h

Filesystem Size Used Avail Use% Mounted on

/dev/sda2 29G 7.6G 20G 28% /

tmpfs 982M 24K 982M 1% /dev/shm

/dev/sda1 194M 70M 115M 38% /boot

/dev/sda3 20G 230M 19G 2% /home

4 # qemu-img info /var/lib/libvirt/images/demo.qcow2image: demo.qcow2
file format: qcow2
virtual size: 1.0G (1073741824 bytes)
disk size: 136K
cluster_size: 65536
backing file: /dev/sda5 (actual path: /dev/sda5)


6 # qemu-img info /dev/sda5

image: /dev/sda5
file format: qcow2
virtual size: 1.0G (1085704704 bytes)
disk size: 0
cluster_size: 65536
backing file: /dev/sda6 (actual path: /dev/sda6)

8 # virsh dumpxml rhel6

......

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/demo.qcow2'/>
      <target dev='hdb' bus='ide'/>
      <alias name='ide0-0-1'/>
      <address type='drive' controller='0' bus='0' unit='1'/>
    </disk>

......

9 # virsh start rhel6

Domain rhel6 started

10 # ls -la /dev/sda2

brw-rw----. 1 qemu qemu 8, 2 Aug 25 01:24 /dev/sda2

# ls -la /dev/sda3

brw-rw----. 1 qemu qemu 8, 3 Aug 24 04:49 /dev/sda3

11. [root@zhpeng ~]# egrep '8:5|8:6' /cgroup/devices/libvirt/qemu/kvm1/devices.list
b 8:5 rw
b 8:6 rw

Notes:
Comments:

		177827 	[Virtual Networks]PXE and TFTP support in virtual network 	gren 	None 	Auto 		Feature 	P1 	100 	Edit
Setup:

1. # yum -y install syslinux tftp-server tftp

    #cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot

2, dowload vmlinuz and initrd.img under the /var/lib/tftpboot folder

 example: http://download.englab.nay.redhat.com/pub/rhel/rel-eng/RHEL6.0-Beta1-4.1/6.0/Server/x86_64/os/images/pxeboot/

3, mkdir /var/lib/tftpboot/pxelinux.cfg

4, create a file name default under the pxelinux.cfg folder

5, edit the file with the contents

DISPLAY boot.txt

DEFAULT rhel6

LABEL rhel6
        kernel vmlinuz
        append initrd=initrd.img

PROMPT 1
TIMEOUT 0
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

1, create a new virtual network , the xml description is as follows

<network>
  <name>netboot</name>
  <bridge name="virbr1" stp='off' delay='1'/>
  <forward/>
  <ip address="192.168.10.1" netmask="255.255.255.0">
    <tftp root="/var/lib/tftpboot" />
    <dhcp>
      <range start="192.168.10.2" end="192.168.10.254" />
      <bootp file="pxelinux.0" />
    </dhcp>
  </ip>
</network>

 # virsh net-define network.xml
Network netboot defined from network.xml

# virsh net-start netboot
Network netboot started

Create a guest using the network netboot.
	
Expected Results:

Verify the network could be created successfully.

 

the guest can create succesfully and network works well(ping google.com)
Notes:
Comments:

		176753 	[configuration] customize the libvirtd.conf 	dyuan 	None 	Manual 		Feature 	P1 	110 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration

bug:

    No bug found

Actions:

1. mv /etc/libvirt/libvirtd.conf /etc/libvirt/libvirtd_new.conf

2.# cat /etc/sysconfig/libvirtd
LIBVIRTD_CONFIG=/etc/libvirt/libvirtd_new.conf

3.service libvirtd restart

4. check the libvirtd  virsh and vm
	
Expected Results:

Step 3 and 4

all normal
Notes:
Comments:

		176963 	[Guest resource control] a <serial> port pointing at a host serial port 	xhu 	None 	Auto 		--default-- 	P2 	110 	Edit
Setup:

Firstly, you must run "56554 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. install a guest using virsh or virt-install or virt-manager
2. shut down the guest after installing completely
3. edit guest XML config and let guest serial port point at a host serial port
   # virsh dumpxml rhel6-demo
   ......
    <serial type='dev'>
      <source path='/dev/ttyS0'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
   ......
4. start the guest
   # virsh start rhel6-demo
5. get /dev/ttyS0 device major:minor number
   # ls -l /dev/ttyS0
6. check whether the above major:minor(4:64) exists in devices.list
   # cat /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list|grep "4:64"


Note: some other test scenarios need be added later, for example,
disk-hotplug/unhotplug + cgroup etc for 'devices' controller testing.
	
Expected Results:

1 the guest should be installed successfully

2 the guest is shutdown completely

3 xml is edited successfully

4 the guest is started successfully

5 # ls -l /dev/ttyS0
   crw-rw----. 1 qemu qemu 4, 64 Aug 22 13:28 /dev/ttyS0

6 # cat /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list|grep "4:64"
   c 4:64 rw
Notes:
Comments:

		177828 	[Scalability] Active-inactive virtual network loop for 1000 times 	jiachen 	None 	Auto 		Stress 	P2 	110 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    scalability

bug:

    No bug found

Actions:

# git clone git+ssh://git@qe-git.englab.nay.redhat.com/~/repo/libvirt/virsh-rail

# cd virsh-rail

# sh virsh-rail cases/qemu/stress/virtual-networks/loop_active_inactive_single_network_1000times.tc
	
Expected Results:

Check no FAIL in log/qemu/stress/virtual-networks/loop_active_inactive_single_network_1000times.log at beginning
Notes:
Comments:

		176754 	[configuration] domain resume when nfs with hard option is not available permanently/temporarily - bug 707202 	gren 	None 	Manual 		Regression 	P1 	120 	Edit
Setup:

1, setup a NFS server(eg: ip 10.66.5.5) and a shared folder.

# cat /etc/exports
/var/lib/libvirt/images        *(rw,no_root_squash)

# service nfs restart

# iptables -F

2, On the test machine, mount the shared folder with option "soft"   10.66.5.5:/var/lib/libvirt/images/ on /mnt/nfsdir type nfs (rw,hard,vers=4,addr=10.66.5.5,clientaddr=<IP>)

# mount -o hard 10.66.5.5:/var/lib/libvirt/images /mnt

# setsebool virt_use_nfs=1

Note: <If neither soft/hard option  is  specified  (or if the hard option is specified), NFS requests are retried indefinitely.  If the soft option is specified, then the NFS client  fails an NFS  request  after  retrans retransmissions have been sent, causing the NFS client to return an error to the calling application.>

<If the option timeo is not specified, requests are retried every 60 seconds for NFS over TCP, If the retrans option is not specified, the NFS client tries each  request 3 times.>

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    configuration

bug:

    No bug found

Actions:

1, On the test machine, install a guest with its disk image placed into the /mnt dir

# virsh list --all

 Id Name                 State
----------------------------------
  5 rhel6                running

# virsh dumpxml rhel6   // Make sure guest img file is in NFS

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='threads'/>
      <source file='/mnt/rhel6.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>

...

2, At the same time, on host use iptables to drop the packet into the NFS server

     iptables -A OUTPUT -d 10.66.5.5 -p tcp -j DROP

 

3. # virsh list --all

4.

4.1 conduct some operation in guest

4.2 # virsh save rhel6

 

5, The guest will not be unlocked until execute the following cmd to delete the rule:

# iptables -D OUTPUT -d 10.66.5.5 -p tcp --dport 2049 -j DROP
	
Expected Results:

 

step 3: The libvirtd will not hang

 Id Name                 State
----------------------------------
  5 rhel6                running

 

step 4:

4.1 the operation in guest will pause, such as scp guest big file to other host

4.2 the virsh save will hang until delete the rule

step 5:

the guest will be unlocked for the operation above.

1. The file can be sent successfully.

2. the guest will be saved automatically

 
Notes:
Comments:

		176964 	[Guest resource control] a guest qcow2 disk with a backing file pointing at a host device 	xhu 	None 	Auto 		--default-- 	P2 	120 	Edit
Setup:

Firstly, you must run "56554 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. choose a host disk partition such as /dev/sda
   # df -h
2. create a qcow2 format image and let it point at /dev/sda
   # qemu-img create -f qcow2 -o backing_file=/dev/sda rhel6.img
3. install a guest using virsh or virt-install or virt-manager and let guest
disk point at /var/lib/libvirt/images/rhel6-demo.qcow2, for example, the
following is guest XML config slice:
   # virsh dumpxml rhel6-demo
   ......
   <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    ......
4. get /dev/sda1 device major:minor number
   # ls -l /dev/sda
5. check whether the above major:minor(8:0) exists in devices.list

   # grep "8:0" /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list
   b 8:0 rw
or 
   # cgget -n -g devices libvirt/qemu/rhel6-demo|grep "8:0"
   devices.list=b 8:0 rw



Note: you don't need to wait the guest installation finish
	
Expected Results:

1 # df -h
   Filesystem            Size  Used Avail Use% Mounted on
   /dev/sda              49G   35G   12G  75% /
   tmpfs                 3.9G  452K  3.9G   1% /dev/shm

2 # qemu-img create -f qcow2 -o backing_file=/dev/sda
/var/lib/libvirt/images/rhel6-demo.qcow2
   Formatting '/var/lib/libvirt/images/rhel6-demo.qcow2', fmt=qcow2
size=52427751424 backing_file='/dev/sda' encryption=off cluster_size=0

3 the guest should be installed succesfully

4 # ls -l /dev/sda

brw-rw----. 1 root disk 8, 0 Aug 11 10:10 /dev/sda

5 # grep "8:0" /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list

   b 8:0 rw

or  

# cgget -n -g devices libvirt/qemu/rhel6-demo|grep "8:0"
   devices.list=b 8:0 rw

 
Notes:
Comments:

		176965 	[Guest resource control] a guest qcow2 disk with a backing file pointing at a host partition 	xhu 	None 	Auto 		--default-- 	P2 	130 	Edit
Setup:

Firstly, you must run "56554 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. choose a host disk partition such as /dev/sda1
   # df -h
2. create a qcow2 format image and let it point at /dev/sda1
   # qemu-img create -f qcow2 -o backing_file=/dev/sda1 /var/lib/libvirt/images/rhel6-demo.qcow2
3. install a guest using virsh or virt-install or virt-manager and let guest
disk point at /var/lib/libvirt/images/rhel6-demo.qcow2, for example, the
following is guest XML config slice:
   # virsh dumpxml rhel6-demo
   ......
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/rhel6-demo.qcow2'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>

    ......
4. get /dev/sda1 device major:minor number
   # ls -l /dev/sda1
5. check whether the above major:minor(8:1) exists in devices.list
   # cat /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list|grep "8:1"
or
   # cgget -n -g devices libvirt/qemu/rhel6-demo|grep "8:1"

Note: you don't need to wait the guest installation finish
	
Expected Results:

1 # df -h
   Filesystem            Size  Used Avail Use% Mounted on
   /dev/sda1              49G   35G   12G  75% /
   tmpfs                 3.9G  452K  3.9G   1% /dev/shm

2 # qemu-img create -f qcow2 -o backing_file=/dev/sda1 /var/lib/libvirt/images/rhel6-demo.qcow2
   Formatting '/var/lib/libvirt/images/rhel6-demo.qcow2', fmt=qcow2
size=52427751424 backing_file='/dev/sda1' encryption=off cluster_size=0

3 the guest should be installed succesfully

4 # ls -l /dev/sda1
   brw-rw----. 1 root disk 8, 1 Aug 11 10:10 /dev/sda1

5 # cat /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list|grep "8:1"
   b 8:1 rw
or
   # cgget -n -g devices libvirt/qemu/rhel6-demo|grep "8:1"
   devices.list=b 8:1 rw
Notes:
Comments:

		177829 	[Scalability] Define-undefine single virtual network loop for 1000 times 	jiachen 	None 	Auto 		Stress 	P2 	130 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    scalability

bug:

    No bug found

Actions:

# git clone git+ssh://git@qe-git.englab.nay.redhat.com/~/repo/libvirt/virsh-rail

# cd virsh-rail

# sh virsh-rail cases/qemu/stress/virtual-networks/loop_define_undefine_single_network_1000times.tc
	
Expected Results:

Check no FAIL in log/qemu/stress/virtual-networks/loop_define_undefine_single_network_1000times.log at beginning
Notes:
Comments:

		176966 	[Guest resource control] a guest raw disk with a backing file pointing at a host device 	xhu 	None 	Auto 		--default-- 	P2 	140 	Edit
Setup:

Firstly, you must run "56554 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. install a guest using virsh, virt-install or virt-manager

2. get disk partition
# df -h

3. using virsh edit to editor the following xml block to shutoff state guest
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/dev/sda'/>
      <target dev='hdb' bus='ide'/>
    </disk>

4. start the guest
# virsh start rhel6

5. get /dev/sda device major:minor number
# ls -la /dev/sda

6. check whether the above major:minor(8:0) exists in devices.list
   # grep "8:0" /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list
or 
   # cgget -n -g devices libvirt/qemu/rhel6-demo|grep "8:0"

	
Expected Results:

2 # df -h

Filesystem Size Used Avail Use% Mounted on

/dev/sda2 29G 7.6G 20G 28% /

tmpfs 982M 24K 982M 1% /dev/shm

/dev/sda1 194M 70M 115M 38% /boot

/dev/sda3 20G 230M 19G 2% /home

4# virsh start rhel6

Domain rhel6 started

5 # ls -la /dev/sda

brw-rw----. 1 qemu qemu 8, 0 Aug 12 00:42 /dev/sda

6 # grep "8:0" /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list

b 8:0 rw

or

# cgget -n -g devices libvirt/qemu/rhel6-demo|grep "8:0"

devices.list=b 8:0 rw
Notes:
Comments:

		176865 	[Docs] The "virt-mem" is removed from manual page - bug 639603 	ccui 	None 	Manual 		Regression 	P3 	150 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    docs
    Regression

bug:

    No bug found

Actions:

1.Check the manual page, the "virt-mem" has been removed.
  # man virsh

2.Unable to find the binary also.
# man 1 virt-mem
No entry for virt-mem in section 1 of the manual
# find / -name virt-mem
# locate virt-mem

	
Expected Results:

1. Output:

---------snip----from #man virsh-----------

LICENSE
       virsh is distributed under the terms of the GNU LGPL v2+.  This is free
software; see
       the source for copying conditions. There is NO warranty; not even for
MERCHANTABILITY
       or FITNESS FOR A PARTICULAR PURPOSE

SEE ALSO
       virt-install(1), virt-xml-validate(1), virt-top(1), virt-df(1),
       <http://www.libvirt.org/>

-----------/snip----------

Notes:
Comments:

		176967 	[Guest resource control] a guest raw disk with a backing file pointing at a host partition 	xhu 	None 	Auto 		--default-- 	P2 	150 	Edit
Setup:

Firstly, you must run "56554 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. install a guest using virsh, virt-install or virt-manager

2. get disk partition
# df -h

3. using virsh edit to editor the following xml block to shutoff state guest
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/dev/sda1'/>
      <target dev='hdb' bus='ide'/>
    </disk>

4. start the guest
# virsh start rhel6

5. get /dev/sda device major:minor number
# ls -la /dev/sda

6. check whether the above major:minor(8:1) exists in devices.list
# grep "8:1" /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list
b 8:1 rw
or 
# cgget -n -g devices libvirt/qemu/rhel6-demo|grep "8:1"
devices.list=b 8:1 rw

	
Expected Results:

2 # df -h

Filesystem Size Used Avail Use% Mounted on

/dev/sda2 29G 7.6G 20G 28% /

tmpfs 982M 24K 982M 1% /dev/shm

/dev/sda1 194M 70M 115M 38% /boot

/dev/sda3 20G 230M 19G 2% /home

4# virsh start rhel6

Domain rhel6 started

5 # ls -la /dev/sda1

brw-rw----. 1 qemu qemu 8, 0 Aug 12 00:42 /dev/sda1

6# grep "8:1" /cgroup/devices/libvirt/qemu/rhel6-demo/devices.list

b 8:1 rwm

or

# cgget -n -g devices libvirt/qemu/rhel6-demo|grep "8:1"

devices.list=b 8:1 rwm
Notes:
Comments:

		177830 	[volume wiping] the volume format of qcow2 is changed to raw after volume wiped 	whuang 	None 	Manual (Autoproposed) 		Function 	P1 	150 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Regression
    storage

bug:

    No bug found

Actions:

Steps to Reproduce:
# qemu-img info /var/lib/libvirt/images/rhel6.img 
image: /var/lib/libvirt/images/rhel6.img
file format: qcow2virtual size: 512K (524288 bytes)
disk size: 392K
cluster_size: 65536

  refresh pool

  # virsh pool-refresh default
  Pool default refreshed

# virsh vol-wipe rhel6.img --pool default
Vol rhel6.img wiped

# qemu-img info /var/lib/libvirt/images/rhel6.img 
image: /var/lib/libvirt/images/rhel6.img
file format: raw
virtual size: 512K (524288 bytes)
disk size: 0

	
Expected Results:

image's format will change 
Notes:
Comments:

		176868 	[Docs] virsh help: network can't be destroyed and dumped xml via id - bug 617439 	ccui 	None 	Manual 		Regression 	P3 	160 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    docs
    Regression

bug:

    No bug found

Actions:

1. # virsh help net-destroy

2. # virsh help net-dumpxml
	
Expected Results:

1. Output:

------------

 NAME
    net-destroy - destroy a network

  SYNOPSIS
    net-destroy <network>

  DESCRIPTION
    Destroy a given network.

  OPTIONS
    [--network] <string>  network name or uuid

-----------

 

2. Output:

-----------

NAME
    net-dumpxml - network information in XML

  SYNOPSIS
    net-dumpxml <network>

  DESCRIPTION
    Output the network information as an XML dump to stdout.

  OPTIONS
    [--network] <string>  network name or uuid

----------

 
Notes:
Comments:

		176970 	[Guest resource control] check whether libvirt default devices appear in devices.list 	xhu 	None 	Auto 		--default-- 	P1 	160 	Edit
Setup:

Firstly, you must run "56554 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. get default devices from qemu.conf
   #  cat /etc/libvirt/qemu.conf

2. get these devices major:minor number using ls -l, for instance:
   #ls -l /dev/ram3
   # ls -l /dev/kvm
   in addition, pty device's major:minor=136, * and sound device's
major:minor=116, *
3. check whether these device's major:minor exists in devices.list,
   assumes guest is rhel6-demo.qcow2
   # cat /cgroup/devices/libvirt/qemu/rhel6-demo.qcow2/devices.list
 
	
Expected Results:

1 #  cat /etc/libvirt/qemu.conf
   ......
   #cgroup_device_acl = [
   #    "/dev/null", "/dev/full", "/dev/zero",
   #    "/dev/random", "/dev/urandom",
   #    "/dev/ptmx", "/dev/kvm", "/dev/kqemu",
   #    "/dev/rtc", "/dev/hpet", "/dev/net/tun",
   #]
   ......

2 #ls -l /dev/ram3
   brw-rw----. 1 root disk 1, 3 Aug 11 10:10 /dev/ram3
   # ls -l /dev/kvm
   crw-rw-rw-+ 1 root kvm 10, 232 Aug 11 10:10 /dev/kvm
   .......

3   # cat /cgroup/devices/libvirt/qemu/rhel6-demo.qcow2/devices.list
   c 136:* rwm
   c 116:* rwm
   c 1:3 rwm
   c 1:7 rwm
   c 1:5 rwm
   c 1:8 rwm
   c 1:9 rwm
   c 5:2 rwm
   c 10:232 rwm
   c 254:0 rwm
   c 10:228 rwm
   c 10:200 rwm
Notes:
Comments:

		177831 	[Volume wiping] wipe logical volume in logical volume pool 	jyang 	None 	Auto 		Feature 	P2 	160 	Edit
Setup:

1. at least one available disk partitions. e.g. /dev/sda3

2. make sure the data of disk partitions are not used in future, all data will be erased.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. create a logical volume pool with xml similar with following:

      <pool type="logical">
        <name>lv_pool</name>
        <source>
          <device path="/dev/sda3"/>
          <device path="/dev/sdb4"/>
          <device path="/dev/sdc5"/>
        </source>
        <target>
          <path>/dev/lv_pool</path>
        </target>
      </pool>

  # virsh pool-define lv_pool.xml
 # virsh pool-build lv_pool 
 # virsh pool-start lv_pool

2. create a logical volume with xml similar with following:
   <volume>
  <name>lv_test</name>
  <key>r4xkCv-MQhr-WKIT-R66x-Epn2-e8hG-1Z5gY0</key>
  <source>
    <device path='/dev/sda3'>
    </device>
  </source>
  <capacity>2080374784</capacity>
  <allocation>2080374784</allocation>
  <target>
    <path>/dev/lv_pool/lv_test</path>
    <permissions>
      <mode>0660</mode>
      <owner>0</owner>
      <group>6</group>
      <label>system_u:object_r:fixed_disk_device_t:s0</label>
    </permissions>
  </target>
</volume>

# virsh vol-create lv_pool lv_test.xml


3. check if the volume exists
 # virsh vol-list lv_pool
Name                 Path                                    
-----------------------------------------
lv_test              /dev/lv_pool/lv_test

4. mount /dev/lv_pool/lv_test
# mkfs.ext3 /dev/lv_pool/lv_test
# mount /dev/lv_pool/lv_test /mnt

5. write some datas to /dev/lv_pool/lv_test
  # cd /mnt
  # for i in {1..100}; do touch "hello${i}"; done
  for i in {1..100};do echo "hello, girl" >> hello${i}; done

6. wipe
  # virsh vol-wipe ${lv_test_path}

7. check if it's empty
#cmp  ${lv_test_path}  /dev/zero
 
8. remount /dev/lv_pool/lv_test


	
Expected Results:

step 7:

#cmp lv_test /dev/zero

cmp: EOF on lv_test

step 8:

# mount /dev/lv_pool/lv_test /mnt/test/

mount: you must specify the filesystem type
Notes:
Comments:

		176971 	[Guest resource control] check whether libvirt group can successfully be created 	xhu 	None 	Auto 		--default-- 	P1 	170 	Edit
Setup:

In rhel6.2, cgroup filesystem will be mounted automatically.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Need to install libcgroup package and make sure libcgroup>=0.36.1-6(support lscgroup)
# yum install libcgroup* -y

2. mount cgroup filesystem

a. check whether cgroup filesystem has been mounted using lscgroup or cat
/proc/mounts|grep cgroup
# lscgroup
or
# cat /proc/mounts|grep cgroup

b. check cgroup configuration file
#cat /etc/cgconfig.conf|tail -11

c. if cgroup filesystem hasn't been mounted, mounts it

#service cgconfig start

OR
# cgconfigparser -l /etc/cgconfig.conf

Note that, you probably need to restart guest if your guest has ever been
controlled by cgroup.  

or manually mount cgroup filesystem

# mkdir /cgroup
# mount -t cgroup -o devices,cpu none /cgroup


3. check whether cgroup filesystem has been mounted
# lscgroup

or

# lssubsys

or
#ll /cgroup/

or
# cat /proc/mounts|grep cgroup

4. check whether libvirt group can successfully create in cgroup:

a. create libvirt group into cgroup
# service libvirtd restart

b. check whether libvirt group exists in cgroup
# lscgroup|grep libvirt

or
# find /cgroup/ -name libvirt

 

Note:

If you finished cgroup related test, you can issue the following commands to clean cgroup environment:. remove cgroup

#service cgconfig stop

Stopping cgconfig service:                                 [  OK  ]

or

# cgclear

or manually remove

b. check whether remove successfully cgroup filesystem including libvirt group

# lscgroup

cgroups can't be listed: Cgroup is not mounted

or

# lssubsys

or

#ll /cgroup/

total 0

or

# cat /proc/mounts|grep cgroup

For qemu driver, Libvirt only supports cpu and device controller now, for lxc driver, I need to add new test items in the future.
	
Expected Results:

1 install libcgroup* packets successfully

2

a. # lscgroup

cgroups can't be listed: Cgroup is not mounted  

OR

# cat /proc/mounts|grep cgroup

b. #cat /etc/cgconfig.conf|tail -11
mount {
 cpuset = /cgroup/cpuset;
 cpu = /cgroup/cpu;
 cpuacct = /cgroup/cpuacct;
 memory = /cgroup/memory;
 devices = /cgroup/devices;
 freezer = /cgroup/freezer;
 net_cls = /cgroup/net_cls;
 blkio = /cgroup/blkio;
}

c.

#service cgconfig start

Starting cgconfig service:                                 [  OK  ]

OR

# cgconfigparser -l /etc/cgconfig.conf

OR

# mkdir /cgroup
# mount -t cgroup -o devices,cpu none /cgroup

3.

# lscgroup
cpuset:/
cpu:/
cpuacct:/
memory:/
devices:/
freezer:/
net_cls:/
blkio:/

OR

# lssubsys
cpuset
cpu
cpuacct
memory
devices
freezer
net_cls
blkio

OR

#ll /cgroup/
total 0
drwxr-xr-x. 2 root root 0 Aug 16 01:27 blkio
drwxr-xr-x. 2 root root 0 Aug 16 01:27 cpu
drwxr-xr-x. 2 root root 0 Aug 16 01:27 cpuacct
drwxr-xr-x. 2 root root 0 Aug 16 01:27 cpuset
drwxr-xr-x. 2 root root 0 Aug 16 01:27 devices
drwxr-xr-x. 2 root root 0 Aug 16 01:27 freezer
drwxr-xr-x. 2 root root 0 Aug 16 01:27 memory
drwxr-xr-x. 2 root root 0 Aug 16 01:27 net_cls

OR

# cat /proc/mounts|grep cgroup
cgroup /cgroup/cpuset cgroup rw,relatime,cpuset 0 0
cgroup /cgroup/cpu cgroup rw,relatime,cpu 0 0
cgroup /cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0
cgroup /cgroup/memory cgroup rw,relatime,memory 0 0
cgroup /cgroup/devices cgroup rw,relatime,devices 0 0
cgroup /cgroup/freezer cgroup rw,relatime,freezer 0 0
cgroup /cgroup/net_cls cgroup rw,relatime,net_cls 0 0
cgroup /cgroup/blkio cgroup rw,relatime,blkio 0 0

4.

a. # service libvirtd restart

Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

b. # lscgroup|grep libvirt
cpuset:/libvirt
cpuset:/libvirt/lxc
cpuset:/libvirt/qemu
cpu:/libvirt
cpu:/libvirt/lxc
cpu:/libvirt/qemu
cpuacct:/libvirt
cpuacct:/libvirt/lxc
cpuacct:/libvirt/qemu
memory:/libvirt
memory:/libvirt/lxc
memory:/libvirt/qemu
devices:/libvirt
devices:/libvirt/lxc
devices:/libvirt/qemu
freezer:/libvirt
freezer:/libvirt/lxc
freezer:/libvirt/qemu

OR

# find /cgroup/ -name libvirt
/cgroup/cpuset/libvirt
/cgroup/freezer/libvirt
/cgroup/cpuacct/libvirt
/cgroup/devices/libvirt
/cgroup/memory/libvirt
/cgroup/cpu/libvirt
Notes:
Comments:

		177832 	[Volume wiping] wipe partition volume in disk pool - bug 738936 	jyang 	None 	Manual 		Feature 	P2 	170 	Edit
Setup:

1. In your host there should be another disk named sdb

# fdisk -l

Disk /dev/sdb: 21.5 GB, 21485322240 bytes
255 heads, 63 sectors/track, 2612 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x000b55f3

And there is no partition in it. If have, please remove it.

# fdisk /dev/sdb

WARNING: DOS-compatible mode is deprecated. It's strongly recommended to
         switch off the mode (command 'c') and change display units to
         sectors (command 'u').

Command (m for help): p

Disk /dev/sdb: 21.5 GB, 21485322240 bytes
255 heads, 63 sectors/track, 2612 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x000b55f3

   Device Boot      Start         End      Blocks   Id  System
/dev/sdb1               1          13      104391   83  Linux

Command (m for help): d
Selected partition 1

Command (m for help): p

Disk /dev/sdb: 21.5 GB, 21485322240 bytes
255 heads, 63 sectors/track, 2612 cylinders
Units = cylinders of 16065 * 512 = 8225280 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x000b55f3

   Device Boot      Start         End      Blocks   Id  System

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    upstream
    storage

bug:

    No bug found

Actions:

1. create a disk pool with xml similar with following:

      <pool type="disk">
        <name>dsk_pool</name>
        <source>
          <device path='/dev/sdb'/>
        </source>
        <target>
          <path>/dev</path>
        </target>
      </pool>

  # virsh pool-define dsk_pool.xml
  # virsh pool-start dsk_pool

2. create a partition volume in pool "dsk_pool" with xml similar with following:
  <volume>
  <name>sdb1</name>
  <key>/dev/sdb1</key>
  <source>
    <device path='/dev/sdb'>
      <extent start='32256' end='106928640'/>
    </device>
  </source>
  <capacity>106896384</capacity>
  <allocation>106896384</allocation>
  <target>
    <path>/dev/sdb1</path>
    <format type='none'/>
    <permissions>
      <mode>0660</mode>
      <owner>0</owner>
      <group>6</group>
      <label>system_u:object_r:fixed_disk_device_t:s0</label>
    </permissions>
  </target>
</volume>

# virsh vol-create dsk_pool sdb1.xml


3.  mount /dev/sdb1 
 # mkfs.ext3  /dev/sdb1
 # mount /dev/sdb1 /mnt

4. write datas into /dev/sdb1
 # cd /mnt
 # for i in {1..100}; do > $i; done

5. check if files are created in /dev/sdb1
 # ls /mnt

6. wipe 
 # virsh vol-wipe --pool dsk_pool sdb1

7. check if it's empty

 # ls /mnt



8. remount /dev/sdb1
 # umount /mnt
 # mount /dev/sdb1/ mnt

	
Expected Results:

step 7: Empty

8.

# mount /dev/sdb1 /mnt/
mount: you must specify the filesystem type

Notes:
Comments:

		176974 	[Guest resource control] Get memory parameter for a guest with option 	ajia 	ajia 	Auto 		Feature 	P2 	180 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:

1. memtune with --live, --config and --current options

# virsh memtune ${domain} ${option}

Note: option is --live, --config, --current

the mixture of --live, --config, --current options should not work since libvirt-0.9.8-1.el6.
	
Expected Results:

current guests status:

# virsh list --all
 Id Name                 State
----------------------------------
  7 vr-rhel6-x86_64-kvm  running
  - vr-rhel6u1-x86_64-kvm shut off

1. if domain is active for --live option, we expect can correct get memory parameter of

the guest, for instance:

# virsh memtune vr-rhel6-x86_64-kvm --live
hard_limit     : unlimited
soft_limit     : unlimited
swap_hard_limit: unlimited

2. if domain is inactive with --live option:

# virsh memtune vr-rhel6u1-x86_64-kvm --live
error: Unable to get number of memory parameters
error: Requested operation is not valid: domain is not running

3. if domain is active for --config option, we expect the following result:

# virsh memtune vr-rhel6-x86_64-kvm --config
hard_limit     : 0 kB
soft_limit     : 0 kB
swap_hard_limit: 0 kB

4. if domain is inactive for --config option, we expect the following result:

 # virsh memtune vr-rhel6u1-x86_64-kvm --config
hard_limit     : 0 kB
soft_limit     : 0 kB
swap_hard_limit: 0 kB

5. if domain is active for --current option, we expect can correct get memory parameter of

the guest, for instance:

# virsh memtune vr-rhel6-x86_64-kvm --current
hard_limit     : unlimited
soft_limit     : unlimited
swap_hard_limit: unlimited

6. if domain is inactive for --current option, we expect the following result:

 # virsh memtune vr-rhel6u1-x86_64-kvm --current
hard_limit     : 0 kB
soft_limit     : 0 kB
swap_hard_limit: 0 kB

7. mixture of --live, --config, --current options:

--current option is exclusive, if you use other option with --current, we expect the following result:

error: --current must be specified exclusively

If you mix --live and --config 

# virsh memtune $guest --live --config
 error: Unable to get number of memory parameters
 error: invalid argument: virDomainGetMemoryParameters

we  must given setting parameters like --hard-limit --soft-limit ....
#virsh memtune $guest  --live --config --hard-limit 100000
error: Unable to change memory parameters
error: Requested operation is not valid: domain is not running

#virsh memtune vm11 --live --config --hard-limit 100000
return none
Notes:
Comments:

		177833 	[Volume wiping] wipe volume(QCOW2) in directory pool 	jyang 	jyang 	Auto 		Feature 	P1 	180 	Edit
Setup:

Make sure you have a good guest image, which is in qcow2 format.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of directory type with the xml similar with following:

<pool type='dir'>
  <name>dir_pool</name>
  <uuid>70a7eb15-6c34-ee9c-bf57-69e8e5ff3fb2</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
      <label>some_label_t</label>
    </permissions>
  </target>
</pool>

  # virsh pool-define dir_pool.xml

  # virsh pool-start dir_pool

 

2. copy the the guest image to the location of pool "dir_pool". e.g

   # cp toy.img ${path_of_dir_pool}

 

3. refresh

   # virsh pool-refresh dir_pool

 

4. check if volume "toy.img" exists in pool "dir_pool"

  # virsh vol-list dir_pool

 

  NOTE: if this step is failed. skip following steps

 

5. wipe the volume

  # virsh vol-wipe --pool dir_pool toy.img

 

6. check

 

#cmp toy.img /dev/zero

OR

  # cat toy.img

 

  NOTE: it should be a long time.
	
Expected Results:

step 4:

        volume with name "toy.img" exists

 

step 6:

        cmp: EOF on toy.img

       OR

         # cat toy.img

        return no output. (cat will return no output if the file is filled with all zeros)

 
Notes:
Comments:

		176975 	[Guest resource control] Get memory parameter for a guest without option 	ajia 	None 	Manual (Autoproposed) 		Feature 	P2 	190 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:

1. get original memory parameter information
# # virsh memtune ${guestname}

2. if guest is active, check whether memory parameters (by virsh displays) are equal to corresponding  value in cgroup

# cgget -nr memory.limit_in_bytes libvirt/qemu/${guestname}

# cgget -nr memory.memsw.limit_in_bytes libvirt/qemu/${guestname}

# cgget -nr memory.soft_limit_in_bytes libvirt/qemu/${guestname}

OR

# cat /cgroup/memory/libvirt/qemu/${guestname}/${memory parameter}

Note: memory parameter may be the above 3 parameters
	
Expected Results:

1. for a active guest such as vr-rhel6-x86_64-kvm, we expect the following result:

# virsh memtune vr-rhel6-x86_64-kvm
hard_limit     : unlimited
soft_limit     : unlimited
swap_hard_limit: unlimited

check  memory parameters and expect the following result:

# cgget -nr memory.limit_in_bytes libvirt/qemu/vr-rhel6-x86_64-kvm
memory.limit_in_bytes: 9223372036854775807

# cgget -nr memory.memsw.limit_in_bytes libvirt/qemu/vr-rhel6-x86_64-kvm
memory.memsw.limit_in_bytes: 9223372036854775807

# cgget -nr memory.soft_limit_in_bytes libvirt/qemu/vr-rhel6-x86_64-kvm
memory.soft_limit_in_bytes: 9223372036854775807

Note: unlimited should be 9223372036854775807

2. for a inactive guest such as vr-rhel6u1-x86_64-kvm, we expect the following result:
# virsh memtune vr-rhel6u1-x86_64-kvm
hard_limit     : 0 kB
soft_limit     : 0 kB
swap_hard_limit: 0 kB
Notes:
Comments:

		177834 	[Volume wiping] wipe volume(QCOW2) in filesystem pool 	jyang 	jyang 	Auto 		Feature 	P2 	190 	Edit
Setup:

1. make sure you have a good guest image, which is in qcow2 format

 

2. a available disk partition. e.g. /dev/sda6
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of filesystem type with the xml similar with following:

<pool type='fs'>
  <name>fs_pool</name>
  <uuid>7641d5a8-af11-f730-a34e-0a7dfcede71f</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
    <device path='/dev/sda6'/>
    <format type='ext3'/>
  </source>
  <target>
    <path>/mnt</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

  # virsh pool-define fs_pool.xml

  # virsh pool-start fs_pool

 

2. copy the the guest image to the location of pool "fs_pool". e.g

   # cp toy.img /mnt

 

3. refresh pool

   # virsh pool-refresh fs_pool

 

4. check if volume "toy.img" exists in pool "fs_pool"

  # virsh vol-list fs_pool

 

  NOTE:  if this step is failed, skip following steps.

 

5. wipe the volume

  # virsh vol-wipe --pool fs_pool toy.img

 

6. check

#cmp toy.img /dev/zero

OR

  # cat toy.img

 

  NOTE: it should be a long time.
	
Expected Results:

step 4:

        volume with name "toy.img" exists

 

step 6:

       cmp: EOF on toy.img

        OR

       # cat toy.img

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		176976 	[Guest resource control] Get the current blkio parameters for a guest 	ajia 	None 	Auto 		Feature 	P2 	200 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:

0. start a guest.

1. get original weight value
# virsh blkiotune guestname


2. check whether weight(by virsh displays) is equal to blkio.weight value in
cgroup if this guestname is running.

# cgget -r blkio.weight libvirt/qemu/${guestname}

OR

# cat /cgroup/blkio/libvirt/qemu/${guestname}/blkio.weight
	
Expected Results:

1 # virsh blkiotune guestname
weight         : 500
device_weight  :

2 # cgget -nr blkio.weight libvirt/qemu/vr-rhel6-x86_64-kvm
blkio.weight: 500

OR

# cat /cgroup/blkio/libvirt/qemu/${guestname}/blkio.weight
500

And make sure step 1's weight value is the same to step 2's.
Notes:
Comments:

		177835 	[Volume wiping] wipe volume(QCOW2) in network filesystem pool 	jyang 	jyang 	Auto 		Feature 	P2 	200 	Edit
Setup:

1. make sure you have a good guest image, which is in QCOW2 format
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of netfs type with the xml similar with following:

<pool type='netfs'>
  <name>nfs_pool</name>
  <uuid>7641d5a8-af11-f730-a34e-0a7dfcede71f</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
    <host name='remote_host'/>
    <dir path='/var/lib/libvirt/images'/>
    <format type='nfs'/>
  </source>
  <target>
    <path>/mnt</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

  # virsh pool-define nfs_pool.xml

  # virsh pool-start nfs_pool

 

2. copy the the guest image to the location of pool "nfs_pool". e.g

   # cp toy.img ${path_of_nfs_pool}

 

3. refresh

   # virsh pool-refresh nfs_pool

 

4. check if volume "toy.img" exists in pool "nfs_pool"

  # virsh vol-list nfs_pool

 

  NOTE: if this step is failed. skip following steps

 

5. wipe the volume

  # virsh vol-wipe --pool nfs_pool toy.img

 

6. check

#cmp toy.img /dev/zero

OR

  # cat toy.img

 

  NOTE: it should be a long time.
	
Expected Results:

step 4:

        volume with name "toy.img" exists

 

step 6:

       #cmp toy.img /dev/zero

       cmp: EOF on toy.img

       OR

      # cat toy.img

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		176977 	[Guest resource control] Get the value of cpu_shares property of the guest 	xhu 	xhu 	Auto 		--default-- 	P2 	210 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    QE consumption
    virsh-rail

bug:

    No bug found

Actions:

1. get original cpu scheduler information
# virsh schedinfo rhel6


2. check whether cpu_shares(by virsh displays) is equal to cpu.shares value in
cgroup

# cd /cgroup/cpu
# cgget -n -r cpu.shares libvirt/qemu/rhel6

OR

# cat /cgroup/cpu/libvirt/qemu/rhel6/cpu.shares
	
Expected Results:

1 # virsh schedinfo rhel6
Scheduler      : posix
cpu_shares     : 1024
vcpu_period    : 100000
vcpu_quota     : -1

2 # cgget -n -r cpu.shares libvirt/qemu/rhel6
cpu.shares=1024

OR

# cat /cgroup/cpu/libvirt/qemu/rhel6/cpu.shares
1024
Notes:
Comments:

		177836 	[Volume wiping] wipe volume(RAW) in directory pool 	jyang 	jyang 	Auto 		Feature 	P1 	210 	Edit
Setup:

1. make sure you have a good guest image, which is in RAW format.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of directory type with the xml similar with following:

<pool type='dir'>
  <name>dir_pool</name>
  <uuid>70a7eb15-6c34-ee9c-bf57-69e8e5ff3fb2</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
      <label>some_label_t</label>
    </permissions>
  </target>
</pool>

  # virsh pool-define dir_pool.xml

  # virsh pool-start dir_pool

 

2. copy the the guest image to the location of pool "dir_pool". e.g

   # cp toy.img ${path_of_dir_pool}

 

3. refresh

   # virsh pool-refresh dir_pool

 

4. check if volume "toy.img" exists in pool "dir_pool"

  # virsh vol-list dir_pool

 

  NOTE: if this step is failed. skip following steps

 

5. wipe the volume

  # virsh vol-wipe --pool dir_pool toy.img

 

6. check

  #cmp toy.img /dev/zero

 OR

  # cat toy.img

 

  NOTE: it should be a long time.
	
Expected Results:

step 4:

        volume with name "toy.img" exists

 

step 6:

       #cmp toy.img /dev/zero

       cmp:EOF on toy.img

       OR

         # cat toy.img

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		176985 	[Guest resource control] Set memory parameter with min_guarantee option 	ajia 	None 	Manual 		Feature 	P2 	220 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:

1. set memory parameter with --min-guarantee option
# virsh memtune ${guestname} --min-guarantee ${number}, for instance:

# virsh memtune vr-rhel6-x86_64-kvm --min-guarantee 1024

# echo $?

Note: should cover active and inactive guest.

2. check return value and libvirtd.log
	
Expected Results:

for step1, expect result as follows:

error: Unable to change memory parameters
error: invalid argument in Memory tunable `min_guarantee' not implemented

and can get above error information by checking libvirtd.log

for step2, expect return value is 1.
Notes:
Comments:

		177837 	[Volume wiping] wipe volume(RAW) in directory pool by vol-key or vol-path 	kxiong 	None 	Manual 		--default-- 	P2 	220 	Edit
Setup:

1. setup a pool of directory type with the xml similar with following:
<pool type='dir'>
  <name>dir_pool</name>
  <uuid>70a7eb15-6c34-ee9c-bf57-69e8e5ff3fb2</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
      <label>some_label_t</label>
    </permissions>
  </target>
</pool>
  # virsh pool-define dir_pool.xml

  # virsh pool-start dir_pool
2. Create a raw vol
cat raw.xml
<volume>
  <name>raw.img</name>
  <source>
 </source>
  <capacity>22300000</capacity>
  <allocation>22300000</allocation>
  <target>
    <path>/var/lib/libvirt/images/raw.img</path>
<format type='raw'/>
  </target>
</volume>
#virsh vol-create --pool dir_pool raw.xml

3. refresh pool
   # virsh pool-refresh dir_pool

4. check if volume "raw.img" exists in pool "dir_pool"
  # virsh vol-list dir_pool

  NOTE:  if this step is failed, skip following steps.

5. check the real file size of sparse file again
   # du -s -B1 raw.img

6.Get the vol key or vol path
# virsh vol-key --pool dir_pool raw.img
# virsh vol-path --pool dir_pool raw.img

7. wipe the volume
  # virsh vol-wipe --pool dir_pool /var/lib/libvirt/images/raw.img

8. check the file contents of sparse file
 #cmp raw.img /dev/zero
 OR
  # cat raw.img

	
Breakdown:

Expected Results:

step 5:
      22302720    

step 6:

/var/lib/libvirt/images/raw.img


step 8:
         #cmp raw.img /dev/zero
          cmp:EOF on raw.img
       OR
          # cat raw.img
        return no output. (cat will return no output if the file is filled with all zeros)

	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		176986 	[Guest resource control] Set the current blkio parameters for a guest without any optional 	ajia 	ajia 	Auto 		Feature 	P2 	230 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:

1. get original weight value
# virsh blkiotune guestname

2. set a new weight value

# virsh blkiotune guestname --weight number

3. repeat 1

4. check whether latest weight(by virsh displays) is equal to blkio.weight value in
cgroup if this guestname is running.

# cgget -r blkio.weight libvirt/qemu/${guestname}

OR

# cat /cgroup/blkio/libvirt/qemu/${guestname}/blkio.weight
	
Expected Results:

weight value is the same to setting in virsh and cgroup/blkio/libvirt/qemu/${guestname}/blkio.weight.
Notes:
Comments:

		177838 	[Volume wiping] wipe volume(RAW) in filesystem pool 	jyang 	None 	Auto 		Feature 	P2 	230 	Edit
Setup:

1. make sure you have a good guest image, which is of raw format.

 

2. a available disk partition. e.g. /dev/sda6
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of filesystem type with the xml similar with following:

<pool type='fs'>
  <name>fs_pool</name>
  <uuid>7641d5a8-af11-f730-a34e-0a7dfcede71f</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
    <device path='/dev/sda6'/>
    <format type='ext3'/>
  </source>
  <target>
    <path>/mnt</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

  # virsh pool-define fs_pool.xml

  # virsh pool-start fs_pool

 

2. copy the the guest image to the location of pool "fs_pool". e.g

   # cp toy.img /mnt

 

3. refresh pool

   # virsh pool-refresh fs_pool

 

4. check if volume "toy.img" exists in pool "fs_pool"

  # virsh vol-list fs_pool

 

  NOTE:  if this step is failed, skip following steps.

 

5. wipe the volume

  # virsh vol-wipe --pool fs_pool toy.img

 

6. check

  #cmp toy.img /dev/zero

  OR

  # cat toy.img

 

  NOTE: it should be a long time.
	
Expected Results:

step 4:

        volume with name "toy.img" exists

 

step 6:

       #cmp toy.img /dev/zero

       cmp: EOF on toy.img

        OR

       # cat toy.img

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		176982 	[Guest resource control] hot-plug/unplug a qcow2 disk with a backing file pointing at a host device - Bug 876828 & 895906 	xhu 	None 	Auto 		--default-- 	P2 	240 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    876828 - From Run 50661
    895906 - From Run 54157

Actions:

1. install a guest using virsh, virt-install or virt-manager

2. get host disk infomation (such as /dev/sda)
# df -h

3. create a qcow2 format image and let it point at /dev/sda
# qemu-img create -f qcow2 /var/lib/libvirt/images/foo.qcow2 -o  backing_file=/dev/sda

4. check whether demo.qcow2 disk pointing at /dev/sda and disk format is qcow2
# qemu-img info /var/lib/libvirt/images/foo.qcow2

5. editor the following xml block to foo.xml
# cat foo.xml
 <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/foo.qcow2'/>
      <target dev='vdb' bus='virtio'/>
 </disk>

6. get the above disk /dev/sda major:minor number 
#ls -la /dev/sda

7. check whether the above major:minor(8:0) number exists in devices.list
# grep "8:0" /cgroup/devices/libvirt/qemu/demo/devices.list
#

8. attach the above foo.xml to the guest
# virsh attach-device demo foo.xml

9. check whether the above major:minor(8:0) number exists in devices.list
# grep "8:0" /cgroup/devices/libvirt/qemu/demo/devices.list

10. detach foo.xml from the guest
# virsh detach-device demo foo.xml

11. check whether the above major:minor(8:0) number exists in devices.list
# grep "8:0" /cgroup/devices/libvirt/qemu/demo/devices.list
#

12. attach the above foo.xml to the guest second time
# virsh attach-device demo foo.xml

13. attach the above foo.xml to the guest  third time
# virsh attach-device demo foo.xml

14.check whether the above major:minor(8:0) number exists in devices.list
# grep "8:0" /cgroup/devices/libvirt/qemu/demo/devices.list

 

 



	
Expected Results:

2 # df -h

Filesystem Size Used Avail Use% Mounted on

/dev/sda7 29G 9.8G 18G 36%

/tmpfs 3.7G 260K 3.7G 1% /dev/shm

/dev/sda6 77G 57G 17G 78% /mnt/vol

3 # qemu-img create -f qcow2 -o backing_file=/dev/sda

/var/lib/libvirt/images/foo.qcow2

Formatting '/var/lib/libvirt/images/foo.qcow2', fmt=qcow2 size=500107862016

backing_file='/dev/sda' encryption=off cluster_size=0

4 # qemu-img info /var/lib/libvirt/images/foo.qcow2

image: /var/lib/libvirt/images/foo.qcow2

file format: qcow2

virtual size: 466G (500107862016 bytes)

disk size: 144K

cluster_size: 65536

backing file: /dev/sda (actual path: /dev/sda)

6 #ls -la /dev/sda

brw-rw----. 1 qemu qemu 8, 0 Aug 25 10:40 /dev/sda

7 # grep "8:0" /cgroup/devices/libvirt/qemu/demo/devices.list

#

8 # virsh attach-device demo foo.xml

Device attached successfully

9 # grep "8:0" /cgroup/devices/libvirt/qemu/demo/devices.list

b 8:0 rw

10 # virsh detach-device demo foo.xml

Device detached successfully

11 # grep "8:0" /cgroup/devices/libvirt/qemu/demo/devices.list

#

The following record should be removed from devices.list.

b 8:0 rw    

12. # virsh attach-device demo foo.xml

Device attached successfully

13.# virsh attach-device demo foo.xml

error: Failed to attach device from foo.xml
error: operation failed: target vdb already exists

14.# grep "8:0" /cgroup/devices/libvirt/qemu/demo/devices.list

b 8:0 rw

The record should be exists in devices.list.

 
Notes:
Add bug 876828 and bug 895906 to summary to trace it by mitian
Comments:

		177839 	[Volume wiping] wipe volume(RAW) in network filesystem pool 	jyang 	jyang 	Auto 		Feature 	P2 	240 	Edit
Setup:

1. make sure you have a good guest image, which is in RAW format
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of netfs type with the xml similar with following:

<pool type='netfs'>
  <name>nfs_pool</name>
  <uuid>7641d5a8-af11-f730-a34e-0a7dfcede71f</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
    <host name='remote_host'/>
    <dir path='/var/lib/libvirt/images'/>
    <format type='nfs'/>
  </source>
  <target>
    <path>/mnt</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

  # virsh pool-define nfs_pool.xml

  # virsh pool-start nfs_pool

 

2. copy the the guest image to the location of pool "nfs_pool". e.g

   # cp toy.img ${path_of_nfs_pool}

 

3. refresh

   # virsh pool-refresh nfs_pool

 

4. check if volume "toy.img" exists in pool "nfs_pool"

  # virsh vol-list nfs_pool

 

  NOTE: if this step is failed. skip following steps

 

5. wipe the volume

  # virsh vol-wipe --pool nfs_pool toy.img

 

6. check

 #cmp toy.img /dev/zero

 OR

  # cat toy.img

 

  NOTE: it should be a long time.
	
Expected Results:

step 4:

        volume with name "toy.img" exists

 

step 6:

       #cmp toy.img /dev/zero

        cmp: EOF on toy.img

        OR

         # cat toy.img

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		176918 	[Graphical framebuffers] spice ssl connection 	vbian 	None 	Manual 		Feature 	P2 	250 	Edit
Setup:

NOTE:

passwdValidTo='2022-02-27T19:46:10' need be not overdue

 

 

 

[Prerequisite]
define the guest with the following xml file
<domain type='kvm' id='4'>
  <name>test</name>
  <uuid>1490cddf-cd72-c1a0-ac25-9a8ad0d81d56</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/cdrom_test.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:b5:d6:4c'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/6'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <serial type='vc'>
      <target port='1'/>
      <alias name='serial1'/>
    </serial>
    <console type='pty' tty='/dev/pts/6'>
      <source path='/dev/pts/6'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <channel type='pty'>
      <source path='/dev/pts/7'/>
      <target type='virtio' name='test.virtio.pty'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='0'/>
    </channel>
    <input type='mouse' bus='ps2'/>ï»¿

<graphics type='spice' port='5900' tlsPort='5901' autoport='yes' listen='0' keymap='en-us' passwd='redhat' passwdValidTo='2022-02-27T19:46:10' connected='disconnect'>
 <listen type='address' address='0'/>
 <channel name='main' mode='secure'/>
 <channel name='inputs' mode='secure'/>
</graphics>

 <channel type='spicevmc'>
 <target type='virtio' name='com.redhat.spice.0'/>
 <address type='virtio-serial' controller='0' bus='0' port='1'/>
 </channel>


    <sound model='ac97'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c360,c691</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c360,c691</imagelabel>
  </seclabel>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers
    QE consumption

bug:

    851397 - From Run 44420

Actions:

1. backup the sysconfig file
   # cp /etc/libvirt/qemu.conf /etc/sysconfig/qemu.conf.bk

2. modify the followings in qemu.conf
   -# spice_listen = "0.0.0.0"
   + spice_listen = "0.0.0.0"

   -# spice_tls = 1
   + spice_tls = 1

   -# spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"
   + spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"

3. perform the following script, to generate the cert files for ssl , and then copy *.pem file info /etc/pkil/libvirt-spice directory
#!/bin/bash

SERVER_KEY=server-key.pem

# creating a key for our ca
if [ ! -e ca-key.pem ]; then
    openssl genrsa -des3 -out ca-key.pem 1024
fi
# creating a ca
if [ ! -e ca-cert.pem ]; then
    openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem  -subj "/C=IL/L=Raanana/O=Red Hat/CN=my CA"
fi
# create server key
if [ ! -e $SERVER_KEY ]; then
    openssl genrsa -out $SERVER_KEY 1024
fi
# create a certificate signing request (csr)
if [ ! -e server-key.csr ]; then
    openssl req -new -key $SERVER_KEY -out server-key.csr -subj "/C=IL/L=Raanana/O=Red Hat/CN=my server"
fi
# signing our server certificate with this ca
if [ ! -e server-cert.pem ]; then
    openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem
fi

# now create a key that doesn't require a passphrase
openssl rsa -in $SERVER_KEY -out $SERVER_KEY.insecure
mv $SERVER_KEY $SERVER_KEY.secure
mv $SERVER_KEY.insecure $SERVER_KEY

# show the results (no other effect)
openssl rsa -noout -text -in $SERVER_KEY
openssl rsa -noout -text -in ca-key.pem
openssl req -noout -text -in server-key.csr
openssl x509 -noout -text -in server-cert.pem
openssl x509 -noout -text -in ca-cert.pem

# copy *.pem file to /etc/pki/libvirt-spice
if [[ -d "/etc/pki/libvirt-spice" ]]
then
    cp ./*.pem /etc/pki/libvirt-spice
else
    mkdir /etc/pki/libvirt-spice
        cp ./*.pem /etc/pki/libvirt-spice
fi

# echo --host-subject
echo "your --host-subject is" \" `openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "` \"

4. restart libvirtd to rescan the configuration
   # service libvirtd restart

5. define and start the guest
   # virsh define guest.xml
   # virsh start guest

6. access the spice interface with ssl connection

should test both two commands:

   # virt-viewer $guestname --spice-host-subject="C=IL,L=Raanana,O=Red Hat,CN=my server" --spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem'
  

   # remote-viewer spice://$ip/?port=5900\&tls-port=5901 --spice-host-subject='C=IL,L=Raanana,O=Red Hat,CN=my server' --spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem'

input password
   Note:
   10.66.93.134 --- your host IP
   5900 --- the port you defined in xml graphic section
   5901 --- the tlsPort you defined in xml graphic section
   redhat --- the passwd you set in the xml graphic section
   "C=IL,L=Raanana,O=Red Hat,CN=my server" --- is got from command #openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "

   Before we access the spice interface, it had better to dump the guest's xml to check the 'port' and 'tlsPort' again, for if there already a running guest which using spcie ï»¿graphic, these two port will changed acutomatically.
	
Expected Results:

6. you could see the spice interface successfully

Note:
if you want to access the spice on another remote machine, please scp -r /etc/pki/libvirt-spice root@$remote_machine first .Second , make the same configuration of qemu.conf with the spice server machine. Then perform the following command to access the spice graphic interface


 #virt-viewer $guestname --spice-host-subject="C=IL,L=Raanana,O=Red Hat,CN=my server" --spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem'


 #remote-viewer spice://$ip/?tls-port=5901 --spice-host-subject="C=IL,L=Raanana,O=Red Hat,CN=my server" --spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem'
Notes:
Comments:

		176983 	[Guest resource control] hot-plug/unplug a qcow2 disk with a backing file pointing at a host partition - Bug 876828 & 895906 	xhu 	None 	Auto 		--default-- 	P2 	250 	Edit
Setup:

Firstly, you must run "56554 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    virsh-rail

bug:

    876828 - From Run 50661
    895906 - From Run 54157

Actions:

1. install a guest using virsh, virt-install or virt-manager

2. get disk partition
# df -h

3. create a qcow2 format image and let it point at /dev/sda7
# qemu-img create -f qcow2 -o backing_file=/dev/sda7 /var/lib/libvirt/images/foo.qcow2

4. check whether foo.qcow2 disk pointing at /dev/sda7 and disk format is qcow2
# qemu-img info /var/lib/libvirt/images/foo.qcow2

5. editor the following xml block to foo.xml
# cat foo.xml
 <disk type='block' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source dev='/var/lib/libvirt/images/foo.qcow2'/>
      <target dev='vdb' bus='virtio'/>
 </disk>

6. get the above /dev/sda7 partition major:minor number 
# ls -la /dev/sda7

7. check whether the above major:minor(8:7) number exists in devices.list
# grep "8:7" /cgroup/devices/libvirt/qemu/demo/devices.list
#

8. attach the above foo.xml to the guest
# virsh attach-device demo foo.xml

9. check whether the above major:minor(8:7) number exists in devices.list
# grep "8:7" /cgroup/devices/libvirt/qemu/demo/devices.list

10. detach foo.xml from the guest
# virsh detach-device demo foo.xml

11. check whether the above major:minor(8:7) number exists in devices.list
# grep "8:7" /cgroup/devices/libvirt/qemu/demo/devices.list
#

12. attach the above foo.xml to the guest second time
# virsh attach-device demo foo.xml

13. attach the above foo.xml to the guest  third time
# virsh attach-device demo foo.xml

14.check whether the above major:minor(8:7) number exists in devices.list
# grep "8:7" /cgroup/devices/libvirt/qemu/demo/devices.list

 

	
Expected Results:

2 # df -h

Filesystem Size Used Avail Use% Mounted on

/dev/sda7 29G 9.8G 18G 36% /

tmpfs 3.7G 260K 3.7G 1% /dev/shm

/dev/sda6 77G 57G 17G 78% /mnt/vol

3 # qemu-img create -f qcow2 -o backing_file=/dev/sda7

/var/lib/libvirt/images/foo.qcow2

Formatting '/var/lib/libvirt/images/foo.qcow2', fmt=qcow2 size=31457280000

backing_file='/dev/sda7' encryption=off cluster_size=0

4 # qemu-img info /var/lib/libvirt/images/foo.qcow2

image: /var/lib/libvirt/images/foo.qcow2

file format: qcow2

virtual size: 29G (31457280000 bytes)

disk size: 140K

cluster_size: 65536

backing file: /dev/sda7 (actual path: /dev/sda7)

6 # ls -la /dev/sda7

brw-rw----. 1 root disk 8, 7 Aug 25 10:40 /dev/sda7

8 # virsh attach-device demo foo.xml

Device attached successfully

9 # grep "8:7" /cgroup/devices/libvirt/qemu/demo/devices.list

b 8:7 rw

10 # virsh detach-device demo foo.xml

Device detached successfully

11 #grep "8:7" /cgroup/devices/libvirt/qemu/demo/devices.list

#

The following record should be removed from devices.list.

b 8:7 rw

12. # virsh attach-device demo foo.xml

Device attached successfully

13.# virsh attach-device demo foo.xml

error: Failed to attach device from foo.xml
error: operation failed: target vdb already exists

14.# grep "8:7" /cgroup/devices/libvirt/qemu/demo/devices.list

b 8:7 rw

The record should be exists in devices.list.

 

 
Notes:
Add bug 876828 and bug 895906 to summary to trace it by mitian
Comments:

		177840 	[Volume wiping] wipe volume(sparse file) in directory pool 	jyang 	jyang 	Auto 		Feature 	P2 	250 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    storage

bug:

    No bug found

Actions:

1. setup a pool of directory type with the xml similar with following:

<pool type='dir'>
  <name>dir_pool</name>
  <uuid>70a7eb15-6c34-ee9c-bf57-69e8e5ff3fb2</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
      <label>some_label_t</label>
    </permissions>
  </target>
</pool>

  # virsh pool-define dir_pool.xml

  # virsh pool-start dir_pool

 

2. create a sparse file in pool "dir_pool". e.g.

   # dd if=/dev/zero of=sparse-file bs=1k count=0 seek=5120

 

3. check the real file size of sparse file

   # du -s -B1 sparse-file

 

4. refresh pool

   # virsh pool-refresh dir_pool

 

5. check if volume "sparse-file" exists in pool "dir_pool"

  # virsh vol-list dir_pool

 

  NOTE:  if this step is failed, skip following steps.

 

6. write datas to sparse file

   # for i in {1..1000}; do echo "hello, you sparse" >> sparse-file; done

 

7. check the real file size of sparse file again

   # du -s -B1 sparse-file

 

8. wipe the volume

  # virsh vol-wipe --pool dir_pool sparse-file

 

9. same as step 7

 

10. check the file contents of sparse file

 #cmp sparse-file /dev/zero

 OR

  # cat sparse-file
	
Expected Results:

step 3:

        zero

 

step 7:

        12288

 

step 9:

        zero

 

step 10:

         #cmp sparse-file /dev/zero

         cmp:EOF on sparse

        OR

          # cat sparse-file

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		176988 	[Guest resource control] Set the value of cpu_shares property of the guest 	xhu 	None 	Auto 		--default-- 	P2 	260 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0
    QE consumption
    virsh-rail

bug:

    No bug found

Actions:

1. check original value of cpu_shares
# cgget -n -r cpu.shares libvirt
or
#cat /cgroup/cpu/libvirt/cpu.shares

2. set value of cpu_shares, such as cpu_shares=2048
# virsh schedinfo rhel6 --set cpu_shares=2048

3. check whether cpu_shares(by virsh displays) is equal to cpu.shares value in
cgroup
# cgget -n -r cpu.shares libvirt/qemu/rhel6

or
# cat /cgroup/cpu/libvirt/qemu/rhel6/cpu.shares
	
Expected Results:

1 # cgget -n -r cpu.shares libvirt
cpu.shares=1024
or
#cat /cgroup/cpu/libvirt/cpu.shares
1024

2 # virsh schedinfo rhel6 --set cpu_shares=2048
Scheduler      : posix
cpu_shares     : 2048

3 # cgget -n -r cpu.shares libvirt/qemu/rhel6
cpu.shares=2048

or
# cat /cgroup/cpu/libvirt/qemu/rhel6/cpu.shares
2048
Notes:
Comments:

		177841 	[Volume wiping] wipe volume(sparse file) in directory pool by pool uuid 	kxiong 	None 	Auto 		Feature 	P2 	260 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of directory type with the xml similar with following:
<pool type='dir'>
  <name>dir_pool</name>
  <uuid>70a7eb15-6c34-ee9c-bf57-69e8e5ff3fb2</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
      <label>some_label_t</label>
    </permissions>
  </target>
</pool>
  # virsh pool-define dir_pool.xml

  # virsh pool-start fs_pool
2. create a sparse file in pool "dir_pool". e.g.
   # dd if=/dev/zero of=sparse-file bs=1k count=0 seek=5120

3. check the real file size of sparse file
   # du -s -B1 sparse-file

4. refresh pool
   # virsh pool-refresh dir_pool

5. check if volume "sparse-file" exists in pool "dir_pool"
  # virsh vol-list dir_pool

  NOTE:  if this step is failed, skip following steps.
6. write datas to sparse file
   # for i in {1..1000}; do echo "hello, you sparse" >> sparse-file; done

7. check the real file size of sparse file again
   # du -s -B1 sparse-file

8.Get the pool uuid
# virsh pool-dumpxml dir_pool
 <pool type='dir'>
  <name>dir_pool</name>
  <uuid>70a7eb15-6c34-ee9c-bf57-69e8e5ff3fb2</uuid>
  <capacity>51605344256</capacity>
  <allocation>41107943424</allocation>
  <available>10497400832</available>
  <source>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
      <label>some_label_t</label>
    </permissions>
  </target>
</pool>

9. wipe the volume
  # virsh vol-wipe --pool 70a7eb15-6c34-ee9c-bf57-69e8e5ff3fb2 sparse-file

10. same as step 7

11. check the file contents of sparse file
 #cmp sparse-file /dev/zero
 OR
  # cat sparse-file
	
Expected Results:

step 3:
        zero


step 7:
        12288


step 10:
        zero


step 11:

         #cmp sparse-file /dev/zero
          cmp:EOF on sparse
       OR
          # cat sparse-file
        return no output. (cat will return no output if the file is filled with all zeros)

Notes:
Comments:

		176968 	[guest resource control] bandwidth info in portgroup 	ydu 	None 	Manual (Autoproposed) 		Function 	P2 	270 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest resource control

bug:

    No bug found

Actions:

1. Define a net
# virsh net-list
Name                 State      Autostart
-----------------------------------------
default              active     yes       
net-portgroup        active     no 

# virsh net-dumpxml net-portgroup
<network>
  <name>net-portgroup</name>
  <uuid>5bb42a33-f3b1-03b6-8202-bb8d5d63f199</uuid>
  <forward mode='nat'/>
  <bridge name='virbr2' stp='on' delay='0' />
  <mac address='52:54:00:A5:69:F9'/>
  <ip address='192.168.120.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.120.2' end='192.168.120.254' />
    </dhcp>
  </ip>
  <portgroup name='engineering' default='yes'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='test'/>
    </virtualport>
    <bandwidth>
      <inbound average='1000' peak='5000' burst='5120'/>
      <outbound average='1000' peak='5000' burst='5120'/>
    </bandwidth>
  </portgroup>
  <portgroup name='sales'>
    <virtualport type='802.1Qbh'>
      <parameters profileid='salestest'/>
    </virtualport>
    <bandwidth>
      <inbound average='500' peak='2000' burst='2560'/>
      <outbound average='128' peak='256' burst='256'/>
    </bandwidth>
  </portgroup>
</network>

2. Edit the guest, replace 'default' interface element with 'net-portgroup', such as:
 <source network='net-portgroup'/>

3. start the guest
# virsh start vr-rhel5u4-x86_64-kvm
Domain vr-rhel5u4-x86_64-kvm started

4. check if the bandwidth parameters work well
# tc class show dev vnet0
class htb 1:1 root prio 0 rate 8000Kbit ceil 40000Kbit burst 5Mb cburst 1600b 

 
	
Expected Results:
Notes:
Comments:

		177842 	[Volume wiping] wipe volume(sparse file) in filesystem pool 	jyang 	jyang 	Auto 		Feature 	P2 	270 	Edit
Setup:

1. a available disk partition. e.g. /dev/sda6
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of filesystem type with the xml similar with following:

<pool type='fs'>
  <name>fs_pool</name>
  <uuid>7641d5a8-af11-f730-a34e-0a7dfcede71f</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
    <device path='/dev/sda6'/>
    <format type='ext3'/>
  </source>
  <target>
    <path>/mnt</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

  # virsh pool-define fs_pool.xml

  # virsh pool-start fs_pool

 

2. create a sparse file in pool "fs_pool". e.g.

   # dd if=/dev/zero of=sparse-file bs=1k count=0 seek=5120

 

3. check the real file size of sparse file

   # du -s -B1 sparse-file

 

4. refresh pool

   # virsh pool-refresh fs_pool

 

5. check if volume "sparse-file" exists in pool "fs_pool"

  # virsh vol-list fs_pool

 

  NOTE:  if this step is failed, skip following steps.

 

6. write datas to sparse file

   # for i in {1..1000}; do echo "hello, you sparse" >> sparse-file; done

 

7. check the real file size of sparse file again

   # du -s -B1 sparse-file

 

8. wipe the volume

  # virsh vol-wipe --pool fs_pool sparse-file

9. same as step 7

 

10. check the file contents of sparse file

 #cmp sparse-file /dev/zero

OR

  # cat sparse-file
	
Expected Results:

step 3:

        zero

 

step 7:

        20480

 

step 9:

        zero

 

step 10:

      #cmp sparse-file /dev/zero

      cmp: EOF on sparse-file

     OR

    # cat sparse-file

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		176972 	[Guest resource control] Control network bandwidth in guest xml 	ajia 	None 	Auto 		Feature 	P2 	280 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    guest resource control

bug:

    No bug found

Actions:

In the guest XML is allowed to add

     <bandwidth>
        <inbound average='1000' peak='5000' burst='5120'/>
        <outbound average='1000' peak='5000' burst='5120'/>
      </bandwidth>

Inside each <interface> element, for instance:

# virsh dumpxml dom
......
    <interface type='network'>
      <mac address='52:54:00:80:4d:ac'/>
      <source network='default' portgroup='engineering'/>
      <target dev='vnet0'/>
      <model type='rtl8139'/>
      <bandwidth>
        <inbound average='2000' peak='6000' burst='5120'/>
        <outbound average='2000' peak='6000' burst='5120'/>
      </bandwidth>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
......

then start the guest and use 'tc' command to check real result:

 # tc qdisc show dev $VETHNAME
 # tc class show dev $VETHNAME
 # tc filter show dev $VETHNAME

Notes, 'VETHNAME' is previous vnet0 in here.

 

 
	
Expected Results:

The following average and peak have KB unit, however, 'tc' uses Kb for them: 
 <inbound average='2000' peak='6000' burst='5120'/>

So you should see the following result by tc:

 # tc class show dev vnet0
class htb 1:1 root prio 0 rate 16000Kbit ceil 48000Kbit burst 5Mb cburst 1590b
Notes:
Comments:

		177843 	[Volume wiping] wipe volume(sparse file) in network filesystem pool 	jyang 	jyang 	Auto 		Feature 	P2 	280 	Edit
Setup:

1. a available disk partition. e.g. /dev/sda6
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of network filesystem type with the xml similar with following:

<pool type='netfs'>
  <name>nfs_pool</name>
  <uuid>7641d5a8-af11-f730-a34e-0a7dfcede71f</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
    <host name='remote_host'/>
    <dir path='/var/lib/libvirt/images'/>
    <format type='nfs'/>
  </source>
  <target>
    <path>/mnt</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

  # virsh pool-define nfs_pool.xml

  # virsh pool-start nfs_pool

 

2. create a sparse file in pool "nfs_pool". e.g.

   # dd if=/dev/zero of=sparse-file bs=1k count=0 seek=5120

 

3. check the real file size of sparse file

   # du -s -B1 sparse-file

 

4. refresh pool

   # virsh pool-refresh nfs_pool

 

5. check if volume "sparse-file" exists in pool "nfs_pool"

  # virsh vol-list nfs_pool

 

  NOTE:  if this step is failed, skip following steps.

 

6. write datas to sparse file

   # for i in {1..1000}; do echo "hello, you sparse" >> sparse-file; done

 

7. check the real file size of sparse file again

   # du -s -B1 sparse-file

 

8. wipe the volume

  # virsh vol-wipe --pool nfs_pool sparse-file

9. same as step 7

 

10. check the file contents of sparse file

  #cmp sparse-file /dev/zero

  OR

  # cat sparse-file
	
Expected Results:

step 3:

        zero

 

step 7:

        12288

 

step 9:

        zero

 

step 10:

    #cmp sparse-file /dev/zero

   cmp: EOF on sparse-file

    OR

   # cat sparse-file

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		176973 	[Guest resource control] Control network bandwidth in virtual network xml 	ajia 	None 	Auto 		Feature 	P2 	290 	Edit
Setup:

Firstly, you must run "95678 [Guest resource control] check whether libvirt group can successfully be created" .
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest resource control
    RHEL6.0

bug:

    No bug found

Actions:



then start the guest and use 'tc' command to check real result:

 # tc qdisc show dev $VETHNAME
 # tc class show dev $VETHNAME
 # tc filter show dev $VETHNAME

Notes, 'VETHNAME' is previous vnet0 in here.

 

The virtual network XML (virsh net-dumpxml net-portgroup) is allowed to have bandwidth data
added to it, inside <portgroup> elements. eg

<network>
 <name>net-portgroup</name>
 <uuid>5bb42a33-f3b1-03b6-8202-bb8d5d63f199</uuid>
 <forward mode='nat'/>
 <bridge name='virbr2' stp='on' delay='0' />
 <mac address='52:54:00:A5:69:F9'/>
 <ip address='192.168.120.1' netmask='255.255.255.0'>
 <dhcp>
 <range start='192.168.120.2' end='192.168.120.254' />
 </dhcp>
 </ip>
 <portgroup name='engineering' default='yes'>
 <virtualport type='802.1Qbh'>
 <parameters profileid='test'/>
 </virtualport>
 <bandwidth>
 <inbound average='1000' peak='5000' burst='5120'/>
 <outbound average='1000' peak='5000' burst='5120'/>
 </bandwidth>
 </portgroup>
</network>



a guest can refer to a portgroup

<interface type='network'>
     <mac address='52:54:00:a4:38:38'/>
    <source network='net-portgroup' portgroup='engineering'/>
</interface>

and thus inherit the bandwidth controls from that.
	
Expected Results:

So you should see the following result by tc:

# tc class show dev vnet0
class htb 1:1 root prio 0 rate 8000Kbit ceil 40000Kbit burst 5Mb cburst 1600b

Notes:
Comments:

		177844 	[Volume wiping] wipe volume(VMDK) in directory pool 	jyang 	jyang 	Auto 		Feature 	P2 	290 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of directory type with the xml similar with following:

<pool type='dir'>
  <name>dir_pool</name>
  <uuid>70a7eb15-6c34-ee9c-bf57-69e8e5ff3fb2</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
      <label>some_label_t</label>
    </permissions>
  </target>
</pool>

  # virsh pool-define dir_pool.xml

  # virsh pool-start dir_pool

 

2. create a volume of vmdk format in pool "dir_pool". e.g.

   # qemu-img create -f vmdk test.vmdk 1G

 

3. refresh

   # virsh pool-refresh dir_pool

 

4. check if volume "test.vmdk" exists in pool "dir_pool"

  # virsh vol-list dir_pool

 

  NOTE: if this step is failed. skip following steps

 

5. check the file size of test.vmdk

  # ls -lh test.vmdk

 

6. cat file contents of test.vmdk

  # cat test.vmdk

 

7. wipe the volume

  # virsh vol-wipe --pool dir_pool test.vmdk

 

8. same as step 5

 

9.

#cmp test.vmdk /dev/zero

OR

# cat test.vmdk
	
Expected Results:

step 5:

        192K

 

step 6:

        outputs texts

 step 8:

        192K

 step 9:

       #cmp test.vmdk /dev/zero

        cmp:EOF on test.vmdk

       OR

        # cat test.vmdk

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		177845 	[Volume wiping] wipe volume(VMDK) in filesystem pool 	jyang 	jyang 	Auto 		Feature 	P2 	300 	Edit
Setup:

1. An available partition with ext3 filesystem. e.g /dev/sda6
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of filesystem type with the xml similar with following:

<pool type='fs'>
  <name>fs_pool</name>
  <uuid>7641d5a8-af11-f730-a34e-0a7dfcede71f</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
    <device path='/dev/sda6'/>
    <format type='ext3'/>
  </source>
  <target>
    <path>/mnt</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

  # virsh pool-define fs_pool.xml

  # virsh pool-start fs_pool

 

2. create a volume of vmdk format in pool "fs_pool". e.g.

   # qemu-img create -f vmdk test.vmdk 1G

 

3. refresh

   # virsh pool-refresh fs_pool

 

4. check if volume "test.vmdk" exists in pool "fs_pool"

  # virsh vol-list fs_pool

 

  NOTE: if this step is failed. skip following steps

 

5. check the file size of test.vmdk

  # ls -lh test.vmdk

 

6. cat file contents of test.vmdk

  # cat test.vmdk

 

7. wipe the volume

  # virsh vol-wipe --pool fs_pool test.vmdk

 

8. same as step 5

 

9.

#cmp test.vmdk /dev/zero

OR

# cat test.vmdk
	
Expected Results:

step 5:

        192K

 

step 6:

        outputs texts

 

step 8:

        192K

 

step 9:

       #cmp test.vmdk /dev/zero

       cmp:EOF on test.vmdk

       OR

        # cat test.vmdk

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		177846 	[Volume wiping] wipe volume(VMDK) in network filesystem pool 	jyang 	jyang 	Auto 		Feature 	P2 	310 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    storage

bug:

    No bug found

Actions:

1. setup a pool of nfs type with the xml similar with following:

<pool type='netfs'>
  <name>nfs_pool</name>
  <uuid>7641d5a8-af11-f730-a34e-0a7dfcede71f</uuid>
  <capacity>0</capacity>
  <allocation>0</allocation>
  <available>0</available>
  <source>
    <host name='remote_host'/>
    <dir path='/var/lib/libvirt/images'/>
    <format type='nfs'/>
  </source>
  <target>
    <path>/mnt</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

  # virsh pool-define nfs_pool.xml

  # virsh pool-start nfs_pool

 

2. create a volume of vmdk format in pool "nfs_pool". e.g.

   # qemu-img create -f vmdk test.vmdk 1G

 

3. refresh

   # virsh pool-refresh nfs_pool

 

4. check if volume "test.vmdk" exists in pool "nfs_pool"

  # virsh vol-list nfs_pool

 

  NOTE: if this step is failed. skip following steps

 

5. check the file size of test.vmdk

  # ls -lh test.vmdk

 

6. cat file contents of test.vmdk

  # cat test.vmdk

 

7. wipe the volume

  # virsh vol-wipe --pool nfs_pool test.vmdk

 

8. same as step 5

 

9.

#cmp test.vmdk /dev/zero

OR

# cat test.vmdk
	
Expected Results:

step 5:

        192K

 

step 6:

        outputs texts

 

step 8:

        192K

 

step 9:

       #cmp test.vmdk /dev/zero

        cmp:EOF on test.vmdk

        OR

        # cat test.vmdk

        return no output. (cat will return no output if the file is filled with all zeros)
Notes:
Comments:

		177801 	[Virtual Networks] deactivate network 	gren 	gren 	Auto 		Feature 	P1 	330 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

1, define a virtual network using "virsh net-define"

<network>
    <name>definebr1</name>
    <uuid>fb09e249-7cce-252e-2417-c47d6d60b3d6</uuid>
    <forward/>
    <bridge name='definebr1' stp='on' forwardDelay='0' />
    <ip address='10.0.0.1' netmask='255.255.255.0'>
      <dhcp>
        <range start='10.0.0.2' end='10.0.0.254' />
      </dhcp>
    </ip>
  </network>

2, activate the newly-defined network by executing "virsh net-start definebr1"

3, # virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            active     no   

4, # virsh net-destroy definebr1
Network definebr1 destroyed
	
Expected Results:

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            inactive no
Notes:
Comments:

		177802 	[Virtual Networks] define network 	gren 	gren 	Auto 		Feature 	P1 	340 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

1, define a virtual network using "virsh net-define"

<network>
    <name>definebr1</name>
    <uuid>fb09e249-7cce-252e-2417-c47d6d60b3d6</uuid>
    <forward/>
    <bridge name='definebr1' stp='on' forwardDelay='0' />
    <ip address='10.0.0.1' netmask='255.255.255.0'>
      <dhcp>
        <range start='10.0.0.2' end='10.0.0.254' />
      </dhcp>
    </ip>
  </network>
	
Expected Results:

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            inactive no
Notes:
Comments:

		177803 	[Virtual Networks] destroy network 	gren 	gren 	Auto 		Feature 	P1 	350 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

1, get a network xml description file

<network>
    <name>definebr1</name>
    <uuid>fb09e249-7cce-252e-2417-c47d6d60b3d6</uuid>
    <forward/>
    <bridge name='definebr1' stp='on' forwardDelay='0' />
    <ip address='10.0.0.1' netmask='255.255.255.0'>
      <dhcp>
        <range start='10.0.0.2' end='10.0.0.254' />
      </dhcp>
    </ip>
  </network>

2, # virsh net-create br1.xml
Network definebr1 created from br1.xml

3, # virsh net-list
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            active     no   

4, # virsh net-destroy definebr1
Network definebr1 destroyed

5. check libvirtd.log.
	
Expected Results:

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes 

 

5. No such error loged:

------
2012-09-14 08:36:37.665+0000: 7575: info : libvirt version: 0.10.1, package: 2.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2012-09-13-00:00:13, x86-009.build.bos.redhat.com)
2012-09-14 08:36:37.665+0000: 7575: error : virCommandWait:2332 : internal error Child process (/sbin/tc qdisc del dev virbr0 root) unexpected exit status 2: RTNETLINK answers: No such file or directory

2012-09-14 08:36:37.670+0000: 7575: error : virCommandWait:2332 : internal error Child process (/sbin/tc qdisc del dev virbr0 ingress) unexpected exit status 2: RTNETLINK answers: Invalid argument
------

 
Notes:
Comments:

		177804 	[Virtual Networks] Enlarge the maximum number of DHCP leases to 253 in libvirt - Bug 524280 	yoyzhang 	yoyzhang 	Manual 		Regression 	P3 	360 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks

bug:

    No bug found

Actions:

1. # ps -aef|grep dnsmasq
	
Expected Results:

Output:

nobody    3516     1  0 Jan12 ?        00:00:00 /usr/sbin/dnsmasq --strict-order --bind-interfaces --pid-file=/var/run/libvirt/network/default.pid --conf-file=  --listen-address 192.168.124.1 --except-interface lo --dhcp-range 192.168.124.2,192.168.124.254 --dhcp-lease-max=253
Notes:
Comments:

		177805 	[Virtual Networks] IPv6 support in Isolated network 	xhu 	None 	Auto 		Feature 	P1 	370 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

1 define and start the isolated network using the following xml:
# cat isolated_net.xml
<network>
    <name>isolatedbr1</name>
        <bridge name='br1' stp='on' forwardDelay='0'/>
        <ip address='10.0.0.1' netmask='255.255.255.0'>
            <dhcp>
                    <range start='10.0.0.2' end='10.0.0.254'/>
                </dhcp>
        </ip>
        <ip family="ipv6" address="2001:db8:ac10:fe01::1" prefix="64">
        </ip>
</network>

# virsh net-define isolated_net.xml
Network isolatedbr1 defined from isolated_net.xml

# virsh net-start isolatedbr1
Network isolatedbr1 started

2 check iptables and ip6tables rules:


3 define and start two guest using isolatedbr1 network:
# virsh dumpxml test1
...
    <interface type='network'>
      <mac address='52:54:00:96:55:51'/>
      <source network='isolatedbr1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
...

# virsh dumpxml test2
...
    <interface type='network'>
      <mac address='52:54:00:f3:32:4d'/>
      <source network='isolatedbr1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
...

4 log into guest test1 and test2, config their ipv6 global addr and ping each other:
log into guest test1
# ifconfig eth0 inet6 add 2001:db8:ac10:fe01::2/64

log into guest test2
# ifconfig eth0 inet6 add 2001:db8:ac10:fe01::3/64


ping guest test2 ipv6 global addr in test1

ping guest test1 ipv6 global addr in test2

ping ipv6 isolate network addr 2001:db8:ac10:fe01::1 in test1

ping ipv6 isolate network addr 2001:db8:ac10:fe01::1 in test2
	
Expected Results:

2 # iptables -vnL FORWARD
Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 ACCEPT     all  --  br1    br1     0.0.0.0/0            0.0.0.0/0           
    0     0 REJECT     all  --  *      br1     0.0.0.0/0            0.0.0.0/0           reject-with icmp-port-unreachable
    0     0 REJECT     all  --  br1    *       0.0.0.0/0            0.0.0.0/0           reject-with icmp-port-unreachable

# ip6tables -vnL FORWARD
Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 ACCEPT     all      br1    br1     ::/0                 ::/0                
    0     0 REJECT     all      *      br1     ::/0                 ::/0                reject-with icmp6-port-unreachable
    0     0 REJECT     all      br1    *       ::/0                 ::/0                reject-with icmp6-port-unreachable

4 ping guest test2 ipv6 global addr in test1:
# ping6 2001:db8:ac10:fe01::3
PING 2001:db8:ac10:fe01::3(2001:db8:ac10:fe01::3) 56 data bytes
64 bytes from 2001:db8:ac10:fe01::3: icmp_seq=1 ttl=64 time=1.25 ms
64 bytes from 2001:db8:ac10:fe01::3: icmp_seq=2 ttl=64 time=0.213 ms
64 bytes from 2001:db8:ac10:fe01::3: icmp_seq=3 ttl=64 time=0.220 ms

ping guest test1 ipv6 global addr in test2:
# ping6 2001:db8:ac10:fe01::2
PING 2001:db8:ac10:fe01::2(2001:db8:ac10:fe01::2) 56 data bytes
64 bytes from 2001:db8:ac10:fe01::2: icmp_seq=1 ttl=64 time=1.29 ms
64 bytes from 2001:db8:ac10:fe01::2: icmp_seq=2 ttl=64 time=0.206 ms
64 bytes from 2001:db8:ac10:fe01::2: icmp_seq=3 ttl=64 time=0.223 ms

 ping ipv6 isolate network addr 2001:db8:ac10:fe01::1 in test1:
PING 2001:db8:ac10:fe01::1(2001:db8:ac10:fe01::1) 56 data bytes
64 bytes from 2001:db8:ac10:fe01::1: icmp_seq=1 ttl=64 time=1.27 ms
64 bytes from 2001:db8:ac10:fe01::1: icmp_seq=2 ttl=64 time=0.203 ms
64 bytes from 2001:db8:ac10:fe01::1: icmp_seq=3 ttl=64 time=0.213 ms

ping ipv6 isolate network addr 2001:db8:ac10:fe01::1 in test2:
PING 2001:db8:ac10:fe01::1(2001:db8:ac10:fe01::1) 56 data bytes
64 bytes from 2001:db8:ac10:fe01::1: icmp_seq=1 ttl=64 time=1.36 ms
64 bytes from 2001:db8:ac10:fe01::1: icmp_seq=2 ttl=64 time=0.211 ms
64 bytes from 2001:db8:ac10:fe01::1: icmp_seq=3 ttl=64 time=0.221 ms
Notes:
Comments:

		177806 	[Virtual Networks] IPv6 support in NAT virtual network 	xhu 	None 	Auto 		Feature 	P1 	380 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

1 define and start the nat network using the following xml:
# cat nat_net.xml
<network>
        <name>natbr1</name>
        <forward mode='nat'/>
        <bridge name='natbr1' stp='on' forwardDelay='0'/>
        <ip address='100.0.0.1' netmask='255.255.255.0'>
              <dhcp>
                      <range start='100.0.0.2' end='100.0.0.254'/>
              </dhcp>
        </ip>
        <ip family="ipv6" address="2002:db8:ac10:fe01::1" prefix="64">
        </ip>
</network>

# virsh net-define nat_net.xml
Network natbr1 defined from nat_net.xml

# virsh net-start natbr1
Network natbr1 started

2 check iptables and ip6tables rules:


3 define and start two guest using natbr1 network:

# virsh dumpxml test1
...
    <interface type='network'>
      <mac address='52:54:00:96:55:51'/>
      <source network='natbr1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
...

# virsh dumpxml test2
...
    <interface type='network'>
      <mac address='52:54:00:f3:32:4d'/>
      <source network='natbr1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
...

4 log into guest test1 and test2, config their ipv6 global addr and ping each other:
log into guest test1
# ifconfig eth0 inet6 add 2002:db8:ac10:fe01::2/64

log into guest test2
# ifconfig eth0 inet6 add 2002:db8:ac10:fe01::3/64

ping guest test2 ipv6 global addr in test1:
# ping6 2002:db8:ac10:fe01::3

ping guest test1 ipv6 global addr in test2:
# ping6 2002:db8:ac10:fe01::2
	
Expected Results:

2 # iptables -vnL FORWARD
Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 ACCEPT     all  --  *      natbr1  0.0.0.0/0            100.0.0.0/24        state RELATED,ESTABLISHED
    0     0 ACCEPT     all  --  natbr1 *       100.0.0.0/24         0.0.0.0/0           
    0     0 ACCEPT     all  --  natbr1 natbr1  0.0.0.0/0            0.0.0.0/0           
    0     0 REJECT     all  --  *      natbr1  0.0.0.0/0            0.0.0.0/0           reject-with icmp-port-unreachable
    0     0 REJECT     all  --  natbr1 *       0.0.0.0/0            0.0.0.0/0           reject-with icmp-port-unreachable

# ip6tables -vnL FORWARD
Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination         
    0     0 ACCEPT     all      *      natbr1  ::/0                 2002:db8:ac10:fe01::/64
    0     0 ACCEPT     all      natbr1 *       2002:db8:ac10:fe01::/64  ::/0                
    0     0 ACCEPT     all      natbr1 natbr1  ::/0                 ::/0                
    0     0 REJECT     all      *      natbr1  ::/0                 ::/0                reject-with icmp6-port-unreachable
    0     0 REJECT     all      natbr1 *       ::/0                 ::/0                reject-with icmp6-port-unreachable
   47  3760 ACCEPT     all      br1    br1     ::/0                 ::/0                
    0     0 REJECT     all      *      br1     ::/0                 ::/0                reject-with icmp6-port-unreachable
    0     0 REJECT     all      br1    *       ::/0                 ::/0                reject-with icmp6-port-unreachable
    0     0 REJECT     all      *      *       ::/0                 ::/0                reject-with icmp6-adm-prohibited

 

4

ping guest test2 ipv6 global addr in test1:
# ping6 2002:db8:ac10:fe01::3
Expect to be successful

ping guest test1 ipv6 global addr in test2:
# ping6 2002:db8:ac10:fe01::2
Expect to be successful
Notes:
Comments:

		177807 	[Virtual Networks] IPv6 support in Shared physical network 	xhu 	None 	Auto 		Feature 	P1 	390 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1  Open the network script for the device you are adding to
the bridge. In this example, ifcfg-eth0 defines the physical
network interface which is set as part of a bridge:

DEVICE=eth0
# change the hardware address to match the hardware address your NIC uses
HWADDR=00:16:76:D6:C9:45
ONBOOT=yes
BRIDGE=phsicalbr0


2 Create a new network script in the /etc/sysconfig/network-scripts
directory called ifcfg-phsicalbr0 or similar. The phsicalbr0 is the name of the bridge,
this can be anything as long as the name of the file is the same as
the DEVICE parameter.

DEVICE=phsicalbr0
TYPE=Bridge
BOOTPROTO=dhcp
ONBOOT=yes
DELAY=0

IPV6ADDR=2004:db8:ac10:fe01::1/64
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6INIT="yes"
IPV6_AUTOCONF=no

3  After configuring, restart networking or reboot.


# service NetworkManager stop
# service network restart

Configure iptables to allow all traffic to be forwarded across the bridge.

# iptables -I FORWARD -m physdev --physdev-is-bridged -j ACCEPT
# service iptables save
# service iptables restart

4 Disable iptables on bridges

net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0

# sysctl -p /etc/sysctl.conf


5 Restart the libvirt daemon.

# service libvirtd reload

6 You should now have a "shared physical device", which guests
can be attached and have full LAN access. Verify your new bridge:

# brctl show
bridge name     bridge id               STP enabled     interfaces
virbr0          8000.000000000000       yes
phsicalbr0      8000.000e0cb30550       no              eth0
 

7 edit guest xml to set type='bridge' and source bridge='phsicalbr0'

#virsh shutdown vm

#virsh edit vm

edit interface partition:

<interface type='bridge'>

<source bridge='phsicalbr0'/>

#virsh start vm

8 log into guest, config its ipv6 global addr and ping gateway:
#ifconfig eth0 inet6 add 2004:db8:ac10:fe01::2/64
ping gateway
# ping6 2004:db8:ac10:fe01::1
	
Expected Results:

8

ping gateway
# ping6 2004:db8:ac10:fe01::1
Expect to be successful
Notes:
Comments:

		177808 	[Virtual Networks] Isolated virtual network 	gren 	yoyzhang 	Auto 		Feature 	P2 	400 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

The test case checks that the bridge have no function of forwarding packets.
It only provides that a completely isolated private network for guests
cannot reach any other machines on the LAN.

As the virtual bridge is for isolated private network, so all
virtual machine attached to it could NOT reach public through the
physical network interface eth0.

hostA: 10.66.70.49 (dynamic)
virtual bridge br1:192.168.144.1
guestA: 192.168.144.2 (dynamic)

1 Define and start the virtual bridge br1

The virtual bridge XML format description:

         <network>
             <name>br1</name>
             <bridge name='br1' stp='on' forwardDelay='0'/>
             <ip address='192.168.144.1' netmask='255.255.255.0'>
                 <dhcp>
                      <range start='192.168.144.2' end='192.168.144.254'/>
                 </dhcp>
             </ip>
         </network>
2 Need to ensure the guestA has a right network interface setting:

......
        <interface type='bridge'>
           <source bridge='br1'/>
        </interface>
......

3 After installation of guestA, we show the route table on hostA:

$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
10.66.70.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0
192.168.144.0        0.0.0.0         255.255.255.0   U     0      0        0 br1
169.254.0.0     0.0.0.0         255.255.0.0     U     0      0        0 eth0
0.0.0.0         10.66.70.254    0.0.0.0         UG    0      0        0 eth0

4 Show the iptables chains and check rules on nat table:

$ iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination         

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination                  

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

5 For now, the guestA can only communicate with other guests which use this bridge(br1) and hostA(private network: br1), but could not reach outside
	
Expected Results:

1. All operations should complete successfully. Check dnsmasq option

# ps aux|grep dnsmasq

nobody   30549  0.0  0.0  12940   568 ?        S    02:48   0:00
/usr/sbin/dnsmasq --strict-order --bind-interfaces
--pid-file=/var/run/libvirt/network/br1.pid --conf-file= --except-interface lo
--dhcp-option=3 --no-resolv --listen-address 192.168.144.1 --dhcp-range
192.168.144.2,192.168.144.254


2. The vm could only reach host OS and guests but not others in LAN
Notes:
Comments:

		177809 	[Virtual networks] libvirtd in guest collides with default host virtual network routing - bug 594494 	eli 	None 	Manual 		Feature 	P3 	410 	Edit
Setup:

1. prepare a linux guest vm installed latest libvirt
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks

bug:

    No bug found

Actions:

1.install latest libvirt inside both host and guest

# rpm -qa | grep libvirt
libvirt-devel-0.8.1-8.el6.x86_64
libvirt-client-0.8.1-8.el6.x86_64
libvirt-0.8.1-8.el6.x86_64
libvirt-python-0.8.1-8.el6.x86_64
libvirt-debuginfo-0.8.1-8.el6.x86_64

2.start the guest vm and check libvirtd is running in both host and guest

# service libvirtd status
libvirtd (pid  1255) is running...

3. check the network connectivity

from host  # ping guest_ip

from guest #ping host_ip   #ping www.google.com

4. check the guest network list in libivrt

#virsh net-list --all

5. try to start default network in guest

#virsh net-start default
	
Expected Results:

3.  all pings work

4. # virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              inactive   yes

The default network in guest should be in inactive status, if not inactive default network manually.

#virsh net-destroy default

5. The collision is detected

# virsh net-start default
error: Failed to start network default
error: internal error Network 192.168.122.1/255.255.255.0 is already in use by interface eth0
Notes:
Comments:

		177810 	[Virtual networks] libvirtd in guest collides with default host virtual network routing - bug 594494 	eli 	eli 	Manual 		Feature 	P3 	420 	Edit
Setup:

 Prepare a linux guest vm installed latest libvirt
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    Regression

bug:

    No bug found

Actions:

1.install latest libvirt inside both host and guest

# rpm -qa | grep libvirt
libvirt-devel-0.8.1-8.el6.x86_64
libvirt-client-0.8.1-8.el6.x86_64
libvirt-0.8.1-8.el6.x86_64
libvirt-python-0.8.1-8.el6.x86_64
libvirt-debuginfo-0.8.1-8.el6.x86_64

2.start the guest vm and check libvirtd is running in both host and guest

# service libvirtd status
libvirtd (pid  1255) is running...

3. check the network connectivity

from host  # ping guest_ip

from guest #ping host_ip   #ping www.google.com

4. check the guest network list in libivrt

#virsh net-list --all

5. try to start default network in guest

#virsh net-start default
	
Expected Results:

3.  all pings work

4. # virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              inactive   yes

The default network in guest should be in inactive status, if not inactive default network manually.

#virsh net-destroy default

5. The collision is detected

# virsh net-start default
error: Failed to start network default
error: internal error Network 192.168.122.1/255.255.255.0 is already in use by interface eth0
Notes:
Comments:

		177811 	[Virtual Networks] migrate guest with the virtual network that doesn't exist on target host 	xhu 	None 	Auto 		--default-- 	P2 	430 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    RHEL6.0
    virsh-rail

bug:

    822052 - From Run 51204

Actions:

On both  migration source(eg. 10.66.93.12) and target(eg. 10.66.93.14) host

1 prepare migration environment by issuing the following commands:

#iptables -F

#mount -t nfs 10.66.90.113:/vol/xenimage /mnt

On   migration source(eg. 10.66.93.12) host

2 define and start network with the following testbr.xml;

<network>
  <name>testbr</name>
  <uuid>8da85d86-fbd9-c2a1-013b-f121e7c42c8a</uuid>
  <forward mode='nat'/>
  <bridge name='testbr' stp='on' delay='0' />
  <ip address='192.168.100.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.100.2' end='192.168.100.6' />
    </dhcp>
  </ip>
</network>

#virsh net-define testbr.xml

#virsh net-start testbr

3 define and start a guest with image on /mnt dir and replace the interface segment with the following xml

<interface type='network'>
      <mac address='52:54:00:c5:66:80'/>
      <source network='testbr'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>

4 issue the following command to migrateguest

# virsh migrate mig --live qemu+ssh://10.66.93.12/system
	
Expected Results:

4

# virsh migrate mig --live qemu+ssh://10.66.93.12/system
root@10.66.93.12's password:
error: Network not found: no network with matching name 'testbr'
Notes:
Comments:

		177812 	[virtual networks] Migrate with Bridged network, eth + bridge + tap BZ 851981 	xhu 	None 	Manual 		Feature 	P1 	440 	Edit
Setup:

1. A RHEL6 host with kvm, with a running domain.

2. Another kvm host, which also is a RHEL6.

3. On both source host, and target host, libvirtd is running.

4. On both source and target machine:
    #iptables -F
    #setenforce 1

   #setsebool virt_use_nfs  on


5. Setup nfs service on Source target machine
 
    5.1 add following line into "/etc/exports"

     /var/lib/libvirt/images 10.66.70.144(rw,no_root_squash,async) 127.0.0.1(rw,no_root_squash,async)

    replace "10.66.70.144" to your destinate host IP

    5.2 service nfs start

6. Mount the nfs filesystem on source host to  both destinate host and source host.
    6.1 on destination machine
     Create /var/lib/libvirt/migrate on destinate host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     # mount -t nfs ${source_host_ip}:/var/lib/libvirt/images/  /var/lib/libvirt/migrate/

    6.2 On source machine
     Create /var/lib/libvirt/migrate on source host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     mount localhost:/var/lib/libvirt/images /var/lib/libvirt/migrate


7. After step 6, make sure your migration domain's disk image locate in "/var/lib/libvirt/migrate/" (such as :/var/lib/libvirt/migrate/migrate.img)on source machine(when you migrate the guest during installation,the iso also should be locate in "/var/lib/libvirt/migrate").
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    Regression

bug:

    No bug found

Actions:

On both source and target host:

1. prepare the following nat network.xml:

# cat nat-network.xml

<network>

  <name>nat-network</name>

    <forward dev='eth0' mode='nat'>

      <interface dev='eth0'/>

    </forward>

   <bridge name='virbr1' stp='on' delay='0' />

     <ip address='192.168.100.1' netmask='255.255.255.0'>

     <dhcp>

        <range start='192.168.100.2' end='192.168.100.254' />

     </dhcp>

     </ip>

</network>

2. define and start nat-network:

# virsh net-define nat-network.xml 

# virsh net-start nat-network

 

On source  host:

3. start a guest with the following interface using nat-network:

<interface type='network'>
   <mac address='52:54:00:1b:6f:e5'/>
   <source network='nat-network'/>
   <target dev='vnet0'/>
   <model type='virtio'/>
   <alias name='net0'/>
   <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</interface>

4. migrate

# virsh migrate --live migrate qemu+ssh://${dest_host_ip}/system 

5. define a bridge network on both machines

6. defina and start a guest

7. # virsh dumpxml testkf8 ..

<interface type='network'>
  <mac address='52:54:00:1e:f2:53'/>
  <source network='bridge-net'/>
  <target dev='macvtap0'/>
  <model type='virtio'/>
  <alias name='net0'/>
   <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</interface> ..

8. migrate guest to dst machine.

# virsh migrate --live testkf8
 qemu+ssh://10.66.85.232/system --verbose

then migrate guest from dst machine to source machine back

9. check check the tap devcice lable

# ls -Z /dev/tap17ï»¿ 

	
Expected Results:

4. The guest should be migrated successfully and the network is ok:

- can get ip like 192.168.100.x

- can ping public network, host and other virtual host on the same network successfully

8. The guest can be migrated successfully in the two direction.

and network works well.

9. The lable was correct in the dev folder

# ls -Z /dev/tap17 
crw-rw----. root root system_u:object_r:tun_tap_device_t:s0:c723,c762 /dev/tap17
There was not any avc denied information in the audit log . 

 
Notes:
Comments:

		177010 	[Host network interface management] Define a bridge interface contains a bond which contains two Ethernet 	yoyzhang 	None 	Manual 		Feature 	P2 	450 	Edit
Setup:

A host with 2 ethernet, eth0 and eth1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

1. Prepare the following xml:

<interface type='bridge' name='br0'>
   <start mode='onboot'/>
   <protocol family='ipv4'>
    <dhcp/>
   </protocol>
  <bridge stp='off'>
  <interface type='bond' name='bond0'>
    <bond mode='active-backup'>
      <miimon freq='100' updelay='10' carrier='ioctl'/>
      <interface type='ethernet' name='eth0'>

      </interface>
       <interface type='ethernet' name='eth1'>
      </interface>
    </bond>
   </interface>
   </bridge>
</interface>

2. Define new br0

# virsh iface-define bridge0.xml

# virsh iface-start br0

3. Check the new br0

3.1 # virsh iface-dumpxml br0

3.2 # cat /etc/syconfig/network-script/ifcfg-br0

3.3 # ifconfig br0

3.4 # ping google.com -I br0
	
Expected Results:

3.

3.1 Output

<interface type='bridge' name='br0'>
  <protocol family='ipv4'>
    <ip address='10.66.93.39' prefix='23'/>
  </protocol>
  <protocol family='ipv6'>
    <ip address='fe80::21b:21ff:fe39:8b18' prefix='64'/>
  </protocol>
  <bridge>
    <interface type='bond' name='bond0'>
      <bond>
        <interface type='ethernet' name='eth0'>
          <mac address='00:1b:21:39:8b:18'/>
        </interface>
        <interface type='ethernet' name='eth1'>
          <mac address='00:1b:21:39:8b:18'/>
        </interface>
      </bond>
    </interface>
  </bridge>
</interface>


3.2
# cat /etc/sysconfig/network-scripts/ifcfg-br0
DEVICE=br0
ONBOOT=yes
TYPE=Bridge
BOOTPROTO=dhcp
STP=off

# cat /etc/sysconfig/network-scripts/ifcfg-bond0
DEVICE=bond0
ONBOOT=yes
BONDING_OPTS="'mode=active-backup primary=eth0 miimon=100 updelay=10 use_carrier=0'"
BRIDGE=br0

# cat /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0
ONBOOT=yes
MASTER=bond0
SLAVE=yes

# cat /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE=eth1
ONBOOT=yes
MASTER=bond0
SLAVE=yes

3.3 # ifconfig br0
br0       Link encap:Ethernet  HWaddr 00:1B:21:39:8B:18  
          inet addr:10.66.93.39  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: fe80::21b:21ff:fe39:8b18/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:188 errors:0 dropped:0 overruns:0 frame:0
          TX packets:30 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:17888 (17.4 KiB)  TX bytes:6549 (6.3 KiB)

3.4 # ping google.com -I br0
PING google.com (74.125.91.147) from 10.66.93.39 br0: 56(84) bytes of data.
64 bytes from qy-in-f147.1e100.net (74.125.91.147): icmp_seq=1 ttl=41 time=298 ms
64 bytes from qy-in-f147.1e100.net (74.125.91.147): icmp_seq=2 ttl=41 time=295 ms

Notes:
Comments:

		177813 	[virtual networks] Migrate with Bridged network, eth + macvtap + bridge 	xhu 	None 	Manual 		Function 	P1 	450 	Edit
Setup:

1. A RHEL6 host with kvm, with a running domain.

2. Another kvm host, which also is a RHEL6.

3. On both source host, and target host, libvirtd is running.

4. On both source and target machine:
    #iptables -F
    #setenforce 1

   #setsebool virt_use_nfs  on


5. Setup nfs service on Source target machine
 
    5.1 add following line into "/etc/exports"

     /var/lib/libvirt/images 10.66.70.144(rw,no_root_squash,async) 127.0.0.1(rw,no_root_squash,async)

    replace "10.66.70.144" to your destinate host IP

    5.2 service nfs start

6. Mount the nfs filesystem on source host to  both destinate host and source host.
    6.1 on destination machine
     Create /var/lib/libvirt/migrate on destinate host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     # mount -t nfs ${source_host_ip}:/var/lib/libvirt/images/  /var/lib/libvirt/migrate/

    6.2 On source machine
     Create /var/lib/libvirt/migrate on source host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     mount localhost:/var/lib/libvirt/images /var/lib/libvirt/migrate


7. After step 6, make sure your migration domain's disk image locate in "/var/lib/libvirt/migrate/" (such as :/var/lib/libvirt/migrate/migrate.img)on source machine(when you migrate the guest during installation,the iso also should be locate in "/var/lib/libvirt/migrate").
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    rhel6.5

bug:

    804951 - From Run 43699
    804951 - From Run 45753
    851981 - From Run 51204

Actions:

On both source and target host:

1. prepare the following bridge-network.xml:

# cat bridge-network.xml

<network>

  <name>bridge-net</name>

  <forward dev='eth0' mode='bridge'>

  <interface dev='eth0'/> </forward>

</network>

2. define and start bridge-net:

# virsh net-define bridge-network.xml

Network bridge-net defined from bridge-network.xml

 

On source  host:

3. start a guest with the following interface using bridge-net:

<interface type='network'>

  <mac address='52:54:00:1b:6f:e5'/>

  <source network='bridge-net'/>

  <target dev='vnet0'/>

  <model type='virtio'/>

  <alias name='net0'/>

  <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

</interface>

 4. migrate

# virsh migrate --live migrate qemu+ssh://${dest_host_ip}/system

 
	
Expected Results:

4 The guest should be migrated successfully and the network is ok:

 - get the ip from LAN

- can ping public network successfully
Notes:
Bug 804951 - Migrate with Bridged network, eth + macvtap + vepa, could not migrate back

This bug was moved to rhel6.5.0 for qemu-kvm, so no need to test it any more in rhel6.4.
Comments:

		177011 	[Host network interface management] Define a bridge interface contains a vlan which is using an Ethernet -878394 	yoyzhang 	None 	Manual 		Feature 	P2 	460 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    878394 - From Run 53754

Actions:

1. # cat br.xml
<interface type="bridge" name="br0">
  <start mode="onboot"/>
  <protocol family="ipv4">
     <ip address='192.168.177.1' prefix='24'/>
  </protocol>
  <bridge stp="off">
  <interface type="vlan" name="eth0.2">
   <vlan tag="2">
     <interface name="eth0"/>
   </vlan>
  </interface>
  </bridge>
</interface>

2. # virsh iface-define br.xml
Interface br0 defined from br.xml

# virsh iface-start br0
Interface br0 started

3 check the new eth0

3.1 # virsh iface-dumpxml br0
<interface type='bridge' name='br0'>
  <protocol family='ipv4'>
    <ip address='192.168.177.1' prefix='24'/>
  </protocol>
  <protocol family='ipv6'>
    <ip address='fe80::ea39:35ff:fe58:f481' prefix='64'/>
  </protocol>
  <bridge>
    <interface type='vlan' name='eth0.2'>
      <vlan tag='2'>
        <interface name='eth0'/>
      </vlan>
    </interface>
  </bridge>
</interface>

3.2 destroy the new br0 , then recheck it

#virsh iface-destroy br0

# virsh iface-list --all
Name                 State      MAC Address
--------------------------------------------
br0                  inactive  
eth0                active   00:25:64:A7:1F:4D
lo                    active     00:00:00:00:00:00

#virsh iface-dumpxml br0

 
	
Expected Results:

2. # virsh iface-list --all
Name                 State      MAC Address
--------------------------------------------
br0                  active     00:25:64:a7:1f:4d
eth0                 active     00:25:64:a7:1f:4d
lo                   active     00:00:00:00:00:00

# ifconfig
br0       Link encap:Ethernet  HWaddr 00:25:64:A7:1F:4D  
          inet addr:192.168.177.1  Bcast:192.168.177.255  Mask:255.255.255.0
          inet6 addr: fe80::225:64ff:fea7:1f4d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:48 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 b)  TX bytes:10628 (10.3 KiB)

eth0      Link encap:Ethernet  HWaddr 00:25:64:A7:1F:4D  
          inet addr:10.66.65.132  Bcast:10.66.65.255  Mask:255.255.254.0
          inet6 addr: fe80::225:64ff:fea7:1f4d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:608400 errors:0 dropped:0 overruns:0 frame:0
          TX packets:621067 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:573265622 (546.7 MiB)  TX bytes:406256396 (387.4 MiB)
          Interrupt:21 Memory:febe0000-fec00000

eth0.2    Link encap:Ethernet  HWaddr 00:25:64:A7:1F:4D  
          UP BROADCAST RUNNING PROMISC MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:38 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 b)  TX bytes:8412 (8.2 KiB)
3.2  should dump correctly and no reportting error ,like

# virsh iface-dumpxml br0
<interface type='ethernet' name='br0'>
</interface>

 
Notes:
Add the checkpoint 3,check the bridge interface's information while the bridge interface was in inactive state. --wangzhenfeng
Comments:

		177814 	[virtual networks] Migrate with Bridged network, eth + macvtap + vepa 	xhu 	None 	Manual 		Function 	P1 	460 	Edit
Setup:

1. A RHEL6 host with kvm, with a running domain.

2. Another kvm host, which also is a RHEL6.

3. On both source host, and target host, libvirtd is running.

4. On both source and target machine:
    #iptables -F
    #setenforce 1

   #setsebool virt_use_nfs  on


5. Setup nfs service on Source target machine
 
    5.1 add following line into "/etc/exports"

     /var/lib/libvirt/images 10.66.70.144(rw,no_root_squash,async) 127.0.0.1(rw,no_root_squash,async)

    replace "10.66.70.144" to your destinate host IP

    5.2 service nfs start

6. Mount the nfs filesystem on source host to  both destinate host and source host.
    6.1 on destination machine
     Create /var/lib/libvirt/migrate on destinate host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     # mount -t nfs ${source_host_ip}:/var/lib/libvirt/images/  /var/lib/libvirt/migrate/

    6.2 On source machine
     Create /var/lib/libvirt/migrate on source host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     mount localhost:/var/lib/libvirt/images /var/lib/libvirt/migrate


7. After step 6, make sure your migration domain's disk image locate in "/var/lib/libvirt/migrate/" (such as :/var/lib/libvirt/migrate/migrate.img)on source machine(when you migrate the guest during installation,the iso also should be locate in "/var/lib/libvirt/migrate").
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    rhel6.5

bug:

    804951 - From Run 43699
    804951 - From Run 45753
    851981 - From Run 46177
    804951 - From Run 51204

Actions:

On both source and target host:

1. prepare the following vepa-network xml:

# cat vepa-network.xml

<network>

  <name>vepa-net</name>

  <forward dev='eth0' mode='vepa'>

    <interface dev='eth0'/>

     <interface dev='eth1'/>

     <interface dev='eth2'/>

     <interface dev='eth3'/>

   </forward>

</network>

2. define and start vepa-network:

# virsh net-define vepa-network.xml

# virsh net-start vepa-net

 

On source  host:

3. start a guest with the following interface using vepa-net:

<interface type='network'>

  <mac address='52:54:00:1b:6f:e5'/>

  <source network='vepa-net'/>

  <target dev='vnet0'/>

  <model type='virtio'/>

  <alias name='net0'/>

  <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

</interface>

4. migrate

# virsh migrate --live migrate qemu+ssh://${dest_host_ip}/system

 

 
	
Expected Results:

4 The guest should be migrated successfully and the network is ok:

- get the ip from LAN

- can ping public network successfully
Notes:

Bug 804951 - Migrate with Bridged network, eth + macvtap + vepa, could not migrate back

This bug was moved to rhel6.5.0 for qemu-kvm, so no need to test it any more in rhel6.4.
Comments:

		177013 	[Host network interface management] Define a bridge using a not existing Ethernet interface 	yoyzhang 	None 	Manual 		Negative test 	P2 	470 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

1. Prepare a bridge xml using a not existing ethernet interface, such as eth1

# cat br.xml
<interface type="bridge" name="br0">
  <start mode="onboot"/>
  <mtu size="1500"/>
  <protocol family="ipv4">
    <dhcp/>
  </protocol>
  <bridge stp="off" delay="0.01">
    <interface type="ethernet" name="eth1">
    </interface>
  </bridge>
</interface>

2. Define the bridge

# virsh iface-define br.xml
Interface br0 defined from br.xml

3. # virsh iface-start br0

	
Expected Results:

3. Should fail to start the bridge

# virsh iface-start br0
error: Failed to start interface br0
error: internal error failed to create (start) interface br0: failed to execute external program - Running 'ifup eth1' failed with exit code 1: Device eth1 does not seem to be present, delaying initialization.


Notes:
Comments:

		177815 	[Virtual Networks] NAT virtual network 	gren 	yoyzhang 	Auto 		Feature 	P2 	470 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

The test case checks that the VMs within a NATed network can reach the
public network, VM could send packets to host and other machines within
the same network with the host. If the host could surf Internet, the
packet from the vm could be forwarded to Internet.

note that we also need to add the following to /etc/sysctl.conf

net.ipv4.ip_forward = 1

For the example, the physical network interface on hostA get a dynamic
IP address.

The table of NAT rules contains three lists called `chains':
each rule is examined in order until one matches.
The two chains are called PREROUTING (for Destination NAT, as packets first come in),
and POSTROUTING (for Source NAT, as packets leave).
The third (OUTPUT) will be ignored here.

hostA: 10.66.70.49 (dynamic)
virtual bridge br1:10.0.0.1
guestA: 10.0.0.2 (dynamic)

1 Define and start the virtual bridge br1

The virtual bridge XML format description:

         <network>
             <name>br1</name>
             <forward mode='nat'/>
             <bridge name='br1' stp='on' forwardDelay='0'/>
             <ip address='10.0.0.1' netmask='255.255.255.0'>
                 <dhcp>
                      <range start='10.0.0.2' end='10.0.0.254'/>
                 </dhcp>
             </ip>
         </network>
2 Need to ensure the guestA has a right network interface setting:

......
        <interface type='bridge'>
           <source bridge='br1'/>
           <mac address='00:16:3e:5d:c7:9e'/>
        </interface>
......

3 After installation of guestA, we show the route table on hostA:

$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
10.66.70.0      0.0.0.0         255.255.255.0   U     0      0        0 eth0
10.0.0.0        0.0.0.0         255.255.255.0   U     0      0        0 br1
169.254.0.0     0.0.0.0         255.255.0.0     U     0      0        0 eth0
0.0.0.0         10.66.70.254    0.0.0.0         UG    0      0        0 eth0

4 show the iptables chains and check rules on nat table:

$ iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination         

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination         
MASQUERADE  all  --  network/24          !network/24          

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

5 As of the setting by libvirt automaticly on iptables,
the gustA could reach Public network.
	
Expected Results:

1. All operations should complete successfully
2. The vm could reach public network through the NATed virtual network
Notes:
Comments:

		177014 	[Host network interface management] Define a bridge using an existing Ethernet interface which is not defined and up 	yoyzhang 	None 	Manual 		Feature 	P2 	480 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

1. Confirm eth1 is undefined and down, that is cannot list it out in #ifconfig

2. # cat br.xml
<interface type='bridge' name='br0'>
   <start mode='onboot'/>
   <mtu size='1500'/>
   <protocol family='ipv4'>
   <dhcp/>
   </protocol>
   <bridge stp='off' delay='0.01'>
   <interface type='ethernet' name='eth1'>
   </interface>
   </bridge>
</interface>

# virsh iface-define br.xml
Interface br0 defined from br.xml

# virsh iface-start br0
Interface br0 started

	
Expected Results:

2. # virsh iface-list --all
Name                 State      MAC Address
--------------------------------------------
br0                  active     00:1b:21:39:8b:19
eth0                 active     00:1b:21:39:8b:18
lo                   active     00:00:00:00:00:00

Verify that both br0 and eth1 is defined and up

# ifconfig
br0       Link encap:Ethernet  HWaddr 00:1B:21:39:8B:19  
          inet addr:10.66.93.63  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: fe80::21b:21ff:fe39:8b19/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:12 errors:0 dropped:0 overruns:0 frame:0
          TX packets:25 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:2219 (2.1 KiB)  TX bytes:5457 (5.3 KiB)

eth0      Link encap:Ethernet  HWaddr 00:1B:21:39:8B:18  
          inet addr:10.66.93.59  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: fe80::21b:21ff:fe39:8b18/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1518763 errors:0 dropped:0 overruns:0 frame:0
          TX packets:262217 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:1965192712 (1.8 GiB)  TX bytes:54810693 (52.2 MiB)
          Memory:f4800000-f4820000

eth1      Link encap:Ethernet  HWaddr 00:1B:21:39:8B:19  
          inet6 addr: fe80::21b:21ff:fe39:8b19/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:142284 errors:0 dropped:0 overruns:0 frame:0
          TX packets:9969 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:36484779 (34.7 MiB)  TX bytes:4898820 (4.6 MiB)
          Memory:f4820000-f4840000

# ping google.com -I br0
PING google.com (72.14.204.104) from 10.66.93.63 br0: 56(84) bytes of data.
64 bytes from iad04s01-in-f104.1e100.net (72.14.204.104): icmp_seq=1 ttl=43 time=351 ms
64 bytes from iad04s01-in-f104.1e100.net (72.14.204.104): icmp_seq=2 ttl=43 time=316 ms

.....
Notes:
Comments:

		177816 	[Virtual Networks] network autostart set 	gren 	gren 	Auto 		Feature 	P1 	480 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

1, define a virtual network using "virsh net-define"

<network>
    <name>definebr1</name>
    <uuid>fb09e249-7cce-252e-2417-c47d6d60b3d6</uuid>
    <forward/>
    <bridge name='definebr1' stp='on' forwardDelay='0' />
    <ip address='10.0.0.1' netmask='255.255.255.0'>
      <dhcp>
        <range start='10.0.0.2' end='10.0.0.254' />
      </dhcp>
    </ip>
  </network>

2, activate the newly-defined network by executing "virsh net-start definebr1"

3, # virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            active     no

4, # virsh net-autostart definebr1
Network definebr1 marked as autostarted

5. #virsh net-destroy definebr1

6. # virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1         inactive     yes

5, Restart libvirtd service to ensure the network with state 'active' and autostart "yes"
	
Expected Results:

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            active     yes
Notes:
Comments:

		177016 	[Host network interface management] Define an arp bond interface that contains two Ethernet -878394 	yoyzhang 	None 	Manual 		Feature 	P2 	490 	Edit
Setup:

A host with 2 ethernet, eth0 and eth1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    878394 - From Run 51207
    878394 - From Run 53754

Actions:

1. Prepare the following xml:

<interface type='bond' name='bond0'>
   <start mode='none'/>
   <protocol family='ipv4'>
    <dhcp/>
   </protocol>
  <bond mode='active-backup'>
    <arpmon interval='100' target='192.168.50.1' validate='active'/>

    <interface type='ethernet' name='eth0'>

     </interface>
    <interface type='ethernet' name='eth1'>
    </interface>
   </bond>
</interface>

2. Define new bond0

# virsh iface-define bond0.xml

# virsh iface-start bond0

3. Check the new bond0

3.1 # virsh iface-dumpxml bond0

3.2 # cat /etc/syconfig/network-script/ifcfg-bond0

3.3 # ifconfig bond0

3.4 # ping google.com -I bond0

3.5 # virsh iface-dumpxml bond0 --inactive

4.destroy the new bond0

4.1 # virsh iface-destroy bond0

Note: ollowing steps will be supported since 6.5.0:

4.2 # virsh iface-dumpxml bond0

 
	
Expected Results:

3.

3.1# virsh iface-dumpxml bond0
<interface type='bond' name='bond0'>
  <protocol family='ipv4'>
    <ip address='10.66.93.39' prefix='23'/>
  </protocol>
  <protocol family='ipv6'>
    <ip address='fe80::21b:21ff:fe39:8b18' prefix='64'/>
  </protocol>
  <bond>
    <interface type='ethernet' name='eth0'>
      <mac address='00:1b:21:39:8b:18'/>
    </interface>
    <interface type='ethernet' name='eth1'>
      <mac address='00:1b:21:39:8b:18'/>
    </interface>
  </bond>
</interface>

3.2 # cat /etc/sysconfig/network-scripts/ifcfg-bond0
DEVICE=bond0
ONBOOT=no
BOOTPROTO=dhcp
BONDING_OPTS="'mode=active-backup primary=eth0 arp_interval=100 arp_ip_target=192.168.50.1 arp_validate=active'"


3.3 # ifconfig bond0
bond0     Link encap:Ethernet  HWaddr 00:1B:21:39:8B:18  
          inet addr:10.66.93.39  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: fe80::21b:21ff:fe39:8b18/64 Scope:Link
          UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
          RX packets:4898 errors:0 dropped:0 overruns:0 frame:0
          TX packets:3891 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:1336526 (1.2 MiB)  TX bytes:379604 (370.7 KiB)

3.4 # ping google.com -I bond0
PING google.com (72.14.204.104) from 10.66.93.39 bond0: 56(84) bytes of data.
64 bytes from iad04s01-in-f104.1e100.net (72.14.204.104): icmp_seq=1 ttl=43 time=290 ms
64 bytes from iad04s01-in-f104.1e100.net (72.14.204.104): icmp_seq=2 ttl=43 time=291 ms

3.5 # virsh iface-dumpxml bond0 --inactive
<interface type='bond' name='bond0'>
  <start mode='none'/>
  <protocol family='ipv4'>
    <dhcp/>
  </protocol>
  <bond mode='active-backup'>
    <arpmon interval='100' target='192.168.50.1' validate='active'/>
    <interface type='ethernet' name='eth0'>
    </interface>
    <interface type='ethernet' name='eth1'>
    </interface>
  </bond>
</interface>

4

4.2 should output correctly

shouldn't report error like this

# virsh iface-dumpxml bond0
error: XML error: bond interface misses the bond element

 
Notes:
add a checkpoint 4,check the bond interface's information while the interface was in inactive state --wangzhenfeng
Comments:

		177817 	[Virtual Networks] nfs access from guest 	jialiu 	None 	Auto 		--default-- 	P1 	490 	Edit
Setup:

libvirt use NAT virtual network for guest's outside access.

Make sure the following iptable rules is shown:

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes

# iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination         

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination         
MASQUERADE  tcp  --  192.168.122.0/24    !192.168.122.0/24    masq ports: 1024-65535
MASQUERADE  udp  --  192.168.122.0/24    !192.168.122.0/24    masq ports: 1024-65535
MASQUERADE  all  --  192.168.122.0/24    !192.168.122.0/24    

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

 

There is a bug about this case: Bug 615144 - iptable rules generated by libvirt will deny nfs access from guest with NAT network.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare two machines, one is nfs server, another is a test machine on which guest is running.

2. On nfs server, share a directory.

# cat /etc/exports
/tmp    *(rw)

# service nfs start

# iptables -F

3. On test machine, make sure a guest with NAT virtual network is running

# virsh dumpxml <guestname>

...

    <interface type='network'>
      <mac address='52:54:00:f9:14:28'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </interface>

...

4. In the guest, run mount command.

# mount <nfs_server_IP>:/tmp /mnt

5. On nfs server, edit /etc/exports as following:

# cat /etc/exports
/tmp    *(rw,insecure)

# service nfs restart

6. In guest, re-mount the nfs share directory.

# mount <nfs_server_IP>:/tmp /mnt
	
Expected Results:

4. The following error message is seen:

mount.nfs: access denied by server while mounting 10.66.92.185:/tmp




6. Mount successfully.

Notes:
Comments:

		177018 	[Host network interface management] Define an ethernet with dhcp ipv4 	yoyzhang 	None 	Manual 		Feature 	P2 	500 	Edit
Setup:

1. save the config file of eth0

  # cp /etc/sysconfig/network-scripts/ifcfg-eth0 /tmp


2. Before destroy eth0, should stop NetworkManager

  #service NetworkManager stop


3. After the following test step is finished, please copy the ifcfg-eth0 back, and restart network to restore netowrk.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

1. Prepare the following xml:

<interface type='ethernet' name='eth0'>
   <start mode='none'/>
   <protocol family='ipv4'>
   <dhcp/>
   </protocol>
</interface>

2. Destroy and undefine the orignal interface.

# virsh iface-destroy eth0

# virsh iface-undefine eth0

3. Define new eth0 with multi static IPv6 address

# virsh iface-define new-eth0-ipv4-ipv6.xml

# virsh iface-start eth0

4. Check the new eth0

# virsh iface-dumpxml eth0

# cat /etc/syconfig/network-script/ifcfg-eth0

# ifconfig eth0
	
Expected Results:

3. Command is run successfully.

4. ping google.com successfully, and ifcfg-eth0 show that the eth0 is using dhcp IPv4 address

DEVICE=eth0
ONBOOT=no
BOOTPROTO=dhcp

# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A7:1F:4D  
          inet addr:10.66.65.132  Bcast:10.66.65.255  Mask:255.255.254.0
          inet6 addr: fe80::225:64ff:fea7:1f4d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:298546 errors:0 dropped:0 overruns:0 frame:0
          TX packets:2317 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:123415270 (117.6 MiB)  TX bytes:265674 (259.4 KiB)
          Memory:febe0000-fec00000

 

 
Notes:
Comments:

		177819 	[Virtual Networks] Route virtual network 	gren 	yoyzhang 	Auto 		Feature 	P2 	500 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

The test case checks that the VMs within a NATed network can reach the
public network, VM could send packets to host and other machines within
the same network with the host. If the host could surf Internet, the
packet from the vm could be forwarded to Internet.

I will give hostA and hostB static IP address,
hostA link to hostB directly.

hostA: 192.168.12.2 (static)
hostB: 192.168.12.3 (static)
virtual bridge br1:10.0.0.1
guestA: 10.0.0.2 (dynamic)

Make sure that the hostA and hostB are in the same network section

1 Define and start the virtual bridge br1

The virtual bridge XML format description:

<network>
  <name>br1</name>
  <uuid>8a2e947d-c1d5-4695-2881-5f877ced63e0</uuid>
  <forward mode='route'/>
  <bridge name='br1' stp='on' delay='0' />
  <ip address='192.168.100.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.100.2' end='192.168.100.254' />
    </dhcp>
  </ip>
</network>

2 Need to ensure the guestA has a right network interface setting:

......
        <interface type='bridge'>
           <source bridge='br1'/>
           <mac address='00:16:3e:5d:c7:9e'/>
        </interface>
......

3 After installation of guestA, we show the route table on hostA:

$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.12.0    0.0.0.0         255.255.255.0   U     0      0        0 eth0
10.0.0.0        0.0.0.0         255.255.255.0   U     0      0        0 br1
169.254.0.0     0.0.0.0         255.255.0.0     U     0      0        0 eth0
0.0.0.0         192.168.12.254  0.0.0.0         UG    0      0        0 eth0

4 So far, test the routed virtual network, ping hostB from guestA

$ ping -c 3 192.168.12.3
PING 192.168.12.3 (192.168.12.3) 56(84) bytes of data.

--- 192.168.12.3 ping statistics ---
3 packets transmitted, 0 received, 100% packet loss, time 2010ms

^C

5, As mentioned above, the guestA has already know how to reach hostB,
but the box hostB don't know the route to guestA, so for my example
I add a route rule in the box hostB as follows:

route add -net 10.0.0.0 netmask 255.255.255.0 gw 192.168.12.2

Now, the hostB have a route rule to response to guestA, the guestA
can reach hostB vice versa.
	
Expected Results:

1. All operations should complete successfully
2. The vm could reach LAN network through the Routed virtual network
Notes:
Comments:

		177023 	[Host network interface management] Define an ethernet wtih multi static ipv6 address 	yoyzhang 	None 	Manual 		Feature 	P2 	510 	Edit
Setup:

1. save the config file of eth1

  # cp /etc/sysconfig/network-scritps/ifcfg-eth1 /tmp

2. After the following test step is finished, please copy the ifcfg-eth0 back, and restart network to restore netowrk.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

1. Prepare the following xml:

<interface type='ethernet' name='eth1'>
  <start mode='onboot'/>
  <protocol family='ipv6'>
    <ip address='3ffe::200' prefix='64'/>
    <ip address='3ffe::201' prefix='64'/>
  </protocol>
</interface>

2. Define eth1 with multi static IPv6 address

# virsh iface-define new-eth1-multi-ipv6.xml

# virsh iface-start eth1

3. Check the new eth1

3.1 # virsh iface-dumpxml eth1

3.2 # cat /etc/syconfig/network-script/ifcfg-eth1

3.3 # ifconfig eth1

3.4 ping another host in the same subnet
	
Expected Results:

3.1 Only display the 1st staic ipv6 address

# virsh iface-dumpxml eth1
<interface type='ethernet' name='eth1'>
  <mac address='78:2b:cb:9a:92:31'/>
  <protocol family='ipv6'>
    <ip address='3ffe::200' prefix='64'/>
    <ip address='fe80::7a2b:cbff:fe9a:9231' prefix='64'/>
  </protocol>
</interface>


3.2 # cat ifcfg-eth1
DEVICE="eth1"
ONBOOT="yes"
IPV6INIT=yes
IPV6_AUTOCONF=no
DHCPV6=no
IPV6ADDR=3ffe::200/64
IPV6ADDR_SECONDARIES="'3ffe::201/64'"

3.3 # ifconfig eth1
eth1      Link encap:Ethernet  HWaddr 00:1B:21:39:8B:19  
          inet6 addr: 3ffe::200/64 Scope:Global
          inet6 addr: fe80::7a2b:cbff:fe9a:9231/64 Scope:Link
          UP BROADCAST RUNNING PROMISC MULTICAST  MTU:1500  Metric:1
          RX packets:36549 errors:0 dropped:0 overruns:0 frame:0
          TX packets:17314 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:20477811 (19.5 MiB)  TX bytes:1295338 (1.2 MiB)
          Interrupt:20 Memory:e1b00000-e1b20000


3.4  Could ping another host in the same sub-net successfully

# ping6 3ffe::202
PING 3ffe::202(3ffe::202) 56 data bytes
64 bytes from 3ffe::202: icmp_seq=1 ttl=64 time=2.24 ms
64 bytes from 3ffe::202: icmp_seq=2 ttl=64 time=0.184 ms

Notes:
Comments:

		177820 	[Virtual Networks] Shared physical network 	gren 	gren 	Manual 		Feature 	P1 	510 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    RHEL6.0

bug:

    No bug found

Actions:

Shared physical device network is used for dedicating a
physical device to a virtual machine. Bridging is often used for
more advanced setups and on servers with multiple network interfaces.

1  Open the network script for the device you are adding to
the bridge. In this example, ifcfg-eth0 defines the physical
network interface which is set as part of a bridge:

DEVICE=eth0
# change the hardware address to match the hardware address your NIC uses
HWADDR=00:16:76:D6:C9:45
ONBOOT=yes
BRIDGE=br0


2 Create a new network script in the /etc/sysconfig/network-scripts
directory called ifcfg-br0 or similar. The br0 is the name of the bridge,
this can be anything as long as the name of the file is the same as
the DEVICE parameter.

DEVICE=br0
TYPE=Bridge
BOOTPROTO=dhcp
ONBOOT=yes
DELAY=0


3  After configuring, restart networking or reboot.


# service NetworkManager stop
# service network restart

Configure iptables to allow all traffic to be forwarded across the bridge.

# iptables -I FORWARD -m physdev --physdev-is-bridged -j ACCEPT
# service iptables save
# service iptables restart

4 Disable iptables on bridges

net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0

# sysctl -p /etc/sysctl.conf


5 Restart the libvirt daemon.

# service libvirtd reload

6 You should now have a "shared physical device", which guests
can be attached and have full LAN access. Verify your new bridge:

# brctl show
bridge name     bridge id               STP enabled     interfaces
virbr0          8000.000000000000       yes
br0             8000.000e0cb30550       no              eth0

 

7 edit the interface element in guest xml

set <interface type='bridge'> and <source bridge='br0'/>

#virsh shutdown vm

#virsh edit vm

edit interface partition:

<interface type='bridge'>

<source bridge='br0'/>

#virsh start vm

8. login the guest

# ping www.google.com
	
Expected Results:

step 8:

The vm will share the physical NIC device to reach the outside network
Notes:
Comments:

		177047 	[Interface hotplug] Attach interface from args with specified model type - Bug 627125 	gsun 	None 	Manual (Autoproposed) 		Regression 	P1 	520 	Edit
Setup:

Make sure your domain is running

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Issue below command in gnome-terminal:
 # virsh attach-interface demo network default vnet1 --model e1000

2. Issue command:
   # virsh dumpxml demo

3. Check demo Hardware tab

4.  from the guest ping the host via the new attached NIC 

	
Expected Results:

1. Output:
Interface attached successfully
2. Verify that the new interface is listed out:

....................

   <interface type='network'>
      <mac address='52:54:00:55:3f:88'/>
      <source network='default'/>
      <target dev='vnet1'/>
      <model type='e1000'/>
      <alias name='net1'/> 
   </interface>

..............

3. Verify that this interface is added

4. 

[root@localhost ~]# ping 192.168.122.1 -I eth1
PING 192.168.122.1 (192.168.122.1) from 192.168.122.188 eth1: 56(84) bytes of
data.
64 bytes from 192.168.122.1: icmp_seq=1 ttl=64 time=0.970 ms
64 bytes from 192.168.122.1: icmp_seq=2 ttl=64 time=0.234 ms
64 bytes from 192.168.122.1: icmp_seq=3 ttl=64 time=0.243 ms
64 bytes from 192.168.122.1: icmp_seq=4 ttl=64 time=0.268 ms
64 bytes from 192.168.122.1: icmp_seq=5 ttl=64 time=0.241 ms
^C
--- 192.168.122.1 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4445ms
rtt min/avg/max/mdev = 0.234/0.391/0.970/0.289 ms

Notes:
Comments:

		177821 	[Virtual Networks] the number of guests exceeds the maximum IP number of DHCP 	xhu 	None 	Manual 		--default-- 	P2 	520 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    RHEL6.0

bug:

    No bug found

Actions:

1. change default network xml configuration with the following xml:
# virsh net-destroy default
# virsh net-edit default 
<network>
  <name>default</name>
  <uuid>502e4540-cbe4-4aca-8b88-62dfe053ef07</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <ip address='192.168.122.1' netmask='255.255.255.252'>
    <dhcp>
      <range start='192.168.122.1' end='192.168.122.2' />
    </dhcp>
  </ip>
</network>
# virsh net-start default

2. check the default network xml
# virsh net-dumpxml default

3. start the first guest rhel5 and check its ip  in guest
# ifconfig
# ping 192.168.122.1

4. start the second guest rhel51 and check its ip in guest
# ifconfig
# ping 192.168.122.1


 
	
Expected Results:

1

# virsh net-destroy default

Network default destroyed

# virsh net-edit default

<network>

<name>default</name>

     <uuid>502e4540-cbe4-4aca-8b88-62dfe053ef07</uuid>

     <forward mode='nat'/>

     <bridge name='virbr0' stp='on' delay='0' />

          <ip address='192.168.122.1' netmask='255.255.255.252'>

          <dhcp>

          <range start='192.168.122.1' end='192.168.122.2' />

          </dhcp>

        </ip>

  </network>

# virsh net-start default

Network default started

2

# virsh net-dumpxml default

<network>

<name>default</name>

   <uuid>eee0b14e-ee66-43f3-a9b6-643d3084f127</uuid>

   <forward mode='nat'/>

    <bridge name='virbr0' stp='on' delay='0' />

     <ip address='192.168.122.1' netmask='255.255.255.252'>

           <dhcp>

               <range start='192.168.122.1' end='192.168.122.2' />

            </dhcp>

      </ip>

</network>
3


# ifconfig

eth0      Link encap:Ethernet  HWaddr 52:54:00:2B:95:EC  
          inet addr:192.168.122.2  Bcast:192.168.122.3  Mask:255.255.255.252
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:350 errors:0 dropped:0 overruns:0 frame:0
          TX packets:118 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:27340 (26.6 KiB)  TX bytes:15981 (15.6 KiB)
          Interrupt:11 Base address:0x6000
# ping 192.168.122.1
PING 192.168.122.1 (192.168.122.1) 56(84) bytes of data.
64 bytes from 192.168.122.1: icmp_seq=1 ttl=64 time=0.060 ms
4
# ifconfig
eth0      Link encap:Ethernet  HWaddr 52:54:00:80:05:34            
             UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
             RX packets:32 errors:0 dropped:0 overruns:0 frame:0
             TX packets:54 errors:0 dropped:0 overruns:0 carrier:0
             collisions:0 txqueuelen:1000 
              RX bytes:3062 (2.9 KiB)  TX bytes:15981 (8 KiB)
              Interrupt:11 Base address:0x6000
# ping 192.168.122.1
connect: Network is unreachable

Notes:
Comments:

		177822 	[Virtual Networks] undefine network 	gren 	gren 	Auto 		Feature 	P1 	530 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks
    virsh-rail

bug:

    No bug found

Actions:

1, define a virtual network using "virsh net-define"

<network>
    <name>definebr1</name>
    <uuid>fb09e249-7cce-252e-2417-c47d6d60b3d6</uuid>
    <forward/>
    <bridge name='definebr1' stp='on' forwardDelay='0' />
    <ip address='10.0.0.1' netmask='255.255.255.0'>
      <dhcp>
        <range start='10.0.0.2' end='10.0.0.254' />
      </dhcp>
    </ip>
  </network>

2, # virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            inactive no

3, # virsh net-undefine definebr1
Network definebr1 has been undefined
	
Expected Results:

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes    
Notes:
Comments:

		177823 	[Virtual Networks] using remote TFTP server in virtual network - bug 683377 	gren 	None 	Manual 		Feature 	P2 	540 	Edit
Setup:


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks

bug:

    No bug found

Actions:

1. Use the below xml scheme to define the virtual network , notice there is a new attribute in bootp element, that means

we need setup the tftp server in another guest using the same virtual network instead of local host.

<network>
  <name>netboot</name>
  <bridge name="virbr1" stp='off' delay='1'/>
  <forward/>
  <ip address="192.168.10.1" netmask="255.255.255.0">
    <tftp root="/var/lib/tftpboot" />
    <dhcp>
      <range start="192.168.10.2" end="192.168.10.254" />
      <bootp file="pxelinux.0" server="192.168.10.3" />
    </dhcp>
  </ip>
</network>

2. Install a guest using the 'netboot' as network interface and give the guest a static ip 192.168.10.3

3. In guest, config tftp server

1). #yum -y install syslinux tftp-server tftp

    #cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot

2) dowload vmlinuz and initrd.img under the /var/lib/tftpboot folder

 example: http://download.englab.nay.redhat.com/pub/rhel/rel-eng/RHEL6.2-20111005.1/6.2/Server/x86_64/os/images/pxeboot/

3) mkdir /var/lib/tftpboot/pxelinux.cfg

4) create a file name default under the pxelinux.cfg folder

5) edit the file with the contents

DISPLAY boot.txt

DEFAULT rhel6

LABEL rhel6
        kernel vmlinuz
        append initrd=initrd.img

PROMPT 1
TIMEOUT 0

6) Edit /etc/xinetd.d/tftp

        server_args             = -c -s /var/lib/tftpboot
        disable                 = no

7) # service xinetd restart

8) # iptables -F

Also, on host, #iptables -F

4. On host, install a guest using PXE with the netboot network.
	
Expected Results:

4. Verify the guest could be installed successfully with PXE
Notes:
Comments:

		177824 	[Virtual Networks] virtual network of ethernet type - bug 593903 	jialiu 	None 	Manual 		Regression 	P3 	550 	Edit
Setup:

This case is from https://bugzilla.redhat.com/show_bug.cgi?id=593903
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    RHEL6.0

bug:

    No bug found

Actions:

1. Edit /etc/libvirt/qemu.conf, change the following lines
   from
   # clear_emulator_capabilities = 1
   to
   clear_emulator_capabilities = 0

   from
   #user = "root"
   to
   user = "root"

2. Restart libvirtd service.
3. Set selinux to permissive.
Note: According to jiri, using ifup script for qemu networking is not the most secure thing in the world
 and if you want to do that, you need to switch selinux to permissive, let libvirt start qemu as root, 
 and prevent libvirtd from clearing capabilities from qemu process.  we want selinux to prevent qemu
 from doing things like that.
 So in enforing selinux status, will fail to boot up guest, it is expected

4. Edit a domain xml as following:
   ....
    <interface type='ethernet'>
      <mac address='52:54:00:d8:05:14'/>
      <script path='/etc/my-qemu-ifup'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
function='0x0'/>
    </interface>
    ....
4. Prepare the script - /etc/my-qemu-ifup
# ll /etc/my-qemu-ifup
-rwxrwxrwx. 1 root root 72 Aug 11 02:41 /etc/my-qemu-ifup
# cat /etc/my-qemu-ifup
#!/bin/sh
/sbin/ifconfig $1 0.0.0.0 up
/usr/sbin/brctl addif virbr0 $1

5. Start the domain
# virsh start rhel6_x86_64
Domain rhel6_x86_64 started

6. In guest, check the network connectivity.
# ifconfig
# ping www.google.com

	
Expected Results:

5. Domain should be started successfully.

6. Guest's network connectivity is fine.
Notes:
Comments:

		177825 	[virtual networks] xml with mutiple dhcp sections - bug 735950 	xhu 	None 	Manual (Autoproposed) 		Regression 	P1 	560 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    Regression

bug:

    869913 - From Run 48358

Actions:

1. prepare the following network xml with multiple dhcp sections:
# cat multi-dhcp.xml 
<network>
  <name>multi-dhcp</name>
  <forward mode='nat'/>
  <bridge name='virbr10' stp='on' delay='0' />
  <ip address='192.168.201.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.201.2' end='192.168.201.254' />
    </dhcp>
  </ip>
  <ip address='192.168.202.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.202.2' end='192.168.202.254' />
    </dhcp>
  </ip>
  <ip family='ipv6' address='2003:db8:ac10:fe01::1' prefix='64'>
  </ip>
</network>

2. define  the multi-dhcp network 

# virsh net-define multi-dhcp.xml


3.  # virsh net-list --all

4. # virsh net-start multi-dhcp

	
Expected Results:

Step 2

# virsh net-define multi-dhcp.xml

error: Failed to define network from multi-dhcp.xml

error: unsupported configuration: Multiple dhcp sections found. dhcp is supported only for a single IPv4 address on each network

# echo $? 1

Step 3

# virsh net-list --all

Name State Autostart

-----------------------------------------

default active yes

There is no multi-dhcp listed.

Step 4

no such device.

 

 
Notes:
confirmed it when libvirt-0.9.13-3.el6.x86_64
Comments:

		177790 	[virtual networks] - VM creation failure should not leave behind stale macvtap interfaces - bug 754621 	xhu 	None 	Manual 		Regression 	P1 	570 	Edit
Setup:

use 2 network cards host to test this
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virtual networks

bug:

    No bug found

Actions:

1. Create network with the xml file and forward mode (or any mode that requires
exclusive access to the device) passthrough and 1 eth interface: 

<network>
  <name>test1</name>
  <uuid>89b10503-0ae6-5e33-b753-20580db197e5</uuid>
  <forward dev='eth1' mode='passthrough'>
    <interface dev='eth1'/>
  </forward>
</network> 

   #virsh net-define network.xml

   #virsh net-start test1

2. Create a VM with two interfaces using the above network , the VM's xml should have interfaces like this:

    <interface type='network'>
      <mac address='52:54:00:f7:41:67'/>
      <source network='test1'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:bf:3b:7a'/>
      <source network='test1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>

 

3.  check macvtap device

# ifconfig | grep -i macvtap
	
Expected Results:

2. VM creation will fail because it cannot find an eth interface to satisfy the second interface requirement for the VM.

#virsh start test1

error: Failed to start domain test1
error: internal error network 'test1' requires exclusive access to interfaces, but none are available

3. There is no macvtap device(eg. macvtap0)

 #ifconfig |grep -i macvtap
Notes:
Comments:

		177791 	[Virtual Networks] activate network 	gren 	gren 	Auto 		Feature 	P1 	580 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1, define a virtual network using "virsh net-define"

<network>
    <name>definebr1</name>
    <uuid>fb09e249-7cce-252e-2417-c47d6d60b3d6</uuid>
    <forward/>
    <bridge name='definebr1' stp='on' forwardDelay='0' />
    <ip address='10.0.0.1' netmask='255.255.255.0'>
      <dhcp>
        <range start='10.0.0.2' end='10.0.0.254' />
      </dhcp>
    </ip>
  </network>

2, activate the newly-defined network by executing "virsh net-start definebr1"
	
Expected Results:

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            active     no   
Notes:
Comments:

		177792 	[virtual networks] Bridged network, eth + bridge + tap 	xhu 	None 	Manual 		Feature 	P1 	590 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1 prepare the following nat network.xml:
# cat nat-network.xml 
<network>
  <name>nat-network</name>
  <forward dev='eth0' mode='nat'>
    <interface dev='eth0'/>
  </forward>
  <bridge name='virbr1' stp='on' delay='0' />
  <ip address='192.168.100.1' netmask='255.255.255.0'>
    <dhcp>
      <range start='192.168.100.2' end='192.168.100.254' />
    </dhcp>
  </ip>
</network>

2 define and start nat-network:
# virsh net-define nat-network.xml 
# virsh net-start nat-network 

3 start a guest with the following interface using nat-network:

<interface type='network'>
   <mac address='52:54:00:1b:6f:e5'/>
   <source network='nat-network'/>
   <target dev='vnet0'/>
   <model type='virtio'/>
   <alias name='net0'/>
   <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</interface>

4 check iptables rule and route table:
# iptables -t nat -L
# route -n


5 log into the guest, check the network: 
# ifconfig
# ping www.baidu.com
# ping <host_ip>
# ping <other_guest_on_the_same_network>

	
Expected Results:

2

# virsh net-define nat-network.xml

Network nat-network defined from nat-network.xml

# virsh net-start nat-network

Network nat-network started

4

# iptables -t nat -L
Chain PREROUTING (policy ACCEPT)
target     prot opt source               destination        

Chain POSTROUTING (policy ACCEPT)
target     prot opt source               destination        
MASQUERADE  tcp  --  192.168.100.0/24    !192.168.100.0/24    masq ports: 1024-65535
MASQUERADE  udp  --  192.168.100.0/24    !192.168.100.0/24    masq ports: 1024-65535
MASQUERADE  all  --  192.168.100.0/24    !192.168.100.0/24

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination

# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
192.168.100.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr1

5

- can get ip like 192.168.100.x

- can ping public network, host and other virtual host on the same network successfully

 

 
Notes:
Comments:

		177793 	[virtual networks] Bridged network, eth + macvtap + bridge 	xhu 	None 	Manual 		Feature 	P1 	600 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1 prepare the following bridge-network.xml:

# cat bridge-network.xml

<network>

  <name>bridge-net</name>

  <forward dev='eth0' mode='bridge'>

  <interface dev='eth0'/> </forward>

</network>

2 define and start bridge-net:

# virsh net-define bridge-network.xml

Network bridge-net defined from bridge-network.xml

# virsh net-start bridge-net

Network bridge-net started

3 start a guest with the following interface using bridge-net:

<interface type='network'>

  <mac address='52:54:00:1b:6f:e5'/>

  <source network='bridge-net'/>

  <target dev='vnet0'/>

  <model type='virtio'/>

  <alias name='net0'/>

  <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

</interface>

4 log into the guest, check the network:

# ifconfig

# ping www.baidu.com

 

 
	
Expected Results:

2

# virsh net-define bridge-network.xml

Network bridge-net defined from bridge-network.xml

# virsh net-start bridge-net

Network bridge-net started

4

 - get the ip from LAN

- can ping public network successfully
Notes:
Comments:

		177794 	[virtual networks] Bridged network, eth + macvtap + passthrough 	xhu 	None 	Manual 		Function 	P1 	610 	Edit
Setup:

1 reload kvm module with allow_unsafe_assigned_interrupts=1
# modprobe -r kvm_intel
# modprobe -r kvm
# modprobe kvm allow_unsafe_assigned_interrupts=1
# modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1 prepare the following vepa-passthrough.xml:

# cat vepa-passthrough.xml
<network>
  <name>vepa-passthrough</name>
  <forward dev='eth0' mode='passthrough'>
    <interface dev='eth0'/>
    <interface dev='eth1'/>
    <interface dev='eth2'/>
    <interface dev='eth3'/>
  </forward>
</network>

2 define and start vepa-passthrough network:

# virsh net-define vepa-passthrough.xml

# virsh net-start vepa-passthrough

3 start a guest with the following interface using vepa-passthrough network:

<interface type='network'>

  <mac address='52:54:00:1b:6f:e5'/>

  <source network='vepa-passthrough'/>

  <target dev='vnet0'/>

  <model type='virtio'/>

  <alias name='net0'/>

  <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

</interface>

4 log into the guest, check the network:

# ifconfig

# ping www.baidu.com

 

 

 
	
Expected Results:

2

# virsh net-define vepa-passthrough.xml
Network vepa-passthrough defined from vepa-passthrough.xml

# virsh net-start vepa-passthrough
Network vepa-passthrough started

4

 - get the ip from LAN

- can ping public network successfully

 

 
Notes:
Comments:

		177795 	[virtual networks] Bridged network, eth + macvtap + vepa 	xhu 	None 	Manual 		Feature 	P1 	620 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1 prepare the following vepa-network xml:

# cat vepa-network.xml 
<network>
  <name>vepa-net</name>
  <forward dev='eth0' mode='vepa'>
    <interface dev='eth0'/>
    <interface dev='eth1'/>
    <interface dev='eth2'/>
    <interface dev='eth3'/>
  </forward>
</network>

 2 define and start vepa-network:

# virsh net-define vepa-network.xml

# virsh net-start vepa-net

3 start a guest with the following interface using vepa-net:

<interface type='network'>

  <mac address='52:54:00:1b:6f:e5'/>

  <source network='vepa-net'/>

  <target dev='vnet0'/>

  <model type='virtio'/>

  <alias name='net0'/>

  <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

</interface>

4 log into the guest, check the network: 

# ifconfig
# ping www.baidu.com

 

 

 
	
Expected Results:

2

# virsh net-define vepa-network.xml

Network vepa-net defined from vepa-network.xml

# virsh net-start vepa-net

Network vepa-net started

4

- get the ip from LAN

- can ping public network successfully

 

 
Notes:
Comments:

		177089 	[libvirtd] Delete UNIX sockets upon libvirtd shutdown 	kxiong 	None 	Manual 		--default-- 	P2 	630 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1.#service libvird start
#ll /var/run/libvirt/libvirt-sock
srwxrwxrwx. 1 root root 0 Dec 29 15:07 /var/run/libvirt/libvirt-sock
2.#service libvirtd stop
#ll /var/run/libvirt/libvirt-sock

	
Expected Results:

ls: cannot access /var/run/libvirt/libvirt-sock: No such file or directory

Notes:
Comments:

		177796 	[Virtual Networks] Bridges created by libvirtd should be started even the network interface has no IP - Bug 532834 	yoyzhang 	yoyzhang 	Manual 		Regression 	P3 	630 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    RHEL6.0

bug:

    No bug found

Actions:

1. # cat test_noip.xml

<network>
  <name>test_noip</name>
  <uuid>9d92939c-208a-9cec-5ff9-86b6645ca2a5</uuid>
  <bridge name='test_noip_br' stp='on' forwardDelay='0' />
</network>
2. # virsh net-define test_noip.xml
3. # virsh net-start test_noip
4. # ip -o link show test_noip_br
OR # ip -o link show test_noip_br | grep -q UP && echo 'Pass' || echo 'Fail'

	
Expected Results:

4. Output

12: test_noip_br: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue \    link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff

OR

PASS
Notes:
Comments:

		177797 	[virtual networks] check /var/lib/libvirt/dnsmasq/default.hostsfile - bug 727982 	xhu 	None 	Auto 		Regression 	P1 	640 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. deactive default network:
# virsh net-destroy default
Network default destroyed

2. 'virsh net-edit default', then add a new static DHCP mapping to the default
network (e.g. <host mac='fe:55:01:7A:E7:44' ip='192.168.122.253' />)

Note: rhel6u4 add a new limit to host mac address, it should be a unicast mac address, that means it only accepts 1 for the 40th bit of mac address.

3. start default network:
# virsh net-start default

4. check default.hostsfile: 
# grep fe:55:01:7A:E7:44 /var/lib/libvirt/dnsmasq/default.hostsfile
fe:55:01:7A:E7:44,192.168.122.253

	
Expected Results:

1

# virsh net-destroy default

Network default destroyed

4

fe:55:01:7A:E7:44,192.168.122.253

 
Notes:
Comments:

		177093 	[libvirtd] Let /var/lib/libvirt/qemu belongs to qemu group 	kxiong 	None 	Manual 		Feature 	P2 	650 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

ls -ld /var/lib/libvirt/qemu/ /var/cache/libvirt/qemu/

	
Expected Results:

drwxr-xr-x. 2 qemu qemu 4096 Dec 22 10:15 /var/cache/libvirt/qemu/
drwxr-x---. 4 qemu qemu 4096 Dec 22 10:03 /var/lib/libvirt/qemu/

Notes:
Comments:

		177798 	[virtual networks] check /var/lib/libvirt/dnsmasq/default.hostsfile - bug 727982 	xhu 	xhu 	Manual 		Regression 	P1 	650 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    Regression

bug:

    No bug found

Actions:

1. deactive default network:
# virsh net-destroy default
Network default destroyed

2. 'virsh net-edit default', then add a new static DHCP mapping to the default
network (e.g. <host mac='FF:FF:EE:EE:DD:DD' ip='192.168.122.254' />)

3. start default network:
virsh net-start default

4. check default.hostsfile: 
grep FF:FF:EE:EE:DD:DD /var/lib/libvirt/dnsmasq/default.hostsfile

	
Expected Results:

1

# virsh net-destroy default

Network default destroyed

4

FF:FF:EE:EE:DD:DD,192.168.122.254

 
Notes:
Comments:

		177098 	[libvirtd] Libvirtd daemon core dump 	kxiong 	None 	Manual 		--default-- 	P2 	660 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1.Configure /etc/sysconfig/libvirtd and add the following line:
DAEMON_COREFILE_LIMIT=unlimited
2.#service libvirtd start
3.#pkill -SEGV libvirtd
4.Look at the dir "/"
	
Expected Results:

A core file is generated under the dir "/".
Notes:
Comments:

		177799 	[Virtual Networks] Create more than 256 nic for guest - Bug 519729 	yoyzhang 	yoyzhang 	Manual 		Regression 	P3 	660 	Edit
Setup:

1.# virsh net-dumpxml default  >default.xml

2.# virsh net-edit default

<network>
  <name>default</name>
  <uuid>a0621ddb-c100-4efb-8779-4e167c4d4d94</uuid>
  <forward mode='nat'/>
  <bridge name='virbr0' stp='on' delay='0' />
  <ip address='192.168.122.1' netmask='255.255.0.0'>
    <dhcp>
      <range start='192.168.122.2' end='192.168.254.254' />
    </dhcp>
  </ip>
</network>
3.# virsh net-destroy default  
Network default destroyed

4.# virsh net-start default  
Network default started

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    Regression

bug:

    No bug found

Actions:

1. Boot 10 guests, which each has 26 virtual nic.

2. Boot the 11th guest
	
Expected Results:

1. No error report, all the 10 guests is started successfully.

2. The 11th guest is started successfuly.

3. Check all NIC can ping to host
Notes:
[Scalability] 256 autostarted network (isolated, route , NAT) reloading
https://tcms.engineering.redhat.com/case/177477/?from_plan=6578
already covered this so disabled it
Comments:

		177800 	[Virtual Networks] create network 	gren 	gren 	Auto 		Feature 	P1 	670 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual networks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1, get a network xml description file

<network>
    <name>definebr1</name>
    <uuid>fb09e249-7cce-252e-2417-c47d6d60b3d6</uuid>
    <forward/>
    <bridge name='definebr1' stp='on' forwardDelay='0' />
    <ip address='10.0.0.1' netmask='255.255.255.0'>
      <dhcp>
        <range start='10.0.0.2' end='10.0.0.254' />
      </dhcp>
    </ip>
  </network>

2, # virsh net-create br1.xml
Network definebr1 created from br1.xml
	
Expected Results:

# virsh net-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       
definebr1            active     no
Notes:
Comments:

		177119 	[libvirtd] Restart libvirtd without killing guests 	kxiong 	kxiong 	Auto 		--default-- 	P1 	680 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    QE consumption

bug:

    No bug found

Actions:

1.Start one VM, do some operation in guest such as "# ls"

# virsh list  --all

Id Name                     State

----------------------------------------------

0  test                         running

2.# service libvirtd restart

3. check the libvirtd stauts

#service libvirtd stauts

4. # virsh list  --all

5. login the guest
	
Expected Results:

step 3:

# service libvirtd status
libvirtd (pid  2129) is running... 

step 4:

The guest is still in running state

Id Name                     State

----------------------------------------------

0  test                         running

 

step5:

keep the status on result of "ls" without restart.
Notes:
Comments:

		177124 	[libvirtd] Upstart libvirtd service 	mzhan 	mzhan 	Auto 		--default-- 	P1 	680 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    libvirtd

bug:

    No bug found

Actions:

1. disable libvirtd sysv initscript:

    # chkconfig libvirtd off

   # service libvirtd stop

Stopping libvirtd daemon:                                  [  OK  ]

2. enable libvirtd upstart job:

    # cp  /usr/share/doc/libvirt-*/libvirtd.upstart /etc/init/libvirtd.conf

    # initctl reload-configuration

    # initctl start libvirtd
libvirtd start/running, process 25413

3. # initctl status libvirtd
libvirtd start/running, process 25413


# service libvirtd status
libvirtd (pid  25413) is running...

# virsh list --all
 Id Name                 State
----------------------------------
  5 rhel6                running
  - cdrom                shut off

4. Kill libvirtd process and re-check

# killall -9 libvirtd

# service libvirtd status
libvirtd (pid  25637) is running...

# virsh list --all
 Id Name                 State
----------------------------------
  5 rhel6                running
  - cdrom                shut off

5. Clear environment

Note: If not clear environment for upstart libvirtd service, then the configuration for libvirtd.conf and others will not take effect.

# initctl stop libvirtd

# rm -rf /etc/init/libvirtd.conf

# initctl reload-configuration

# service libvirtd restart
Stopping libvirtd daemon:                                  [ FAILED ]
Starting libvirtd daemon:                                  [  OK  ]
	
Expected Results:

All checkpoints are passed.
Notes:
Comments:

		177136 	[libvirtd]Qemu get hang will not lead libvirtd to hang 	kxiong 	None 	Auto 		--default-- 	P2 	710 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    QE consumption

bug:

    No bug found

Actions:

1.Start a guest
# virsh list --all
 Id Name                 State
----------------------------------
  2 RHEL6                running
  - example              shut off

2.Stop qemu process with the following:
 kill -STOP  `ps aux | grep qemu | grep -v grep | awk '{print $2}'`

3.Run virsh list --all

	
Expected Results:

The guest cann't be connected.
but the libvirtd can show the guests and will not hang.

# virsh list --all
 Id Name                 State
----------------------------------
  2 RHEL6                running
  - example              shut off

Notes:
Comments:

		177748 	[VirtFS] Invalid virtfs parameters handling 	jialiu 	None 	Manual 		Feature 	P2 	710 	Edit
Setup:

Do this case first.

[VirtFS] Run a guest with virtio p9fs supported

<https://tcms.engineering.redhat.com/case/124914/?from_plan=5066>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtFS

bug:

    No bug found

Actions:

1. Make sure your current qemu-kvm does support virtFS.

# /usr/libexec/qemu-kvm --help

2. Add the following lines to the guest xml, make sure the source dir does NOT existing, or specify a file to it, not a dir

    <filesystem type='mount'>
      <source dir='/non-existing-dir'/>
      <target dir='test_mount'/>
    </filesystem>

 

3. Try to start the domain.
	
Expected Results:

1. Check if your qemu support p9fs:
# [qemu-kvm] --help
...

File system options:
-fsdev local,id=id,path=path,security_model=[mapped|passthrough|none]
Virtual File system pass-through options:
-virtfs local,path=path,mount_tag=tag,security_model=[mapped|passthrough|none]
...

# qemu-kvm -device ?
...
name "virtio-9p-pci", bus PCI
...

If any of the above two options is missing, that indicates your qemu does NOT support 9pfs.

3. Fail to start the domain, there should be some error mesgae to prompt user source dir specifid for virtFS does NOT existing.
Notes:
Comments:

		177197 	[Managed save] Domains are *not* automatically restore via initscript with "ON_BOOT=ignore" when host boots up 	dyuan 	None 	Auto 		Feature 	P2 	720 	Edit
Setup:

Domains with autostart *not* set are *not* automatically restore via initscript with "ON_BOOT=ignore"
      Domains marked as autostart will still be automatically started by libvirtd regardless on the ON_BOOT setting in libvirt-guests.

 

The file is intended to be sourced (if i'm not mistaken) not executed directly.

# ll /etc/sysconfig/libvirt-guests

-rw-r--r--. 1 root root 1095 Dec 24 04:36 /etc/sysconfig/libvirt-guests

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Start up 2 domains and shutdown 1 domain:
   # virsh start <domain1>
   # virsh start <domain2>
   # virsh shutdown <domain3>

Note: make sure the autostart should be disabled for the 3 domains, or they will start by libvirtd on host bootup.

2. Edit configure in /etc/sysconfig/libvirt-guests
ON_SHUTDOWN=suspend
ON_BOOT=ignore

3. Make sure the libvirt-guests script is on:
   # chkconfig libvirt-guests on

 

4. reboot the *host*:
   # reboot




	
Expected Results:

1. During boot, the libvirt-guests script should NOT restore all 3 domains

domain1 shutoff
domain2 shutoff
domain3 shutoff

Notes:
Comments:

		177749 	[VirtFS] Run a guest with virtio p9fs supported 	jialiu 	None 	Manual 		Feature 	P1 	720 	Edit
Setup:

Make sure the guest kernel support p9fs.

How to check if your current kernel supports 9pfs?

# grep 9P /boot/config-$(uname -r)
CONFIG_NET_9P=m
CONFIG_NET_9P_VIRTIO=m
CONFIG_NET_9P_RDMA=m
# CONFIG_NET_9P_DEBUG is not set
CONFIG_9P_FS=m
CONFIG_9P_FSCACHE=y

If the option value is "y", that indicates p9fs driver is built into the kernel ("=y").
If the option value is "m", that indicates p9fs driver will be built as a module ("=m"), user can load it manually via "modprobe" command.
If the option value is "n" or no the config line, that indicates p9fs driver is not selected. User need re-compile kernel.

 

Acctually F14 release kernel has supported p9fs. So we can use FC14 or later to test this case.

 

NOTE:

Not all the qemu emulator is support virtFS feature, so before run this case, make sure your qemu emulator suport virtFS.

Check if your qemu support p9fs:
# [qemu-kvm | qemu | qemu-system-x86_64] --help
...
File system options:
-fsdev local,id=id,path=path,security_model=[mapped|passthrough|none]
Virtual File system pass-through options:
-virtfs local,path=path,mount_tag=tag,security_model=[mapped|passthrough|none]

...

# [qemu-kvm | qemu | qemu-system-x86_64] -device ?
...
name "virtio-9p-pci", bus PCI
...

If any of the above two options is missing, that indicates your qemu does NOT support 9pfs.

The upstream qemu does support p9fs, so you need compile the latest qemu emulator.

Get and build latest qemu binary.

1). Get the latest git repository from http://git.qemu.org/.
# git clone git://git.qemu.org/qemu.git

Notes:By now, upstream qemu not support virtFS, so we should use previous stable build to testing

# git clone git://git.qemu.org/qemu-stable-1.0.git


2). cd to the downloaded qemu source directory

3). Configure QEMU for the desired target.
# ./configure '--target-list=x86_64-softmmu' '--enable-debug' '--enable-kvm' '--enable-attr' '--prefix=/usr'

NOTE:
Make sure install zlib-devel, libattr & libattr-devel

4). Compile QEMU
# make
# make install

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtFS

bug:

    No bug found

Actions:

1. Create a guest with p9fs supported.

2. Shut down the guest.

3. Add the following lines to the guest xml.

    <filesystem type='mount'>
      <source dir='/tmp'/>
      <target dir='test_mount'/>
    </filesystem>

NOTE:
"/tmp" is what host expose to guest
"test_mount" is not actually a directory, it is merely a arbitrary string tag that is exported to the guest as a hint for where to mount it.

Example:

<domain type='kvm'>
  <name>p9fs</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='pc-0.13'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/qemu-system-x86_64</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/p9fs.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <filesystem type='mount'>
      <source dir='/tmp'/>
      <target dir='test_mount'/>
    </filesystem>
    <interface type='network'>
      <mac address='52:54:00:09:79:6f'/>
      <source network='default'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
  </devices>
</domain>

NOTE:

<emulator>/root/qemu_bin_9p/bin/qemu-system-x86_64</emulator> is the emulator which does support virtFS feature.


4. Make sure user=root and group=root in /etc/libvirt/qemu.conf, then start the domain.

# ps -ef|grep qemu

 

5. In guest, check the virtFS functionality.

# lsmod|grep  9p

# mount -t 9p -o trans=virtio test_mount /tmp/shared/

# cd /tmp/shared/

# ls

Try to write and read some data in this folder.
	
Expected Results:

4. Output:

# ps -ef|grep qemu
/root/qemu_bin_9p/bin/qemu-system-x86_64 -S -M pc-0.13 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name p9fs -uuid 6acf1b13-e551-5930-2e39-f1e8858ae74c -nodefconfig -nodefaults -chardev socket,id=monitor,path=//var/lib/libvirt/qemu/p9fs.monitor,server,nowait -mon chardev=monitor,mode=readline -rtc base=utc -boot c -drive file=/var/lib/libvirt/images/f14.img,if=none,id=drive-ide0-0-0,format=raw -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -fsdev local,security_model=passthrough,id=fsdev-fs0,path=/tmp -device virtio-9p-pci,id=fs0,fsdev=fsdev-fs0,mount_tag=test_mount,bus=pci.0,addr=0x5 -netdev tap,fd=17,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:86:83:1c,bus=pci.0,addr=0x3 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc 127.0.0.1:0 -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4

NOTE:

passthrough is the default security model for virtFS.

5. Output:

# lsmod|grep  9p
9p                     28949  0
fscache                44467  1 9p
9pnet_virtio            3717  0
virtio                  4242  1 9pnet_virtio
virtio_ring             6110  1 9pnet_virtio
9pnet                  40618  2 9p,9pnet_virtio

After mount successfully, find ls output is listing context from your host dir - /tmp

Write and read data should be successful.

Note: Currently, you may fail to  modify and list existing files and touch/mkdir new file/folder in the guest. To workaround this problem, change selinux to permissive on host. A patch is on the external list which allows SElinux to identify 9p(VirtFS) as a filesystem.
Notes:
Comments:

		177202 	[Managed save] Domains are automatically start via initscript when host start 	yimwang 	None 	Auto 		--default-- 	P2 	730 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Start up two RHEL-6 domains (the domains do not have to be RHEL-6, but they need
   to be new enough to handle ACPI events, so RHEL-3 will not work here).
   # virsh start <domain1>
   # virsh start <domain2>

2. Wait for the domains to fully boot up.

Conduct some operations in the guest, eg # ls.

3. Edit /etc/sysconfig/libvirt-guests, and set
   "ON_SHUTDOWN" to "shutdown":
    ON_SHUTDOWN=shutdown

4. Edit /etc/sysconfig/libvirt-guests, and set
    "ON_BOOT" to "start":
     ON_BOOT=start

5. Edit /etc/sysconfig/libvirt-guests, and set
   SHUTDOWN_TIMEOUT to 60:
   SHUTDOWN_TIMEOUT=60

6. Reboot the *host*:
   # reboot


	
Expected Results:

6.0 While the host is shutting down, the "libvirt-domains" script should shutdown
    both of the active domains.

6.1 When the host comes back up, both domains should bootup normally but not at the same place in step 2.

 

Notes:
Comments:

		177750 	[VirtFS] Try to start a guest with a emulator that does not support virtfs 	jialiu 	None 	Manual 		Feature 	P2 	730 	Edit
Setup:

Do this case first.

[VirtFS] Run a guest with virtio p9fs supported

<https://tcms.engineering.redhat.com/case/124914/?from_plan=5066>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    alsa-lib

Tag:

    virtFS

bug:

    No bug found

Actions:

1. Check your current qemu-kvm does NOT support virtFS.

# /usr/libexec/qemu-kvm --help

2. Add the following lines to the guest xml.

    <filesystem type='mount'>
      <source dir='/tmp'/>
      <target dir='test_mount'/>
    </filesystem>

NOTE:
"/tmp" is what host expose to guest
"test_mount" is not actually a directory, it is merely a arbitrary string tag that is exported to the guest as a hint for where to mount it.

3. Try to start the domain.
	
Expected Results:

1. Check if your qemu support p9fs:
# [qemu-kvm] --help
...
File system options:
-fsdev local,id=id,path=path,security_model=[mapped|passthrough|none]
Virtual File system pass-through options:
-virtfs local,path=path,mount_tag=tag,security_model=[mapped|passthrough|none]
...

# qemu-kvm -device ?
...
name "virtio-9p-pci", bus PCI
...

If any of the above two options is missing, that indicates your qemu does NOT support 9pfs.

3. Fail to start the domain, there should be some error mesgae to prompt user your qemu-kvm does NOT support virtFS.
Notes:
Comments:

		177751 	[VirtFS]Run a guest with different access mode for virtfs mount type 	jialiu 	None 	Manual 		Feature 	P1 	740 	Edit
Setup:
Setup:

Make sure the guest kernel support p9fs.

How to check if your current kernel supports 9pfs?

# grep 9P /boot/config-$(uname -r)
CONFIG_NET_9P=m
CONFIG_NET_9P_VIRTIO=m
CONFIG_NET_9P_RDMA=m
# CONFIG_NET_9P_DEBUG is not set
CONFIG_9P_FS=m
CONFIG_9P_FSCACHE=y

If the option value is "y", that indicates p9fs driver is built into the kernel ("=y").
If the option value is "m", that indicates p9fs driver will be built as a module ("=m"), user can load it manually via "modprobe" command.
If the option value is "n" or no the config line, that indicates p9fs driver is not selected. User need re-compile kernel.

 

Acctually F14 release kernel has supported p9fs. So we can use FC14 or later to test this case.

 

NOTE:

Not all the qemu emulator is support virtFS feature, so before run this case, make sure your qemu emulator suport virtFS.

Check if your qemu support p9fs:
# [qemu-kvm | qemu | qemu-system-x86_64] --help
...
File system options:
-fsdev local,id=id,path=path,security_model=[mapped|passthrough|none]
Virtual File system pass-through options:
-virtfs local,path=path,mount_tag=tag,security_model=[mapped|passthrough|none]
...

# [qemu-kvm | qemu | qemu-system-x86_64] -device ?
...
name "virtio-9p-pci", bus PCI
...

If any of the above two options is missing, that indicates your qemu does NOT support 9pfs.

The upstream qemu does support p9fs, so you need compile the latest qemu emulator.

Get and build latest qemu binary.

1). Get the latest git repository from http://git.qemu.org/.
# git clone git://git.qemu.org/qemu.git

2). cd to the downloaded qemu source directory

3). Configure QEMU for the desired target.
# ./configure '--target-list=x86_64-softmmu' '--enable-debug' '--enable-kvm' '--enable-attr' '--prefix=/usr'

NOTE:
Make sure install zlib-devel, libattr & libattr-devel

4). Compile QEMU
# make
# make install
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtFS

bug:

    No bug found

Actions:

1. Create a guest with p9fs supported.

2. Shut down the guest.

3. Add the following lines to the guest xml seperately for three different access mode.

==========mapped============

For mapped security model, you need to enable xattr on the host FS.

By default, xattr is enabled generally.

    <filesystem type='mount'  accessmode='mapped' >
      <source dir='/tmp'/>
      <target dir='test_mount'/>
    </filesystem>

===========passthrough============

passthrough needs QEMU run as root.

So make sure user=root and group=root in /etc/libvirt/qemu.conf

    <filesystem type='mount'  accessmode='passthrough' >
      <source dir='/tmp'/>
      <target dir='test_mount'/>
    </filesystem>

==========squash============

Here I run qemu process as qemu user.

So make sure user=qemu and group=qemu in /etc/libvirt/qemu.conf, or coment out this two lines

    <filesystem type='mount'  accessmode='squash' >
      <source dir='/tmp'/>
      <target dir='test_mount'/>
    </filesystem>

======================

NOTE:
"/tmp" is what host expose to guest
"test_mount" is not actually a directory, it is merely a arbitrary string tag that is exported to the guest as a hint for where to mount it.


4. Start the domain.

# ps -ef|grep qemu

 

5. In guest, check the virtFS functionality.

# lsmod|grep  9p

# mount -t 9p -o trans=virtio test_mount /tmp/shared/

# cd /tmp/shared/

 

6. Create a shell script in the virtFS directory - /tmp/shared as root user, and run it

# touch test.sh

( Is SELinux is enabled on the host? If so, please disable it or set it to permissive mode.
  A patch is on the external list which allows SELinux to identify 9p(VirtFS) as a filesystem. )

give excutable permisson to the script file.

# chmod a+x test.sh

# cat test.sh
echo "hello"

# ls -l test.sh

# ./tset.sh

# ln -s test.sh test-new.sh
	
Expected Results:

4.

========mapped========

qemu      6367     1 80 15:06 ?        00:00:14 /usr/bin/qemu_9p/bin/qemu-system-x86_64 -S -M pc-0.14 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name fc14 -uuid 1d9eee1d-09b2-904e-8349-f67ab6114bed -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/fc14.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -boot c -drive file=/var/lib/libvirt/images/f14.img,if=none,id=drive-ide0-0-0,format=raw -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -fsdev local,security_model=mapped,id=fsdev-fs0,path=/tmp -device virtio-9p-pci,id=fs0,fsdev=fsdev-fs0,mount_tag=test_mount,bus=pci.0,addr=0x5 -netdev tap,fd=25,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:f2:28:50,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4

========passthrough========

# ps -ef|grep qemu
qemu      5035     1 35 14:18 ?        00:00:14 /usr/bin/qemu_9p/bin/qemu-system-x86_64 -S -M pc-0.14 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name fc14 -uuid 1d9eee1d-09b2-904e-8349-f67ab6114bed -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/fc14.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -boot c -drive file=/var/lib/libvirt/images/f14.img,if=none,id=drive-ide0-0-0,format=raw -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -fsdev local,security_model=passthrough,id=fsdev-fs0,path=/tmp -device virtio-9p-pci,id=fs0,fsdev=fsdev-fs0,mount_tag=test_mount,bus=pci.0,addr=0x5 -netdev tap,fd=24,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:f2:28:50,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4

 

========squash========

# ps -ef|grep qemu
qemu      4849     1 92 14:08 ?        00:00:08 /usr/bin/qemu_9p/bin/qemu-system-x86_64 -S -M pc-0.14 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name fc14 -uuid 1d9eee1d-09b2-904e-8349-f67ab6114bed -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/fc14.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -boot c -drive file=/var/lib/libvirt/images/f14.img,if=none,id=drive-ide0-0-0,format=raw -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -fsdev local,security_model=none,id=fsdev-fs0,path=/tmp -device virtio-9p-pci,id=fs0,fsdev=fsdev-fs0,mount_tag=test_mount,bus=pci.0,addr=0x5 -netdev tap,fd=24,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:f2:28:50,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4

6.

========mapped========

For special files, such as, link file, corresponding regular files are created
on file server and appropriate mode bits are added to extended attributes.

# ll -l test2.sh
-rwxr-xr-x. 1 root 107 13 Feb 15 15:12 test2.sh

# ./test2.sh
hello

# ln -s test2.sh test2-new.sh

# ll (on guest)

lrwxrwxrwx. 1 root      107    9 Feb 15 15:12 test2-new.sh -> test2.sh
-rwxr-xr-x. 1 root      107   13 Feb 15 15:12 test2.sh

# ll (on host)

-rw-------. 1 qemu   qemu      9 Feb 15 15:12 test2-new.sh
-rw-------. 1 qemu   qemu     13 Feb 15 15:12 test2.sh

# ./test2-new.sh
hello

========passthrough========

 


# ls -l test1.sh
-rwxr-xr-x. 1 root root 13 Feb 15 14:52 test1.sh


# ./test1.sh
hello


# ln -s test1.sh test1-new.sh

# ll (on guest)
lrwxrwxrwx. 1 root   root      8 Feb 15 14:54 test1-new.sh -> test1.sh
-rwxr-xr-x. 1 root   root     13 Feb 15 14:52 test1.sh

# ll (on host)
lrwxrwxrwx. 1 root   root      8 Feb 15 14:54 test1-new.sh -> test1.sh
-rwxr-xr-x. 1 root   root     13 Feb 15 14:52 test1.sh


# ./test1-new.sh
hello

========squash========

In 'squash' mode, the (filesystem) server attempts to preserve user/group
ownership from guest, however:
- If the server is running as root this mode is equivalent to passthrough.
- If the server is running as non-root, all files just have uid/gid matching
  the server process.

# cat test.sh
echo "hello"

# ll (on guest)
lrwxrwxrwx. 1    107    107    7 Feb 15  2011 test-new.sh -> test.sh
-rwxr-xr-x. 1    107    107   22 Feb 15 14:21 test.sh

# ll (on host)
lrwxrwxrwx. 1 qemu   qemu      7 Feb 15 15:00 test-new.sh -> test.sh
-rwxr-xr-x. 1 qemu   qemu     22 Feb 15 14:21 test.sh

NOTE:

107 is qemu uid, the qemu process is running as qemu user.

# ./test.sh
hello

Notes:
Comments:

		177227 	[Memory management] Deactivate memory balloon with type of none BZ 833674 	mzhan 	None 	Manual 		Feature 	P2 	750 	Edit
Setup:

Make sure a guest works well with memoryballoon, Rhel5u5 and above guest should support
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    memory

bug:

    No bug found

Actions:

1. Prepare a guest with shutdown status, Rhel5u5 and above guest should support memoryballoon

2. Modify the guest xml to deactivate memory balloon,Change memballoon model value as none.

# virsh dumpxml ccc
    ...
    <memballoon model='none'>
    </memballoon>
    ...

3. # virsh start ccc
Domain ccc started

4. # virsh setmem ccc 524288
error: Requested operation is not valid: Unable to change memory of active domain without the balloon device and guest OS balloon driver

5. # virsh destroy ccc
Domain ccc destroyed

6. # virsh setmem ccc 524288
error: Requested operation is not valid: domain is not running

	
Expected Results:

2. The value is saved correctly

3. Guest start successfully

4. as steps

6. as steps
Notes:
Comments:

		177762 	[Virtio] Create domain with virtio block device driver (only for Linux) 	yimwang 	None 	Auto 		Regression 	P2 	750 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1.Create  disk image.

# qemu-img create /var/lib/libvirt/images/virtio.img 8G

2. Defined  virtio guest.

For Windows:

#cat virtio.xml

<domain type='kvm'>
  <name>virtio</name>+
  <uuid>6785f9df-12e9-3f5a-32b3-39de72020e12</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>2</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/sr0'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/virtio.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <controller type='ide' index='0'>
    </controller>
    <controller type='fdc' index='0'/>
    <interface type='network'>
      <mac address='52:54:00:e7:7e:04'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <sound model='ac97'>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
    </video>
    <memballoon model='virtio'>
    </memballoon>
  </devices>
</domain>

2. Start guest.

# virsh start virtio

3.Install the Linux domain guest

4.After Install finished,edit domain XML.

  #virsh edit  virtio

 Delete that xml " <boot dev='cdrom'/>" in domain  XML

 5.Start domain guest.

#virsh start virtio

 
	
Expected Results:

3. The guest could be installed successfully

no error output

5.1 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

5.2 firefox or IE application should be launched

5.3 The output value should be nearly equal to the following value given in xml confile file

  <memory>1048576</memory>

5.4  vcpu number should be equal to the following value given in xml config file

  <vcpu>2</vcpu>

5.5. Should ping to host successfully
Notes:
Comments:

		177228 	[Memory management] Disable ksm for per VM - BZ#635419 	mzhan 	mzhan 	Manual (Autoproposed) 		Feature 	P2 	760 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    memory

bug:

    No bug found

Actions:

1. Prepare a guest with shutdown status

2.Add the following xml info into guest

 <name>qcow2</name>
  <uuid>e7acd650-2c62-b599-cf63-835c65a0006e</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <memoryBacking>
    <nosharepages/>
  </memoryBacking>
  <vcpu>1</vcpu>

3. Start the guest 

# virsh start rhel6

4. # ps aux|grep kvm | grep ksm -i

qemu 8063 9.0 4.7 1330152 376908 ? Sl 21:11 0:38 /usr/libexec/qemu-kvm -S -M rhel6.1.0 -enable-kvm -m 1024 -redhat-disable-KSM -smp 1,sockets=1,cores=1,threads=1 -name rhel6 -uuid 092dcbfe-ec20-9ff6-9a34-4c516c233e33 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/rhel6.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -boot c -drive file=/var/lib/libvirt/images/rhel6.img,if=none,id=drive-virtio-disk0,format=raw,cache=none -device virtio-blk-pci,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0 -netdev tap,fd=25,id=hostnet0,vhost=on,vhostfd=26 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:3f:c4:20,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -device usb-tablet,id=input0 -vnc 127.0.0.1:0 -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x4 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6

 
	
Expected Results:
Notes:
Comments:

		177231 	[Memory management] Memory usage works fine in libvirtd udev backend - bug 595490 	mzhan 	None 	Manual 		Regression 	P3 	770 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    memory

bug:

    No bug found

Actions:

1. In host

# lspci |grep -i hba
04:00.0 Fibre Channel: QLogic Corp. ISP2432-based 4Gb Fibre Channel to PCI Express HBA (rev 03)
04:00.1 Fibre Channel: QLogic Corp. ISP2432-based 4Gb Fibre Channel to PCI Express HBA (rev 03)

2.

# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host2
scsi_host3
scsi_host4
scsi_host5

# virsh nodedev-dumpxml scsi_host5
<device>
  <name>scsi_host5</name>
  <parent>pci_0000_04_00_1</parent>
  <capability type='scsi_host'>
    <host>5</host>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9da4e</wwpn>
    </capability>
    <capability type='vport_ops' />
  </capability>
</device>

# ll /sys/class/fc_host/
total 0
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host4 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.0/host4/fc_host/host4
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host5 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.1/host5/fc_host/host5

# ll /sys/class/scsi_host/
total 0
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host0 -> ../../devices/pci0000:00/0000:00:07.0/host0/scsi_host/host0
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host1 -> ../../devices/pci0000:00/0000:00:07.0/host1/scsi_host/host1
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host2 -> ../../devices/pci0000:00/0000:00:08.0/host2/scsi_host/host2
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host3 -> ../../devices/pci0000:00/0000:00:08.0/host3/scsi_host/host3
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host4 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.0/host4/scsi_host/host4
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host5 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.1/host5/scsi_host/host5

# cat /sys/class/fc_host/host5/max_npiv_vports
127

# cat /sys/class/fc_host/host5/port_state
Online

3. Create new hba on the host

# cat virtualhba.xml
<device>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9f012</wwpn>
    </capability>
  </capability>
</device>

# virsh nodedev-create virtualhba.xml
Node device scsi_host143 created from virtualhba.xml

# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host143
scsi_host2
scsi_host3
scsi_host4
scsi_host5

# virsh nodedev-dumpxml scsi_host143
<device>
  <name>scsi_host143</name>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <host>143</host>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9f012</wwpn>
    </capability>
  </capability>
</device>

4. Destroy this hba

# virsh nodedev-destroy scsi_host143
Destroyed node device 'scsi_host143'

# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host2
scsi_host3
scsi_host4
scsi_host5

# ll /sys/class/fc_host/
total 0

lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host4 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.0/host4/fc_host/host4
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host5 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.1/host5/fc_host/host5

5. Check all the steps are correct. Then do an extended run (about 8 hours) of creating and destroying device in a loop and check memory usage. If the numbers stabilize, there is no leak. A leak will case to increase without bound.

# cat test.sh

#!/bin/bash

hba_xml=$1
timeout=28800    # 8h
interval=2
log="/var/log/mem_monitor.log"

#Output memory usage of libvirt into log
output_log(){
  sleep $interval
  date "+%F %T" >> $log
  ps -C libvirtd -o rss,size,vsize >> $log
  echo >> $log
}

#Get new creation node device name
get_latest_node_name(){
  node_name=scsi_$(ls --time=ctime /sys/class/fc_host/|head -1)
  echo $node_name
}


if [ $# -ne 1 ]; then
  echo "Usage: <mem_monitor> <virtual_hba.xml>"
  exit 1
fi

lspci | grep -i hba
virsh nodedev-list --cap=scsi_host
ls -l /sys/class/fc_host/
ls -l /sys/class/scsi_host/


while [ $timeout -gt 0 ]
do
  output_log
  virsh nodedev-create $hba_xml
  output_log
  virsh nodedev-destroy $(get_latest_node_name)
  output_log
  let timeout=$timeout-3*$interval
done

echo "log info: $log"


# chmod +x test.sh

# sh test.sh virtualhba.xml

Final check the log in /var/log/mem_monitor.log if memory leaks or not.

	
Expected Results:

5. Script use this to monitor memory:

ps -C libvirtd -o rss,size,vsize

and the result can be checked in following file. RSS range from 11620 to 12920, and SZ=456836, VSZ=601868

# cat /var/log/mem_monitor.log

2011-01-11 07:22:31
  RSS    SZ    VSZ
12920 456836 601868

2011-01-11 07:22:33
  RSS    SZ    VSZ
12268 456836 601868

2011-01-11 07:22:35
  RSS    SZ    VSZ
12232 456836 601868

2011-01-11 07:22:37
  RSS    SZ    VSZ
12232 456836 601868

2011-01-11 07:22:39
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:22:41
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:22:43
  RSS    SZ    VSZ
12272 456836 601868

2011-01-11 07:22:45
  RSS    SZ    VSZ
12272 456836 601868

2011-01-11 07:22:47
  RSS    SZ    VSZ
12272 456836 601868

2011-01-11 07:22:49
  RSS    SZ    VSZ
12272 456836 601868

2011-01-11 07:22:52
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:22:54
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:22:56
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:22:58
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:23:00
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:23:02
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:23:04
  RSS    SZ    VSZ

11620 456836 601868


2011-01-11 07:23:06
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:23:08
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:23:10
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:23:12
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:23:14
  RSS    SZ    VSZ
11620 456836 601868

......
Notes:
Comments:

		177267 	[migration] migrate a guest for 1024 rounds through TCP connection --live 	jiachen 	None 	Auto 		Stress 	P2 	810 	Edit
Setup:

there is no auto script in virsh-rail
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration

bug:

    No bug found

Actions:

#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live"
TRANSPORT="tcp"
#TRANSPORT="tls"
#TRANSPORT="ssh"

date
for i in `seq 1 1024`;
do
    echo "Loop ${i}: Migrating ${GUEST} from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system
done
date
	
Expected Results:
Notes:
Comments:

		177308 	[Migration] SSL connected spice migration 	weizhan 	None 	Manual 		--default-- 	P2 	840 	Edit
Setup:

Note:

Because there is passwd for spice in the following case:
95445 [Graphical framebuffer] spice ssl connection


so we should add a checkpoint for passwd after migration on dest host.

Bug 725009 - No Spice password is set on target host after migration

 

make sure install virt-viewer pkg
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    QE consumption

bug:

    853197 - From Run 44420
    852668 - From Run 44628

Actions:

1. Prepare 2 host and setting the virt_use_nfs boolean on both sides

   # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Mount nfs on both sides

   # mount 10.66.90.113:/vol/xenimage /mnt -o vers=3

3. Do "95445 [Graphical framebuffer] spice ssl connection" by changing the disk path to /mnt/xxx.img on source host

4. Copy ca-cert.pem  server-cert.pem  server-key.pem to target host on the path /etc/pki/libvirt-spice

also modify the followings in qemu.conf  on target host
   -# spice_listen = "0.0.0.0"
   + spice_listen = "0.0.0.0"

   -# spice_tls = 1
   + spice_tls = 1

   -# spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"
   + spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"

and restart libvirtd

# service libvirtd restart

5.Connect the guest on source host via virt-viewer

# virt-viewer $guest --spice-host-subject='C=IL,L=Raanana,O=Red Hat,CN=my server' --spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem'

6. Do migration

# virsh migrate --live guest qemu+ssh://{target ip}/system

7. Close the spice console, connect the guest on target host with the same command with step 5:

# virt-viewer $guest --spice-host-subject='C=IL,L=Raanana,O=Red Hat,CN=my server' --spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem'

8. On dest host :
# virsh destroy guest 

9. On source host:

# virsh start guest

10.connect the guest on source host via remote-viewer

# remote-viewer spice://$ip/?port=5900\&tls-port=5901 --spice-host-subject='C=IL,L=Raanana,O=Red Hat,CN=my server' --spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem'

11. Do migration

# virsh migrate --live guest qemu+ssh://{target ip}/system

12. Close the spice console, connect the guest on target host with the same command with step 10:

# remote-viewer spice://$ip/?port=5900\&tls-port=5901 --spice-host-subject='C=IL,L=Raanana,O=Red Hat,CN=my server' --spice-ca-file='/etc/pki/libvirt-spice/ca-cert.pem' 

 
	
Expected Results:

After step 5, the spicec shows the screen of the guest after migration

Check file /var/log/libvirt/libvirtd.log, make sure it doesn't appear with the following errors in log:

16:22:24.992: 15397: info : libvirt version: 0.9.3, package: 3.el6 (Red Hat,
Inc. <http://bugzilla.redhat.com/bugzilla>, 2011-07-13-23:20:52,
x86-002.build.bos.redhat.com)
16:22:24.992: 15397: error : qemuDomainExtractTLSSubject:151 : internal error
cannot initialize cert object: ASN1 parser: Element was not found.
16:22:24.992: 15397: warning : qemuMigrationPrepareDirect:1386 : Unable to
encode migration cookie

 

step 6:

The spicec is still keeping connected after migration.

step 7:

need to input the passwd for reconnection.

step 10:

Need to input passwd for connection.

step 11:

The spicec is still keeping connected after migration.

step 12:

need to input the passwd for reconnection.

 
Notes:
Comments:

		177729 	[Update device flags] change the media in an existing FLOPPY device on the fly 	jialiu 	None 	Auto 		Feature 	P2 	840 	Edit
Setup:

NOTE:

For RHEL6 guest, the floppy driver is not loaded by default, so if you want to use the floppy device in rhel6 device, u need load floppy driver manually.

# modprobe floppy
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Make 2 floppy disk image file

# dd if=/dev/zero of=/var/lib/libvirt/images/fd1.img count=1024 bs=1024

# mkfs.ext3 /var/lib/libvirt/images/fd1.img

# dd if=/dev/zero of=/var/lib/libvirt/images/fd2.img count=1024 bs=1024

# mkfs.ext3 /var/lib/libvirt/images/fd2.img

2. Define a domain with floppy device connected with in config xml file

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd1.img'/>
      <target dev='fda' bus='fdc'/>
    </disk>


3. Start the domain

4. In guest, mount floppy device, do some write opertion to floppy device.

# mount /dev/fd0 /media

# echo "hello">/media/test.log

# cat /media/test.log

5. Prepare a xml as flollowing:

# cat floppy.xml
    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd2.img'/>
      <target dev='fda' bus='fdc'/>
    </disk>

6. Umount the media in floppy device (Make sure you umount first, direct update will cause guest io error)

# umount /media

7. Try to update the media in floppy device

# virsh update-device rhel6 floppy.xml

9. In guest, mount the new media and check the context of the media.

# mount /dev/fd2 /media

# ls /media

 
	
Expected Results:

4. test file is created successfully in floppy device with fd1.img

7.

# virsh update-device rhel6 cdrom.xml
Device updated successfully

9. Makes sure the new floppy image take effect, the test.log file is not exsitant.
Notes:
Comments:

		177314 	[Migration] Tunnelled spice migration - bug 591974 	weizhan 	None 	Manual 		Regression 	P2 	850 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Prepare 10 guests on each side (20 total) with shared image on nfs named mig0 ~ mig9 on source and mig10 ~ mig19 on target

3. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}

4: make sure install virt-viewer pkg
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    QE consumption

bug:

    851397 - From Run 44420
    852668 - From Run 44628

Actions:

1. Define a domain with spice graphic.
...
<graphics type='spice' port='5900' autoport='no' keymap='en-us'/>

 <channel type='spicevmc'>
 <target type='virtio' name='com.redhat.spice.0'/>
 <address type='virtio-serial' controller='0' bus='0' port='1'/>
 </channel>


...

2. Edit the following line in /etc/libvirt/qemu.conf both in source host and target host:
from:
# spice_listen = "0.0.0.0"
to 
spice_listen = "0.0.0.0"

3. Restart libvirtd service

4. Start the domain on source host (10.66.93.84)
# virsh start guest

5. Use virt-viewer/remote-viewer to connect the domain on source host.
#virt-viewer $guest

#remote-viewer spice://10.66.93.84:5900
 
(should test all these two command)

6. check  mouse after ï»¿ https://bugzilla.redhat.com/show_bug.cgi?id=787974 fixed

Do not start X  so need  init 3 

#init 3 

start a mouse 

# /etc/init.d/gpm start 

then check mouse whether works well 

 



6. Migrate the domain to destination host (10.66.92.185)
# virsh migrate --live --p2p --tunnelled guest qemu+ssh://10.66.92.185/system

 

8.   do step 6 again 

	
Expected Results:

Check that the spice client remains connected & active throughout migration & after completion.

 

mouse need works well 
Notes:
Comments:

		177732 	[update device flags] live modification 	jyang 	None 	Manual 		Feature 	P4 	850 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    device flags

bug:

    No bug found

Actions:

TBD
	
Expected Results:

TBD
Notes:
Comments:

		177317 	[Migration]Bi-directional p2p migration between 2 hosts. 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P2 	860 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Prepare 10 guests on each side (20 total) with shared image on nfs named mig0 ~ mig9 on source and mig10 ~ mig19 on target

3. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1.Start all the guest on both sides

   on source:

   for i in {0..9}; do virsh start mig$i; done

   on target:

   for i in {10..19}; do virsh start mig$i; done

2. Do migration on both sides concurrently with script

  # cat test.sh

  #!/bin/sh

  for i in {0..9};
  do
    ssh root@{target_ip} ./migrate-cmd.sh mig1$i &
    ./migrate-cmd.sh mig$i
  done

  # cat migrate-cmd.sh ( have this sh on both side with setting different ip of their target )

  #!/bin/sh

  virsh migrate --p2p $1 qemu+ssh://{ip}/system &

Run: # sh test.sh
	
Expected Results:

1. The migration of all guests on both sides should be successful.

2. Log in all the guests to check that they all work fine include network

3. Check the disk type, cpu and mem on all the guest are the same as before migration
Notes:
Comments:

		177734 	[Update device flags] report cdrom tray status - bug 723272 	gsun 	None 	Manual 		--default-- 	P2 	860 	Edit
Setup:
	
Breakdown:

TBD because the bug 723272
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    update device flags
    Regression

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177319 	[Migration]Change domain name during migration 	weizhan 	None 	Auto 		--default-- 	P1 	870 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    virsh-rail

bug:

    No bug found

Actions:

1. Start a domain with shared storage on nfs

    # virsh start guest

2. Do migration

   #virsh migrate --live test1 --dname ${new_guest_name} qemu+ssh://{target ip}/system 
	
Expected Results:

1. Migration succeed with ${new_guest_name} named guest running on target

2. Log in to the guests to check that it works fine include network

3. Check the disk type, cpu and mem on the guest are the same as before migration
Notes:
Comments:

		177735 	[Update device flags] RFE: implement insert-media and eject-media virsh commands - bug 713932 	gsun 	None 	Manual 		--default-- 	P2 	870 	Edit
Setup:

1. Prepare a running guest with cdrom empty.

# virsh dumpxml aaa

 ... 

 <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
          <target dev='hdc' bus='ide'/>
      <readonly/>
          <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>

 ...

2. prepare iso files for insert and change

# mkisofs -o /var/lib/libvirt/images/aa.iso /tmp

write a new file into /tmp, then

# mkisofs -o /var/lib/libvirt/images/bb.iso /tmp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Update device flags

bug:

    No bug found

Actions:

1. insert a CDROM

Create an XML file cd.xml containing the following:

<disk type='file' device='cdrom'>
      <source file='/var/lib/libvirt/images/aa.iso'/>
      <target dev='hdc' bus='ide'/>
</disk>

and execute:

# virsh update-device aaa cd.xml

2. check the cdrom content in the guest

    log in the guest

    # mount /dev/sr0 /mnt



3. change CDROM

Create an XML file cd-change.xml containing the following:

<disk type='file' device='cdrom'>
      <source file='/var/lib/libvirt/images/bb.iso'/>
      <target dev='hdc' bus='ide'/>
</disk>

and execute:

# virsh update-device aaa cd.xml


4. eject a CDROM
Ceate an XML file cd-eject.xml, containing the following:

<disk type='file' device='cdrom'>
  <target dev='hdc' bus='ide'/>
  <readonly/>
</disk>

and execute:

# virsh update-device aa cd-eject.xml

check whether the cd is ejected in guest.

	
Expected Results:

1.

Device updated successfully

2.

# ll /mnt

the content is as desired.

3.

error: Failed to update device from cd-change.xml
error: internal error unable to execute QEMU command 'change': Device 'drive-ide0-1-0' is locked

This not desired now, should be updated successfully

 

4.

error: Failed to update device from cd-eject.xml
error: internal error unable to execute QEMU command 'eject': Device 'drive-ide0-1-0' is locked

This not desired now, should be updated successfully

log in the guest

the cdrom should be empty
Notes:
Comments:

		177328 	[Migration]Migration name collision check 	weizhan 	None 	Manual (Autoproposed) 		Negative test 	P1 	880 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

  # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Define 2 vms on each side with same domain name (image and uuid are different)

2. Migration one vm from one host to another

    # virsh migrate --live vm qemu+ssh://{target ip}/system

3.Check if this have memory leak

# valgrind -v --leak-check=full virsh migrate <domain> qemu+ssh://<target
ip>/system --unsafe

 
	
Expected Results:

Step 2 Should be an error:

error: operation failed: domain 'kvm1' already exists with uuid 7e09e785-cf34-f6be-3ad4-3f749a4a2b63

Step 3. No memory leak
Notes:
Comments:

		177736 	[update device flags] update floppy disk on the fly with <readonly/> - bug 740702 	gsun 	None 	Manual 		--default-- 	P2 	880 	Edit
Setup:

This bug is deferred to 6.3, so this case is only for 6.3
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    update device flags

bug:

    No bug found

Actions:

ï»¿ï»¿1. Make 2 floppy disk image file

# dd if=/dev/zero of=/var/lib/libvirt/images/fd1.img count=1024 bs=1024

# mkfs.ext3 /var/lib/libvirt/images/fd1.img

# dd if=/dev/zero of=/var/lib/libvirt/images/fd1.img count=1024 bs=1024

# mkfs.ext3 /var/lib/libvirt/images/fd2.img

2. Define a domain with floppy device connected with in config xml file

<disk type='file' device='floppy'>
<driver name='qemu' type='raw'/>
<source file='/var/lib/libvirt/images/fd1.img'/>
<target dev='fda' bus='fdc'/>
<readonly/>
</disk>

3. Start the domain

4. Prepare a xml as flollowing:

# cat floppy.xml
<disk type='file' device='floppy'>
<driver name='qemu' type='raw'/>
<source file='/var/lib/libvirt/images/fd2.img'/>
<target dev='fda' bus='fdc'/>
<readonly/>
</disk>

5. Try to update the media int floppy device

# virsh update-device rhel6 floppy.xml

	
Expected Results:

5.

Actual results:
error: Failed to update device from floppy.xml
error: internal error unable to execute QEMU command 'change': Could not open
'/var/lib/libvirt/images/fd2.img'

Expected results:

Succes updated.
Notes:
Comments:

		177741 	[Video devices] cirrus 	gren 	gren 	Auto 		Feature 	P3 	890 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh-rail
    input/output device

bug:

    No bug found

Actions:

Only on KVM hypervisor
1. Add cirrus xml description into vm xml file, redefine the vm.
<video>
      <model type='cirrus' vram='9216' heads='1'/>
</video>
2.Start the vm and run lspci|grep VGA
Note:This is default model of video device kvm could support.
	
Expected Results:

00:02:0 VGA compatible controller: Cirrus Logic GD 5446
Notes:
Comments:

		177742 	[Video devices] Compare the resolution of display between 2 video types 	xhu 	None 	Manual 		Feature 	P3 	900 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    input/output device

bug:

    No bug found

Actions:

1 install a windows guest vm1 with cirrus video model type

the guest xml should include the following xml:

<video>
      <model type='cirrus' vram='9216' heads='1'/>
</video>

2 install a windows guest vm2 with vga video model type

the guest xml should include the following xml:

<video>
      <model type='vga' vram='9216' heads='1'/>

</video>

3 start vm1 and vm2
	
Expected Results:
1 Verify specifying video device with 'vga' should be able to give much higher resolution in a Windows guests than the 'cirrus' adapter.

For cirrus:
Min: 800 x 600
Max: 1280 x 1024

For vga:
Min: 800 x 600
Max: 2560 x 1600

Notes:
Comments:

		177743 	[Video devices] qxl---For Linux 	kxiong 	None 	Manual 		Feature 	P3 	910 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    input/output device

bug:

    No bug found

Actions:

1.Add qxl xml description into vm xml file,redefine the vm.
<graphics type='spice' port='5903' autoport='no' listen='$HostIP'/>
<video>
      <model type='qxl' vram='65536' heads='1'/>
</video>
2.Start the vm.
3.On a machine with spice-client package installed
run /usr/libexec/spicec -h $HostIP(such as 10.66.93.194) -p 5903
4.Login to the guest, and run lspci|grep VGA
	
Expected Results:

1.After step 3,you can connect to the guest by spice
2.After step 4,you can see 00:02:0 VGA compatible controller:Red Hat, Inc. Device 0100 (rev 01)
Notes:
Comments:

		177744 	[Video devices] qxl---For Windows 	kxiong 	None 	Manual 		Feature 	P3 	920 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    input/output device

bug:

    No bug found

Actions:

1.Add qxl xml description into vm xml file,redefine the vm.
<graphics type='spice' port='5903' autoport='no' listen='$HostIP'/>
<video>
      <model type='qxl' vram='65536' heads='1'/>
</video>
2.Start the vm.
3.On a machine with spice-client package installed
run /usr/libexec/spicec -h $HostIP(such as 10.66.93.194) -p 5903
4.Login to the guest,and install the qxl driver
Take win2008R2 for example,
Go to Device Manager->Display adapters,
select "Standard Vga",then right click the mouse,
select "Update Driver Software..."->
select "Browse my computer for driver software"->
select "Let me pick from a list of device drivers on my computer"-->
then select the hard drive,then browse for your windows driver and click Ok.
You can get the qxl drivers at:

https://brewweb.devel.redhat.com/buildinfo?buildID=190823

qxl_w7_x64.zip is for win7 x86_64
qxl_w7_x86.zip is for win7 x86_32
qxl_xp_x86.zip is for all xp x86_32  systems

Note: you must delete the dispaler driver clearly before you update the "Red Hat QXL GPU" driver 


5.Restart the vm,go to Device Manager->Display adapters.

	
Expected Results:

1.After step 3,you can connect to the guest by spice
2.After step 5,you can see the display adapter is "Red Hat QXL GPU"

   Since the digital signature of this driver is not verify by MS, there will be yellow exclamation mark about "Red Hat QXL GPU". That's normal.
Notes:
Comments:

		177745 	[Video devices] Remove video device 	ydu 	None 	Manual 		Feature 	P3 	930 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    input/output device

bug:

    No bug found

Actions:

1. Prepare a  healthy guest with video device.

#virsh list --all

Id Name                 State
----------------------------------
  - rhel6-x86_64         shut off

#virsh dumpxml  rhel6-x86_64

...
<video>
<model type='qxl' vram='9216' heads='1'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
</video>
...

2. Remove video device from guest

#virsh edit rhel6-x86_64

*remove the element: <video></video>

3. Check the guest's XML about video device

#virsh dumpxml  rhel6-x86_64

4. Remove elements: <graphics> and <video>

5. Check the guest's XML again

6. Start the guest.

 
	
Expected Results:

1.

2.

3. There's a 'cirrus' type video in the XML

...

<video>
<model type='cirrus' vram='9216' heads='1'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
</video>
...

4. 

5.  Both <graphic> and <video> elements don't exist.

6. The guest can boot up successfully.
Notes:
Comments:

		177746 	[Video devices] vga 	xhu 	None 	Auto 		Feature 	P3 	940 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh-rail
    input/output device

bug:

    No bug found

Actions:

1.Add vga xml description into vm xml file, define the vm.
<video>
      <model type='vga' vram='9216' heads='1'/>
</video>
2.Start the vm and run lspci|grep VGA

	
Expected Results:

Step 2:

rhel6 guest:

00:02:0 VGA compatible controller:Technical Corp.Device 1111

rhel5 guest:

00:02:0 VGA compatible controller:Technical Corp.Device 1111

 

If the guest is installed with rhel5, also verify the following string exists in /etc/X11/xorg.conf:
  Driver "vesa"
Notes:
Comments:

		177724 	[Update device flags] "persistent" update CD-ROM device - bug 598792 	xhu 	None 	Both 		Feature 	P2 	990 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    update device flags

bug:

    No bug found

Actions:

1. Prepare 2 different ISO files.

Create temp1.iso and temp2.iso

# mkisofs -o /var/lib/libvirt/image/temp1.iso /tmp

Do some changes in /tmp directory, such as, add/change/remove some new file to make a different iso file

# mkisofs -o /var/lib/libvirt/image/temp2.iso /tmp

2. Start a guest with a cdrom devcie which is connected with temp1.iso. Make sure the following is seen in your domain xml file.

......

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/temp1.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>
.......

3. Prepare a xml as following:

# cat cdrom.xml
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/temp2.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>

4. Try to update the media in cdrom device

# virsh update-device rhel6 cdrom.xml --persistent

	
Expected Results:

This is a known limitation of the implementation for the QEMU that is not
expected to be changed at this time.

Actual result now:

4 # virsh update-device rhel6 cdrom.xml --persistent

error: Failed to update device from cdrom.xml
error: Requested operation is not valid: cannot modify the persistent
configuration of a domain

Notes:
Comments:

		177725 	[Update device flags] "persistent" update VNC password 	xhu 	None 	Manual 		Feature 	P2 	1000 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    update device flags

bug:

    No bug found

Actions:


1. Define a domain with a vnc password in xml config file.

...

    <graphics type='vnc' port='-1' autoport='yes' listen='127.0.0.1' keymap='en-us' passwd='aaabbb'/>

...

2. Start the domain.

3. Use virt-viewer and virt-manager to log in guest windows

4. Input your vnc password in domain config xml file

5. Change vnc password on the fly.

# cat vnc_passwd.xml
    <graphics type='vnc' port='0' autoport='yes' listen='127.0.0.1' keymap='en-us' passwd='111111'/>

# virsh update-device nfs_test ps.xml   --persistent

	
Expected Results:

This is a known limitation of the implementation for the QEMU that is not
expected to be changed at this time.

actual result at this time

#  virsh update-device nfs_test ps.xml   --persistent
error: Failed to update device from ps.xml
error: Requested operation is not valid: cannot modify the persistent configuration of a domain

Notes:
Comments:

		177755 	[Virtio-serial] Hotplug of channel device 	nzhang 	None 	Manual (Autoproposed) 		Feature 	P2 	1000 	Edit
Setup:

there is a bug

https://bugzilla.redhat.com/show_bug.cgi?id=704955 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtio-serial

bug:

    704955 - From Run 43004

Actions:

1. Save the following XML as a file channel.xml.
# cat channel.xml
<controller type='virtio-serial' index='0'/>
<channel type='pty'>
  <target type='virtio' name='org.linux-kvm.port.0'/>
</channel>

2. Boot the guest domain.
# virsh start <domain>

3. Attach the XML file to a guest domain.
# virsh attach-device <domain> channel.xml

4. Verify if the channel device was attached to the guest.
# virsh dumpxml <domain>

5. Detach the channel device from the XML file.
# virsh detach-device channel.xml
	
Expected Results:

Channel device can be hotplug and unplug.
Notes:
Comments:

		177726 	[Update device flags] (QMPEjectNotify) QEMU driver does not notice when a guest OS ejects CDROM/Floppy device media - bug 575160 	gsun 	None 	Manual 		--default-- 	P2 	1010 	Edit
Setup:

Duplicate with case 124061
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    update device flags
    Regression

bug:

    No bug found

Actions:
	
Expected Results:



Notes:
Comments:

		177664 	[Storage] libvirt should places new added IDE disks after virtio/scsi disks - Bug 521053 	yoyzhang 	yoyzhang 	Auto 		Regression 	P3 	1020 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0

bug:

    No bug found

Actions:

1. Install a guest with virtio/ scsi disk image

2. Shut off the guest and add an IDE disk

3. Start the guest

4. # ps -ef|grep kvm

5.#virsh dumpxml <Domain>

checkpoint: -device (bold word in the "Expected Result") type in step 4 should consistent with the disk type of 'dumpxml' in step 5
	
Expected Results:

3.The guest is started successfully.

4. New added IDE disk should be placed after virtio / scsi disk

qemu      9318 31.0  0.4 808532 17348 ?        Sl   22:42   0:01
/usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp
1,sockets=1,cores=1,threads=1 -name rhel5_5_32_2 -uuid
6f569164-0a5a-b8c1-54e0-53095d311dc5 -nodefaults -chardev
socket,id=monitor,path=/var/lib/libvirt/qemu/rhel5_5_32_2.monitor,server,nowait
-mon chardev=monitor,mode=control -rtc base=utc -boot c -drive
file=/var/lib/libvirt/images/RHEL-Server-5.5-32-virtio-2.qcow2,if=none,id=drive-virtio-disk1,
boot=on,format=qcow2 -device virtio-blk-pci,bus=pci.0,addr=0x4,drive=drive-virtio-disk1,id=virtio-disk1
-drive file=/var/lib/libvirt/images/bb.img,if=none,id=drive-ide0-0-1,cache=off -device
ide-drive,bus=ide.0,unit=1,drive=drive-ide0-0-1,id=ide0-0-1 -netdev
tap,fd=20,id=hostnet0 -device
virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:92:92:17,bus=pci.0,addr=0x5
-chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc
127.0.0.1:0 -k en-us -vga cirrus -device
virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3

 5. 

.....
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel6_x86_64_kvm_raw.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/kvm1.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

.....
   




Notes:
Comments:

		177665 	[Storage] List vols 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P1 	1030 	Edit
Setup:

Save the following strings to an XML file 'pool-mig.xml'
---------------------------------
<pool type="netfs">
  <name>pool-mig</name>
  <source>
    <host name="10.66.90.121"/>
    <dir path="/vol/S3/libvirtmanual"/>
    <format type="auto"/>
  </source>
  <target>
    <path>/var/lib/libvirt/images</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

[Prerequisites]
1. Issue below command in gnome-terminal
   # virsh pool-define pool-mig.xml
2. Issue below command in gnome-terminal
   # virsh pool-start pool-mig
3. # virsh pool-list
4. # mount
[Steps]
5. Issue below comamnd:
   # virsh vol-list pool-mig
	
Expected Results:

1. Output:
    Pool pool-mig defined from pool-mig.xml
2. Output:
    Pool pool-mig started
3. List pool-mig, output sample:
   Name                 State      Autostart
-----------------------------------------
pool-mig             active     no    
4. Output sample:
   ..........
   10.66.71.226:/vol/kvm1/auto/images_good on /var/lib/libvirt/images type nfs (rw,addr=10.66.71.226)
5. List vols by pool-mig, output sample:
   Name                 Path                                   
-----------------------------------------
b.qcow2              /var/lib/libvirt/images/b.qcow2        
F11-PAE.qcow2        /var/lib/libvirt/image/F11-PAE.qcow2
...........................
Notes:
Comments:

		177666 	[Storage] Logical based storage pool 	nzhang 	yoyzhang 	Auto 		Feature 	P1 	1040 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Define a pool from an XML file, check if the pool was defined.
   Create an XML file, save as test-logical.xml:
     <pool type='logical'>
       <name>HostVG</name>
       <source>
         <name>HostVG</name>
         <format type='lvm2'/>
         <device path='/dev/sda11'/>
       </source>
       <target>
         <path>/dev/HostVG</path>
       </target>
     </pool>

   # virsh pool-define test-logical.xml
   Pool HostVG defined from test-logical.xml

2. Built the pool
# virsh pool-build HostVG


3. Start the inactive pool, check if the pool state was active.
   # virsh pool-start HostVG
   Pool HostVG started
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   HostVG         active     no

4. Destroy the pool, check if the pool state was inactive.
   # virsh pool-destroy HostVG
   Pool HostVG destroyed
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   HostVG         inactive   no



5. Delete the pool

# virsh pool-delete HostVG

6. Undefine the inactive pool, check if the pool can be undefined.
   # virsh pool-undefine HostVG
   Pool HostVG has been undefined
	
Expected Results:

Confirm if all the steps is correct.

 

step2:

# pvdisplay
  --- Physical volume ---
  PV Name               /dev/sda11
  VG Name               HostVG
  PV Size               53.16 GiB / not usable 3.14 MiB
  Allocatable           yes
  PE Size               4.00 MiB
  Total PE              13607
  Free PE               13607
  Allocated PE          0
  PV UUID               OuZh16-a3eH-p4mw-x1of-tPYH-F6L0-3BWTMY
   
# vgdisplay
  --- Volume group ---
  VG Name               HostVG
  System ID             
  Format                lvm2
  Metadata Areas        1
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                1
  Act PV                1
  VG Size               53.15 GiB
  PE Size               4.00 MiB
  Total PE              13607
  Alloc PE / Size       0 / 0   
  Free  PE / Size       13607 / 53.15 GiB
  VG UUID               gleybT-22Q9-QMo6-jI3G-BXkt-4Edq-EDusnG

step 5:

# pvdisplay

# vgdisplay
Notes:
Comments:

		177667 	[Storage] mpath based storage pool 	jialiu 	None 	Auto 		Feature 	P2 	1050 	Edit
Setup:

1. Confirm the rpm package is installed.
     device-mapper-multipath-0.4.9-41.el6.x86_64

2. # cp /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf /etc/multipath.conf

3. # /etc/init.d/multipathd start

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1.  Create a mpth pool xml file:

# cat mpath.xml
<pool type="mpath">
  <name>mpath-test</name>
  <target>
    <path>/dev/mapper</path>
  </target>
</pool>

2.  Define the mpath pool:

# virsh pool-define mpath.xml
Pool mpath-test defined from mpath.xml

[root@intel-5130-16-1 libvirt-test-API]# virsh pool-list --all
Name                 State      Autostart
-----------------------------------------
mpath-test           inactive   no     

3. Start the mpath pool

# virsh pool-start mpath-test
Pool mpath-test started

[root@intel-5130-16-1 libvirt-test-API]# virsh pool-list --all
Name                 State      Autostart
-----------------------------------------
mpath-test           active     no       

4. # virsh pool-refresh mpath-test
Pool mpath-test refreshed

5. # cat /var/log/libvirt/libvirtd.log, no error info for pool refreshing
02:21:11.869: 4677: debug : virStorageBackendMpathRefreshPool:319 :
conn=0x7fb7ec07f100, pool=0x7fb7e4002b80

6. Destroy the mpath-test pool

# virsh pool-destroy mpath-test
Pool mpath-test destroyed

# virsh pool-list --all
Name                 State      Autostart
-----------------------------------------
mpath-test           inactive   no        


7. Undefine the mpath pool

# virsh pool-undefine mpath-test
Pool mpath-test has been undefined

# virsh pool-list --all
Name                 State      Autostart
-----------------------------------------
default              active     yes       

	
Expected Results:
Notes:
Comments:

		177668 	[Storage] netfs based storage pool - bug 878400 	nzhang 	None 	Auto 		Feature 	P1 	1060 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    QE consumption
    virsh-rail

bug:

    889099 - From Run 54156

Actions:

1. Define a pool from an XML file, check if the pool was defined.
   Create an XML file, save as test-nfs.xml:
      <pool type="netfs">
        <name>test-nfs</name>
        <source>
          <host name="10.66.90.115"/>
          <dir path="/vol/libvirt1/auto"/>
        </source>
        <target>
          <path>/tmp/netfs</path>
        </target>
      </pool>

   # virsh pool-define test-nfs.xml
   Pool test-nfs defined from test-nfs.xml


2. Build the netfs pool, the non-existant mount point (/tmp/netfs) will be created successfully.

# virsh pool-build test-nfs
Pool test-nfs built


3. Start the inactive pool, check if the pool state was active.
   # virsh pool-start test-nfs
   Pool test-nfs started
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   test-nfs              active     yes


4. Create a volume in the netfs pool:

# cat vol-netfs-pool.xml

<volume>
    <name>vol_netfs_pool.qcow2</name>
    <capacity unit="G"> 1</capacity>
    <allocation unit="G">1</allocation>
    <target>
        <path>
            /tmp/netfs/vol_netfs_pool.qcow2
        </path>
        <format type="qcow2"/>
    </target>
</volume>

# virsh vol-create test-nfs vol-netfs-pool.xml
Vol vol_netfs_pool.qcow2 created from vol-netfs-pool.xml

Use "ll  /tmp/netfs/vol_netfs_pool.qcow2" to check the image is created successfully

5. Delete the crated volume, and check if the image file is deleted

# virsh vol-delete /tmp/netfs/vol_netfs_pool.qcow2
Vol /tmp/netfs/vol_netfs_pool.qcow2 deleted

# ll /tmp/netfs/vol_netfs_pool.qcow2
ls: cannot access /tmp/netfs/vol_netfs_pool.qcow2: No such file or directory

Add some steps to track the bug 878400:

a. Create vol_netfs_pool.qcow2 again:

# virsh vol-create test-nfs vol-netfs-pool.xml
Vol vol_netfs_pool.qcow2 created from vol-netfs-pool.xml

b. Attach it to a guest with virt-manager or virsh command, and keep guest running.


6. Destroy the pool, check if the pool state was inactive.
   # virsh pool-destroy test-nfs
   Pool test-nfs destroyed

Note: this command should fail, please refer to Expected Results

   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   test-nfs              inactive   yes

Note: and the pool state should be still active.

Destroy the guest, then redo step 6 and 7, should succeed.

7. Undefine the inactive pool, check if the pool can be undefined.
   # virsh pool-undefine test-nfs
   Pool test-nfs has been undefined
	
Expected Results:

Confirm if all the steps is correct.

Step 6:

After bug 878400 fixed, should get error like:

# virsh pool-destroy mypool
error: Failed to destroy pool mypool
error: internal error Child process (/bin/umount /var/lib/libvirt/images/mypool) status unexpected: exit status 1

 
Notes:
Comments:

		177671 	[storage] vol-info get the wrong "Type" for a directory --bug 750683 	nzhang 	None 	Manual 		Function 	P2 	1070 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage
    Regression

bug:

    No bug found

Actions:

# virsh vol-list default --details
Name          Path                                  Type   Capacity  Allocation
-------------------------------------------------------------------------------
demo-new.qed  /var/lib/libvirt/images/demo-new.qed  file    2.00 GB   320.00 KB
demo.img      /var/lib/libvirt/images/demo.img      file  100.00 MB   100.00 MB
demo.qed      /var/lib/libvirt/images/demo.qed      file    4.00 GB     1.32 GB
foo.img       /var/lib/libvirt/images/foo.img       file    6.00 GB     1.78 GB
pool-dir      /var/lib/libvirt/images/pool-dir      dir       0.00        0.00
win7.img      /var/lib/libvirt/images/win7.img      file   12.00 GB    11.18 GB

# virsh vol-info pool-dir --pool default
Name:           pool-dir
Type:           dir
Capacity:       0.00
Allocation:     0.00
	
Expected Results:

The volume type should be 'dir' instead of 'block'.

verify :

Name:           pool-dir
Type:           dir
Capacity:       0.00
Allocation:     0.00
Notes:
Comments:

		177681 	[storage]libvirt should use vgchange -aly/-aln instead of vgchange -ay/-an for clustered volume groups BZ#748248 	whuang 	whuang 	Manual 		Regression 	P1 	1080 	Edit
Setup:

a) vgchange -ay/-an
1. Build 2-node cluster
2. Create clustered volume group
3. Create logical storage pool in libvirt
4. Open/mount any volume on the first node
5. Try to destroy storage pool on the second node

 

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Regression
    storage

bug:

    No bug found

Actions:

 step

1) host1 and host2  use the same lvm via iscsi 
For build iscsi environment , you can refer case [Migration] migrate a guest based on iscsi storage


2) add lvm in the libvirt pool two hosts
<pool type='logical'>
  <name>vg</name>
  <uuid>7092394e-d5ee-301d-f2cc-21c0c0bb51a1</uuid>
  <capacity>1044381696</capacity>
  <allocation>104857600</allocation>
  <available>939524096</available>
  <source>
    <device path='/dev/sdf1'/>
    <name>vg</name>
    <format type='lvm2'/>
  </source>
  <target>
    <path>/dev/vg</path>
    <permissions>
      <mode>0700</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>


3)  start and create a lv 

#virsh vol-list vg
Name                 Path                                    
-----------------------------------------
ctest                /dev/vg/ctest  

  format lv
   #mkfs.ext3 /dev/vg/ctest

4) in the host1  mount it 
#mount  /dev/vg/ctest  /media 

5) in the host2  destroy the pool 
# virsh pool-destroy vg
Pool vg destroyed

 

	
Expected Results:

 

should not show error message like this : 

virsh pool-destroy vg

error: Failed to destroy pool vg
error: internal error '/sbin/vgchange -an vg'     

 

after step 5:

check the lv status is NOT available
# lvs
  LV      VG            Attr   LSize   Origin Snap%  Move Log Copy%  Convert
  ctest   vg            -wi--- 100.00m    

in the host1  check the lv status is available
# lvs
  LV      VG              Attr   LSize    Origin Snap%  Move Log Copy%  Convert
  ctest   vg              -wi-ao  100.00m

 
Notes:
Comments:

		177642 	[Storage] correct work flow for pool management 	vbian 	None 	Manual 		Function 	P1 	1090 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

# virsh pool-list --all

Name                 State      Autostart
-----------------------------------------
default              active     yes   

# virsh pool-undefine default
error: Failed to undefine pool default
error: Requested operation is not valid: pool is still active
	
Expected Results:

Make sure you can't remove active pool
Notes:
Comments:

		177646 	[Storage] Delete a vol - Bug 510450 	yoyzhang 	yoyzhang 	Auto 		Regression 	P2 	1100 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

[Prerequisites]
Create a vol using virt-manager or virsh command
[Steps]
1. Issue below command in gnome-terminal:
  # virsh vol-delete --pool pool-mig vol-test
OR # virsh vol-delete /var/lib/libvirt/images/vol-test 
2. Issue command:
 # virsh vol-list pool-mig
	
Expected Results:

1. Output:
    Vol vol-test deleted
2. List all vol by pool-mig, vol-test is removed, sample:
       ..............
   RHEL5.3-64-virtio.raw /var/lib/libvirt/images/RHEL5.3-64-virtio.raw      
win2003-32-virtio.qcow2 /var/lib/libvirt/images/win2003-32-virtio.qcow2
win2003-32-virtio.raw /var/lib/libvirt/images/win2003-32-virtio.raw
..........
Notes:
Comments:

		177647 	[Storage] Delete storage volumes from a pool residing on a root-squashed NFS share - BZ#612341 	nzhang 	None 	Manual 		Function 	P1 	1110 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage
    upstream

bug:

    No bug found

Actions:

1) create an nfs share on a server with the following line in /etc/exports:

       /export    *(rw,sync,root_squash)

   and run "exportfs -afrv" to make sure that the



2) create /export, owned by 107:107 (the qemu
   user:group) and permission 755 (NB: it is important that the
   permission *NOT* be 700 for the purposes of this test)

3) Create the file /tmp/pool.xml on the libvirtd host with the
   following data:

<pool type='netfs'>
  <name>netfs-pool</name>
  <capacity>408721293312</capacity>
  <allocation>65552777216</allocation>
  <available>343168516096</available>
  <source>
    <host name='10.66.4.232'/>
    <dir path='/export'/>
    <format type='nfs'/>
  </source>
  <target>
    <path>/var/lib/libvirt/images/netfs-pool</path>
    <permissions>
      <mode>0755</mode>
      <owner>107</owner>
      <group>107</group>
    </permissions>
  </target>
</pool>

(substitute your NFS server's IP address for 10.66.4.232)(AGAIN
Note that the mode is 755, *NOT* 700).

4) run the following in virsh (the lines in parentheses are the
   messages you should see from virsh):

   virsh # pool-define /tmp/pool.xml
   (Pool netfs-pool defined from /tmp/pool.xml)
   virsh # pool-build netfs-pool
   (Pool netfs-pool built)
   virsh # pool-start netfs-pool
   (Pool netfs-pool started)

5) Create /tmp/vol.xml containing the following:

<volume>
  <name>test.img</name>
  <key>/var/lib/libvirt/images/netfs-pool/test.img</key>
  <source>
  </source>
  <capacity>1048576000</capacity>
  <allocation>0</allocation>
  <target>
    <path>/var/lib/libvirt/images/netfs-pool/test.img</path>
    <format type='raw'/>
    <permissions>
      <mode>0644</mode>
      <owner>107</owner>
      <group>107</group>
      <label>system_u:object_r:nfs_t:s0</label>
    </permissions>
  </target>
</volume>

(again, note that the mode is 644, NOT 600).

6) in virsh, run the following:

   virsh # vol-create netfs-pool /tmp/vol.xml
   (Vol test.img created from /tmp/vol.xml)

Now examine the file /export/qemu/netfs-pool/test.img on the NFS
server. It should be owned by qemu:qemu (ie 107:107).

7) in virsh, run the following:

virsh # vol-delete --pool netfs-pool test.img
error: Failed to delete vol test.img
error: cannot unlink file '/var/lib/libvirt/images/netfs-pool/test.img':
Permission denied

 
	
Expected Results:

Confirm it doesn't be caused any errors on vol-delete.
Notes:
Comments:

		177648 	[Storage] Dir based storage pool 	nzhang 	yoyzhang 	Auto 		Feature 	P1 	1120 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Define a pool from an XML file, check if the pool was defined.
   Create an XML file, save as pool-dir.xml:
     <pool type="dir">
        <name>pool-dir</name>
        <target>
          <path>/var/lib/libvirt/images/pool-dir</path>
        </target>
      </pool>


# virsh pool-define pool-dir.xml
Pool pool-dir defined from pool-dir.xml

# virsh pool-list --all
Name                 State      Autostart
-----------------------------------------     
pool-dir             inactive   no      

2. Make sure /var/lib/libvirt/images/pool-dir is not exsting:

# ll /var/lib/libvirt/images/pool-dir
ls: cannot access /var/lib/libvirt/images/pool-dir: No such file or directory


3. Build the fs pool

# virsh pool-build pool-dir
Pool pool-dir built


4. Make sure /var/lib/libvirt/images/test-dir is created:

# ll  /var/lib/libvirt/images/pool-dir
total 0



5. Start the inactive pool, check if the pool state was active.
# virsh pool-start pool-dir
Pool pool-dir started

# virsh pool-list --all
Name                 State      Autostart
-----------------------------------------     
pool-dir             active     no   

 

6. Prepare a vol xml and create a volume in the pool:

<volume>
  <name>sparse.img</name>
  <source/>
  <capacity unit="G">1</capacity>
  <allocation>0</allocation>
  <target>
    <path>/var/lib/libvirt/images/pool-dir/sparse.img</path>
    <permissions>
      <label>virt_image_t</label>
    </permissions>
  </target>
</volume>

# virsh vol-create pool-dir vol-file.xml
Vol sparse.img created from vol-file.xml

7. Make sure volume is created:

# ll -Z /var/lib/libvirt/images/pool-dir/sparse.img
-rw-------. root root unconfined_u:object_r:virt_image_t:s0 /var/lib/libvirt/images/pool-dir/sparse.img

8. Delete the crated volume, and check if the image file is deleted

# virsh vol-delete /var/lib/libvirt/images/pool-dir/sparse.img
Vol /var/lib/libvirt/images/pool-dir/sparse.img deleted

# ll -Z /var/lib/libvirt/images/pool-dir/sparse.img
ls: cannot access /var/lib/libvirt/images/pool-dir/sparse.img: No such file or directory


9. Destroy the pool, check if the pool state was inactive.
# virsh pool-destroy pool-dir
Pool pool-dir destroyed

# virsh pool-list --all
Name                 State      Autostart
-----------------------------------------  
pool-dir             inactive   no   

10. Delete the pool, and check if the created dir is deleted.

# virsh pool-delete pool-dir
Pool pool-dir deleted

# ll /var/lib/libvirt/images/pool-dir
ls: cannot access /var/lib/libvirt/images/pool-dir: No such file or directory


11. Undefine the inactive pool, check if the pool can be undefined.
# virsh pool-undefine pool-dir
Pool pool-dir has been undefined

# virsh pool-list --all
Name                 State      Autostart
-----------------------------------------
	
Expected Results:

Confirm if all the steps is correct.
Notes:
Comments:

		177650 	[Storage] Disk based storage pool -- Bug 570286 	nzhang 	nzhang 	Both 		Feature 	P2 	1130 	Edit
Setup:

1, find a machine with two hard disk,

2, install an OS on only one harddisk, leaving the other without filesystem

 

such as:

intel-i7-12-1.englab.nay.redhat.com
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    Regression

bug:

    No bug found

Actions:

1. Prepare a disk pool xml:
# cat pool-disk-test.xml

ï»¿<pool type='disk'>
  <name>sdb</name>
  <source>
    <device path='/dev/sdb'>
    </device>
    <format type='dos'/>
  </source>
  <target>
    <path>/dev</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

2. define the disk pool
   # virsh pool-define pool-disk-test.xml
   Pool sdb defined from pool-disk-test.xml

3. Build the disk pool, that will format the /dev/sdb

# virsh pool-build sdb
Pool sdb built


4. Start the pool
   # virsh pool-start sdb
   Pool sdb started

   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   sdb            active     no

5. Parepare a vol xml file

# cat vol-partition-test.xml
<volume>
  <name>sdb5</name>
  <key>/dev/sdb5</key>
  <source>
    <device path='/dev/sdb'>
    </device>
  </source>
  <capacity unit='M'>100</capacity>
  <target>
    <path>/dev/sdb5</path>
  </target>
</volume>

6. Create a vol in the disk pool.
   # virsh vol-create sdb vol-partition-test.xml
   # virsh vol-list --pool sdb
   Name                 Path                                    
   -----------------------------------------
   sdb5                 /dev/sdb5                             

7. Mount the vol and test
   # mount /dev/sdb5 /mnt
   # cd /mnt
   # for i in {1..100}; do touch "hello${i}"; done
   # umount /mnt

8. Delete the vol in the disk pool
   # virsh vol-delete /dev/sdb5

9. Destroy the pool, check if the pool state was inactive.
   # virsh pool-destroy sdb
   Pool test-disk destroyed
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   sdb            inactive   no

10. Delete the inactive pool, check if the pool can be delete
    # virsh pool-delete sdb
    Pool sdb deleted

11. Undefine the inactive pool, check if the pool can be undefined.
   # virsh pool-undefine sdb
   Pool sdb has been undefined

	
Expected Results:

5. Use fdisk -l or parted tools to check the new created partition is seen in ouput.

7. Check if the file can be touched successfully

8. Use fdisk -l or parted tools to check the created partition is deleted successfully.

 

Confirm if all the steps is correct.
Notes:
Comments:

		177653 	[Storage] FileSystem based storage pool 	nzhang 	None 	Auto 		Feature 	P1 	1140 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Define a pool from an XML file, check if the pool was defined.
   Create an XML file, save as test-fs.xml:
     <pool type="fs">
       <name>test-fs</name>
       <source>
         <device path="/dev/sda3"/>
       </source>
       <target>
         <path>/media/images</path>
       </target>
     </pool>
   # virsh pool-define test-fs.xml
   Pool test-fs defined from test-fs.xml

2. Start the inactive pool, check if the pool state was active.
   # virsh pool-start test-fs
   Pool test-fs started
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   test-fs              active     yes

3. Destroy the pool, check if the pool state was inactive.
   # virsh pool-destroy test-fs
   Pool test-fs destroyed
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   test-fs              inactive   yes

4. Undefine the inactive pool, check if the pool can be undefined.
   # virsh pool-undefine test-fs
   Pool test-fs has been undefined
	
Expected Results:

Confirm if all the steps is correct.
Notes:
Comments:

		177654 	[Storage] Find potential storage pool sources - logical - Bug 509979 	yoyzhang 	yoyzhang 	Auto 		Regression 	P2 	1150 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Create a physical volume.

# pvcreate /dev/sda8

NOTE: make sure the data on the partition is not important, becasue this operation may damage the data.

2. Create a volume group

# vgcreate myvg /dev/sda8

3.find potential logical stroage pool

# virsh find-storage-pool-sources logical

4.
	
Expected Results:

1.

2.

3. This command should list all existing LVM volume groups on the system (even if libvirt has none set up). That's the point of the sources commands, it will enumerate existing shares that may exist outside of libvirt.


For 'find-storage-pool-sources', you can use:

echo "<source/>" > find.xml
virsh find-storage-pool-sources logical find.xml 

 

# virsh find-storage-pool-sources logical
<sources>
  <source>
    <device path='/dev/sda8'/>
    <name>myvg</name>
    <format type='lvm2'/>
  </source>
</sources>
Notes:
Comments:

		177655 	[Storage] Find potential storage pool sources - netfs 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	1160 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    837470 - From Run 42300

Actions:

1. # cat pool.xml 
  <source>
    <host name='10.66.90.115'/>
    <dir path='/vol/xenimage'/>
    <format type='nfs'/>
  </source>

2. # virsh find-storage-pool-sources netfs pool.xml 

	
Expected Results:

After step2, output

<sources>
  <source>
    <host name='10.66.90.115'/>
    <dir path='/vol/iscsi21'/>
    <format type='nfs'/>
  </source>
  <source>
    <host name='10.66.90.115'/>
    <dir path='/vol/iscsi22'/>
    <format type='nfs'/>
  </source>
  <source>
    <host name='10.66.90.115'/>
    <dir path='/vol/libvirt2/manual'/>
    <format type='nfs'/>
  </source>
  <source>
    <host name='10.66.90.115'/>
    <dir path='/vol/mshao/vdctest'/>
    <format type='nfs'/>
  </source>
......

Notes:
Comments:

		177656 	[Storage] Get vol basic info 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	1170 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

[Steps]
1. Issue below command:
 # virsh vol-info /var/lib/libvirt/images/winXP-32.raw
OR # virsh vol-info /var/lib/libvirt/images/winXP-32.raw --pool pool-mig

	
Expected Results:

1. Output vol basic info, sample:
   Name:           winXP-32.raw
Type:           file
Capacity:       10.00 GB
Allocation:     56.00 KB
Notes:
Comments:

		177657 	[Storage] Get vol detail info 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P1 	1180 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

[Steps]
1. Issue below command:
    # virsh vol-dumpxml /var/lib/libvirt/images/winXP-32.raw 
OR # virsh vol-dumpxml /var/lib/libvirt/images/winXP-32.raw --pool pool-mig
	
Expected Results:

1. Vol detail info is dumped out, sample:
    <volume>
  <name>winXP-32.raw</name>
  <key>/var/lib/libvirt/images/winXP-32.raw</key>
  <source>
  </source>
  <capacity>10737418240</capacity>
  <allocation>57344</allocation>
  <target>
    <path>/var/lib/libvirt/images/winXP-32.raw</path>
    <format type='raw'/>
    <permissions>
      <mode>0644</mode>
.....................
Notes:
Comments:

		191110 	[Storage] check vol sticky bit - bug 839463 	gsun 	gsun 	Manual (Autoproposed) 		--default-- 	P3 	1180 	Edit
Setup:

Please update the case after bug 839463 is fixed

 

# ll /var/lib/libvirt/images/rhel6u2
-rw-r--r--. 1 root root 10737418240 Aug  3 18:15 /var/lib/libvirt/images/rhel6u2
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage

bug:

    839463 - From Run 44254
    839463 - From Run 47630
    839463 - From Run 48367
    839463 - From Run 50663
    839463 - From Run 54156

Actions:

[Steps]
1. Issue below command:
    # virsh vol-dumpxml /var/lib/libvirt/images/rhel6u2
OR # virsh vol-dumpxml /var/lib/libvirt/images/rhel6u2 --pool pool-mig

2. Change the permission of image to 677

# chmod 677 /var/libvirt/images/rhel6u2

# virsh vol-dumpxml /var/lib/libvirt/images/rhel6u2

3. change vol with sticky bit

# chmod 1777 /var/lib/libvirt/images/rhel6u2

 # virsh vol-dumpxml /var/lib/libvirt/images/rhel6u2

4. change user:group

# chmod 107:107 /var/libvirt/images/rhel6u2

 # virsh vol-dumpxml /var/lib/libvirt/images/rhel6u2

5. create a new vol with sticky bit

# qemu-img /var/lib/libvirt/images/rhel6q.img

# chmod 7777 rhel6q.img 
# ll
total 136
-rwsrwsrwt. 1 root root 262144 Jul 13 09:53 rhel6q.img

# cat /tmp/vol.xml 
<volume>
  <name>rhel6q.img</name>
  <key>/var/lib/libvirt/images/rhel6q.img</key>
  <source>
  </source>
  <capacity unit='bytes'>10737418240</capacity>
  <allocation unit='bytes'>139264</allocation>
  <target>
    <path>/var/lib/libvirt/images/rhel6q.img</path>
    <format type='qcow2'/>
    <permissions>
      <mode>7777</mode>
      <owner>0</owner>
      <group>0</group>
      <label>unconfined_u:object_r:virt_image_t:s0</label>
    </permissions>
  </target>
</volume>

# virsh vol-create default /tmp/vol.xml 

 
	
Expected Results:

1. Vol detail info is dumped out, sample:
<volume>
  <name>libvirt-test-api</name>
  <key>/var/lib/libvirt/images/rhel6u2</key>
  <source>
  </source>
  <capacity unit='bytes'>10737418240</capacity>
  <allocation unit='bytes'>1177894912</allocation>
  <target>
    <path>/var/lib/libvirt/images/rhel6u2</path>
    <format type='raw'/>
    <permissions>
      <mode>0644</mode>
      <owner>0</owner>
      <group>0</group>
      <label>system_u:object_r:virt_image_t:s0</label>
    </permissions>
  </target>
</volume>

2.

...

    <permissions>
      <mode>0677</mode>
      <owner>0</owner>
      <group>0</group>
      <label>system_u:object_r:virt_image_t:s0</label>
    </permissions>

...

3.

...

    <permissions>
      <mode>1777</mode>
      <owner>0</owner>
      <group>0</group>
      <label>system_u:object_r:virt_image_t:s0</label>
    </permissions>

...

4.

...

    <permissions>
      <mode>1777</mode>
      <owner>107</owner>
      <group>107</group>
      <label>system_u:object_r:virt_image_t:s0</label>
    </permissions>

...

 

5.

For now with bug:

error: Failed to create vol from /tmp/vol.xml
error: XML error: malformed octal mode

 This should succes.
Notes:
Comments:

		177658 	[Storage] Get vol key by name or path 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	1190 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

[Steps]
1. Get vol key by path with below command:
  # virsh vol-key /var/lib/libvirt/images/winXP-32.raw
	
Expected Results:

1. Output vol key correctly, sample:
   /var/lib/libvirt/images/winXP-32.raw
Notes:
Comments:

		177659 	[Storage] Get vol name by key or path 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	1200 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

[Steps]
1. Get vol name by path with below command:
  # virsh vol-name /var/lib/libvirt/images/winXP-32.raw
OR get vol name by key with below command:
  # virsh vol-name /var/lib/libvirt/images/winXP-32.raw
	
Expected Results:

1. Output vol name correctly, sample:
   winXP-32.raw
Notes:
Comments:

		177660 	[Storage] Get vol path by name or key - Bug 509306 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	1210 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0

bug:

    No bug found

Actions:

[Prerequisites]
Have finished test case '17262' with summary 'Get vol detail info'
[Steps]
1. Get vol path by key with below command:
  # virsh vol-path /var/lib/libvirt/images/winXP-32.raw
OR get vol path by name with below command:
  # virsh vol-path  winXP-32.raw --pool pool-mig
	
Expected Results:

1. Output vol path correctly, sample:
   /var/lib/libvirt/images/winXP-32.raw
Notes:
Comments:

		177661 	[Storage] Get vol path by name or key - Bug 509306 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	1220 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    Regression
    virsh-rail

bug:

    No bug found

Actions:

[Prerequisites]
Have finished test case '17262' with summary 'Get vol detail info'
[Steps]
1. Get vol path by key with below command:
  # virsh vol-path /var/lib/libvirt/images/winXP-32.raw
OR get vol path by name with below command:
  # virsh vol-path  winXP-32.raw --pool pool-mig
	
Expected Results:

1. Output vol path correctly, sample:
   /var/lib/libvirt/images/winXP-32.raw
Notes:
Comments:

		177663 	[Storage] iSCSI based storage pool 	nzhang 	yoyzhang 	Auto 		Feature 	P2 	1230 	Edit
Setup:

you can get the iscsi storage by setting /etc/iscsi/initiatorname.iscsi

InitiatorName=iqn.1994-05.com.redhat:libvirt

Then run

#service iscsid restart

# iscsiadm --mode discovery --type sendtargets --portal10.66.90.100

to discover the iscsi disk
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Define a pool from an XML file, check if the pool was defined.
   Create an XML file, save as test-iscsi.xml:
     <pool type='iscsi'>
       <name>test-iscsi</name>
       <source>
         <host name="10.66.90.100"/>
         <device path="iqn.2001-05.com.equallogic:0-8a0906-26f1f7d03-1c7267779594c286-dyuan-1"/>
       </source>
       <target>
         <path>/dev/disk/by-path</path>
       </target>
     </pool>
   # virsh pool-define test-iscsi.xml
   Pool test-iscsi defined from test-iscsi.xml

2. Start the inactive pool, check if the pool state was active.
   # virsh pool-start test-iscsi
   Pool test-iscsi started
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   test-iscsi           active     yes

3. 

[root@zhpeng ~]# scsi_id --export --whitelisted /dev/disk/by-path/ip-10.66.6.209\:3260-iscsi-iqn.2011-12.zhpeng\:disk1-lun-1 
ID_SCSI=1
ID_VENDOR=IET
ID_VENDOR_ENC=IET\x20\x20\x20\x20\x20
ID_MODEL=VIRTUAL-DISK
ID_MODEL_ENC=VIRTUAL-DISK
ID_REVISION=0001
ID_TYPE=disk
ID_SERIAL_RAW="1IET     00010001"
ID_SERIAL=1IET_00010001
ID_SERIAL_SHORT=IET_00010001
ID_SCSI_SERIAL=                              beaf11

 

4. # virsh vol-list --pool test-iscsi
Name Path
-----------------------------------------
unit:0:0:0           /dev/disk/by-path/ip-10.66.6.209:3260-iscsi-iqn.2011-12.zhpeng:disk1-lun-1

5. # virsh vol-dumpxml --pool test-iscsi unit\:0\:0\:0

<volume>
  <name>unit:0:0:0</name>
  <key>1IET_00010001</key>
  <source>
  </source>
  <capacity>10737418240</capacity>
  <allocation>10737418240</allocation>
  <target>
    <path>/dev/disk/by-path/ip-10.66.6.209:3260-iscsi-iqn.2011-12.zhpeng:disk1-lun-1</path>
    <format type='lvm2'/>
    <permissions>
      <mode>0660</mode>
      <owner>0</owner>
      <group>6</group>
      <label>system_u:object_r:fixed_disk_device_t:s0</label>
    </permissions>
  </target>
</volume>



6. Destroy the pool, check if the pool state was inactive.
   # virsh pool-destroy test-iscsi
   Pool test-iscsi destroyed
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   test-iscsi           inactive   yes

7. Undefine the inactive pool, check if the pool can be undefined.
   # virsh pool-undefine test-iscsi
   Pool test-iscsi has been undefined
	
Expected Results:

Confirm the following:

1) volume name is changed to unit:*:*:*  

2) key is changed to serial identifier
Notes:
Comments:

		177618 	[SR-IOV] Save a guest with assigned VF 	yoyzhang 	None 	Auto 		Feature 	P2 	1240 	Edit
Setup:

A guest with assigned VF
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. Save the guest with VF

# virsh save nfs_test /tmp/nfs_test.save
	
Expected Results:

2. Fail with error info

# virsh save nfs_test /tmp/nfs_test.save
error: Failed to save domain nfs_test to /tmp/nfs_test.save
error: internal error unable to execute QEMU command 'migrate': An undefined error has ocurred

Notes:
Comments:

		177619 	[SR-IOV] Shutdown guest with assigned VF 	yoyzhang 	yoyzhang 	Manual 		Feature 	P2 	1250 	Edit
Setup:

The guest is assigned with VF
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. Shutdown the guest wtih assigned VF

2. Start the guest again
	
Expected Results:

1. Check guest dump xml file to confirm assigned VF still exists

2. Make sure the assigned VF works well in the guest after restart
Notes:
Comments:

		177620 	[SR-IOV] Suspend and resume guest with assigned VF 	yoyzhang 	yoyzhang 	Auto 		Feature 	P2 	1260 	Edit
Setup:

A guest with assigned VF/PF
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. Suspend the guest with VF/PF with command

# virsh suspend kvm

2. Resume the guest

# virsh resume kvm
	
Expected Results:

2. After the guest is resumed, the VF/PF should still work normally

- could be listed out by #lspci

- could get ip and ping to google.com
Notes:
Comments:

		177623 	[Stable guest ABI] Stable domain compatibility for Linux guest 	yimwang 	None 	Manual 		Feature 	P3 	1260 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Install a Linux guest (with virtio or with non-virtio disk & interface) on RHEL-5.4 or later version RHEL host.

2. Login the guest and check the PCI address.
   #lspci

3. Move the Linux guest to latest RHEL host.
   # virsh dumpxml foo > foo.xml
   # cat foo.xml
     ...
     <os>
       <type arch='x86_64' machine='rhelX.X.X'>hvm</type>
       <boot dev='hd'/>
     </os>
     ...

4. Keep the machine type with 'rhelX.X.X' and define the dumped XML in RHEL-6.0 host.
   # virsh define foo.xml
   
5. Copy the guest image from RHEL-5.4 or later host to latest host, and place in appropriate location.

6. Start domain.
   # virsh start guestname  

7.Login the guest and check the PCI address.
   #lspci
	
Expected Results:

6. VM start successfully.


7. Confirm all the PCI address allocation using lspci command and verify they have not be changed in guest.
Notes:
Comments:

		177621 	[SR-IOV] tight loop of hotplug/unplug PF to guest on SR-IOV host-- Bug 635669 	vbian 	None 	Manual 		Regression 	P1 	1270 	Edit
Setup:

1. Enable VT-d on your host                                               
2. edit the /boot/grub/grub.conf like this
Add the kernel option 'intel_iommu=on'

default=0
timeout=0
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu
title Fedora (2.6.31.5-127.fc12.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.31.5-127.fc12.x86_64 ro
root=/dev/mapper/vg_intelx5550121-lv_root  LANG=en_US.UTF-8
SYSFONT=latarcyrheb-sun16 KEYBOARDTYPE=pc KEYTABLE=us intel_iommu=on rhgb
quiet
        initrd /initramfs-2.6.31.5-127.fc12.x86_64.img
3.Power off your testing machine ,then power on

4.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. Start a guest

On host

2. # lspci |grep 82576

3. Select one PF device

# lspci -n | grep 42:00.0

4. # virsh nodedev-dettach pci_8086_10c9_0

5. # virsh nodedev-reset pci_8086_10c9_0

6. In the guest load acpiphp mode

# modprobe acpiphp

7. # virsh nodedev-dumpxml pci_8086_10c9_0

8. Edit vtd.xml (according to the bus,slot,function number you got from nodedev-dumpxml command)

   <hostdev mode='subsystem' type='pci'>
            <source>
                <address bus='66' slot='0' function='0'/>
            </source>
    </hostdev>
9.run the following script:
   # cat script.sh
   #!/bin/sh
   for ((i=0; i < 300; i++))
   do
   echo $i
   virsh attach-device vm vtd.xml
   sleep 5
   virsh detach-device vm vtd.xml
   sleep 5
   done
   # sh script.sh

	
Expected Results:

1. After 300 times , you won't get Call Trace ,kernel panic ,and also libvirtd keeps running after the for loop.
Notes:
Comments:

		177625 	[Stable guest ABI] Stable domain compatibility for Windows guest 	yimwang 	None 	Manual 		Feature 	P3 	1270 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Install a Windows guest (with virtio or with non-virtio disk & interface) on RHEL-5.4 or later verison RHEL host.

2. Move the Windows guest to latest RHEL host.
   # virsh dumpxml foo > foo.xml
   # cat foo.xml
     ...
     <os>
       <type arch='x86_64' machine='rhelX.X.X'>hvm</type>
       <boot dev='hd'/>
     </os>
     ...
     
3. Keep the machine type with 'rhelX.X.X' and define the dumped XML in latest host.
   # virsh define foo.xml

4. Copy the guest image from RHEL-5.4 or later host to latest host, and place in appropriate location.

5. Start domain.
   # virsh start guestname 
	
Expected Results:

5. VM start successfully.

5.1. Confirm there is no blue screen during booting, no prompt of re-activate the license keys
   and no warning or errors in Hardware list for the guest os.

Notes:
Comments:

		177193 	[Managed save] A managed save file for a domain can be manually removed 	yimwang 	None 	Auto 		--default-- 	P1 	1280 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Run a domain:
   # virsh start rhel6

conduct some operation in guest.

2. Take a managed save of the domain:
# virsh managedsave rhel6


3. # ls /var/lib/libvirt/qemu/save/rhel6.save -lh
-rw-------. 1 root root 280M Jul 19 18:00 /var/lib/libvirt/qemu/save/rhel6.save

4. Remove the managed save file:
   # virsh managedsave-remove rhel6

5. 
# ls /var/lib/libvirt/qemu/save/rhel6.save -lh


6. Run the domain:
   # virsh start rhel6


	
Expected Results:

step 2:

# virsh managedsave rhel6
Domain rhel6 state saved by libvirt

 

step 4:

# virsh managedsave-remove rhel6
Removed managedsave image for domain rhel6

step 5:

ls: cannot access /var/lib/libvirt/qemu/save/rhel6.save: No such file or directory

step 6:

The domain should boot normally, and NOT resume where it left off.
Notes:
Comments:

		177622 	[SR-IOV]tight loop of hotplug/unplug VF to guest on SR-IOV host-- Bug 635669 	vbian 	None 	Manual 		Regression 	P1 	1280 	Edit
Setup:

1. Enable VT-d on your host                                               
2. edit the /boot/grub/grub.conf like this
Add the kernel option 'intel_iommu=on'

default=0
timeout=0
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu
title Fedora (2.6.31.5-127.fc12.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.31.5-127.fc12.x86_64 ro
root=/dev/mapper/vg_intelx5550121-lv_root  LANG=en_US.UTF-8
SYSFONT=latarcyrheb-sun16 KEYBOARDTYPE=pc KEYTABLE=us intel_iommu=on rhgb
quiet
        initrd /initramfs-2.6.31.5-127.fc12.x86_64.img
3.Power off your testing machine ,then power on

4.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. List  pci device with SR-IOV function in the box

( in my example , I use the intel 82576 network interface card which supports
the SR-IOV function )

# lspci |grep 82576

2. Confirm the driver is built as a module

# lsmod |grep igb

3.  max_vfs value controls the number  of virtual network card
Valid Range:   0-7

# modprobe -r igb


# modprobe igb max_vfs=7

4. Relist the pci device relevant Intel 82576 network device

# lspci |grep 82576

5. Show the VF product ID and vendor ID

# lspci -n | grep 42:10.0

6. List availiable node devices on host

# virsh nodedev-list

7. Dump VF detail info

# virsh nodedev-dumpxml pci_8086_10ca

8. Add this to vtd.xml
   # cat vtd.xml
          <hostdev mode='subsystem' type='pci'>
            <source>
              <address bus='66' slot='16' function='1'/>
            </source>
          </hostdev>

9.Detach this VF device from host

# virsh nodedev-dettach pci_8086_10ca

10. # virsh define guest.xml

11. # virsh start guest


12.run the following script:
   # cat script.sh
   #!/bin/sh
   for ((i=0; i < 300; i++))
   do
   echo $i
   virsh attach-device guest vtd.xml
   sleep 5
   virsh detach-device guest vtd.xml
   sleep 5
   done
   # sh script.sh
	
Expected Results:

1. After 300 times , you won't get Call Trace ,kernel panic ,and also libvirtd keeps running after the for loop.
Notes:
Comments:

		177633 	[Start & Destroy] start & destroy 1000 guest for 10 rounds on host (96 cores CPU, 1TB mem) 	jiachen 	None 	Auto 		Stress 	P2 	1290 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177634 	[Start & Destroy] start & destroy a guest for 1000 rounds on host (64 cores CPU, 512GB mem) 	jiachen 	None 	Auto 		Stress 	P2 	1300 	Edit
Setup:

have no auto script on virsh-rail
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177635 	[Start & Destroy] start & destroy a guest for 1000 rounds on host (96 cores CPU, 1TB mem) 	jiachen 	None 	Auto 		Stress 	P2 	1310 	Edit
Setup:

should be the same with https://tcms.engineering.redhat.com/case/95556/?from_plan=4050
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177636 	[storage] Client blocks indefinitely on virStream API BZ#741337 	whuang 	whuang 	Manual 		Regression 	P1 	1320 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Regression
    storage

bug:

    No bug found

Actions:

#virsh -c qemu+ssh://10.66.4.233/system vol-download foo.img /var/lib/libvirt/images/foo1.img --pool default

root@10.66.4.233's password:
	
Expected Results:

get vol successfully
Notes:
Comments:

		177701 	[sVirt] Guest with sVirt on root_squash NFS 	jialiu 	None 	Auto 		Feature 	P2 	1320 	Edit
Setup:

Make sure your os is in enforcing mode, and allow virt nfs access.

# setenforce 1

# getenforce
Enforcing

# setsebool virt_use_nfs on

# getsebool -a|grep virt_use_nfs
virt_use_nfs --> on

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    QE consumption
    virsh-rail

bug:

    No bug found

Actions:

1. Setup a nfs server shrare dir with root_squash

# cat /etc/exports
/var/lib/libvirt/images/     *(rw,root_squash)

2. On test machine(nfs client), mount the share dir.

# mount <nfs_server_ip>:/var/lib/libvirt/images/ /var/lib/libvirt/images/

3. Edit /etc/libvirt/qemu.conf

dynamic_ownership = 0

4. Make sure the share image file is owned by qemu:qemu

#service libvirtd restart

# ll /var/lib/libvirt/images/guest.img
-rw-------. 1 qemu qemu 8589934592 Jan 13  2011 /var/lib/libvirt/images/guest.img

If not, please log onto NFS server, change ownership to qemu:qemu via chown command

5. Start a domain using the nfs image file.

# virt-install -n guest -r 1024 -f /var/lib/libvirt/images/guest.img --import 
	
Expected Results:

5. Start the vm successfully.
Notes:
Comments:

		177637 	[storage] Libvirtd hangs forever when destroying multiple domains with disks on an unreachable NFS storage. BZ#746666 	whuang 	whuang 	Manual (Autoproposed) 		Regression 	P1 	1330 	Edit
Setup:

Description:
Libvirtd hangs forever when destroying multiple domains with disks on an
unreachable NFS storage.

 

 

if test it via pure libvirt : 

1, setup a NFS server(eg: ip 10.66.5.5) and a shared folder.

# cat /etc/exports
/var/lib/libvirt/images        *(async,rw,all_squash,anonuid=36,anongid=36)

# service nfs restart

# iptables -F

2, On the test machine, mount the shared folder with option "soft"   10.66.5.5:/var/lib/libvirt/images/ on /mnt/nfsdir type nfs (rw,soft,vers=4,addr=10.66.5.5,clientaddr=<IP>)   NOT -o vers=3

# mount -o soft 10.66.5.5:/var/lib/libvirt/images /mnt

# setsebool virt_use_nfs=1

Note: <If neither soft/hard option  is  specified  (or if the hard option is specified), NFS requests are retried indefinitely.  If the soft option is specified, then the NFS client  fails an NFS  request  after  retrans retransmissions have been sent, causing the NFS client to return an error to the calling application.>

<If the option timeo is not specified, requests are retried every 60 seconds for NFS over TCP, If the retrans option is not specified, the NFS client tries each  request 3 times.>

 

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Regression
    storage
    QE consumption

bug:

    837485 - From Run 42159
    851397 - From Run 44420

Actions:

Reproduce steps:
1) Run 2 vms with disks on a NFS share. in RHEVM 
2) Block connection to NFS server. (iptables)

iptable -A OUTPUT -d $IP -j DROP
3) Destroy the vms.


pure libvirt : 

# mount -o soft 10.66.5.5:/var/lib/libvirt/images /mnt      

add iptalbes for NFS server

#iptables -A OUTPUT -d $NFS-server-ip -p tcp --dport 2049 -j DROP

 

	
Expected Results:

libvirtd should not hang forever.

Notes:
Comments:

		177713 	[sVirt] Save/Restore guest on no_root_squash NFS 	jialiu 	None 	Auto 		Feature 	P2 	1330 	Edit
Setup:

Make sure your os is in enforcing mode, and allow virt nfs access.

# setenforce 1

# getenforce
Enforcing

# setsebool virt_use_nfs on

# getsebool -a|grep virt_use_nfs
virt_use_nfs --> on
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    virsh-rail

bug:

    No bug found

Actions:

1. Setup a nfs shrare dir with root_squash

# cat /etc/exports
/var/lib/libvirt/migrate/     *(rw,no_root_squash)

2. On test machine, mount the nfs share.

# mount <nfs_server_ip>:/var/lib/libvirt/migrate/ /var/lib/libvirt/migrate/
3. Make sure qemu user have write permission to the share dir.

# ls -ld /var/lib/libvirt/migrate/
-rwx-------. 1 qemu qemu 4096 Jan 13  2011 /var/lib/libvirt/migrate/

If not, please log onto NFS server, change ownership and permission to qemu:qemu via chown command.

#chown qemu:qemu /var/lib/libvirt/migration/

4. Start a domain.

5. Save the running domain to the nfs share direcotory.

# virsh save <guestname> /var/lib/libvirt/migrate/save

# ll /var/lib/libvirt/migrate/

6. Restore the domain.

# virsh restore /var/lib/libvirt/migrate/save
	
Expected Results:

5. Save operation is fine, and saved file is generated successfully.

6. Restore operation is fine.
Notes:
Comments:

		177638 	[storage] creating libvirt storage pools fails for lvm volume groups with striped volumes - Bug 727474 	whuang 	whuang 	Manual 		Regression 	P1 	1340 	Edit
Setup:

1) need two disk partation for pvcreate

  > pvcreate /dev/sda6 Physical volume "/dev/sda6" successfully created

> pvcreate /dev/sda7 Physical volume "/dev/sda7" successfully created

2) add two pv into a vg

> vgcreate vg_ssd /dev/sda6 /dev/sda7
  Volume group "vg_ssd" successfully created

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Regression
    storage

bug:

    No bug found

Actions:

> lvcreate --size 2GB --name test_nostripes vg_ssd
  Logical volume "vmtest" created
> vgchange -a y vg_ssd
  1 logical volume(s) in volume group "vg_ssd" now active

> lvcreate --size 2GB --stripes 2 --stripesize 8kb --name test_stripes vg_ssd
  Logical volume "test_stripes" created
> vgchange -a y vg_ssd
  2 logical volume(s) in volume group "vg_ssd" now active

> stat /dev/vg_ssd/test_stripes 
  File: `/dev/vg_ssd/test_stripes' -> `../dm-1'
  Size: 7          Blocks: 0          IO Block: 4096   symbolic link
Device: 5h/5d Inode: 2166057     Links: 1
Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Access: 2011-10-12 13:51:55.633639378 +0800
Modify: 2011-10-12 13:51:55.633639378 +0800
Change: 2011-10-12 13:51:55.633639378 +0800


> virsh -d 5 pool-create-as vg_ssd logical --target /dev/vg_ssd
Pool vg_ssd created


> virsh pool-list 
Name                 State      Autostart 
-----------------------------------------
default              active     yes       
r6                   active     yes       
sda5                 active     no        
vg_ssd               active     no        

> virsh vol-list vg_ssd
Name                 Path                                    
-----------------------------------------
test_nostripes       /dev/vg_ssd/test_nostripes              
test_stripes         /dev/vg_ssd/test_stripes

	
Expected Results:

no error
Notes:
Comments:

		177714 	[sVirt] Save/Restore guest on root_squash NFS 	jialiu 	None 	Auto 		Feature 	P1 	1340 	Edit
Setup:

Make sure your os is in enforcing mode, and allow virt nfs access.

# setenforce 1

# getenforce
Enforcing

# setsebool virt_use_nfs on

# getsebool -a|grep virt_use_nfs
virt_use_nfs --> on

 

Reference bug:

Bug 643884 - cannot save/restore domain from root_squashing nfs export even if qemu gid matches nfs
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    virsh-rail

bug:

    No bug found

Actions:

1. Setup a nfs shrare dir with root_squash

# cat /etc/exports
/var/lib/libvirt/migrate/     *(rw,root_squash)

2. On test machine, mount the nfs share.

# mount <nfs_server_ip>:/var/lib/libvirt/migrate/ /var/lib/libvirt/migrate/
3. Make sure qemu user have write permission to the share dir.

# ls -ld /var/lib/libvirt/migrate/
-rwx-------. 1 qemu qemu 4096 Jan 13  2011 /var/lib/libvirt/migrate/

If not, please log onto NFS server, change ownership and permission to qemu:qemu via chown command.

#chown qemu:qemu /var/lib/libvirt/migration/

#chmod 755 /var/lib/libvirt/migration/

Note: Do not use 777 or 774 mod of /var/lib/libvirt/migration/ dir

4. Start a domain.

5. Save the running domain to the nfs share direcotory.

# virsh save <guestname> /var/lib/libvirt/migrate/save

# ll /var/lib/libvirt/migrate/

6. Restore the domain.

# virsh restore /var/lib/libvirt/migrate/save



	
Expected Results:

5. Save operation is fine, and saved file is generated successfully.

6. Restore operation is fine.
Notes:
Comments:

		177639 	[storage] libvirt should skip not activated/suspended volumes during pool-create/pool-refresh for logical pools BZ#748282 	whuang 	whuang 	Manual 		Regression 	P1 	1350 	Edit
Setup:

1) create  a pv 

#pvcreate ...

2) create a vg  

#vgcreate ... 

3)  add a vg pool in the libvirt 

<pool type='logical'>
  <name>vg</name>
  <uuid>9af81e46-41e2-3a2a-bb6e-07c9c0ceab73</uuid>
  <capacity>21470642176</capacity>
  <allocation>1048576000</allocation>
  <available>20422066176</available>
  <source>
    <device path='/dev/sdb2'/>
    <name>vg01</name>
    <format type='lvm2'/>
  </source>
  <target>
    <path>/dev/vg01</path>
    <permissions>
      <mode>0700</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    Regression
    rhel6.5

bug:

    No bug found

Actions:

 

 

1)  create 2 lv in the pool   lv1 , lv2

 

2) disable lv1   

#ï»¿lvchange -an /dev/vg01/lv1

 

3)check lv1 

#lvdisplay 

  --- Logical volume ---
  LV Name                /dev/vg01/lv1
  VG Name                vg01
  LV UUID                bEexiA-nQTl-WS4v-kLns-xxkA-96mT-BSJeml
  LV Write Access        read/write
 LV Status NOT available
  LV Size                1000.00 MiB
  Current LE             250
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto

 

refresh  vg pool  

4)#virsh pool-refresh vg

 

 

ï»¿
	
Expected Results:

 

4)  should not show the message like this: 

error: Failed to refresh pool vg
error: End of file while reading data: Input/output error

 

libvirtd should not crash 

 

Notes:
Comments:

		177722 	[sVirt] sVirt error message handling for NFS permission denial 	jialiu 	None 	Auto 		Regression 	P1 	1350 	Edit
Setup:

Make sure your os is in enforcing mode, and allow virt nfs access.

# setenforce 1

# getenforce
Enforcing

Edit /etc/libvirt/libvirtd.conf, add 
log_outputs="1:file:/tmp/libvirtd.log"

# service libvirtd restart

 

Reference bug:

Bug 589922 - permission denied error for NFS image, should libvirt error message mention virt_use_nfs?
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt
    QE consumption
    virsh-rail

bug:

    No bug found

Actions:

1. Setup a nfs server shrare dir with no_root_squash

# cat /etc/exports
/var/lib/libvirt/images/     *(rw,no_root_squash)

2. On test machine, mount the share dir.

# mount <nfs_server_ip>:/var/lib/libvirt/images/ /var/lib/libvirt/images/
3. Make sure virt_use_nfs is off

# getsebool -a|grep virt_use_nfs
virt_use_nfs --> off

4. Start a domain.

# virt-install -n guest -r 1024 -f /var/lib/libvirt/images/guest.img --import

5. Permission deny error is seen


error: unable to open disk path /var/lib/libvirt/images/guest.img: Permission denied
.

6. After virt_use_nfs is on, domain is started successfully.

# setsebool virt_use_nfs on
	
Expected Results:

5. Out put:

In /tmp/libvirtd.log:

15:50:51.844: 26844: warning : SELinuxSetFilecon:436 : Setting security context
'system_u:object_r:virt_image_t:s0' on '/var/lib/libvirt/images/guest.img' not
supported. Consider setting virt_use_nfs




Notes:
Comments:

		177640 	[storage] Clone a new vol from the source vol - Bug 671104 	nzhang 	None 	Manual 		Regression 	P2 	1360 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage

bug:

    No bug found

Actions:

# virsh vol-create-as --pool default --name foo --capacity 6G --allocation 0
--format qcow2
Vol foo created

# virsh vol-info --pool default foo
Name:           foo
Type:           file
Capacity:       6.00 GB
Allocation:     140.00 KB

# virsh vol-clone --pool default --vol foo --newname foo1
Vol foo1 cloned from foo

	
Expected Results:

The volume should be cloned successfully, verify no following error occurs.

error: Failed to clone vol from foo
error: Cannot run /usr/bin/qemu-img to create /var/lib/libvirt/images/foo1:
Invalid argument

ï»¿
Notes:
Comments:

		177731 	[Update device flags] Eject CD-ROM in use with --force option 	gsun 	None 	Manual 		--default-- 	P2 	1360 	Edit
Setup:

1.Create ISO file.
# mkisofs -o  /var/lib/libvirt/images/test.iso  /tmp

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks

bug:

    No bug found

Actions:

1.Add this XML in the domain config XML.
..............................   
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/test.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
................................ 

 2.Define and start the domain
# virsh define test.xml 
# virsh start $guest

3.Log in the guest domain.
# mount /dev/sr0 /mnt  

4.create eject XML
# cat eject.xml 
 <disk type='block' device='cdrom'>
      <target dev='hdc' bus='ide'/>
    </disk> 

5.Force eject cdrom 
# virsh update-device $guest eject.xml --force  

6.Check guest xml
# virsh dumpxml $guest

7. Log in the guest to check the cdrom 

	
Expected Results:

2. Domain http_test defined from test.xml
    
    Domain http_test started

3. 

mount: block device /dev/sr0 is write-protected, mounting read-only

5. 

Device updated successfully


6.

...
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
...

The cdrom is successfuly ejected.

7. Check whether the cdrom content is changed. 

Notes:
Comments:

		177597 	[SR-IOV] Check nodedev-dumpxml show SR-IOV detail info correctly - bug665446 	yoyzhang 	None 	Manual 		Regression 	P2 	1370 	Edit
Setup:

- VT-d enabled host

- The host is inserted wtih SR-IOV card

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1.  max_vfs value controls the number  of virtual network card
Valid Range:   0-7

 

# modprobe -r igb

# modprobe igb max_vfs=2

2. # virsh nodedev-list --tree
computer
 |
  +- net_eth0_4_00_1b_21_39_8b_18
  +- net_lo_00_00_00_00_00_00
  +- net_vnet0_fe_54_00_43_e7_d1
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_0
  |   |   |
  |   |   +- net_eth5_46_cd_be_cc_46_f1
  |   |     
  |   +- pci_0000_03_10_1
  |       |
  |       +- net_eth7_3e_7d_75_0c_68_42

3. # virsh nodedev-dumpxml pci_0000_03_00_0

<device>
  <name>pci_0000_03_00_0</name>
  <parent>pci_0000_00_01_0</parent>
  <driver>
    <name>igb</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>3</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x10c9'>82576 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
    <capability type='virt_functions'>
      <address domain='0x0000' bus='0x03' slot='0x10' function='0x0'/>
      <address domain='0x0000' bus='0x03' slot='0x10' function='0x2'/>
    </capability>
  </capability>
</device>

# virsh nodedev-dumpxml pci_0000_03_10_0
<device>
  <name>pci_0000_03_10_0</name>
  <parent>pci_0000_00_01_0</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>3</bus>
    <slot>16</slot>
    <function>0</function>
    <product id='0x10ca'>82576 Virtual Function</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
    <capability type='phys_function'>
      <address domain='0x0000' bus='0x03' slot='0x00' function='0x0'/>
    </capability>
  </capability>
</device>

# ls -l /sys/bus/pci/devices/0000\:03\:00.0/virtfn0
lrwxrwxrwx. 1 root root 0 Jan 10 16:32
/sys/bus/pci/devices/0000:03:00.0/virtfn0 -> ../0000:03:10.0

# ls -l /sys/bus/pci/devices/0000\:03\:00.0/virtfn1
lrwxrwxrwx. 1 root root 0 Jan 10 16:32
/sys/bus/pci/devices/0000:03:00.0/virtfn1 -> ../0000:03:10.2

# ls -l /sys/bus/pci/devices/0000\:03\:10.0/physfn
lrwxrwxrwx. 1 root root 0 Jan 10 16:32 /sys/bus/pci/devices/0000:03:10.0/physfn
-> ../0000:03:00.0

	
Expected Results:

From step3, we could see the PF and VF relationship from nodedev-dumpxml detail information
Notes:
Comments:

		177598 	[SR-IOV] Check nodedev-dumpxml show SR-IOV detail info correctly - bug665446 	yoyzhang 	yoyzhang 	Manual 		Regression 	P2 	1380 	Edit
Setup:

- VT-d enabled host

- The host is inserted wtih SR-IOV card

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV
    Regression

bug:

    No bug found

Actions:

1.  max_vfs value controls the number  of virtual network card
Valid Range:   0-7

 

# modprobe -r igb

# modprobe igb max_vfs=2

2. # virsh nodedev-list --tree
computer
 |
  +- net_eth0_4_00_1b_21_39_8b_18
  +- net_lo_00_00_00_00_00_00
  +- net_vnet0_fe_54_00_43_e7_d1
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_0
  |   |   |
  |   |   +- net_eth5_46_cd_be_cc_46_f1
  |   |     
  |   +- pci_0000_03_10_1
  |       |
  |       +- net_eth7_3e_7d_75_0c_68_42

3. # virsh nodedev-dumpxml pci_0000_03_00_0

<device>
  <name>pci_0000_03_00_0</name>
  <parent>pci_0000_00_01_0</parent>
  <driver>
    <name>igb</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>3</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x10c9'>82576 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
    <capability type='virt_functions'>
      <address domain='0x0000' bus='0x03' slot='0x10' function='0x0'/>
      <address domain='0x0000' bus='0x03' slot='0x10' function='0x2'/>
    </capability>
  </capability>
</device>

# virsh nodedev-dumpxml pci_0000_03_10_0
<device>
  <name>pci_0000_03_10_0</name>
  <parent>pci_0000_00_01_0</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>3</bus>
    <slot>16</slot>
    <function>0</function>
    <product id='0x10ca'>82576 Virtual Function</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
    <capability type='phys_function'>
      <address domain='0x0000' bus='0x03' slot='0x00' function='0x0'/>
    </capability>
  </capability>
</device>

# ls -l /sys/bus/pci/devices/0000\:03\:00.0/virtfn0
lrwxrwxrwx. 1 root root 0 Jan 10 16:32
/sys/bus/pci/devices/0000:03:00.0/virtfn0 -> ../0000:03:10.0

# ls -l /sys/bus/pci/devices/0000\:03\:00.0/virtfn1
lrwxrwxrwx. 1 root root 0 Jan 10 16:32
/sys/bus/pci/devices/0000:03:00.0/virtfn1 -> ../0000:03:10.2

# ls -l /sys/bus/pci/devices/0000\:03\:10.0/physfn
lrwxrwxrwx. 1 root root 0 Jan 10 16:32 /sys/bus/pci/devices/0000:03:10.0/physfn
-> ../0000:03:00.0

	
Expected Results:

From step3, we could see the PF and VF relationship from nodedev-dumpxml detail information
Notes:
Comments:

		177599 	[SR-IOV] Confirm a VF that is assigned to guest cannot be used by the host or other guests - bug 559122 	yoyzhang 	yoyzhang 	Auto 		Feature 	P2 	1390 	Edit
Setup:

1.Have executed test case with the summary  [SR-IOV] Prepare: Enable iommu.

2.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. Assign one VF to guest1.

2. Add the VF info in another guest2 xml config file, and try to start the guest2

# virsh define kvm-2

# virsh start kvm-2

3. Try to attach this assigned VF to host

# virsh nodedev-reattach pci_0000_03_11_2

4. Shutdown the guest1

5. Try to attach this assigned VF to the guest2

# virsh attach-device kvm-3 nodedev.xml
	
Expected Results:

2. The same VF cannot be used by the other guest

Output

# virsh start kvm-2
error: Failed to start domain kvm-2
error: Requested operation is not valid: PCI device 0000:03:10.1 is in use by domain guest1

# readlink /sys/bus/pci/devices/0000\:03\:00.0/driver

../../../../bus/pci/drivers/pci-stub

Check the device works well in guest1,
#ping g.cn -I eth*
eth* is the attached pci device

3. Output

error: Failed to re-attach device pci_0000_03_11_2
error: internal error Not reattaching active device 0000:03:11.2

# readlink /sys/bus/pci/devices/0000\:03\:00.0/driver
../../../../bus/pci/drivers/pci-stub 

Check the device works well in guest1
#ping g.cn -I eth* 

eth* is the attached device

5. output

# virsh attach-device kvm-3 nodedev.xml

 Device attached successfully

 

 
Notes:
Comments:

		177600 	[SR-IOV] Confirm a VF that is assigned to guest cannot be used by the host or other guests - bug 559122 	yoyzhang 	yoyzhang 	Manual 		Feature 	P2 	1400 	Edit
Setup:

1.Have executed test case '39116'  with the summary  [SR-IOV] Prepare: Enable iommu.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    Regression

bug:

    No bug found

Actions:

1. Assign one VF to one guest.

2. Add the VF info in another guest xml config file, and try to start the guest

# virsh define kvm-2

# virsh start kvm-2

3. Try to attach this assigned VF to host

# virsh nodedev-reattach pci_8086_10ca_0

4. Shutdown the guest

5. Try to attach this assigned VF to the other guest

# virsh attach-device kvm-3 nodedev.xml
	
Expected Results:

2. The same VF cannot be used by the other guest

Output

error: Failed to start domain rhel4u8_x86_64_kvm
error: internal error unable to start guest: char device redirected to /dev/pts/1
char device redirected to /dev/pts/2

3. Output

error: Failed to re-attach device pci_8086_10ca_0
error: internal error Not resetting active device 0000:42:10.7

5. output

Device pci_8086_10ca_0 re-attached
Notes:
Comments:

		177601 	[SR-IOV] Create up to MAX VFs 	yoyzhang 	yoyzhang 	Auto 		Feature 	P2 	1410 	Edit
Setup:

VT-D is enabled ( including bios vt-d enable and kernel line "intel_iommu=on" adding for intel host)

1. For 82576 card, the max function is 8 (0~7)

2. For 82599 card, the max function is 128 (0~127)

3. For X3100 card, in default sr-iov mode, the max function is 15 and cannot be modified
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    875016 - From Run 54141
    895048 - From Run 54141

Actions:

The following test steps are created on 82576 card, the tester also need loop the testing on 82599 card.( For 82599, the module is ixgbe)

1. # rmmod igb
2. # modprobe igb max_vfs=7
3. # lspci |grep 82576

4. # virsh nodedev-list --tree

 

for 82599:

modprobe -r ixgbe;modprobe ixgbe max_vfs=XXX
	
Expected Results:

step3

7 VF is generated for each PF

03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.7 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

 

step4

7 VF is generated for each PF

computer
 |
  +- net_lo_00_00_00_00_00_00
  +- net_vnet0_fe_54_00_33_21_24
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_0
  |   |   |
  |   |   +- net_eth5_36_9b_0b_6f_c6_30
  |   |     
  |   +- pci_0000_03_10_1
  |   |   |
  |   |   +- net_eth7_f6_b5_02_ca_74_02
  |   |     
  |   +- pci_0000_03_10_2
  |   |   |
  |   |   +- net_eth9_02_58_0c_d5_7a_20
  |   |     
  |   +- pci_0000_03_10_3
  |   |   |
  |   |   +- net_eth8_ea_a7_3a_37_bd_45
  |   |     
  |   +- pci_0000_03_10_4
  |   |   |
  |   |   +- net_eth10_ca_f3_9b_d6_b0_b9
  |   |     
  |   +- pci_0000_03_10_5
  |   |   |
  |   |   +- net_eth11_aa_92_bc_a9_09_a6
  |   |     
  |   +- pci_0000_03_10_6
  |   |   |
  |   |   +- net_eth16_de_35_49_a7_2a_5f
  |   |     
  |   +- pci_0000_03_10_7
  |   |   |
  |   |   +- net_eth13_f6_c9_ad_ff_59_52
  |   |     
  |   +- pci_0000_03_11_0
  |   |   |
  |   |   +- net_eth12_56_5c_d4_b9_96_0d
  |   |     
  |   +- pci_0000_03_11_1
  |   |   |
  |   |   +- net_eth15_22_09_53_1c_88_64
  |   |     
  |   +- pci_0000_03_11_2
  |   |   |
  |   |   +- net_eth18_2a_e8_cf_87_e9_6f
  |   |     
  |   +- pci_0000_03_11_3
  |   |   |
  |   |   +- net_eth19_b2_00_1b_30_c6_cf
  |   |     
  |   +- pci_0000_03_11_4
  |   |   |
  |   |   +- net_eth14_52_9e_4d_bb_c0_10
  |   |     
  |   +- pci_0000_03_11_5
  |       |
  |       +- net_eth17_92_24_88_df_41_79

Notes:
Comments:

		177602 	[SR-IOV] Hot unplug PF 	yoyzhang 	yoyzhang 	Manual 		--default-- 	P2 	1420 	Edit
Setup:

1.Have executed test case  with the summary  [SR-IOV] Prepare: Enable iommu.

2.Have a running guest with PF
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. # virsh detach-device <guestname> PF.xml
	
Expected Results:

1. This device PF could be unplug successfully, and the device should not see with #lspci in guest any more
Notes:
Comments:

		177603 	[SR-IOV] Hot unplug VF 	yoyzhang 	yoyzhang 	Manual 		--default-- 	P1 	1430 	Edit
Setup:

1.Have executed test case '39116'  with the summary  [SR-IOV] Prepare: Enable iommu.

2.Have a running guest with VF
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. # virsh detach-device <guestname>VF.xml
	
Expected Results:

1. This VF device could be unplug successfully, and the device should not see with #lspci in guest any more

 
Notes:
Comments:

		177604 	[SR-IOV] Hotplug PF 	yoyzhang 	yoyzhang 	Manual 		--default-- 	P2 	1440 	Edit
Setup:

1.Have executed test case  with the summary  [SR-IOV] Prepare: Enable iommu.

2.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. Start a guest

On host

2. # lspci |grep 82576

3. Select one PF device

# lspci -n | grep 42:00.0

4. Dump the details of the pci device.

# virsh nodedev-dumpxml pci_8086_10c9_0

5. Edit PF.xml

   <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
                <address bus='66' slot='0' function='0'/>
            </source>
    </hostdev>

6. if the guest is rhel5, execute the following command in guest before attach-device: # modprobe acpiphp

# virsh attach-device <guestname> PF.xml

7.In guest, # lspci | grep 82576

# service network restart

# ifconfig

# ping {host} -I eth* (with the PF device.)
8. # virsh shutdown <guest>

# virsh start <guest>
9. log in guest

run # lspci ,#ping host
	
Expected Results:

2. Output

42:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

3. Output

42:00.0 0200: 8086:10c9 (rev 01)

5. Output:

<device>
  <name>pci_8086_10c9_0</name>
  <parent>pci_8086_340a</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>66</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x10c9'>82576 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

7. hot-plug sucessfully

8.  The PF device is hotpluged to guest, in the guest could list the PF, PF could get ip and guest could ping to host successfully

# lspci | grep 82576

00:05.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

# service network restart

# ifconfig

# ping {host}

9. boot successfully

10. Could not see the assigned device, ping successfully
Notes:
Comments:

		177605 	[SR-IOV] Hotplug PF to guest should fail when the same VF is already assigned - bug669634 	yoyzhang 	None 	Manual 		Negative test 	P2 	1450 	Edit
Setup:

VT-d is enabled
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. # rmmod igb

2. # modprobe igb max_vfs=1

3. # virsh nodedev-list --tree

4. Detach the 1st VF from host

# virsh nodedev-dettach pci_0000_03_10_0
Device pci_0000_03_10_0 dettached

# virsh nodedev-reset pci_0000_03_10_0
Device pci_0000_03_10_0 reset

5. Hotplug the 1st VF to guest

 # cat nodedev.xml
          <hostdev mode='subsystem' type='pci' managed='yes'>
                      <source>
                                    <address bus='3' slot='0x10' function='0'/>
                                                </source>
                                                          </hostdev>

# virsh attach-device cdrom_test nodedev.xml
Device attached successfully

6. Detach the 1st PF which the 1st VF is belonging to from host

# virsh nodedev-dettach pci_0000_03_00_0
Device pci_0000_03_00_0 dettached

# virsh nodedev-reset pci_0000_03_00_0
Device pci_0000_03_00_0 reset

7. Hotplug the 1st PF to guest

# cat nodedev.xml
          <hostdev mode='subsystem' type='pci' managed='yes'>
                      <source>
                                    <address bus='3' slot='0' function='0'/>
                                                </source>
                                                          </hostdev>

# virsh attach-device cdrom_test nodedev.xml
Device attached successfully

	
Expected Results:

3. # virsh nodedev-list --tree
computer
 |
  +- net_lo_00_00_00_00_00_00
  +- net_vnet0_fe_54_00_0e_b4_7b
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_0
  |   |   |
  |   |   +- net_eth5_76_b3_40_11_7f_3f
  |   |     
  |   +- pci_0000_03_10_1
  |       |
  |       +- net_eth7_b6_88_9e_af_00_7e

7. Should fail to hot plug the PF to guest
Notes:
Comments:

		177606 	[SR-IOV] Hotplug VF 	yoyzhang 	yoyzhang 	Manual 		--default-- 	P1 	1460 	Edit
Setup:

1.Have executed test case '39116'  with the summary  [SR-IOV] Prepare: Enable iommu.

2.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

For windowns guest, 82576 VF drivers don't exist for Windows except for Windows 2008.

Bug 637502 - VF cannot get IP address in windows guests except win2008 64bit/32bit
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. Start a guest

On host

2. # lspci |grep 82576

3. Select one VF device

# lspci -n | grep 42:11.1

4.In the guest load acpiphp mode

# modprobe acpiphp

5. # virsh nodedev-dumpxml pci_8086_10ca

6. Edit VF.xml

<hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='66' slot='16' function='1'/>
            </source>
  </hostdev>

7. if the guest is rhel5, execute the following command in guest before attach-device: # modprobe acpiphp

#virsh attach-device <guestname> VF.xml

8.  For Linux guest

# lspci | grep 82576

# service network restart

# ifconfig

# ping {host}

For Windows guest

Need download & install Network Adapter Driver to use the PCI NIC, give window 2008 for example

<http://downloadcenter.intel.com/Detail_Desc.aspx?agr=Y&ProdId=3003&DwnldID=18720&ProductFamily=Ethernet+Components&ProductLine=Ethernet+Controllers&ProductProduct=Intel%C2%AE+82567+Gigabit+Ethernet+Controller&lang=eng>
	
Expected Results:

2. Output

42:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.7 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

3. Output

42:11.1 0200: 8086:10ca (rev 01)

5. Output:

<device>
  <name>pci_8086_10ca</name>
  <parent>pci_8086_340a</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>66</bus>
    <slot>16</slot>
    <function>1</function>
    <product id='0x10ca'>82576 Virtual Function</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

7. hot-plug sucessfully

8.  The VF device is hotpluged to guest, in the guest could list the VF, VF could get ip and guest could ping to host successfully

For Linux guest

# lspci | grep 82576

00:05.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

# service network restart

# ifconfig

For Wlindows guest

Could link to such as www.google.com

# ping {host}
Notes:
Comments:

		177607 	[SR-IOV] Hotplug VF to guest should fail when the same PF is assigned 	yoyzhang 	None 	Manual 		Negative test 	P2 	1470 	Edit
Setup:

iommu is enabled
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. # rmmod igb

2. # modprobe igb max_vfs=1

3. ]# lsmod |grep igb
igbvf                  35038  0
igb                   100394  0
dca                     7099  1 igb

4. # virsh nodedev-list --tree

5. Attach the 1st PF to guest

# cat nodedev.xml
          <hostdev mode='subsystem' type='pci' managed='yes'>
                      <source>
                                    <address bus='3' slot='0' function='0'/>
                                                </source>
                                                          </hostdev>

# virsh attach-device 1 nodedev.xml
Device attached successfully

6. Check VFs belonging to the 1st PF don't exist any more

# virsh nodedev-list --tree

7. Try to detach VFs belonging to the 1st PF
	
Expected Results:

4. List out

computer
 |
  +- net_bond0_00_00_00_00_00_00
  +- net_lo_00_00_00_00_00_00
  +- net_sit0_00_00_00_00
  +- net_vnet0_fe_54_00_0e_b4_7b
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_0
  |   |   |
  |   |   +- net_eth5_42_9e_76_9a_d0_21
  |   |     
  |   +- pci_0000_03_10_1
  |       |
  |       +- net_eth7_76_87_62_ba_7b_6e

 

6. computer
 |
  +- net_bond0_00_00_00_00_00_00
  +- net_lo_00_00_00_00_00_00
  +- net_sit0_00_00_00_00
  +- net_vnet0_fe_54_00_0e_b4_7b
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_1
  |       |
  |       +- net_eth7_76_87_62_ba_7b_6e

 

7. # virsh nodedev-dettach pci_0000_03_10_0
error: Could not find matching device 'pci_0000_03_10_0'
error: Node device not found

Notes:
Comments:

		177608 	[SR-IOV] Hotplug/hotunplug MAX VFs to guest - bug670787 	yoyzhang 	yoyzhang 	Manual 		Feature 	P2 	1480 	Edit
Setup:

- iommu is enabled

- For 82576 card, the max function value is 8 (0~7)

  For 82599 card, the max function value is 128 (0~127)

But max VF could be attached to guest should be limited by pci slot supported by guest and it equals to 32 - occupied pci slot by the other pci device (could be count by #lspci)

According to bug 670787, now only allow assigning max 8 VFs to guest

According to bug 678963, the error report in step 7 should be improved
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV
    Regression

bug:

    No bug found

Actions:

The following test steps are created on 82576 card, for tester should loop the test steps on 82599 card

1. # rmmod igb
2. # modprobe igb max_vfs=7
3. # lspci |grep 82576
4. # virsh nodedev-list --tree

5. Prepare VF*.xml for each VF

# cat vf.xml
<hostdev mode='subsystem' type='pci' managed='yes'>
       <source>
            <address bus='3' slot='16' function='0'/>
       </source>
</hostdev>

6. Hotplug 8 VFs to guest

# virsh start rhel6
Domain rhel6 started

# for i in {0..7}; do  sed -e s/0/$i/ vf.xml > vf1.xml; virsh attach-device rhel6 vf1.xml; done
Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

7. Try to detach the 9th VF from host and attach it to guest

8. Hot unplug the 8 VFs from guest

# for i in {0..7}; do  sed -e s/0/$i/ vf.xml > vf1.xml; virsh detach-device rhel6 vf1.xml; done
Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully
	
Expected Results:

3.

03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.7 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

4.

computer
 |
  +- net_lo_00_00_00_00_00_00
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_0
  |   |   |
  |   |   +- net_eth5_2a_cc_b2_a1_da_67
  |   |     
  |   +- pci_0000_03_10_1
  |   |   |
  |   |   +- net_eth7_a2_1f_b9_53_1e_04
  |   |     
  |   +- pci_0000_03_10_2
  |   |   |
  |   |   +- net_eth9_56_41_9c_f4_3c_15
  |   |     
  |   +- pci_0000_03_10_3
  |   |   |
  |   |   +- net_eth8_ea_56_5d_1f_de_a2
  |   |     
  |   +- pci_0000_03_10_4
  |   |   |
  |   |   +- net_eth10_5a_9c_c4_10_f5_a4
  |   |     
  |   +- pci_0000_03_10_5
  |   |   |
  |   |   +- net_eth11_2a_a9_85_86_db_72
  |   |     
  |   +- pci_0000_03_10_6
  |   |   |
  |   |   +- net_eth16_52_8f_73_02_5f_f9
  |   |     
  |   +- pci_0000_03_10_7
  |   |   |
  |   |   +- net_eth13_5a_c4_2d_cb_42_8d
  |   |     
  |   +- pci_0000_03_11_0
  |   |   |
  |   |   +- net_eth12_66_1c_e7_92_4a_4b
  |   |     
  |   +- pci_0000_03_11_1
  |   |   |
  |   |   +- net_eth15_ae_36_43_f0_e1_6d
  |   |     
  |   +- pci_0000_03_11_2
  |   |   |
  |   |   +- net_eth18_72_eb_5b_7d_be_93
  |   |     
  |   +- pci_0000_03_11_3
  |   |   |
  |   |   +- net_eth19_f2_12_af_86_39_00
  |   |     
  |   +- pci_0000_03_11_4
  |   |   |
  |   |   +- net_eth14_b2_b6_cd_e0_8c_f2
  |   |     
  |   +- pci_0000_03_11_5
  |       |
  |       +- net_eth17_b2_47_c6_22_8b_ec

6. In guest # lspci could see all the 8 VFs

config /etc/sysconfig/network-scripts/ifcfg-eth* for 8 VFs, then # service network restart, verify all 8 VFs can get ip and could ping to google.com

7. Should fail to assign it to guest and got a error

8. Should cannot see the 8 VFs in guest any more by #lspci
Notes:
Comments:

		177609 	[SR-IOV] Hotplug/hotunplug MAX VFs to guest - bug670787, 616415 	yoyzhang 	None 	Manual 		Feature 	P2 	1490 	Edit
Setup:

- iommu is enabled

- For 82576 card, the max function value is 8 (0~7)

  For 82599 card, the max function value is 128 (0~127)

But max VF could be attached to guest should be limited by pci slot supported by guest and it equals to 32 - occupied pci slot by the other pci device (could be count by #lspci)

According to bug 670787, now only allow assigning max 8 VFs to guest

According to bug 678963, the error report in step 7 should be improved

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 

NOTE: After the bug https://bugzilla.redhat.com/show_bug.cgi?id=616415 QMP: does not report the real cause of PCI device assignment failure fixed, we should change the expect result step 7 output
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

The following test steps are created on 82576 card, for tester should loop the test steps on 82599 card

1. # rmmod igb
2. # modprobe igb max_vfs=7
3. # lspci |grep 82576
4. # virsh nodedev-list --tree

5. Prepare VF*.xml for each VF

# cat vf.xml
<hostdev mode='subsystem' type='pci' managed='yes'>
       <source>
            <address bus='3' slot='16' function='0'/>
       </source>
</hostdev>

6. Hotplug 8 VFs to guest

# virsh start rhel6
Domain rhel6 started

# for i in {0..7}; do  sed -e s/0/$i/ vf.xml > vf1.xml; virsh attach-device rhel6 vf1.xml; done
Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

7. Try to detach the 9th VF from host and attach it to guest

8. Hot unplug the 8 VFs from guest

# for i in {0..7}; do  sed -e s/0/$i/ vf.xml > vf1.xml; virsh detach-device rhel6 vf1.xml; done
Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully

Device attached successfully
	
Expected Results:

3.

03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.7 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

4.

computer
 |
  +- net_lo_00_00_00_00_00_00
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_0
  |   |   |
  |   |   +- net_eth5_2a_cc_b2_a1_da_67
  |   |     
  |   +- pci_0000_03_10_1
  |   |   |
  |   |   +- net_eth7_a2_1f_b9_53_1e_04
  |   |     
  |   +- pci_0000_03_10_2
  |   |   |
  |   |   +- net_eth9_56_41_9c_f4_3c_15
  |   |     
  |   +- pci_0000_03_10_3
  |   |   |
  |   |   +- net_eth8_ea_56_5d_1f_de_a2
  |   |     
  |   +- pci_0000_03_10_4
  |   |   |
  |   |   +- net_eth10_5a_9c_c4_10_f5_a4
  |   |     
  |   +- pci_0000_03_10_5
  |   |   |
  |   |   +- net_eth11_2a_a9_85_86_db_72
  |   |     
  |   +- pci_0000_03_10_6
  |   |   |
  |   |   +- net_eth16_52_8f_73_02_5f_f9
  |   |     
  |   +- pci_0000_03_10_7
  |   |   |
  |   |   +- net_eth13_5a_c4_2d_cb_42_8d
  |   |     
  |   +- pci_0000_03_11_0
  |   |   |
  |   |   +- net_eth12_66_1c_e7_92_4a_4b
  |   |     
  |   +- pci_0000_03_11_1
  |   |   |
  |   |   +- net_eth15_ae_36_43_f0_e1_6d
  |   |     
  |   +- pci_0000_03_11_2
  |   |   |
  |   |   +- net_eth18_72_eb_5b_7d_be_93
  |   |     
  |   +- pci_0000_03_11_3
  |   |   |
  |   |   +- net_eth19_f2_12_af_86_39_00
  |   |     
  |   +- pci_0000_03_11_4
  |   |   |
  |   |   +- net_eth14_b2_b6_cd_e0_8c_f2
  |   |     
  |   +- pci_0000_03_11_5
  |       |
  |       +- net_eth17_b2_47_c6_22_8b_ec

6. In guest # lspci could see all the 8 VFs

config /etc/sysconfig/network-scripts/ifcfg-eth* for 8 VFs, then # service network restart, verify all 8 VFs can get ip and could ping to google.com

7. Should fail to assign it to guest and got a error

# virsh attach-device test-yp vf.xml
error: Failed to attach device from vf.xml
error: internal error unable to execute QEMU command 'device_add': Device 'pci-assign' could not be initialized

8. Should cannot see the 8 VFs in guest any more by #lspci
Notes:
Comments:

		177610 	[SR-IOV] Hotplug/Hotunplug VF for 500 times 	jialiu 	None 	Manual 		Stress 	P2 	1500 	Edit
Setup:

Have executed test case '39116'  with the summary  [SR-IOV] Prepare: Enable iommu.

# modprobe acpiphp
if the guest is rhel5, execute the following command in guest before attach-disk:

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_inte

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    SR-IOV

bug:

    No bug found

Actions:

1. Generate some VF.
# modprobe -r igb
# modprobe igb max_vfs=2

2. # lspci |grep 82576

3. Define a domain, and start the domain

4. Prepare the following xml - vf.xml.
<hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='66' slot='16' function='1'/>
            </source>
</hostdev>

5. Hotplug a VF to guest.
# virsh attach-device <guestname> vf.xml

6. In guest, check the new VF card.
# lspci
# ip link
# ifconfig
# ping www.google.com -I eth[N]

7. Hot unplug the NIC from guest.
# virsh detach-device <guest name> vf.xml

8. In guest, check the NIC in step 6 is not seen.
# lspci | grep -i eth
# ip link

9. Hotplug / hot-unplug the nic device for 500 times.
# for i in `seq 500`; \
do echo "---${i}---"; virsh attach-device <guestname> vf.xml;virsh detach-device <guestname> vf.xml \
done

10. loop once again from step 5 to step 9

	
Expected Results:

2. Output

42:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

6. The VF device is hotpluged to guest, in the guest could list the VF, VF could get ip, and guest could ping to host

# lspci | grep 82576

00:05.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 1000
    link/ether 52:54:00:80:d9:6a brd ff:ff:ff:ff:ff:ff
36: eth4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:80:d9:bb brd ff:ff:ff:ff:ff:ff

NOTE:

A new interface - eth4 is seen


# ifconfig eth4
eth4      Link encap:Ethernet  HWaddr 52:54:00:80:D9:BB  
          inet addr:192.168.122.221  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fe80:d9bb/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:15 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:1348 (1.3 KiB)  TX bytes:1390 (1.3 KiB)
          Interrupt:11 Base address:0x8000

# ping www.google.com -I eth4
PING www.l.google.com (72.14.204.147) from 192.168.122.221 eth4: 56(84) bytes of data.
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=1 ttl=42 time=303 ms
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=2 ttl=42 time=277 ms

7. hot-unplug the NIC successfully

8. the eth4 is not seen

9. all the virsh command is finished without any error

10. After 500 times hotplug/hotunplug, the nic still could be work properly in guest
Notes:
Comments:

		177612 	[SR-IOV] Migrate a guest with assigned VF 	yoyzhang 	yoyzhang 	Auto 		Feature 	P2 	1510 	Edit
Setup:

1.Prepare 2 same brand host, one acts as source host and the other acts as target host

- Both host are iommu enabled

- Both host are inserted with 82576 interface card

- PCI slot number for 82576 interface card should be the same

2.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

On Migration Source Host (eg. 10.66.92.29)

1 issue the below commands prepare migration environment

      #iptables -F
      #setenforce 1

      #setsebool virt_use_nfs=1
      #mount -t nfs 10.66.90.113:/vol/xenimage /mnt

2 create  82576 VF
     #modprobe -r igb
     #modprobe igb max_vfs=1
      #lspci|grep 82576
      #virsh nodedev-dumpxml pci_0000_03_10_0
 
3 Add the following lines into guest xml  and start the guest:
       <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
             <address domain='0x0000' bus='0x03' slot='0x10' function='0x0'/>
             </source>
             <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
       </hostdev>
   
   # virsh start rhel6

On Migration Target Host (eg. 10.66.72.102)
4 issue the below commands to prepare migration environment
    #iptables -F
    #setenforce 1     
    #setsebool virt_use_nfs=1
    #mount -t nfs 10.66.90.113:/vol/xenimage /mnt

On Migration Source Host (eg. 10.66.92.29)

5 issue the below command to migrate from 10.66.92.29 to 10.66.72.102
#virsh migrate --live rhel6 qemu+ssh://10.66.72.102/system
	
Expected Results:

2. #lspci|grep 82576
03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
#virsh nodedev-dumpxml pci_0000_03_10_0
       <device>
            <name>pci_0000_03_10_0</name>
             <parent>pci_0000_00_01_0</parent>
             <capability type='pci'>
             <domain>0</domain>
             <bus>3</bus>
             <slot>16</slot>
             <function>0</function>
             <product id='0x10ca'>82576 Virtual Function</product>
             <vendor id='0x8086'>Intel Corporation</vendor>
             </capability>
       <device>
3 # virsh start rhel6
Domain rhel6 started
4
#iptables -F
#setenforce 0
#mount -t nfs 10.66.90.113:/vol/xenimage /mnt
5 
Migration should be forbidden as the guest has PCI device
# virsh migrate --live test-mig qemu+ssh://10.66.5.10/system
root@10.66.5.10's password:
error: Requested operation is not valid: Domain with assigned host devices cannot be migrated
Notes:
Comments:

		177613 	[SR-IOV] Prepare: Enable VT-D 	yoyzhang 	yoyzhang 	Manual 		Feature 	P1 	1520 	Edit
Setup:

Prepare:

- A host with Intel VT-d or AMD IOMMU support

- The host with a network card which supports SR-IOV

Note:

If only one network card is available on the host, it will not be able to access the network during the test if PV is assigned to guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. The SR-IOV need vt-d support, so we need a box with vt-d availiable firstly

--------------- For ADM host ------------------------

IOMMU is enabled by default

--------------- For Intel host -------------------------

Need edit /etc/grub/grub.conf, then reboot the host. For xen or kvm kernel, it is a little differrent.

If it is xen kernel

Add the kernel option 'iommu=force' and 'pci_pt_e820_access=on'

default=0
timeout=5
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu
title Red Hat Enterprise Linux Server (2.6.18-185.el5xen)
        root (hd0,0)
        kernel /xen.gz-2.6.18-185.el5 iommu=force
        module /vmlinuz-2.6.18-185.el5xen ro root=/dev/VolGroup01/LogVol00 pci_pt_e820_access=on
        module /initrd-2.6.18-185.el5xen.img

If it is kvm kernel

Add the kernel option 'intel_iommu=on'

default=0
timeout=0
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu
title Fedora (2.6.31.5-127.fc12.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.31.5-127.fc12.x86_64 ro
root=/dev/mapper/vg_intelx5550121-lv_root  LANG=en_US.UTF-8
SYSFONT=latarcyrheb-sun16 KEYBOARDTYPE=pc KEYTABLE=us intel_iommu=on rhgb
quiet
        initrd /initramfs-2.6.31.5-127.fc12.x86_64.img
	
Expected Results:

1. --------------- For Intel host -------------------------

If it is xen kernel

After reboot

# xm dmesg | grep -i vt-d
(XEN) Intel VT-d has been enabled
(XEN) Intel VT-d snoop control disabled

If it is kvm kernel

After reboot

# dmesg | grep -i iommu

Command line: ro root=/dev/VolGroup01/LogVol00 rhgb quiet intel_iommu=on
Kernel command line: ro root=/dev/VolGroup01/LogVol00 rhgb quiet intel_iommu=on
Intel-IOMMU: enabled
Notes:
Comments:

		177614 	[SR-IOV] Prepare: intel network adapters includes 82576, 82599, x3100 	nzhang 	None 	Manual 		--default-- 	P2 	1530 	Edit
Setup:

Prepare:

- A host with Intel VT-d or AMD IOMMU support

- Assign a network cards of different type includes 82576, 82599, x3100 which supports SR-IOV
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    sr-iov

Tag:

    SR-IOV

bug:

    No bug found

Actions:

Do each test cases for SR-IOV. X3100 has a little higher priority but in fact the priority of the 3 cards are almost the same
	
Expected Results:
Notes:
Comments:

		177615 	[SR-IOV] Put VFs to a different VLAN 	yoyzhang 	None 	Manual 		Feature 	P2 	1540 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

Create 2 VLANs containing 2 different VFs, then create 2 virtual networks
basing on different 2 VLANs. Assign the 2 virtual networks to 2 guests. The 2
virtual network should get ip in guest but the 2 guests should not ping to
each other for they are in different VLAN.
	
Expected Results:
Notes:
Comments:

		177757 	[Virtio-serial] Create a channel with socket-virtconsole type 	gren 	None 	Manual 		virtio-serial 	P1 	1540 	Edit
Setup:

The host and guest should be RHEL6 above.

And you have to create a non-root user such as test in the guest

# useradd test

# passwd test 

   set the passwd to redhat
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtio-serial

bug:

    No bug found

Actions:

    1, Add the following XML to a guest domain, remove other console and serial XML elements

    <controller type='virtio-serial' index='0'/>
    <console type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/serial.sock'/>
      <target type='virtio' port='0'/>
    </console>

    The snippet corresponds to the following qemu command line.

    -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x6
    -chardev socket,id=console0,path=/var/lib/libvirt/qemu/serial.sock,server,nowait
    -device virtconsole,chardev=console0

    2, Start the guest,

    In the guest:  # agetty /dev/hvc0 9600 vt100
        In the host:
        Download the socat utility: http://www.dest-unreach.org/socat/download/socat-1.7.1.3.tar.bz2
        After compiling, in the socat building directory, run

 

               # wget 

               #  tar -jxvf socat-1.7.13.tar.bz2

               # cd socat-1.7.13

               # ./configure --disable-fips

               # make

               # make install
               # ./socat /var/lib/libvirt/qemu/serial.sock -

    you will see the login promt list this:

        Red Hat Enterprise Linux Server release 6.0 (Santiago)
        Kernel 2.6.32-71.el6.x86_64 on an x86_64

        localhost.localdomain login: test
        test
        password: redhat

        Would you like to enter a security context? [N]  



	
Expected Results:

2. After you input username 'gren', only if you see 'password:', that means the socket communication is created successfully
Notes:
Comments:

		177616 	[SR-IOV] Reboot guest with assigned VF 	yoyzhang 	yoyzhang 	Auto 		Feature 	P2 	1550 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. Reboot the guest with VF

# virsh reboot <guestname>

OR execute # reboot in guest
	
Expected Results:

1. After guest reboot, the assigned VF should work well in guest.

- Could be listed out by #lspci

- could get ip and ping to outside ip

 
Notes:
Comments:

		177758 	[Virtio-serial] Migrate a guest with assigned virtio-serial 	yoyzhang 	None 	Manual 		Feature 	P2 	1550 	Edit
Setup:

Both the host and guest should be rhel6
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtio-serial

bug:

    704955 - From Run 43004

Actions:

Set up migration env on 2 hosts

1, On source host,  Add the following XML into <devices> tag for a domain:

    <controller type='virtio-serial' index='0'/>
        <channel type='pty'>
          <target type='virtio' name='org.linux-kvm.port.1'/>
    </channel>

2. Start the domain
3. Migrate the guest to target host
4. On target host

use "virsh dumpxml $domain-name" to determine the which pty the domain use,
    like the the example, the pty is /dev/pts/6 in the host.
      
    # virsh dumpxml rhel6

    <channel type='pty'>
      <source path='/dev/pts/6'/>
      <target type='virtio' name='org.linux-kvm.port.1'/>
          <alias name='channel0'/>
          <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>

    In the host to receive the data:
    
        # cat /dev/pts/6
        a


     In the guest, send data:
        
        # echo a > /dev/vport0p1
     in the host you will see "a"

    In the guest, receive the data:
        
        # cat /dev/vport0p1


    In the host to send out a data:
        
        # echo "This is from host" > /dev/pts/6
  you will see "This is from host" in guest


	
Expected Results:
Notes:
Comments:

		177617 	[SR-IOV] Released VF from guest can be used in host or the other guests 	dyuan 	dyuan 	Manual 		Feature 	P2 	1560 	Edit
Setup:

1.Have executed test case  with the summary  [SR-IOV] Prepare: Enable iommu.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. Release / Hot-unplug VF from a guest with VF

2. Check the released VF could be used by the host

    # virsh nodedev-reattach pci_8086_10ca_0

   # virsh nodedev-reset pci_8086_10ca_0

3. Check the released VF could be used by the other guests

   Use virt-manager to hotplug the released VF to the other guest following test steps in test case Hotplug VF
	
Expected Results:

2. Output

Device pci_8086_10ca_0 re-attached

Device pci_8086_10ca_0 re-seted

3. The VF could be used by the other guest successfully
Notes:
Comments:

		177759 	[Virtio-serial] Verify virtio-serial functionality with MSI Enabled/Disabled 	yoyzhang 	None 	Manual 		Feature 	P2 	1560 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtio-serial

bug:

    No bug found

Actions:

The host and guest should be RHEL6.0

1. Remove all the controller, serial and channel tab in guest config file, then add the following xml in guest config

<controller type='virtio-serial' index='0'/>
<channel type='pty'>
  <target type='virtio' name='org.linux-kvm.port.0'/>
</channel>

2. Start the guest

3. Shut down the guest, then remove all the controller, serial and channel tab in guest config file, then add the following xml in guest config

<controller type='virtio-serial' vectors='0' />
<channel type='pty'>
  <target type='virtio' name='org.linux-kvm.port.0'/>
</channel>

4. Start the guest
	
Expected Results:

2. Log in guest, could see MSI is enabled

# lspci -vvv

...

00:07.0 Communication controller: Red Hat, Inc Virtio console
        Subsystem: Red Hat, Inc Device 0003
        Physical Slot: 7
        Control: I/O+ Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
        Interrupt: pin A routed to IRQ 11
        Region 0: I/O ports at c960 [size=32]
        Region 1: Memory at f2022000 (32-bit, non-prefetchable) [size=4K]
       Capabilities: [40] MSI-X: Enable+ Count=32 Masked-
                Vector table: BAR=1 offset=00000000
                PBA: BAR=1 offset=00000800
        Kernel driver in use: virtio-pci
        Kernel modules: virtio_pci

...

4. Log in guest, could see MSI is disabled

# lspci -vvv

...

00:07.0 Communication controller: Red Hat, Inc Virtio console
        Subsystem: Red Hat, Inc Device 0003
        Physical Slot: 7
        Control: I/O+ Mem+ BusMaster- SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap- 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
        Interrupt: pin A routed to IRQ 11
        Region 0: I/O ports at c960 [size=32]
        Kernel driver in use: virtio-pci
        Kernel modules: virtio_pci

...
Notes:
Comments:

		177577 	[snapshot]revert should be forbidden without --force BZ 742615 	zhpeng 	zhpeng 	Manual (Autoproposed) 		Regression 	P1 	1570 	Edit
Setup:

Prepare a guest with one disk

#virsh dumpxml test

<domain type='kvm'>
  <name>testkf</name>
  <uuid>10a32bd1-07af-6c73-0af3-1f737093972b</uuid>
  <memory unit='KiB'>2097152</memory>
  <currentMemory unit='KiB'>2097152</currentMemory>
  <vcpu placement='static' current='1'>2</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel62.qcow2'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <interface type='network'>
      <mac address='52:54:00:1e:f2:53'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' />
    <sound model='ich6'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

# virsh shutdown test

# virsh snapshot-create-as test snap1
Domain snapshot snap1 created


# virsh start test
Domain test started

#virsh edit test

Add the second disk to the guest

<disk type='file' device='disk'>
<driver name='qemu' type='qcow2' cache='none'/>
<source file='/var/lib/libvirt/images/test2.img'/>
 <target dev='vdb' bus='virtio'/>
    </disk>
Domain test XML configuration  changed.
# virsh shutdown test
# virsh list
 Id    Name                           State
----------------------------------------------------

# virsh start test
Domain test started

# virsh snapshot-revert test snap1 --running
error: revert requires force: must respawn qemu to start inactive snapshot

# virsh snapshot-revert test snap1 --running --force

# virsh list
 Id    Name                           State
----------------------------------------------------
 11    test                           running

# virsh dumpxml test   

<disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/rhel62.qcow2'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
 </disk>
	
Expected Results:

Follow the steps, the second disk is gone.
Notes:
Comments:

		177581 	[snapshot]snapshot XML description --bug 741510 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	1580 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1 # virsh snapshot-list rhel6
 Name                 Creation Time             State
------------------------------------------------------------
 s1                   2010-02-05 20:02:36 -0500 running

2 # virsh snapshot-dumpxml rhel6 s1
<domainsnapshot>
.....
   <domain type='kvm'>
.......
   </domain>
</domainsnapshot>

	
Expected Results:

Step 2 not like:

<domainsnapshot>
.....
<domain type='kvm'>
......
</domain>
</domainsnapshot>

 
Notes:
Comments:

		177583 	[snapshot]snapshot-parent should error when current snapshot is a root --bug 742410 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	1590 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:


when current snapshot is a root, you can't get any error information by virsh
snapshot-parent, however, the command will return 1.

Make sure you have no snapshots of your guest. So the "hello" snapshot is the root snapshot.

1. cat /tmp/snap.xml 
<domainsnapshot>
  <name>hello</name>
  <state>shutoff</state>
</domainsnapshot>

2. virsh snapshot-create vr-rhel5u4-x86_64-kvm snap.xml

3. virsh snapshot-list vr-rhel5u4-x86_64-kvm --parent

4. virsh snapshot-parent vr-rhel5u4-x86_64-kvm hello

5. echo $?

	
Expected Results:


The Step4 shoult error like:
error: Domain snapshot not found: snapshot 'hello' does not have a parent

The step 5 should be 1

Notes:
Comments:

		177586 	[snapshot]virsh snapshot-edit should edit disk snapshots --bug 744071 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	1600 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1. virsh start dom
2. virsh snapshot-create-as dom snap
3. virsh snapshot-edit dom snap # try adding <description>
like:
<domainsnapshot>
<description>Snapshot of dom</description>
  <name>snap</name>
  <state>running</state>
  <parent>
    <name>1319531063</name>
  </parent>
  <creationTime>1319532663</creationTime>
.......



	
Expected Results:

Step3 should not error like:

error: Failed to update snap
error: argument unsupported: unable to handle disk requests in snapshot

should be:
Snapshot snap edited.

 
Notes:
Comments:

		177588 	[Sound devices] ac97, es1370 - VNC 	jyang 	None 	Both 		Feature 	P3 	1610 	Edit
Setup:

a healthy guest, which is in shutoff
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sound
    virsh-rail

bug:

    No bug found

Actions:

1. dump guest's xml

   # virsh dumpxml toy > toy.xml

make sure graphics type is vnc:

<graphics type='vnc' port='-1' autoport='yes'/>

2. insert following xml into node "<device>"

   <sound model="ac97"/>

 

3. undefine & define

   # virsh undefine toy

    # virsh define toy.xml

 

4. start guest, and login into it to do check like following:

    1>. # lspci | grep -i ac

 

5. download a multi-media file, install mplayer or such other multi-media player to play the file we downloaded to see if there is sound.

 

6. loop upper steps with:

    <sound model="es1370"/>
	
Expected Results:

step 4:

        will get output like:

        00:05.0 Multimedia audio controller: Intel Corporation 82801AA AC'97 Audio Controller (rev 01)

 

step 5:

     Can not hear the sound so far
Notes:
Comments:

		177589 	[Input/Output devices] Get sound under different graphics type - spice 	ydu 	None 	Manual 		Feature 	P3 	1620 	Edit
Setup:

ï»¿Prepare a healthy guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Input/Output devices

bug:

    No bug found

Actions:

1. Edit guest's XML,
   #virsh edit domain

insert a sound device like:

<souce model="ac97"/>

2. Edit guest's XML,
   #virsh edit domain

insert a graphics device like:

<graphics type='spice' port='5903' autoport='no' keymap='en-us'/>

3. start guest

# virsh start domain

4. Install virt-viewer 

   #yum install virt-viewer

5. Use virt-viewer to check guest windows. 

   # virt-viewer domain

.........................................
6. login and verify.
   # lspci | grep -i ac   

7. Download a multi-media file, install mplayer or similar, play the file and verify ther is sound.


8. Repeat upper steps with other sound models.
   <sound model="es1370"/>
	
Expected Results:

7. should get sound from guest.

Now for win7-32,win7-64 can hear the sound with <sound model='ich6'/>.

       also win7-32 can hear the sound with <souce model="ac97"/>
Notes:
Comments:

		177590 	[Sound devices] Intel HDA support 	ydu 	None 	Manual 		Feature 	P3 	1630 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sound
    input/output device

bug:

    No bug found

Actions:

Note: you should do the case on local machine, do not use ssh connect, or you will hear nothing from the guest.

1. dump guest's xml

   # virsh dumpxml toy > toy.xml

2. insert following xml into node "<device>"

      <sound model='ich6'>
    </sound>

3. undefine & define

   # virsh undefine toy

    # virsh define toy.xml

4. start guest, and login into it to do check like following:

    1>. # lspci | grep -i audio

5. download a multi-media file, install mplayer or such other multi-media player to play the file we downloaded to see if there is sound.
	
Expected Results:

4. In guest, output similar like

00:04.0 Audio device: Intel Corporation 82801FB/FBM/FR/FW/FRW (ICH6 Family) High Definition Audio Controller (rev 01)

5. Cannot hear sound with vnc till now,can hear sound with spice

The driver info: https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/RHEL6.1%20vdsm/spice%20integration
Notes:
Comments:

		177591 	[SR-IOV] Assign one PF to one guest 	yoyzhang 	yoyzhang 	Manual 		Feature 	P2 	1640 	Edit
Setup:

1.Have executed test case with the summary  [SR-IOV] Prepare: Enable iommu.

2.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. List  pci device with SR-IOV function in the box

( in my example , I use the intel 82576 network interface card which supports
the SR-IOV function )

# lspci |grep 82576

2. Confirm the driver is built as a module

# lsmod |grep igb

3.  max_vfs value controls the number  of virtual network card
Valid Range:   0-7

# modprobe -r igb

# modprobe igb max_vfs=1

4. Relist the pci device relevant Intel 82576 network device

# lspci |grep 82576

5. Show the PF product ID and vendor ID

# lspci -n | grep 42:00.0

6. List availiable node devices on host

# virsh nodedev-list

7. Dump PF detail info

# virsh nodedev-dumpxml pci_8086_10c9_0

8. Add this PF xml info to VM config xml (kvm.xml)

...
          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='66' slot='0' function='0'/>
            </source>
          </hostdev>
          ...

9 # virsh define kvm.xml

10. # virsh start kvm

11. On guest, check whether PF is already assigned successfully

#lscpi | grep 82576

# service network restart

# ifconfig

# ping {host}
	
Expected Results:

1. Output

03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

2. Output

igb                    87592  0
dca                     6708  1 igb

4. Output:

42:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

5. Output:

42:00.0 0200: 8086:10c9 (rev 01)

But it should be pci_8086_10c9_0, this can be confirmed in step 7

6. Output:

....................

pci_8086_10c9
pci_8086_10c9_0
pci_8086_10ca
pci_8086_10ca_0

..............................

7. Output:

<device>
  <name>pci_8086_10c9_0</name>
  <parent>pci_8086_340a</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>66</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x10c9'>82576 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

10. Output:

Domain kvm started

11. Output:

00:04.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

And could get ip and ping to host successfully
Notes:
Comments:

		177592 	[SR-IOV] Assign one VF to one guest 	yoyzhang 	yoyzhang 	Auto 		Feature 	P1 	1650 	Edit
Setup:

1.Have executed test case  with the summary  [SR-IOV] Prepare: Enable iommu.

 2.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

For windowns guest, 82576 VF drivers don't exist for Windows except for Windows 2008.

Bug 637502 - VF cannot get IP address in windows guests except win2008 64bit/32bit
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

1. List  pci device with SR-IOV function in the box

( in my example , I use the intel 82576 network interface card which supports
the SR-IOV function )

# lspci |grep 82576

2. Confirm the driver is built as a module

# lsmod |grep igb

3.  max_vfs value controls the number  of virtual network card
Valid Range:   0-7

# modprobe -r igb


# modprobe igb max_vfs=7

4. Relist the pci device relevant Intel 82576 network device

# lspci |grep 82576

5. Show the VF product ID and vendor ID

# lspci -n | grep 42:10.0

6. List availiable node devices on host

# virsh nodedev-list

7. Dump VF detail info

# virsh nodedev-dumpxml pci_8086_10ca

8. Add this VF xml info to VM config xml (kvm.xml)

...
          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='66' slot='16' function='1'/>
            </source>
          </hostdev>
          ...

9. # virsh define kvm.xml

10. # virsh start kvm

11. On guest, check whether VF is already assigned successfully

For Linux guest

#lspci | grep 82576

# service network restart

# ifconfig

For Windows guest

Need download & install Network Adapter Driver to use the PCI NIC, give window 2008 for example

<http://downloadcenter.intel.com/Detail_Desc.aspx?agr=Y&ProdId=3003&DwnldID=18720&ProductFamily=Ethernet+Components&ProductLine=Ethernet+Controllers&ProductProduct=Intel%C2%AE+82567+Gigabit+Ethernet+Controller&lang=eng>

# ping {host}
	
Expected Results:

1. Output

03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

2. Output

igb                    87592  0
dca                     6708  1 igb

4. Output:

42:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
42:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:10.7 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
42:11.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

5. Output:

42:10.0 0200: 8086:10ca (rev 01)

6. Output:

....................

pci_14e4_1684_0
pci_8086_10b9
pci_8086_10c9
pci_8086_10c9_0
pci_8086_10ca
pci_8086_10ca_0

..............................

7. Output:

<device>
  <name>pci_8086_10ca</name>
  <parent>pci_8086_340a</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>66</bus>
    <slot>16</slot>
    <function>1</function>
    <product id='0x10ca'>82576 Virtual Function</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

10. Output:

Domain kvm started

11. Output:

For Linux guest

00:05.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

And could get ip and ping to host successfully

For Windows guest

Could see the VF device in guest and ping to google.com successfully
Notes:
Comments:

		177593 	[SR-IOV] Assign seven VFs to the same guest. 	yoyzhang 	yoyzhang 	Manual 		Feature 	P2 	1660 	Edit
Setup:

1. Host supports VT-d and iommu is enabled in kernel cmdline, for example:

# cat /boot/grub/grub.conf

title Red Hat Enterprise Linux Server (2.6.32-349.el6.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.32-349.el6.x86_64 ro root=/dev/mapper/VolGroup-lv_root rd_NO_LUKS LANG=en_US.UTF-8 rd_NO_MD rd_LVM_LV=VolGroup/lv_swap SYSFONT=latarcyrheb-sun16 crashkernel=auto rd_LVM_LV=VolGroup/lv_root  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet intel_iommu=on

Note that, need to reboot host after enabling iommu in kernel cmdline.

2.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm kvm_intel

 3. To generate 7 VFs

# lspci|grep Ethernet
01:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5764M Gigabit Ethernet PCIe (rev 10)
03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

# rmmod igb
# modprobe igb max_vfs=7

# lspci|grep Ethernet
01:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5764M Gigabit Ethernet PCIe (rev 10)
03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
03:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.6 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.7 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.4 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:11.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

Notes, 03:11.5 is bus:slot.function, and the domain default is 0000 in here, so a udev style PCI name

is pci_$domain_$bus_$slot.$function such as pci_0000_03_11_5.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV
    RHEL6.0

bug:

    No bug found

Actions:

 

1. To dump PCI device(VF) details in XML

# virsh nodedev-dumpxml pci_0000_03_10_0

2. To edit the following XML into inactive guest XML

          ...
          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address domain='0x0000' bus='0x03' slot='0x00' function='0x0'/>
            </source>
          </hostdev>
          ...

3. To Repeat step 1-2 and replace function number from 0 to 1, 2, 3, 4, 5, 6 and then 7 VFS should

be assigned to a guest.

 

4. Starting the guest and check if these 7 VFs work well in the guest

On the guest

#lscpi | grep 82576

# service network restart

# ifconfig

# ping {host} -I eth1

Notes, replace name of device from eth1 to eth{2..7}
	
Expected Results:

1. The guest is successfully started and these 7 VFs is assigned to guest successfully

You should see these 7 NICs via 'lspci|grep 82576' on the guest.

2. These 7 NICs should work well

You should successfull ping local host or external network via each network interface

such as from eth1 to eth7.
Notes:
Comments:

		177851 	[Watchdog device] Watchdog device "i6300esb" - shutdown 	yimwang 	None 	Auto 		Feature 	P2 	1660 	Edit
Setup:

For rhel6 guest,make sure the guest is not mini installed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    watchdog device
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='i6300esb' action='shutdown'/> to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, check the watchdog dirvier is installed:

# dmesg | grep i6300

# /sbin/lsmod | grep i6300esb

# lspci

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

NOTE:(RHEL-3U9-i386 guest does not have ACPI mode)

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm
17108 ?        Sl     0:19 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name rhel6 -uuid 2292db8e-9f74-ea3a-5fc4-2240a3f8534e -nodefconfig -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/rhel6.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -device virtio-serial-pci,id=virtio-serial0,max_ports=16,vectors=4,bus=pci.0,addr=0x5 -device virtio-serial-pci,id=virtio-serial1,bus=pci.0,addr=0x6 -drive file=/var/lib/libvirt/images/nfs_test.img,if=none,id=drive-ide0-0-0,boot=on,format=raw,cache=none -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=20,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:61:a6:9e,bus=pci.0,addr=0x3 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -chardev pty,id=channel0 -device virtserialport,bus=virtio-serial0.0,nr=0,chardev=channel0,name=org.linux-kvm.port.0 -chardev pty,id=channel1 -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=channel1,name=org.linux-kvm.port.1 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device i6300esb,id=watchdog0,bus=pci.0,addr=0x7 -watchdog-action shutdown  -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4
17255 pts/11   S+     0:00 grep qemu-kvm

4.The oupt is similar to the following:

# dmesg | grep i6300
i6300ESB timer: initialized (0xffffc20000036000). heartbeat=30 sec (nowayout=0)
# /sbin/lsmod | grep i6300esb
i6300esb     40557     0

5.watchdong software is installed successfully.

8.After 130 seconds, the guest is shutdown automatically.
Notes:
Comments:

		177595 	[SR-IOV] Assign VF to guest with non-ACS capability SR-IOV devices and enable relaxed_acs_check in qemu.conf 	ajia 	None 	Manual 		Feature 	P1 	1670 	Edit
Setup:

Firstly, you require a SR-IOV devices with non-ACS.(intel-e5530-24-1)

How to determine a SR-IOV devices is non-ACS?

if you met one of the following case, the SR-IOV is non-ACS:

1.  lspci can't get PCIe ACS capability structure, in other words, capability structure hasn't capability ID 13 (or 0xD) such as intel82576

For 1, it is "downstream port lacks ACS".

2.  ACSCtl register haven't SrcValid+ ReqRedir+ CmpltRedir+ UpstreamFwd+ capabilities enabled

For 2, it is "downstream port has ACS disabled".

For details:

# lspci | grep 82576
05:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
05:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
06:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
06:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

# lspci -t

......

 \-[0000:00]-+-00.0
                      +-01.0-[03-06]----00.0-[04-06]--+-02.0-[05]--+-00.0           <--05:00.0
                      |                                                       |                      \-00.1           <--05:00.1
                      |                                                        \-04.0-[06]--+-00.0
                      |                                                                               \-00.1

......

Notes:

we see that 05:00.0/1 are connected via 00:01.0 -> 00:00.0.  That means
that both 00:01.0 and 00:00.0 need to implement ACS, so we can check
whether 00:01.0 and 00:00.0 have ACS capabilites support and enable.  

# lspci -vvv -xxxx -s 00:01.0          

......

    Capabilities: [150] Access Control Services
        ACSCap:    SrcValid+ TransBlk+ ReqRedir+ CmpltRedir+ UpstreamFwd+ EgressCtrl- DirectTrans-
        ACSCtl:      SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-

......

Although we can see "Capabilities: [150] Access Control Services", ACSCtl hasn't SrcValid+ ReqRedir+ CmpltRedir+ UpstreamFwd+ capabilities enabled, so it is a SR-IOV with non-ACS device.

--------------------------------------------------------------------------------------------------------------------------------------------------------------

NOTE: If you see SrcValid+ ReqRedir+ CmpltRedir+ UpstreamFwd+ on hp-z600(e5530), you can still finish this case.

--------------------------------------------------------------------------------------------------------------------------------------------------------------

 

 

Secondly, you require a guest , if not, please install it by virsh/virt-manager/virt-install

Third.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtio
    Acceptance
    SR-IOV

bug:

    No bug found

Actions:

1.# lspci|grep -i ethernet
01:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5764M Gigabit Ethernet PCIe (rev 10)
05:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
05:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
06:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
06:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

2.# virsh nodedev-list --tree
computer
 |
  +- net_lo_00_00_00_00_00_00
  +- net_virbr0_nic_52_54_00_27_cb_91
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |       |
  |       +- pci_0000_04_02_0
  |       |   |
  |       |   +- pci_0000_05_00_0
  |       |   +- pci_0000_05_00_1
  |       |       |
  |       |       +- net_eth2_00_1b_21_55_b3_b9
  |       |
  |       +- pci_0000_04_04_0
  |           |
  |           +- pci_0000_06_00_0
  |           |   |
  |           |   +- net_eth3_00_1b_21_55_b3_bc
  |           |
  |           +- pci_0000_06_00_1
  |               |
  |               +- net_eth4_00_1b_21_55_b3_bd

......

3.# virsh nodedev-dumpxml pci_0000_06_00_0
<device>
  <name>pci_0000_06_00_0</name>
  <parent>pci_0000_04_04_0</parent>
  <driver>
    <name>igb</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>6</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x10e8'>82576 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

4.# readlink -f /sys/bus/pci/devices/0000\:06\:00.0/driver/
/sys/bus/pci/drivers/igb

5.# virsh nodedev-dettach pci_0000_06_00_0
Device pci_0000_06_00_0 dettached

6.# readlink -f /sys/bus/pci/devices/0000\:06\:00.0/driver/
/sys/bus/pci/drivers/pci-stub

7.# virsh edit guestname, edit the following content into inactive guest xml configuration

    <hostdev mode='subsystem' type='pci' managed='no'>
      <source>
        <address domain='0x0000' bus='0x06' slot='0x00' function='0x0'/>
      </source>
    </hostdev>

8.# virsh dumpxml demo
<domain type='kvm' id='2'>
  <name>demo</name>
  <uuid>0352c36f-a172-99be-28c6-228f16ba9339</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
......

    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x06' slot='0x00' function='0x0'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </hostdev>

......

9. # vim /etc/libvirt/qemu.conf (to uncomment the following line)

relaxed_acs_check = 1

10.# service libvirtd restart

11.# virsh start demo
Domain demo started

12. log in guest and check whether this 82576 NICs exists by lspci and works well.

Note: if yo want this NICs work, you may need to configure related file, and restart network service.
	
Expected Results:

each step is successful, and you should can assign a sr-iov with non-ACS device to guest when

enable relaxed_acs_check = 1 in /etc/libvirt/qemu.conf.
Notes:
Comments:

		177853 	[Watchdog device] Watchdog device "i6300esb" - none 	yimwang 	None 	Auto 		Feature 	P3 	1670 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    watchdog device
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='i6300esb' action='none'/> to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, check the watchdog dirvier is installed:

# dmesg | grep i6300

# /sbin/lsmod | grep i6300esb

# lspci

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm
17108 ?        Sl     0:19 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name rhel6 -uuid 2292db8e-9f74-ea3a-5fc4-2240a3f8534e -nodefconfig -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/rhel6.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -device virtio-serial-pci,id=virtio-serial0,max_ports=16,vectors=4,bus=pci.0,addr=0x5 -device virtio-serial-pci,id=virtio-serial1,bus=pci.0,addr=0x6 -drive file=/var/lib/libvirt/images/nfs_test.img,if=none,id=drive-ide0-0-0,boot=on,format=raw,cache=none -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=20,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:61:a6:9e,bus=pci.0,addr=0x3 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -chardev pty,id=channel0 -device virtserialport,bus=virtio-serial0.0,nr=0,chardev=channel0,name=org.linux-kvm.port.0 -chardev pty,id=channel1 -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=channel1,name=org.linux-kvm.port.1 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device i6300esb,id=watchdog0,bus=pci.0,addr=0x7 -watchdog-action none -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4
17255 pts/11   S+     0:00 grep qemu-kvm

4.The oupt is similar to the following:

# dmesg | grep i6300
i6300ESB timer: initialized (0xffffc20000036000). heartbeat=30 sec (nowayout=0)
# /sbin/lsmod | grep i6300esb
i6300esb     40557     0

5.watchdong software is installed successfully.

8.After 130 seconds, guest status have not changed.

Notes:
Comments:

		177596 	[SR-IOV] Assign VF to guest with non-ACS capability SR-IOV devices and use qemu.conf default configuration 	ajia 	None 	Manual 		Feature 	P1 	1680 	Edit
Setup:

Firstly, you require a SR-IOV devices with non-ACS.(10.66.4.241)

How to determine a SR-IOV devices is non-ACS?

if you met one of the following case, the SR-IOV is non-ACS:

1.  lspci can't get PCIe ACS capability structure, in other words, capability structure hasn't capability ID 13 (or 0xD) such as intel82576

For 1, it is "downstream port lacks ACS".

2.  ACSCtl register haven't SrcValid+ ReqRedir+ CmpltRedir+ UpstreamFwd+ capabilities enabled

For 2, it is "downstream port has ACS disabled".

For details:

# lspci | grep 82576
05:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
05:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
06:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
06:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

# lspci -t

......

 \-[0000:00]-+-00.0
                      +-01.0-[03-06]----00.0-[04-06]--+-02.0-[05]--+-00.0           <--05:00.0
                      |                                                       |                      \-00.1           <--05:00.1
                      |                                                        \-04.0-[06]--+-00.0
                      |                                                                               \-00.1

......

Notes:

we see that 05:00.0/1 are connected via 00:01.0 -> 00:00.0.  That means
that both 00:01.0 and 00:00.0 need to implement ACS, so we can check
whether 00:01.0 and 00:00.0 have ACS capabilites support and enable.  

# lspci -vvv -xxxx -s 00:01.0          

......

    Capabilities: [150] Access Control Services
        ACSCap:    SrcValid+ TransBlk+ ReqRedir+ CmpltRedir+ UpstreamFwd+ EgressCtrl- DirectTrans-
        ACSCtl:      SrcValid- TransBlk- ReqRedir- CmpltRedir- UpstreamFwd- EgressCtrl- DirectTrans-

.....

Although we can see "Capabilities: [150] Access Control Services", ACSCtl hasn't SrcValid+ ReqRedir+ CmpltRedir+ UpstreamFwd+ capabilities enabled, so it is a SR-IOV with non-ACS device.

--------------------------------------------------------------------------------------------------------------------------------------------------------------

NOTE: If you see SrcValid+ ReqRedir+ CmpltRedir+ UpstreamFwd+ on hp-z600(10.66.4.241), you can still finish this case.

--------------------------------------------------------------------------------------------------------------------------------------------------------------

 

 

Secondly, you require a guest , if not, please install it by virsh/virt-manager/virt-install

Third.For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Acceptance
    SR-IOV

bug:

    No bug found

Actions:

1.# lspci|grep -i ethernet
01:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5764M Gigabit Ethernet PCIe (rev 10)
05:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
05:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
06:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)
06:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

2.# virsh nodedev-list --tree
computer
 |
  +- net_lo_00_00_00_00_00_00
  +- net_virbr0_nic_52_54_00_27_cb_91
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |       |
  |       +- pci_0000_04_02_0
  |       |   |
  |       |   +- pci_0000_05_00_0
  |       |   +- pci_0000_05_00_1
  |       |       |
  |       |       +- net_eth2_00_1b_21_55_b3_b9
  |       |
  |       +- pci_0000_04_04_0
  |           |
  |           +- pci_0000_06_00_0
  |           |   |
  |           |   +- net_eth3_00_1b_21_55_b3_bc
  |           |
  |           +- pci_0000_06_00_1
  |               |
  |               +- net_eth4_00_1b_21_55_b3_bd

......

3.# virsh nodedev-dumpxml pci_0000_06_00_0
<device>
  <name>pci_0000_06_00_0</name>
  <parent>pci_0000_04_04_0</parent>
  <driver>
    <name>igb</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>6</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x10e8'>82576 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

4.# readlink -f /sys/bus/pci/devices/0000\:06\:00.0/driver/
/sys/bus/pci/drivers/igb

5.# virsh nodedev-dettach pci_0000_06_00_0
Device pci_0000_06_00_0 dettached

6.# readlink -f /sys/bus/pci/devices/0000\:06\:00.0/driver/
/sys/bus/pci/drivers/pci-stub

7.# virsh edit guestname, edit the following content into inactive guest xml configuration

    <hostdev mode='subsystem' type='pci' managed='no'>
      <source>
        <address domain='0x0000' bus='0x06' slot='0x00' function='0x0'/>
      </source>
    </hostdev>

8.# virsh dumpxml demo
<domain type='kvm' id='2'>
  <name>demo</name>
  <uuid>0352c36f-a172-99be-28c6-228f16ba9339</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
......

<hostdev mode='subsystem' type='pci' managed='no'>
      <source>
        <address domain='0x0000' bus='0x06' slot='0x00' function='0x0'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </hostdev>

......

9.# virsh start demo


error: Failed to start domain demo
error: Requested operation is not valid: PCI device 0000:06:00.1 is not assignable
	
Expected Results:

each step is successful, and you should see the following information on step9:

error: Failed to start domain demo
error: Requested operation is not valid: PCI device 0000:06:00.1 is not assignable

however if you  check the log /var/log/libvirt/libvirt.log ,you can see a record like this


 internal error Device 0000:06:00.0 is behind a switch lacking ACS and cannot be assigned
Notes:
Comments:

		177856 	[Watchdog device] Watchdog device "ib700" - poweroff(bug667090) 	yimwang 	None 	Auto 		Regression 	P3 	1680 	Edit
Setup:

Bug 667090 - kernel needs ib700 driver
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    watchdog device
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='ib700' action='poweroff'/> to the <devices> section of the XML, and start the guest.

 watch dog ib700 kernel mod is ib700wdt. and you must load  the module by hand. 

add "modprobe ib700wdt" to /etc/rc.d/rc.sysinit and reboot.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, load the ib700 mode and check the watchdog dirvier is installed:

#modprobe ib700wdt

# dmesg | grep ib700

# /sbin/lsmod | grep ib700

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm

.........................

-device ib700,id=watchdog0 -watchdog-action poweroff

5.watchdong software is installed successfully.

8. After about 2 minutes, the guest is power off automatically.
Notes:
Comments:

		177549 	[Snapshot] Domain snapshot with FC 	mzhan 	mzhan 	Manual 		Function 	P1 	1690 	Edit
Setup:

Make sure the host has HBA card with FC storage connected.

- For FC storage please apply from S4. 

- For host has HBA card you can use 10.66.83.204

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    snapshot

bug:

    837544 - From Run 42159
    837544 - From Run 42935
    837544 - From Run 42300
    852668 - From Run 44628

Actions:

1. Create an mpath storage pool

1)# yum install device-mapper-multipath

2) # cp /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf /etc/multipath.conf

3) Edit /etc/multipath.conf:

PLS do not remove local disk config and luns config in multipath.conf.

Make sure localdisk is not in your multipath -ll

4) Start multipath:
/etc/init.d/multipathd start

5) # cat mpath.xml

<pool type="mpath">
  <name>mpath</name>
  <target>
    <path>/dev/mapper</path>
  </target>
</pool> 

# virsh pool-create mpath.xml
Pool mpath created from mpath.xml

# virsh vol-list mpath
Name                 Path                                   
-----------------------------------------
dm-3                 /dev/mapper/mpathb   

2. Create a guest storage is in fc storage
# qemu-img create -f qcow2 /dev/mapper/mpathb 10G
Formatting '/dev/mapper/mpathb', fmt=qcow2 size=10737418240 encryption=off cluster_size=0
# qemu-img info /dev/mapper/mpathb
image: /dev/mapper/mpathb
file format: qcow2
virtual size: 10G (10737418240 bytes)
disk size: 0
cluster_size: 65536
Install a guest, make sure using /dev/mapper/mpathb as storage.

# virsh dumpxml <guest>
...
    <disk type='block' device='disk'>
      <driver name='qemu' type='qcow2' cache='none' io='native'/>
      <source dev='/dev/mapper/mpathb'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
...

3. Create an snapshot of the shutdown domain.

# virsh destroy test
Domain test destroyed

# virsh snapshot-create-as <domain> snap1 
Domain snapshot snap1 created

4. List the snapshot of created domain.

# virsh snapshot-list test
 Name                 Creation Time             State
---------------------------------------------------
 snap1                2011-07-15 16:56:11 +0800 shutoff 

 

5. create live snapshot for domain 

#virsh start <domain>

# virsh snapshot-create-as <domain> snap2 --disk-only --diskspec vda,file=/tmp/iscsi.image 
	
Expected Results:

Check step 4  and step 5 that if the created snapshot is correct state.

like: 

ï»¿Domain snapshot snap created
Notes:
Comments:

		177550 	[Snapshot] Domain snapshot with iSCSI 	nzhang 	None 	Manual 		Regression 	P1 	1700 	Edit
Setup:

Make sure you host have connect iscsi storage, such as

# iscsiadm --mode discovery --type sendtargets --portal 10.66.90.100
10.66.90.100:3260,1 iqn.2001-05.com.equallogic:0-8a0906-26f1f7d03-1c7267779594c286-dyuan-1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    snapshot

bug:

    837544 - From Run 42159
    837544 - From Run 42935
    837544 - From Run 42300
    852668 - From Run 44628

Actions:

1. Create an iSCSI storage pool (Refer to the case [iscsi storage pool creation]).

# virsh pool-list
Name                 State      Autostart
-----------------------------------------
default              active     yes      
iscsi                active     yes

# virsh pool-dumpxml iscsi
<pool type='iscsi'>
  <name>iscsi</name>
  <source>
    <host name='10.66.90.100'/>
    <device path='iqn.2001-05.com.equallogic:0-8a0906-26f1f7d03-1c7267779594c286-dyuan-1'/>
  </source>
  <target>
    <path>/dev/disk/by-path</path>
  </target>
</pool>

2. Prepare a disk of block type for the guest.

# qemu-img create -f qcow2 /dev/disk/by-path/ip-10.66.90.100:3260-iscsi-iqn.2001-05.com.equallogic:0-8a0906-26f1f7d03-1c7267779594c286-dyuan-1-lun-0 30G

# virsh dumpxml <domain>
....
    <disk type='block' device='disk'>
      <driver name='qemu' type='qcow2' cache='none' io='native'/>
      <source dev='/dev/disk/by-path/ip-10.66.90.100:3260-iscsi-iqn.2001-05.com.equallogic:0-8a0906-26f1f7d03-1c7267779594c286-dyuan-1-lun-0'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
....
# virsh list --all

 Id Name                 State
----------------------------------
  - <domain>                shut off

3. Create a snapshot of the shutdown domain.

# virsh snapshot-create-as <domain> snap1
Domain snapshot snap1 created

4. List the snapshot of created domain.

# virsh snapshot-list <domain>
 Name                 Creation Time             State
---------------------------------------------------
 snap1                2011-07-12 23:57:17 -0400 shutoff

5. create live snapshot for domain 

#virsh start <domain>

# virsh snapshot-create-as <domain> snap2 --disk-only --diskspec vda,file=/tmp/iscsi.image 

 
	
Expected Results:

Check step 4  and step 5 that if the created snapshot is correct state.

like: 

ï»¿Domain snapshot snap created
Notes:
Comments:

		177859 	[Watchdog device]Watchdog device "ib700" - none(bug667090) 	yimwang 	None 	Manual 		Regression 	P3 	1700 	Edit
Setup:

Bug 667090 - kernel needs ib700 driver
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    watchdog device

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='ib700' action='none'/>t to the <devices> section of the XML, and start the guest.

 watch dog ib700 kernel mod is ib700wdt. and you must load  the module by hand. 

add "modprobe ib700wdt" to /etc/rc.d/rc.sysinit and reboot.


3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, check the watchdog dirvier is installed:

# dmesg | grep ib700

# /sbin/lsmod | grep ib700

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm

.........................

-watchdog ib700

-watchdog-action none

5.watchdong software is installed successfully.

8.After 130 seconds, guest status have not changed.
Notes:
Duplicate with
https://tcms.engineering.redhat.com/case/177848/?from_plan=6578
Comments:

		177551 	[Snapshot] Domain snapshot with NFS 	nzhang 	None 	Manual (Autoproposed) 		Feature 	P1 	1710 	Edit
Setup:

Setup nfs server:

/var/lib/libvirt/images  *(rw)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    QE consumption
    snapshot

bug:

    837544 - From Run 42159
    837544 - From Run 42935
    837544 - From Run 42300
    852668 - From Run 44628

Actions:

1. Mount an NFS to local directory.

# mount nfs-sever-ip:/var/lib/libvirt/images /var/lib/libvirt/images

2. Create a guest which the image file rhel6.qcow2 is on the NFS storage and format is qcow2.

# virsh list --all
 Id Name                 State
----------------------------------
  - <domain>                shut off

3. Create an snapshot of the shutdown domain.

# virsh snapshot-create-as <domain> snap1 
Domain snapshot snap1 created

4. List the snapshot of created domain.

# virsh snapshot-list <domain>
 Name                 Creation Time             State
---------------------------------------------------
 snap1                2011-07-12 23:47:17 -0400 shutoff

5. Make the guest with running state.

# virsh start <domain>
Domain <domain> started

# virsh list
 Id Name                 State
----------------------------------
  4 <domain>                running

6. Create an snapshot of the running domain.

# virsh snapshot-create-as <domain> snap2 --disk-only
Domain snapshot snap2 created

7. List the snapshot of created domain.

# virsh snapshot-list <domain>
 Name                 Creation Time             State
---------------------------------------------------
 snap1                2011-07-12 23:47:17 -0400 shutoff
 snap2                2011-07-12 23:50:52 -0400 disk-snapshot
	
Expected Results:

Check step 4 & 7 that if the created snapshot is correct state.

 
Notes:
Comments:

		177568 	[snapshot]libvirt should give error when qemu-kvm disabled live snapshot support --bug 747115 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	1720 	Edit
Setup:


Remove live snapshot support from RHEL 6.2 qemu-kvm package.

Live snapshot support should be removed from qemu-kvm and deferred until RHEL 6.3.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot

bug:

    No bug found

Actions:

1. # rpm -q libvirt qemu-kvm
   libvirt-0.9.4-19.el6.x86_64
   qemu-kvm-0.12.1.2-2.199.el6.x86_64

   # virsh snapshot-list foo
    Name                 Creation Time             State
   ------------------------------------------------------------
    s1                   2010-02-16 19:42:28 -0500 running

2. # virsh snapshot-create --disk-only foo
   error: operation failed: Failed to take snapshot: unknown command:
   'snapshot_blkdev'

3. # virsh snapshot-list foo
    Name                 Creation Time             State
   ------------------------------------------------------------
    s1                   2010-02-16 19:42:28 -0500 running

4. # qemu-img info /var/lib/libvirt/images/foo.img 
   image: /var/lib/libvirt/images/foo.img
   file format: qcow2
   virtual size: 5.9G (6291456000 bytes)
   disk size: 2.4G
   cluster_size: 65536
   Snapshot list:
   ID        TAG                 VM SIZE                DATE       VM CLOCK
   1         s1                     3.1M 2010-02-16 19:42:28   00:00:04.605

	
Expected Results:

Step 2 should show error information like:


   error: operation failed: Failed to take snapshot: unknown command:
   'snapshot_blkdev'

 
Notes:
Comments:

		177318 	[Migration]Break migration during migrating-bug635353 	yimwang 	None 	Manual (Autoproposed) 		--default-- 	P2 	1730 	Edit
Setup:

    Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. Prepare migration env and guest.

# setenforce 0

# iptables -F

# mount <nfs_ip>:<share_storage_path> /mnt

Start a guest with image located on /mnt, the xml of disk should with "cache=none"

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/mnt/vm1'/>
      <target dev='vda' bus='virtio'/>
    </disk>

2. Migrate the guest from host1 to host2.

   # virsh migrate-setspeed vm1 1; virsh migrate  vm1 --live  qemu+ssh://10.66.93.197/system --verbose


3.During migrating use "Ctrl+c" to break migrate .

4. Migrate the guest from host1 to host2 with tunnelled migration

   # virsh migrate-setspeed vm1 1; virsh migrate  vm1 --live  --p2p --tunnelled qemu+ssh://10.66.93.197/system --verbose

5.During migrating use "Ctrl+c" to break migrate .
	
Expected Results:

3.  Guest  vm1 on source host will work fine.

      Guest vm1 on the target host must be deleted automaticly.

      Should see the error after break migration

            error: operation aborted: migration job: canceled by client  

5.  Same as step 3
Notes:
Comments:

		177518 	[Scalability] migrate a guest for 1024 rounds through TCP connection --p2p 	weizhan 	None 	Manual 		Stress 	P1 	1730 	Edit
Setup:

Following case:

39253 [Remote access] Connect to the hypervisor on host using an unsecured TCP connection

<https://tcms.engineering.redhat.com/case/39253/?from_plan=1950>

Run the case on both source host and target host to enable tcp listen for libvirtd.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    scalability

bug:

    No bug found

Actions:

1. Define and start a domain on SOURCE host

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live  --p2p"
TRANSPORT="tcp"
#TRANSPORT="tls"
#TRANSPORT="ssh"

date
for i in `seq 1 1024`;
do
    echo "Loop ${i}: Migrating ${GUEST} from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system
done
date

3. Start to migrate domain for 1024 rounds

# sh migrate.sh <guestname> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 1024 round should be fininshed successfully.

Output:

.....


Loop 1: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.512s
user    0m0.000s
sys    0m0.005s
Loop 1: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.445s
user    0m0.002s
sys    0m0.007s
Loop 2: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.235s
user    0m0.003s
sys    0m0.003s
Loop 2: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.201s
user    0m0.001s
sys    0m0.004s

......

4. The migration time in the above output should be acceptable, range should be in 0 ~ 5 sec.

no big degradation is seen.
Notes:
Comments:

		177521 	[Scalability] migrate 3 guests for 512 rounds through TLS connection --p2p 	weizhan 	None 	Manual 		Stress 	P1 	1740 	Edit
Setup:
Finished case:

39342 [Remote access] Connect to the hypervisor running on host using TLS

Run the case on both source host and target host to enable tls listen for libvirtd.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability
    migration

bug:

    No bug found

Actions:

1. Define and start 3 guests on SOURCE host named test1 test2 test3

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live --p2p"
TRANSPORT="tls"


date
for i in `seq 1 512`;
do
    echo "Loop ${i}: Migrating ${GUEST}123 from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}123 qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}1 qemu+${TRANSPORT}://root@${HOST2}/system & pid1=$!
   time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}2 qemu+${TRANSPORT}://root@${HOST2}/system & pid2=$!
   time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}3 qemu+${TRANSPORT}://root@${HOST2}/system & pid3=$!
    wait $pid1
    wait $pid2
    wait $pid3

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}123 qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}1 qemu+${TRANSPORT}://root@${HOST1}/system & pid1=$!
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}2 qemu+${TRANSPORT}://root@${HOST1}/system & pid2=$!
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}3 qemu+${TRANSPORT}://root@${HOST1}/system & pid3=$!
    wait $pid1
    wait $pid2
    wait $pid3


done
date

3. Start to migrate domain for 512 rounds

# sh migrate.sh <guest_prefix> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 512 round should be fininshed successfully.

4. The migration time in the above output should be acceptable, range should be in 0 ~ 5 sec.

no big degradation is seen.
Notes:
Comments:

		177524 	[Scalability] Multiple concurrent events catching 	vbian 	None 	Manual 		Function 	P1 	1750 	Edit
Setup:

modify /etc/libvirt/libvirtd.conf

enable max_clients = 100
enable max_workers = 100
enable max_requests =100
enable max_client_requests = 100

 download http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/vbian/Scalability/reproducor-scripts.tar.gz

 

make sure you have at lease ten guests

and modify multi_lifecycle.sh , and change the guest name in Guests array to the guest name of your own guests 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

# sh multi_lifecycle.sh

on the second terminal

# python /usr/share/doc/libvirt-python-xxxx/events-python/event-test.py
	
Expected Results:

Make sure

1. on the first terminal , libvirtd won't crash during the excution of the script

2. on the second terminal , you won't see any failure report .
Notes:
Comments:

		177766 	[Virtio] Hot plug a virtio disk 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	1750 	Edit
Setup:

For virtio driver support, rhel OS version must be rhel4.8+, include rhel4.8
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Create a domain.

2. Prepare the following xml:
# cat virtio_disk.xml
    <disk type='file' device='disk'>
      <source file='/var/lib/libvirt/images/newdisk.img'/>
      <target dev='vdb' bus='virtio'/>
    </disk>

3. Create a disk image file
# qemu-img create /var/lib/libvirt/images/newdisk.img 5G

4. For the rhel4 and rhel5 guest, please load acpiphp module, if it is rhel6, ignore this step
# modprobe acpiphp

5. Hot-plug the virtio disk to the running guest.
# virsh attach-device <guestname> virtio_disk.xml

6. Check the new disk is added successfully, and works fine.
# lspci
# fdisk -l
# fdisk /dev/vdb

in fdisk command line, input: n, p, 1, enter, enter, w


# mkfs.ext3 /dev/vdb1
# mount /dev/vdb1 /mnt
# >/mnt/test
	
Expected Results:

5. Output:

# virsh attach-device rhel6-32 virtio_disk.xml
Device attached successfully

6. Output:
# lspci
...
00:05.0 SCSI storage controller: Red Hat, Inc Virtio block device (The orignal disk)
...
00:07.0 SCSI storage controller: Red Hat, Inc Virtio block device (The new added disk)

# fdisk -l
...
Disk /dev/vdb doesn't contain a valid partition table

# fdisk /dev/vdb

# mkfs.ext3 /dev/vdb1

# mount /dev/vdb1 /mnt/

# >/mnt/test

# ll /mnt/test
-rw-r--r--. 1 root root 0 Dec 23 10:35 /mnt/test

Notes:
Comments:

		177528 	[Scalability] Running guests with more than 256 disks - Bug 740899 	weizhan 	None 	Manual 		Regression 	P1 	1760 	Edit
Setup:

prepare guest xml

Download http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/test1.xml

1. make 21 copy of the xml

# for i in {2..22};do cp test1.xml test$i.xml;done

2. modify xml

# vim test2.xml

in vim use :%s/guest1/guest2/g

follow this util done with test22.xml

3. create guest imgs

# for i in {1..22};do qemu-img create /var/lib/libvirt/images/guest$i 10M;done

# for i in {1..22}; do for j in {b..x};do qemu-img create /var/lib/libvirt/images/guest$i-$j.img 10M;done;done

4. define guests

# for i in {1..22};do virsh define test$i.xml; done
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability
    virtual disks

bug:

    No bug found

Actions:

1. check the aio limit

#cat /proc/sys/fs/aio-max-nr

1048576

if it is not 1048576 please reboot the host and check again

2. Start 22 guests with 24 disks on each guest and aio=native ( > 512 disks )

# for i in {1..22};do virsh start guest$i; done

# virsh dumpxml guest9

<domain type='kvm' id='20'>
  <name>guest9</name>
  <uuid>427268fa-aba4-4966-ed1d-bbaea7702a8f</uuid>
  <memory>256288</memory>
  <currentMemory>257024</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.1.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native'/>
      <source dev='/nfs/images/guest9'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native'/>
      <source dev='/nfs/images/guest9-b.img'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </disk>
    ......
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native'/>
      <source dev='/nfs/images/guest9-x.img'/>
      <target dev='vdx' bus='virtio'/>
      <alias name='virtio-disk23'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x1b' function='0x0'/>
    </disk>
    <serial type='pty'>
      <source path='/dev/pts/4'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/4'>
      <source path='/dev/pts/4'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5901' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux' relabel='yes'>
    <label>system_u:system_r:svirt_t:s0:c347,c791</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c347,c791</imagelabel>
  </seclabel>
</domain>
	
Expected Results:

Step 1:

The result is

1048576

Step 2:

All the guests can be started successfully
Notes:
Comments:

		177767 	[Virtio] Hot plug a virtio NIC 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	1760 	Edit
Setup:

For virtio driver support, rhel OS version must be rhel4.8+, include rhel4.8
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Create a domain.

2. Prepare the following xml:
# cat virtio_nic.xml
    <interface type='network'>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
3. Hot-plug the virtio disk to the running guest.

# virsh attach-device <guestname> virtio_nic.xml

4. Check the new virtio NIC is seen in guest

# lspci

# ip link

5. Create the follwoing config file for the new added interface

# ip link

..............

3: eth1:......

.................

# vim /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE="eth1"
ONBOOT=yes
TYPE=Ethernet
BOOTPROTO=dhcp

6. Check if the new added virtio interface works fine.
# ifup eth1

# ifconfig eth1

# ping www.google.com -I eth1
	
Expected Results:

3, OUTPUT:


Device attached successfully

4. OUTPUT:

# lspci
...
00:03.0 Ethernet controller: Red Hat, Inc Virtio network device   (This is the orignal one)
...
00:07.0 Ethernet controller: Red Hat, Inc Virtio network device    (This is the new added one)

# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 1000
    link/ether 52:54:00:de:0c:27 brd ff:ff:ff:ff:ff:ff
3: eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 52:54:00:95:a3:fe brd ff:ff:ff:ff:ff:ff

NOTE:

eth1 is the new added one.

6. OUTPUT:

# ifup eth1

Determining IP information for eth1... done.

# ifconfig eth1
eth1      Link encap:Ethernet  HWaddr 52:54:00:95:A3:FE  
          inet addr:192.168.122.157  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fe95:a3fe/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:236 errors:0 dropped:0 overruns:0 frame:0
          TX packets:10 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:12998 (12.6 KiB)  TX bytes:1292 (1.2 KiB)

# ping www.google.com -I eth1
PING www.l.google.com (72.14.204.147) from 192.168.122.157 eth2: 56(84) bytes of data.
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=1 ttl=42 time=285 ms
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=2 ttl=42 time=283 ms
^C
--- www.l.google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1956ms
rtt min/avg/max/mdev = 283.088/284.361/285.634/1.273 ms

Notes:
Comments:

		177535 	[Scalability] Support 500 concurrent guests - bug 638996 	gsun 	None 	Manual 		--default-- 	P2 	1770 	Edit
Setup:

Finished 56369 [Scalability] 256 RHEL guests boot up â 256 guests â 100%
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

After boot up 256 guests, push it to reach 500 guests

1.  According to step 5 of 56359 [Scalability] 256 RHEL guests boot up â 51 guests â 20%, clone another 244 vms:

    Before clone, first modify the template xml network interface part to:

    <interface type='network'>
      ...
      <source network='default1'/>
      ...
    </interface>

    Then:

    # cd clone_vm

    # ./clone_vm.sh clone_number_start=257 clone_number_end=500 source_image_file=/mnt/guest_name.img guestname_prefix=<STRING> guest_template_xml=/root/clone_vm/rhel6_Server_x86_64_kvm.xml image_target_dir=/mnt


    Note: <STRING> should be replace with the name you desired.

2. start the new cloned 244 guests.

    # for i in {257..267};do virsh start <STRING>$i;done

    ...

    # for i in {496..500};do virsh start <STRING>$i;done

    NOTE: The boot up should be graduatlly , like boot the existing 256 guest, 10 by 10 guests or less base on the response of the machine.

3. After guest all started, log in the guests to check whether they can get the ip address.

    # ifconfig

    Note: It will take a while for all guests to boot up.


4. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

For step 3,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

# ifconfig

...

inet addr:192.168.122.79  Bcast:192.168.122.255  Mask:255.255.255.0

...

Or run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

......

For step 4:
TBD
Check 256 guests case for compare.
Notes:
Comments:

		177768 	[Virtio] Hot plug multiple (up to 29 or so) virtio disks 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	1770 	Edit
Setup:

For virtio driver support, rhel OS version must be rhel4.8+, include rhel4.8
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

The steps are the same as "[Disk hotplug] Attach 32 disks to guest"
	
Expected Results:
Notes:
Comments:

		177769 	[Virtio] Hot plug multiple (up to 29 or so) virtio NIC 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	1780 	Edit
Setup:

For virtio driver support, rhel OS version must be rhel4.8+, include rhel4.8
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

The steps is almost the same as "[Interface hotplug] Attach 32 NIC to guest"

https://tcms.engineering.redhat.com/case/57085/?from_plan=1950

 

The main difference is step 2:

2. Prepare the following file:

# cat nic_templ.xml
    <interface type='network'>
      <source network='default'/>
      <model type='virtio'/>
      <target dev='#TARGET_DEV#'/>
    </interface>
	
Expected Results:
Notes:
Comments:

		177537 	[Scalability]destroy the 256 guests 	vbian 	None 	Manual 		Regression 	P1 	1790 	Edit
Setup:

Since the slowness coming from cgroups in 6.0 GA s now fixed with 6.1 kernel, no need to disable cgroup at the start. If the slowness problem still exist, do it again under disable cgroup as following:

    If the host is a large SMP boxes (like with 96 cpus), first need to disable cgroup.

             # service cgconfig stop

             # chkconfig cgconfig off

             # service libvirtd restart

2.run 256 vms

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1.Try to shutdown all the guest in a loop

e.g.

for i in {1..256}

> do

> virsh destroy vm$i

> done

2. List all of the guest

# virsh list --all
	
Expected Results:

Make sure all of the 256 guests are shutdown

 

# time for i in {1..256}; do virsh destroy vm$i; done

real    8m51.384s
user    0m1.258s
sys    0m7.722s

Notes:
Comments:

		177770 	[Virtio] Hot unplug a virtio disk 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	1790 	Edit
Setup:

Following steps in [Virtio] Hot plug a virtio disk

https://tcms.engineering.redhat.com/case/75606/?from_plan=4050
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Following steps in [Virtio] Hot plug a virtio disk

2. hot unplug the new added disk

# virsh detach-device <guestname> virtio_disk.xml

3. In guest, check if the disk is not existing

# lspci

# fdisk -l

# ls /dev/vdb*
	
Expected Results:

2. Output:


Device detached successfully

3. There is no any info about vdb device.
Notes:
Comments:

		177538 	[Scalability]kill SIGSTOP qemu processes 	vbian 	None 	Manual 		Regression 	P1 	1800 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1.run more than 20 vms
2.kill -SIGSTOP to 5 qemu processes 
3.perform following command to check libvirtd status
   # service libvirtd status
   # virsh list --all 

	
Expected Results:

3.1 libvirtd is running

3.2 virsh list --all could list all of the guest immidietely

The guest still remain the status(running) before kill.
Notes:
Comments:

		177771 	[Virtio] Hot unplug a virtio NIC 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	1800 	Edit
Setup:

Following steps in [Virtio] Hot plug a virtio nic

https://tcms.engineering.redhat.com/case/95607/?from_plan=4050
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Following steps in [Virtio] Hot plug a virtio nic

NOTE:

You add some identify element into virtio_nic.xml to let libvirt know you want to remove which NIC, such as; mac;

Just like:

    <interface type='network'>
     <mac address='52:54:00:95:a3:fe'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>

Here, the mac address is the new added virtio NIC's mac.

2. hot unplug the new added nic

# virsh detach-device <guestname> virtio_nic.xml

3. In guest, check if the nic is not existing

# lspci

# ifconfig

# ip link

# virsh dumpxml <guestname>
	
Expected Results:

2. OUTPUT:

# virsh detach-device rhel6-32 virtio_nic.xml
Device detached successfully

3. The added virtio NIC is removed now, there is no any info about it, include domain xml.
Notes:
Comments:

		177772 	[Virtio] Migrate a guest with virtio NICs and disks 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	1810 	Edit
Setup:

For virtio driver support, rhel OS version must be rhel4.8+, include rhel4.8
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Create a domain with virtio NICs and disks

2. Make sure in guest user can write data to the disks, ping outside (such as: ping -I eth[X] google.com) via the virtio card successfully.
NOTE: eth[X] is the your virtio NICs

3. Mimgrate the domain to target machines.
# virsh migrate <guestname> qemu+ssh://<target IP>/system

4. On the target machine, log into the guest, do the same operation as step 2.
	
Expected Results:

3. Migrationg shoule work fine.

4. virtio NICs and disks work fine.
Notes:
Comments:

		177539 	[Scalability]migrating vm and restarting libvirtd 	vbian 	None 	Manual 		--default-- 	P1 	1820 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. Run about 35 guest

2. download multi-migrate.py script from http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/vbian/Scalability/

3. python multi-migrate.py --guestname_prefix=vm --start_number=101 --end_number=115 --remote_host_address=10.66.82.169

4. during running the script restart libvirtd

5. virsh list --all on both source machine and destination machine
	
Expected Results:

5. on source machine all of the guest are off after migration, on destination machine all of the guest are running
Notes:
Comments:

		177775 	[Virtio] Save/restore a guest with virtio NICs and disks 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	1820 	Edit
Setup:

For virtio driver support, rhel OS version must be rhel4.8+, include rhel4.8
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks
    virtual disks

bug:

    No bug found

Actions:

1. Create a domain with virtio NICs and disks

2. Make sure in guest user can write data to the disks, ping outside (such as: ping -I eth[X] google.com) via the virtio card successfully.
NOTE: eth[X] is the your virtio NICs

3. Save the domain, then restore it.
# virsh save <guestname> /tmp/saved

# virsh restore /tmp/saved

4. Check if disks and NICs work, just like step 2.
	
Expected Results:

4. Write some data to virtio disk successfully.

    Ping outside via the virtio NIC successfully.
Notes:
Comments:

		177015 	[Host network interface management] Define a vlan 	yimwang 	None 	Manual 		Feature 	P2 	1830 	Edit
Setup:


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. # cat vlan.xml

<interface type='vlan' name='eth0.1'>
   <start mode='onboot'/>
   <protocol family='ipv4'>
      <ip address='192.168.177.1' prefix='24'/>
   </protocol>
   <vlan tag='1'>
     <interface name='eth0'/>
   </vlan>
</interface>

2. # virsh iface-define vlan.xml
Interface eth0.1 defined from vlan.xml

3. # virsh iface-start eth0.1
Interface eth0.1 started

4.  # ifconfig

5. Create a virtual network named vlan via virt-manager

Click Edit-> Connection Details, select Virtual Networks tab, click '+' -> Forward-> input 'vlan' as name -> click 'Forward'

Select 'Forwarding to physical network', select 'Physical device eth0.1' as Destination, 'NAT' as mode, then click Forward, till finish

6. Create 2 guests and hot unplug all the existing networks.

7. Hotplug the vlan to both the guest

# virsh attach-interface $guest --type network --source vlan
	
Expected Results:

4. # ifconfig

eth0      Link encap:Ethernet  HWaddr 00:25:64:A7:1F:4D  
          inet addr:10.66.65.132  Bcast:10.66.65.255  Mask:255.255.254.0
          inet6 addr: fe80::225:64ff:fea7:1f4d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:529297 errors:0 dropped:0 overruns:0 frame:0
          TX packets:556126 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:499891051 (476.7 MiB)  TX bytes:369496710 (352.3 MiB)
          Interrupt:21 Memory:febe0000-fec00000

eth0.1    Link encap:Ethernet  HWaddr 00:25:64:A7:1F:4D  
          inet addr:192.168.177.1  Bcast:192.168.177.255  Mask:255.255.255.0
          inet6 addr: fe80::225:64ff:fea7:1f4d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:34 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 b)  TX bytes:4813 (4.7 KiB)

.......

7. Check could see the network in guests, the guest could get ip, could ping to host and ping to the other guest.
Notes:
Comments:

		177540 	[Scalability]reconnect to the 256 guests 	vbian 	None 	Manual 		--default-- 	P1 	1830 	Edit
Setup:

Since the slowness coming from cgroups in 6.0 GA s now fixed with 6.1 kernel, no need to disable cgroup at the start. If the slowness problem still exist, do it again under disable cgroup as following:

    If the host is a large SMP boxes (like with 96 cpus), first need to disable cgroup.

             # service cgconfig stop

             # chkconfig cgconfig off

             # service libvirtd restart
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1.run 256 vms
2.restart libvird
3.# virsh list

	
Expected Results:

Make sure all of the 256 guests are listed after libvirtd restarted

before:

# time virsh list |grep vm |wc -l
256

real    0m3.813s
user    0m0.091s
sys    0m0.162s

 

After:

# time virsh list |grep vm |wc -l
256

real    0m3.724s
user    0m0.092s
sys    0m0.143s

Notes:
Comments:

		177541 	[smbios]add new smbios information - bug 671326 	gren 	None 	Manual 		Feature 	P1 	1840 	Edit
Setup:

libvirt version libvirt-0.8.6-1.el6 above
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    smbios

bug:

    No bug found

Actions:

1, Add the following <sysinfo> section into a guest XML defination, and 

<domain type='kvm' id='55'>
  <name>rhel6.1</name>
  <uuid>1849d614-50cf-b09d-dbf7-e5909a55c561</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <sysinfo type='smbios'>
    <bios>
      <entry name='vendor'>LENOVO</entry>
      <entry name='version'>6FET82WW (3.12 )</entry>
    </bios>
    <system>
      <entry name='manufacturer'>Fedora</entry>
      <entry name='product'>Virt-Manager</entry>
      <entry name='version'>0.8.2-3.fc14</entry>
      <entry name='serial'>32dfcb37-5af1-552b-357c-be8c3aa38310</entry>
      <entry name='sku'>1234567890</entry>
      <entry name='family'>Red Hat</entry>
    </system>
  </sysinfo>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>

    <smbios mode='sysinfo'/>

  </os>
  ...

2, Start the guest, run "dmidecode" command to examine appropriate seabios informatin

# dmidecode 2.10
SMBIOS 2.4 present.
10 structures occupying 306 bytes.
Table at 0x3FFFFEC0.

Handle 0x0000, DMI type 0, 24 bytes
BIOS Information
        Vendor: LENOVO
        Version: 6FET82WW (3.12 )
        Release Date: 01/01/2007
        Address: 0xE8000
        Runtime Size: 96 kB
        ROM Size: 64 kB
        Characteristics:
                BIOS characteristics not supported
                Targeted content distribution is supported
        BIOS Revision: 1.0

Handle 0x0100, DMI type 1, 27 bytes
System Information
        Manufacturer: Fedora
        Product Name: Virt-Manager
        Version: 0.8.2-3.fc14
        Serial Number: 32dfcb37-5af1-552b-357c-be8c3aa38310
        UUID: 784F384B-7FA0-29B9-2D9E-A99F0CEC6BF0
        Wake-up Type: Power Switch
        SKU Number: 1234567890
        Family: Red Hat
	
Expected Results:
Notes:
Comments:

		177542 	[smbios]add new smbios information - bug 671326 	gren 	gren 	Manual 		Feature 	P1 	1850 	Edit
Setup:

libvirt version libvirt-0.8.6-1.el6 above
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    smbios
    Regression

bug:

    No bug found

Actions:

1, Add the following <sysinfo> section into a guest XML defination, and 

<domain type='kvm' id='55'>
  <name>rhel6.1</name>
  <uuid>1849d614-50cf-b09d-dbf7-e5909a55c561</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <sysinfo type='smbios'>
    <bios>
      <entry name='vendor'>LENOVO</entry>
      <entry name='version'>6FET82WW (3.12 )</entry>
    </bios>
    <system>
      <entry name='manufacturer'>Fedora</entry>
      <entry name='product'>Virt-Manager</entry>
      <entry name='version'>0.8.2-3.fc14</entry>
      <entry name='serial'>32dfcb37-5af1-552b-357c-be8c3aa38310</entry>
      <entry name='sku'>1234567890</entry>
      <entry name='family'>Red Hat</entry>
    </system>
  </sysinfo>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>

    <smbios mode='sysinfo'/>

  </os>
  ...

2, Start the guest, run "dmidecode" command to examine appropriate seabios informatin

# dmidecode 2.10
SMBIOS 2.4 present.
10 structures occupying 306 bytes.
Table at 0x3FFFFEC0.

Handle 0x0000, DMI type 0, 24 bytes
BIOS Information
        Vendor: LENOVO
        Version: 6FET82WW (3.12 )
        Release Date: 01/01/2007
        Address: 0xE8000
        Runtime Size: 96 kB
        ROM Size: 64 kB
        Characteristics:
                BIOS characteristics not supported
                Targeted content distribution is supported
        BIOS Revision: 1.0

Handle 0x0100, DMI type 1, 27 bytes
System Information
        Manufacturer: Fedora
        Product Name: Virt-Manager
        Version: 0.8.2-3.fc14
        Serial Number: 32dfcb37-5af1-552b-357c-be8c3aa38310
        UUID: 784F384B-7FA0-29B9-2D9E-A99F0CEC6BF0
        Wake-up Type: Power Switch
        SKU Number: 1234567890
        Family: Red Hat
	
Expected Results:
Notes:
Comments:

		177201 	[Managed save] Domains are automatically shutdown PARALLELLY via initscript when host shuts down 	dyuan 	dyuan 	Auto 		--default-- 	P2 	1860 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Start up two domains (the domains need to be new enough to handle ACPI events, so RHEL-3 will not work here, and minimal installation of rhel6 will not work here too).
   # virsh start <domain1>
   # virsh start <domain2>

   # virsh start <domain3>
   # virsh start <domain4>

Note: make sure the autostart should be disabled for all the domains, or they will start by libvirtd on host bootup.


2. Wait for the domains to fully boot up.

Conduct some operations in the guest, eg # ls.

3. Edit /etc/sysconfig/libvirt-guests, and set

ON_SHUTDOWN=shutdown
PARALLEL_SHUTDOWN=3
SHUTDOWN_TIMEOUT=300
ON_BOOT=ignore

4. Reboot the *host*:
   # reboot
or restart libvirt-guests service

# service libvirt-guests restart

	
Expected Results:

4.0. While the host is shutting down, the "libvirt-guests" script should shutdown
   3 of the active domains parallelly.

# service libvirt-guests stop

Running guests on default URI: rhel58, rhel62, rhel62-1, vr-guest_managedsave
Shutting down guests on default URI...
Starting shutdown on guest: domain1
Starting shutdown on guest: domain2
Starting shutdown on guest: domain3
Shutdown of guest domain1 complete.
Starting shutdown on guest: domain4
Shutdown of guest domain2 complete.
Shutdown of guest domain3 complete.
Shutdown of guest domain4 complete.

  
4.1.When the host comes back up or libvirt-guests start, the domains should not be active.

no such managedsave file /var/lib/libvirt/qemu/save/domain1.save for the domains.

 

Notes:
Comments:

		177203 	[Managed save] Domains that take too long to shutdown PARALLELLY via initscript are killed when host shuts down 	dyuan 	dyuan 	Auto 		--default-- 	P2 	1860 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Start up four domains (the domains need to be new enough to handle ACPI events, so RHEL-3 will not work here, and minimal installation of rhel6 will not work here too).
   # virsh start <domain1>
   # virsh start <domain2>

   # virsh start <domain3>
   # virsh start <domain4>

Note: make sure the autostart should be disabled for all the domains, or they will start by libvirtd on host bootup.


2. Wait for the domains to fully boot up.

Conduct some operations in the guest, eg # ls.

3. Edit /etc/sysconfig/libvirt-guests, and set

ON_SHUTDOWN=shutdown
PARALLEL_SHUTDOWN=3
SHUTDOWN_TIMEOUT=1
ON_BOOT=ignore

4. Reboot the *host*:
   # reboot
or restart libvirt-guests service

# service libvirt-guests  stop

	
Expected Results:

4.0. While the host is shutting down, the "libvirt-guests" script should shutdown
   3 of the active domains parallelly.

# service libvirt-guests stop

Running guests on default URI: domain1, domain2, domain3, domain4
Shutting down guests on default URI...
Starting shutdown on guest: domain1
Starting shutdown on guest: domain2
Starting shutdown on guest: domain3
Timeout expired while shutting down domains

but the UUID for domain4 is recorded in libvirt-guests.(the libvirt-guests will record the UUID when the libvirt-guests  service is stopped)
# cat /var/lib/libvirt/libvirt-guests 
default 83e69755-f692-6413-0c90-2213eddbbbde
6a6839c3-8b51-125e-262f-f2d384367c49 05d9a9f8-3def-491c-e649-87718ea2d98a
3862afa0-3 ff8-80d1-51f2-cff6ec3880a6

  
4.1.When the host comes back up or libvirt-guests start, the domains should not be active.

no such managedsave file /var/lib/libvirt/qemu/save/domain1.save for the domains.

 

Notes:
Comments:

		177475 	[Scalability] qemu-kvm process status monitoring kill -SIGSEGV qemu pid 	vbian 	None 	Manual 		Function 	P1 	1860 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

# for i in {1..256}

do

virsh start guest$i

done

# Kill SIGSEGV $(pidof $one_guest) . Such as 

    a. #ps -aef | grep guest1

    b. # kill -SIGSEGV $guest1_pid

# virsh list --all |grep guest1
	
Expected Results:

make sure libvirtd could recognize guest1 has been shut off
Notes:
Comments:

		177204 	[Managed save] Domains that take too long to shutdown SERIALLY via initscript are killed when host shuts down 	yimwang 	None 	Auto 		--default-- 	P2 	1870 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Start up two domains (the domains need to be new enough to handle ACPI events, so RHEL-3 will not work here, 
minimal installation of rhel6 will not work too).
   # virsh start <domain1>
   # virsh start <domain2>

Note: make sure the autostart is disabled for the 2 domains, or they will start by libvirtd on host bootup.

2. Wait for the domains to fully boot up.

3. Edit /etc/sysconfig/libvirt-guests, and set

ON_SHUTDOWN=shutdown
PARALLEL_SHUTDOWN=0
SHUTDOWN_TIMEOUT=1

ON_BOOT=ignore



6. Reboot the *host*:
   # reboot
or restart libvirt-guests
# service libvirt-guests restart

	
Expected Results:

6. While the host is shutting down, the "libvirt-guests" script should try to shutdown both the active domains,
but should timeout before they successfully shutdown with an error message: "failed to shutdown in time"

# service libvirt-guests stop

Running guests on default URI: domain1, domain2
Shutting down guests on default URI...
Shutting down domain1: failed to shutdown in time
Shutting down domain2: failed to shutdown in time

When the host comes back up, the domains should not be active.

no such managedsave file /var/lib/libvirt/qemu/save/domain1.save for the domains.


Notes:
Comments:

		177476 	[Scalability] Spice WAN performance 	vbian 	None 	Manual 		Function 	P1 	1870 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

TBD
	
Expected Results:
Notes:
Comments:

		177034 	[Input devices] absolute cursor function 	gren 	yoyzhang 	Manual 		Feature 	P3 	1890 	Edit
Setup:

 Windows, latest RHEL5 and latest RHEL6 should all support the tablet device with zero extra config

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    input/output device

bug:

    No bug found

Actions:

On kvm hypervisor

1, Firstly, install a new vm , using virsh command line tool to dump out xml description

2, add a new <input> element with tablet attribute into devices container .like the following :

<domain type='kvm'>
  <name>Inputtest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/Inputtest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:5c:8c:4a'/>
      <source network='default'/>
      <target dev='vnet0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
    </serial>
    <console type='pty' tty='/dev/pts/2'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
    </console>
    <input type='tablet'/>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
  </devices>
</domain>

3, Using "virsh define" to define the new vm xml description ,and boot up the vm
	
Expected Results:

The graphics tablet configured as the default pointer in the guest OS will ensure

that the virtual cursor moves in sync with the local desktop cursor.

if no , that would be some problems
Notes:
Comments:

		177479 	[Scalability] 256 mixed guests boot up 	gsun 	None 	Manual 		--default-- 	P2 	1900 	Edit
Setup:

Setup

Since the slowness coming from cgroups in 6.0 GA s now fixed with 6.1 kernel, no need to disable cgroup at the start. If the slowness problem still exist, do it again under disable cgroup as following:

    If the host is a large SMP boxes (like with 96 cpus), first need to disable cgroup.

             # service cgconfig stop

             # chkconfig cgconfig off

             # service libvirtd restart
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Log on the test machine

    # ssh root@10.66.83.79 -X

2. Mount the extra storage to /mnt dir, could be device connect by optical fiber or nfs storage, optical connected storage will be preferred, size should be larger than 500G, where all guest imgs will be stored.


    # mount /dev/sdb1 /mnt

3. Download the clone_vm scripts to the host

  # mount 10.66.90.113:/vol/xenimage /tmp

    # cp -r /tmp/gsun/clone_vm /root/

    # umount /tmp

4. Prepare 16 different guest images

    RHEL:

    # wget http://fileshare.englab.nay.redhat.com/pub/yoyo_images/RHEL-3.9-32-virtio.qcow2 -P /mnt

    # wget http://fileshare.englab.nay.redhat.com/pub/yoyo_images/RHEL-3.9-64-virtio.qcow2 -P /mnt

    # wget http://fileshare.englab.nay.redhat.com/pub/yoyo_images/RHEL-4.8-32-virtio.qcow2 -P /mnt

    # wget http://fileshare.englab.nay.redhat.com/pub/yoyo_images/RHEL-4.8-64-virtio.qcow2 -P /mnt

    # wget http://fileshare.englab.nay.redhat.com/pub/yoyo_images/RHEL-Server-5.5-32-virtio.qcow2 -P /mnt

    # wget http://fileshare.englab.nay.redhat.com/pub/yoyo_images/RHEL-Server-5.5-64-virtio.qcow2 -P /mnt

    # mount 10.66.90.113:/vol/xenimage /tmp

    # cp /tmp/gsun/RHEL-Server-6-32-virtio.qcow2 /mnt

    # cp /tmp/gsun/RHEL-Server-6-64-virtio.qcow2 /mnt

    Windows:

    # wget http://fileshare.englab.nay.redhat.com/pub/windows-images/kvm-tanjun/winXP-32-virtio.qcow2 -P /mnt

    # wget http://fileshare.englab.nay.redhat.com/pub/windows-images/kvm-tanjun/win2003-32-virtio-el6.qcow2 -P /mnt

    # wget http://fileshare.englab.nay.redhat.com/pub/windows-images/kvm-tanjun/win2003-64-virtio.qcow2 -P /mnt

    # wget http://fileshare.englab.nay.redhat.com/pub/windows-images/kvm-tanjun/win7-32-virtio.qcow2 -P /mnt

 

    # cp /mnt/gsun/win2008-32.qcow2 /mnt

    # cp /mnt/gsun/win2008-64.qcow2 /mnt

    # cp /mnt/gsun/win2008r2.qcow2 /mnt

    # cp /mnt/gsun/win7-64.qcow2 /mnt

    # umount /tmp

5. # cd clone_vm

    # ./clone_vm.sh clone_number_start=1 clone_number_end=16 source_image_file=/mnt/RHEL-3.9-32-virtio.qcow2 guestname_prefix=rhel3u9_32_ guest_template_xml=/root/clone_vm/template.xml image_target_dir=/mnt

    # ./clone_vm.sh clone_number_start=1 clone_number_end=16 source_image_file=/mnt/RHEL-3.9-64-virtio.qcow2 guestname_prefix=rhel3u9_64_ guest_template_xml=/root/clone_vm/template.xml image_target_dir=/mnt

    # ./clone_vm.sh clone_number_start=1 clone_number_end=16 source_image_file=/mnt/RHEL-4.8-32-virtio.qcow2 guestname_prefix=rhel4u8_32_ guest_template_xml=/root/clone_vm/template.xml image_target_dir=/mnt

    # ./clone_vm.sh clone_number_start=1 clone_number_end=16 source_image_file=/mnt/RHEL-4.8-32-virtio.qcow2 guestname_prefix=rhel4u8_64_ guest_template_xml=/root/clone_vm/template.xml image_target_dir=/mnt    

    ...

    From 16 guests images, each define 16 guests, the total will be 256 guests.

    Note: template.xml should be modify according to different disk type.

6. Start the 256 defined guests

    # for i in {1..16};do virsh start <STRING>$i;done

    Start the guests one type a time, at last 128 guests all started.

7. After guest all started, log in the guests to check whether they can get the ip address.

    #ifconfig


    Note: It will take a while for all guests to boot up.


8. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

7. All guest can get ip address.

Run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
16
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

8.

# time virsh list>/tmp/list

real    0m0.627s
user    0m0.050s
sys    0m0.042s

# w
 14:53:44 up 23:23,  3 users,  load average: 15.96, 14.06, 19.85

# virsh list
 Id Name                 State
----------------------------------
 11 win7_32_1            running
 12 win7_32_2            running
 13 win7_32_3            running
 14 win7_32_4            running
 15 win7_32_5            running
 16 win7_32_6            running
 17 win7_32_7            running
 18 win7_32_8            running
 19 rhel6_64_1           running
 20 rhel6_64_2           running
 21 rhel6_64_3           running
 22 rhel6_64_4           running
 23 rhel6_64_5           running
 24 rhel6_64_6           running
 25 rhel6_64_7           running
 26 rhel6_64_8           running
 27 rhel6_32_1           running
 28 rhel6_32_2           running
 29 rhel6_32_3           running
 30 rhel6_32_4           running
 31 rhel6_32_5           running
 32 rhel6_32_6           running
 33 rhel6_32_7           running
 34 rhel6_32_8           running
 35 win2008_64_1         running
 36 win2008_64_2         running
 37 win2008_64_3         running
 38 win2008_64_4         running
 39 win2008_64_5         running
 40 win2008_64_6         running
 41 win2008_64_7         running
 42 win2008_64_8         running
 43 rhel5u5_64_1         running
 44 rhel5u5_64_2         running
 45 rhel5u5_64_3         running
 46 rhel5u5_64_4         running
 47 rhel5u5_64_5         running
 48 rhel5u5_64_6         running
 49 rhel5u5_64_7         running
 50 rhel5u5_64_8         running
 51 win2008_32_1         running
 52 win2008_32_2         running
 53 win2008_32_3         running
 54 win2008_32_4         running
 55 win2008_32_5         running
 56 win2008_32_6         running
 57 win2008_32_7         running
 58 win2008_32_8         running
 59 rhel5u5_32_1         running
 60 rhel5u5_32_2         running
 61 rhel5u5_32_3         running
 62 rhel5u5_32_4         running
 63 rhel5u5_32_5         running
 64 rhel5u5_32_6         running
 65 rhel5u5_32_7         running
 66 rhel5u5_32_8         running
 67 rhel4u8_32_1         running
 68 rhel4u8_32_2         running
 69 rhel4u8_32_3         running
 70 rhel4u8_32_4         running
 71 rhel4u8_32_5         running
 72 rhel4u8_32_6         running
 73 rhel4u8_32_7         running
 74 rhel4u8_32_8         running
 75 rhel4u8_64_1         running
 76 rhel4u8_64_2         running
 77 rhel4u8_64_3         running
 78 rhel4u8_64_4         running
 79 rhel4u8_64_5         running
 80 rhel4u8_64_6         running
 81 rhel4u8_64_7         running
 82 rhel4u8_64_8         running
 83 rhel3u9_64_1         running
 84 rhel3u9_64_2         running
 85 rhel3u9_64_3         running
 86 rhel3u9_64_4         running
 87 rhel3u9_64_5         running
 88 rhel3u9_64_6         running
 89 rhel3u9_64_7         running
 90 rhel3u9_64_8         running
 91 rhel3u9_32_1         running
 92 rhel3u9_32_2         running
 93 rhel3u9_32_3         running
 94 rhel3u9_32_4         running
 95 rhel3u9_32_5         running
 96 rhel3u9_32_6         running
 97 rhel3u9_32_7         running
 98 rhel3u9_32_8         running
 99 winxp_1              running
100 winxp_2              running
101 winxp_3              running
102 winxp_4              running
103 winxp_5              running
104 winxp_6              running
105 winxp_7              running
106 winxp_8              running
107 win2003_64_1         running
108 win2003_64_2         running
109 win2003_64_3         running
110 win2003_64_4         running
111 win2003_64_5         running
112 win2003_64_6         running
113 win2003_64_7         running
114 win2003_64_8         running
115 win2003_32_1         running
116 win2003_32_2         running
117 win2003_32_3         running
118 win2003_32_4         running
119 win2003_32_5         running
120 win2003_32_6         running
121 win2003_32_7         running
122 win2003_32_8         running
123 win2008r2_1          running
124 win2008r2_2          running
125 win2008r2_3          running
126 win2008r2_4          running
127 win2008r2_5          running
128 win2008r2_6          running
129 win2008r2_7          running
130 win2008r2_8          running
131 win7_64_1            running
132 win7_64_2            running
133 win7_64_3            running
134 win7_64_4            running
135 win7_64_5            running
136 win7_64_6            running
137 win7_64_7            running
138 win7_64_8            running
Notes:
Comments:

		177035 	[Input devices] relative cursor function 	gren 	gren 	Manual 		Feature 	P3 	1910 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    input/output device

bug:

    No bug found

Actions:

1 The relative cursor movement is the default virtual pointer in the guest OS, When enabling the framebuffer, the input device is automatically provided.

No <input type='tablet'/> in xml.

2, We just need to move the cursor in the graphical framebuffer to perform some operation.
	
Expected Results:

We can use the type of input device to perform normal operation.

And cursor can be released by using ctrl+alt.
Notes:
Comments:

		177480 	[Scalability] 256 RHEL guests boot up â 128 guests â 50% 	gsun 	None 	Manual 		--default-- 	P2 	1910 	Edit
Setup:

Finished 56359 [Scalability] 256 RHEL guests boot up â 51 guests â 20%
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the 128 defined guests


    # for i in {52..128};do virsh start <STRING>$i;done


2. After guest all started, log in the guests to check whether they can get the ip address.

    # ifconfig

    Note: It will take a while for all guests to boot up.


3. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

For step 2,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

# ifconfig

...

inet addr:192.168.122.79  Bcast:192.168.122.255  Mask:255.255.255.0

...

Or run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

......

For step 3:

# time virsh list > /tmp/list

real 0m0.434s
user 0m0.132s
sys 0m0.031s

The time elapse should be fast, also can run #virsh list to check.

 

# tail -5 /tmp/list

 

125 rheltest125           running
126 rheltest126           running
127 rheltest127           running
128 rheltest128           running

The guest status should be running.

 

#w

load average should be normal, not high.
Notes:
Comments:

		177481 	[Scalability] 256 RHEL guests boot up â 204 guests â 80% 	gsun 	None 	Manual 		--default-- 	P2 	1920 	Edit
Setup:

Finished 56367 [Scalability] 256 RHEL guests boot up â 128 guests â 50%
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the 204 defined guests


    # for i in {129..204};do virsh start <STRING>$i;done


2. After guest all started, log in the guests to check whether they can get the ip address.

    # ifconfig

    Note: It will take a while for all guests to boot up.


3. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

For step 2,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

# ifconfig

...

inet addr:192.168.122.79  Bcast:192.168.122.255  Mask:255.255.255.0

...

Or run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

......

For step 3:

# time virsh list > /tmp/list

real 0m0.834s
user 0m0.132s
sys 0m0.031s

The time elapse should be fast, also can run #virsh list to check.

 

# tail -5 /tmp/list

 

201 rheltest201           running
202 rheltest202           running
203 rheltest203           running
204 rheltest204           running

The guest status should be running.

 

#w

load average should be high, but the machine still responsive well.
Notes:
Comments:

		177482 	[Scalability] 256 RHEL guests boot up â 256 guests â 100% 	gsun 	None 	Manual 		--default-- 	P2 	1930 	Edit
Setup:

Finished 56368 [Scalability] 256 RHEL guests boot up â 204 guests â 80%
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the 256 defined guests


    # for i in {205..255};do virsh start <STRING>$i;done

    Modify the source network as different to exist 255 guests.

# virsh net-list
Name                 State      Autostart
-----------------------------------------
default              active     yes       
default1             active     yes    

    # virsh edit <STRING>256

    <interface type='network'>
      ...
      <source network='default1'/>
      ...
    </interface>

 

    # virsh start <STRING>256


2. After guest all started, log in the guests to check whether they can get the ip address.

    # ifconfig

    Note: It will take a while for all guests to boot up.


3. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

For step 2,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

# ifconfig

...

inet addr:192.168.122.79  Bcast:192.168.122.255  Mask:255.255.255.0

...

Or run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

......

For step 3:

# time virsh list > /tmp/list

real 0m0.934s
user 0m0.132s
sys 0m0.031s

The time elapse should be fast, also can run #virsh list to check.

 

# tail -5 /tmp/list

 

253 rheltest253           running
254 rheltest254           running
255 rheltest255           running
256 rheltest256           running

The guest status should be running.

 

#w

load average should be high, 200%, but the machine still responsible well.
Notes:
Comments:

		177483 	[Scalability] 256 RHEL guests boot up â 51 guests â 20% 	gsun 	None 	Manual 		--default-- 	P2 	1940 	Edit
Setup:

Setup

Since the slowness coming from cgroups in 6.0 GA s now fixed with 6.1 kernel, no need to disable cgroup at the start. If the slowness problem still exist, do it again under disable cgroup as following:

    If the host is a large SMP boxes (like with 96 cpus), first need to disable cgroup.

             # service cgconfig stop

             # chkconfig cgconfig off

             # service libvirtd restart
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Log on the test machine

    # ssh root@10.66.83.79 -X

2. Mount the extra storage to /mnt dir, could be device connect by optical fiber or nfs storage, optical connected storage will be preferred, size should be larger than 500G, where all guest imgs will be stored.


    # mount /dev/sdb1 /mnt

3. Download the clone_vm scripts to the host

    # mount 10.66.90.113:/vol/xenimage /tmp

    # cp -r /tmp/gsun/clone_vm /root/

    # umount /tmp

4. Prepare one RHEL image, basic installation is preferred.

    # cp /var/lib/libvirt/images/guest_name.img /mnt

5. # cd clone_vm

    # ./clone_vm.sh clone_number_start=1 clone_number_end=256 source_image_file=/mnt/guest_name.img guestname_prefix=<STRING> guest_template_xml=/root/clone_vm/rhel6_Server_x86_64_kvm.xml image_target_dir=/mnt


    Note: <STRING> should be replace with the name you desired.

6. Start the 51 defined guests

    # for i in {1..51};do virsh start <STRING>$i;done

7. After guest all started, log in the guests to check whether they can get the ip address.

    #ifconfig


    Note: It will take a while for all guests to boot up.


8. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

For step 7,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

#ifconfig

...

inet addr:192.168.122.79  Bcast:192.168.122.255  Mask:255.255.255.0

...

Or run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

.....

Step 8,

# time virsh list > /tmp/list

real 0m0.434s
user 0m0.132s
sys 0m0.031s

The time elapse should be fast, also can run #virsh list to check.

 

# tail -5 /tmp/list

 

48 rheltest48           running
49 rheltest49           running
50 rheltest50           running
51 rheltest51           running

The guest status should be running.

 

#w

load average should be normal, not high.
Notes:
Comments:

		177484 	[Scalability] 256 windows guests boot up â 128 guests â 50% 	gsun 	None 	Manual 		--default-- 	P2 	1950 	Edit
Setup:

Finished 56371 [Scalability] 256 windows guests boot up â 50 guests â 20%
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the 128 defined guests


    # for i in {52..128};do virsh start <STRING>$i;done


2. After guest all started, log in the guests to check whether they can get the ip address.

    Run cmd, then run ipconifg to show the ip address.

    Note: It will take a while for all guests to boot up.


3. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

For step 2,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

Run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

......

For step 3:

# time virsh list > /tmp/list

real 0m0.634s
user 0m0.132s
sys 0m0.031s

The time elapse should be fast, also can run #virsh list to check.

 

# tail -5 /tmp/list

 

125 wintest125           running
126 wintest126           running
127 wintest127           running
128 wintest128           running

The guest status should be running.

 

#w

load average should be normal, not high.
Notes:
Comments:

		177485 	[Scalability] 256 windows guests boot up â 204 guests â 80% 	gsun 	None 	Manual 		--default-- 	P2 	1960 	Edit
Setup:

Finished 56372 [Scalability] 256 windows guests boot up â 128 guests â 50%
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the 204 defined guests


    # for i in {129..204};do virsh start <STRING>$i;done


2. After guest all started, log in the guests to check whether they can get the ip address.

    Run cmd, then run ipconifg to show the ip address.

    Note: It will take a while for all guests to boot up.


3. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

For step 2,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

Run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

......

For step 3:

# time virsh list > /tmp/list

real 0m0.834s
user 0m0.132s
sys 0m0.031s

The time elapse should be fast, also can run #virsh list to check.

 

# tail -5 /tmp/list

 

201 wintest201           running
202 wintest202           running
203 wintest203           running
204 wintest204           running

The guest status should be running.

 

#w

load average should be high, but the machine still responsive well.
Notes:
Comments:

		177486 	[Scalability] 256 windows guests boot up â 256 guests â 100% 	gsun 	None 	Manual 		--default-- 	P2 	1970 	Edit
Setup:

Finished 56373 [Scalability] 256 windows guests boot up â 204 guests â 80%
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the 256 defined guests


    # for i in {205..256};do virsh start <STRING>$i;done


2. After guest all started, log in the guests to check whether they can get the ip address.

    Run cmd, then run ipconifg to show the ip address.

    Note: It will take a while for all guests to boot up.


3. # time virsh list > /tmp/list

    # tail -5 /tmp/list

    # w
	
Expected Results:

For step 2,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

Run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

......

For step 3:

# time virsh list > /tmp/list

real 0m0.834s
user 0m0.132s
sys 0m0.031s

The time elapse should be fast, also can run #virsh list to check.

 

# tail -5 /tmp/list

 

253 rheltest253           running
254 rheltest254           running
255 wintest255           running
256 wintest256           running

The guest status should be running.

 

#w

load average should be high, over 200%, but the machine still responsive well.
Notes:
Comments:

		177487 	[Scalability] 256 windows guests boot up â 50 guests â 20% 	gsun 	None 	Manual 		--default-- 	P2 	1980 	Edit
Setup:

Setup

Since the slowness coming from cgroups in 6.0 GA s now fixed with 6.1 kernel, no need to disable cgroup at the start. If the slowness problem still exist, do it again under disable cgroup as following:

    If the host is a large SMP boxes (like with 96 cpus), first need to disable cgroup.

             # service cgconfig stop

             # chkconfig cgconfig off

             # service libvirtd restart

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

1. Log on the test machine


    # ssh root@10.66.83.79 -X


2. Mount the extra storage to /mnt dir, could be device connect by optical fiber or nfs storage, optical connected storage will be preferred, size should be larger than 500G, where all guest imgs will be stored.


    # mount /dev/sdb1 /mnt


3. Download the clone_vm scripts to the host

    # mount 10.66.90.113:/vol/xenimage /tmp

    # cp -r /tmp/gsun/clone_vm /root/

    # umount /tmp


4. Prepare one windows image, core version is preferred.


    # cp /var/lib/libvirt/images/guest_name.img /mnt


5. # cd clone_vm


    # sh clone_vm.sh clone_number=256 source_image_file=/mnt/guest_name.img target_guestname=<STRING>

    # ./clone_vm.sh clone_number_start=1 clone_number_end=256 source_image_file=/mnt/guest_name.img guestname_prefix=<STRING> guest_template_xml=/root/clone_vm/win.xml image_target_dir=/mnt


    Note: <STRING> should be replace with the name you desired.


6. Start the 51 defined guests


    # for i in {1..51};do virsh start <STRING>$i;done


7. After guest all started, log in the guests to check whether they can get the ip address.


    Run cmd, then run ipconifg to show the ip address.


    Note: It will take a while for all guests to boot up.


8. # time virsh list > /tmp/list


    # tail -5 /tmp/list


    # w
	
Expected Results:

For step 7,

All guests can get ip address. Also can use nmap to scan the guest's net segment which will take time.

Run on host:

# ./clone_vm.sh --prompt
Input clone_number_start: <INT>
1
Input clone_number_end: <INT>
8
Input guestname prefix: <STRING>
win7_64_
1: define guest; 2: copy guest image file; 3: start guest; 4: check guest IP; 5: destroy and undefine guest
4

......

 

Step 8,

# time virsh list > /tmp/list

real 0m0.434s
user 0m0.132s
sys 0m0.031s

The time elapse should be fast, also can run #virsh list to check.

 

# tail -5 /tmp/list

 

48 wintest48           running
49 wintest49           running
50 wintest50           running
51 wintest51           running

The guest status should be running.

 

#w

load average should be normal, not high.
Notes:
Comments:

		177488 	[Scalability] 35 guests migration 	gsun 	None 	Manual 		--default-- 	P2 	1990 	Edit
Setup:

  1.   If the host is a large SMP boxes (like with 48 cpus):

        Since the slowness coming from cgroups in 6.0 GA s now fixed with 6.1 kernel, no need to disable cgroup at the start. If the slowness problem still exist, do it again under disable cgroup as following:

             # service cgconfig stop

             # chkconfig cgconfig off

             # service libvirtd restart

   2. If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

   3. dispatch ssh publick key of source host to target host.  so that we don't need input the passphrase.

       - Creating your local public key pair (by running "ssh-keygen -t rsa ", just give the default answer to the request questions.)
       - Copying the public key to a remote host by "$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@somehost "

      Do this on both source and target host, make sure no password need from both side.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0

bug:

    No bug found

Actions:

On KVM hypervisor

1. Select two machine with the same CPU types

2. Log on both test machines in two console

    # ssh root@10.66.83.79 -X

    # ssh root@10.66.83.80 -X

    Note: Assume 10.66.83.79 as host1 and 10.66.83.80 as host2.
             The host ip address should be modified to the box be used.

3. Mount the share storage to both box. It could be nfs or iscsi storage, size should be larger than 500G, where all guest imgs will be stored.
   On both box:
     For nfs storage:
   

      # mount -t nfs ${nfs_address} /mnt -o vers=3

      Define the pool test using xml:

      <pool type="dir">
        <name>test</name>
        <target>
          <path>/mnt</path>
        </target>
      </pool>
      # virsh pool-define test.xml

      # virsh pool-start test

      # virsh pool-autostart test  
       
     For iscsi storage:
   
   Define a pool from an XML file, check if the pool was defined.
   Create an XML file, save as test-iscsi.xml:
     <pool type='iscsi'>
       <name>test-iscsi</name>
       <source>
         <host name="10.66.90.115"/>
         <device path="iqn.1992-08.com.netapp:sn.135053389"/>
       </source>
       <target>
         <path>/dev/disk/by-path</path>
       </target>
     </pool>
   
   # virsh pool-define test-iscsi.xml
   Pool test-iscsi defined from test-iscsi.xml

   Start the inactive pool, check if the pool state was active.
   # virsh pool-start test-iscsi
   Pool test-iscsi started
   # virsh pool-autostart test-iscsi
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   default        active     yes
   test-iscsi           active     yes

4. Download the clone_vm scripts to the host1

    # mount 10.66.90.113:/vol/xenimage /tmp

    # cp -r /tmp/gsun/clone_vm /root/

    # cp /tmp/gsun/multi-migrate.py /root/

    # umount /tmp

5. Prepare one RHEL image, basic installation is preferred.

    # cp /var/lib/libvirt/images/guest_name.img /mnt

6. # cd clone_vm

    # ./clone_vm.sh clone_number_start=1 clone_number_end=35 source_image_file=/mnt/guest_name.img guestname_prefix=<STRING> guest_template_xml=/root/clone_vm/rhel6_Server_x86_64_kvm.xml image_target_dir=/mnt

    Note: <STRING> should be replace with the name you desired.

7. Start the 35 defined guests

    # for i in {1..35};do virsh start <STRING>$i;done

8. After guest all started, log in the guests to check whether they can get the ip address.

    #ifconfig


    Note: It will take a while for all guests to boot up.


9. Start migration for multiple guests from host1 to host2 at the same time.
   
   # cd /root

   # python multi-migrate.py --guestname_prefix=<STRING> --start_number=1 --end_number=35 --remote_host_address=10.66.83.80
	
Expected Results:

9. Check wether guests can be migrated at the same time

    After all guest migrated to host2, log in each guest and check the status of guest.
Notes:
Comments:

		177497 	[Scalability] bidirectional migrate multiple guests simultaneously 	gsun 	None 	Manual 		Stress 	P2 	2000 	Edit
Setup:
Pleas make sure the files is here
http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/vbian/Scalability/clone_vm.tar
http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/vbian/Scalability/multi-migrate.py

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0
    migration

bug:

    No bug found

Actions:

On KVM hypervisor

1. Select two machine with the same CPU types

2. Log on both test machines in two console

    # ssh root@10.66.83.79 -X

    # ssh root@10.66.83.80 -X

    Note: Assume 10.66.83.79 as host1 and 10.66.83.80 as host2.
             The host ip address should be modified to the box be used.

3. Mount the share storage to both box. It could be nfs or iscsi storage, size should be larger than 500G, where all guest imgs will be stored.
   On both box:
     For nfs storage:
   

      # mount -t nfs ${nfs_address} /mnt -o vers=3

      Define the pool test using xml:

      <pool type="dir">
        <name>test</name>
        <target>
          <path>/mnt</path>
        </target>
      </pool>
      # virsh pool-define test.xml

      # virsh pool-start test

      # virsh pool-autostart test  


     For iscsi storage:
   
   Define a pool from an XML file, check if the pool was defined.
   Create an XML file, save as test-iscsi.xml:
     <pool type='iscsi'>
       <name>test-iscsi</name>
       <source>
         <host name="10.66.90.115"/>
         <device path="iqn.1992-08.com.netapp:sn.135053389"/>
       </source>
       <target>
         <path>/dev/disk/by-path</path>
       </target>
     </pool>
   
   # virsh pool-define test-iscsi.xml
   Pool test-iscsi defined from test-iscsi.xml

   Start the inactive pool, check if the pool state was active.
   # virsh pool-start test-iscsi
   Pool test-iscsi started
   # virsh pool-autostart test-iscsi
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   default        active     yes
   test-iscsi           active     yes

    Note : If there is a :error: cannot open volume '/mnt/fcoemon.dcbd.1673': No such device or address.

               Please just only mount -t nfs ${nfs_address} /mnt -o vers=3 , and skip others in 3.

4. Download the clone_vm scripts to the host1
    #wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/vbian/Scalability/clone_vm.tar  /tmp
   
    #wget http://fileshare.englab.nay.redhat.com/pub/section3/libvirtmanual/Share/vbian/Scalability/multi-migrate.py    /tmp

    # cp  /tmp/clone_vm.tar  /root/

    # cp /tmp/multi-migrate.py /root/

    # cd /root

    # tar -xvf  clone_vm.tar


5. Prepare one RHEL or Windows image, basic installation is preferred.

    # cp /var/lib/libvirt/images/guest_name.img /mnt

6. # cd clone_vm

    # ./clone_vm.sh clone_number_start=1 clone_number_end=32 source_image_file=/mnt/guest_name.img guestname_prefix=<STRING> guest_template_xml=/root/clone_vm/rhel6_Server_x86_64_kvm.xml image_target_dir=/mnt

    Note: <STRING> should be replace with the name you desired.

               For instance , if guestname_prefix=test-  , the guest will be defined as test-1....test-32

7. Start the 32 defined guests

    # for i in {1..32};do virsh start <STRING>$i;done

8. Repeat 4 - 7 steps on host2

    But change "clone_number_start=1 clone_number_end=32" to "clone_number_start=33 clone_number_end=64" in step 6.

8. After guest all started, check whether guests they can get the ip address.


    Note: It will take a while for all guests to boot up.


9. Start migration for multiple guests between host1 and host2 bidrectional at the same time.

   On host1:

   # cd /root

   # python multi-migrate.py --guestname_prefix=<STRING> --start_number=1 --end_number=32 --remote_host_address=10.66.83.80

   On host2:

   # cd /root

   # python multi-migrate.py --guestname_prefix=<STRING> --start_number=33 --end_number=64 --remote_host_address=10.66.83.79

    Note : If your mechine's password is  not redhat , pleas change it in multi-migrate.py 

                line:78  ret = remote_exec_pexpect('redhat', command).
               Change 'redhat' to your password.
	
Expected Results:

9. Check wether guests can be migrated at the same time

    After all guest migrated, log in each guest and check the status of guest.
Notes:
Comments:

		177498 	[Scalability] Cgroup operation -- BZ# 623712 	vbian 	None 	Manual 		Function 	P1 	2010 	Edit
Setup:

make sure cgroup is enabled

# service cgconfig start
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. Create 150  guest XML files
2. for i in `seq 1 150` ; do virsh define $i.xml ; done
3. time virsh list --all > /dev/null
4. cd /cgroup/cpu/libvirt/qemu
5. for i in `seq 1 150` ; do mkdir test$i ; done
6. time virsh list --all > /dev/null

	
Expected Results:

No slowdown as cpu cgroups are created

3. Before cgroups created:
# time virsh list --all > /dev/null

real    0m1.415s
user    0m0.057s
sys    0m0.061s


6. After cgroups created:

# time virsh list --all > /dev/null

real    0m1.459s
user    0m0.057s
sys    0m0.056s

Notes:
Comments:

		177510 	[Scalability] Libvirt event handler investigation - scenario 1 	vbian 	None 	Manual 		Function 	P1 	2020 	Edit
Setup:

# cat template.sh

#!/bin/sh

Save_dir=/test/save-dir
Dump_dir=/test/dump-dir
XML_dir=/test/guest-xml



echo "this is the lifecycle oeprations for guest" $1

virsh dumpxml $1 > $XML_dir/$1.xml
for i in {1..10}
do
virsh start $1
virsh destroy $1
done

virsh start $1
for i in {1..50}
do
virsh suspend $1
virsh resume $1
done

for i in {1..50}
do
virsh save $1 $Save_dir/$1.save
virsh restore $Save_dir/$1.save
done

# dump [--live] [--crash] <domain> <file>
for i in {1..50}
do
virsh dump --crash $1 $Dump_dir/$1.dump
virsh start $1
done

sleep 30

virsh destroy $1

for i in {1..50}
do
virsh undefine $1
virsh define $XML_dir/$1.xml
done

 


# cat multi_life.sh

Guests=("win2k3-sp2" "win2k8-64-sp2" "win2k8-R2" "win2k8_32" "win2k8_64" "win7-32" "win7-64" "XP-sp3" "RHEL62-64" "RHEL62-32" "RHEL57-64" "RHEL57-32" "RHEL49-64" "RHEL49-32" "RHEL39-32")

for guest in "${Guests[@]}"
do
sh ./template.sh $guest &
done
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

1. open two terminal

[terminal 1]

# python ï»¿/usr/share/doc/libvirt-python-0.9.3/events-python/event-test.py

[terminal 2]

# sh multi_life.sh
	
Expected Results:

Make sure all of the events are caught , and there is no error msg in the terminal 1 .
Notes:
Comments:

		177511 	[scalability] Libvirt event handler investigation - scenario 2 	gsun 	None 	Manual 		Function 	P3 	2030 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

Do local and remote multiple guests lifecycle events combination loop catching about 1000 times

( i/o error , RTC events , watchdog , graphical framebuffer)

refer to

124255 [Libvirt domain event handler] Run 'hwclock --systohc'(on the domain) to change the domain clock

124256 [Libvirt domain event handler] Running watchdog action such as pasue action

124259 [Libvirt domain event handler]VNC & SPICE connecting & disconnecting

For i/o error, prepare 1 disk and attach to 2 running guests, then write date on both guests, may generate io error
	
Expected Results:

Check that all event can be get in libvirt-python
Notes:
Comments:

		177451 	[Remote access] Connect to the hypervisor on host using TCP connection with SASL via ipv6 	yoyzhang 	None 	Manual 		Feature 	P2 	2040 	Edit
Setup:

ipv6 configuration on client and server

1)stop ip6tables service  in two hosts

2)two host ifcfg-eth* config file need add these lines 

IPV6INIT="yes"
IPV6_AUTOCONF="yes"

3)sysctl -w ï»¿net.ipv6.conf.eth0.forwarding=1 

 

 

a) on client host

I configure ipv6 global addr

ifconfig eth0 inet6 add 3ffe::101/64

II configure ipv6 hostname

add the following line into /etc/hosts

3ffe::102    dhcpv6.93.14.bne.redhat.com    bne.redhat.com

b) on server host

III configure ipv6 global addr

ifconfig eth0 inet6 add 3ffe::102/64

 

make sure two hosts can ping6 each other.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access

bug:

    No bug found

Actions:

On server host  (3ffe::102)
1. Edit /etc/sysconfig/libvirtd
       LIBVIRTD_ARGS="--listen"

2. Edit /etc/libvirt/libvirtd.conf
       listen_tls = 0
       listen_tcp=1
       auth_tcp="sasl"

3. Add a user named "fred", and setting his password as "redhat"
   # saslpasswd2 -a libvirt fred
   Password: xxxxxx
   Again (for verification): xxxxxx

4. # service libvirtd restart

On client host  (3ffe::101)

# virsh -c qemu+tcp://[3ffe::102]/system

or

# virsh -c qemu+tcp://dhcpv6.93.14.bne.redhat.com/system
	
Expected Results:

Connect sucessfully, output # virsh -c qemu+tcp://[3ffe::101]/system
Please enter your authentication name: fred
Please enter your password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh
Notes:
Comments:

		177452 	[Remote access] Connect to the hypervisor running on host over an SSH connection via ipv4 	yoyzhang 	None 	Auto 		Feature 	P2 	2050 	Edit
Setup:

- 2 host installed with kvm kernel
- libvirt service is running
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

On host A, run command
# virsh -c qemu+ssh://10.66.70.159/system

(10.66.70.159 is host B's ip)
	
Expected Results:

Output:

The authenticity of host '10.66.70.159 (10.66.70.159)' can't be established.
RSA key fingerprint is 24:fb:07:de:0d:b3:4c:6d:a3:e4:99:d2:a3:e3:a3:18.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '10.66.70.159' (RSA) to the list of known hosts.
root@10.66.70.159's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #
Notes:
Comments:

		177453 	[Remote access] Connect to the hypervisor running on host over an SSH connection via ipv6 - bug 785164 	yoyzhang 	None 	Manual 		Feature 	P2 	2060 	Edit
Setup:

1, Make sure you have at least 2 host running KVM
2, Make sure the libvirtd is running.
3, Make sure the iptables let the ssh though

4, ipv6 configuration on client and server

a) on client host

I configure ipv6 global addr

ifconfig eth0 inet6 add 3ffe::101/64

II configure ipv6 hostname

add the following line into /etc/hosts

3ffe::102    dhcpv6.93.14.bne.redhat.com    bne.redhat.com

b) on server host

III configure ipv6 global addr

ifconfig eth0 inet6 add 3ffe::102/64
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access
    regression

bug:

    No bug found

Actions:

on client:


# virsh -c qemu+ssh://dhcpv6.93.14.bne.redhat.com/system

OR # virsh -c qemu+ssh://[3ffe::102]/system
	
Expected Results:

After password auth, could be connected to server successfully
Notes:
Comments:

		177454 	[Remote access] Connect to the hypervisor running on host using TLS with SASL via ipv6 	xhu 	None 	Manual 		Feature 	P2 	2070 	Edit
Setup:

- 2 host installed with kvm kernel

- libvirtd service is running on both system

-require package "gnutls-utils" installed

-  ipv6 configuration on client and server

a) on client host

I configure ipv6 global addr

ifconfig eth0 inet6 add 3ffe::101/64

II configure ipv6 hostname

add the following line into /etc/hosts

3ffe::102    dhcpv6.93.14.bne.redhat.com    dhcpv6.93.14

b) on server host

III configure ipv6 global addr

ifconfig eth0 inet6 add 3ffe::102/64
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access
    RHEL6.0

bug:

    No bug found

Actions:

On server (3ffe::102)

1. Set up a Certificate Authority (CA)

1.1 # certtool --generate-privkey > cakey.pem

1.2 self-sign cakey.pem by creating a file with the signature details called ca.info containing:

cn = 3ffe::102
ca
cert_signing_key

1.3 # certtool --generate-self-signed --load-privkey cakey.pem --template ca.info --outfile cacert.pem

2. Create server certificates

2.1 certtool --generate-privkey > serverkey.pem

2.2 sign that key with the CA's private key by first creating a template file called server.info
    
organization = Red Hat
cn = 3ffe::102
tls_www_server
encryption_key
signing_key

2.3 # certtool --generate-certificate --load-privkey serverkey.pem --load-ca-certificate cacert.pem \
--load-ca-privkey cakey.pem --template server.info --outfile servercert.pem

3. Copy CA key and server key to correct directory

3.1 # cp cakey.pem cacert.pem /etc/pki/CA

3.2 # mkdir -p /etc/pki/libvirt/private

3.3 # cp serverkey.pem /etc/pki/libvirt/private

3.4 # cp servercert.pem /etc/pki/libvirt


3.5 # cp serverkey.pem /etc/pki/libvirt/private/


4. Copy CA key to client(3ffe::101) into correct directory

4.1 # scp cakey.pem cacert.pem root@[3ffe::101]:/etc/pki/CA

5. Add a user named "fred", and setting his password as "redhat"
   # saslpasswd2 -a libvirt fred
   Password: xxxxxx
   Again (for verification): xxxxxx


6. Turn on libvird monitor listening in /etc/sysconfig/libvirtd
  -- uncomment LIBVIRTD_ARGS="--listen"

7. Edit /etc/libvirt/libvirtd.conf
  -- enbale listen_tls = 1

  -- enable auth_tls = "sasl"


8. # service libvirtd restart

9. # service ip6tables stop


On client (3ffe:101)

10.  Create client certificates

10.1 # certtool --generate-privkey > clientkey.pem

10.2 Act as CA and sign the certificate.  Create client.info containing:

country = GB
state = London
locality = London
organization = Red Hat
cn = 3ffe::101
tls_www_client
encryption_key
signing_key

10.3 # certtool --generate-certificate  --load-privkey clientkey.pem --load-ca-certificate /etc/pki/CA/cacert.pem \
--load-ca-privkey /etc/pki/CA/cakey.pem --template client.info --outfile clientcert.pem

11. Copy client key to correct directory

11.1 # mkdir -p /etc/pki/libvirt/private

11.2 # cp clientkey.pem /etc/pki/libvirt/private

11.3 # cp clientcert.pem /etc/pki/libvirt/

12. Conect to server hypervisor using fred

# virsh -c qemu+tls://[3ffe::102]/system

13. disable user "fred"
   # saslpasswd2 -a libvirt -d fred

14. try to connect to libvirtd using "fred"

# virsh -c qemu+tls://[3ffe::101]/system
	
Expected Results:

12. Connect sucessfully, output

# virsh -c qemu+tls://[3ffe::101]/system
Please enter your authentication name: fred
Please enter your password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #

 

14 Fail to connect to tls server,output

# virsh -c qemu+tls://[3ffe::101]/system

Please enter your authentication name: fred
Please enter your password:
error: authentication failed
error: failed to connect to the hypervisor
Notes:
Comments:

		177455 	[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4 	yoyzhang 	yoyzhang 	Auto 		Feature 	P1 	2080 	Edit
Setup:

1.

- 2 host installed with kvm kernel

- libvirtd service is running on both system

-require package "gnutls-utils" installed

2.

Make sure 2 hosts UTC time was same.

#date -u

if not please set it.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    remote access
    virsh-rail

bug:

    No bug found

Actions:

On server (dhcp-66.70.159.nay.redhat.com)

1. Set up a Certificate Authority (CA)

1.1 # certtool --generate-privkey > cakey.pem

1.2 self-sign cakey.pem by creating a file with the signature details called ca.info containing:

cn = dhcp-66.70.159.nay.redhat.com
ca
cert_signing_key

1.3 # certtool --generate-self-signed --load-privkey cakey.pem --template ca.info --outfile cacert.pem

2. Create server certificates

2.1 certtool --generate-privkey > serverkey.pem

2.2 sign that key with the CA's private key by first creating a template file called server.info
    
organization = Red Hat
cn = dhcp-66.70.159.nay.redhat.com
tls_www_server
encryption_key
signing_key

2.3 # certtool --generate-certificate --load-privkey serverkey.pem --load-ca-certificate cacert.pem \
--load-ca-privkey cakey.pem --template server.info --outfile servercert.pem

3. Copy CA key and server key to correct directory

3.1 # cp cakey.pem cacert.pem /etc/pki/CA

3.2 # mkdir -p /etc/pki/libvirt/private

3.3 # cp serverkey.pem /etc/pki/libvirt/private

3.4 # cp servercert.pem /etc/pki/libvirt

4. Copy CA key to client(10.66.70.162) into correct directory

4.1 # scp cakey.pem cacert.pem root@10.66.70.162:/etc/pki/CA

5. Turn on libvird monitor listening in /etc/sysconfig/libvirtd
  -- uncomment LIBVIRTD_ARGS="--listen"

6. Edit /etc/libvirt/libvirtd.conf
  -- enbale listen_tls = 1

auth_tls = "none"

7. # service libvirtd restart

8. # service iptables stop


On client (dhcp-66.70.162.nay.redhat.com)

9.  Create client certificates

9.1 # certtool --generate-privkey > clientkey.pem

9.2 Act as CA and sign the certificate.  Create client.info containing:

country = GB
state = London
locality = London
organization = Red Hat
cn = dhcp-66.70.162.nay.redhat.com
tls_www_client
encryption_key
signing_key

9.3 # certtool --generate-certificate  --load-privkey clientkey.pem --load-ca-certificate /etc/pki/CA/cacert.pem \
--load-ca-privkey /etc/pki/CA/cakey.pem --template client.info --outfile clientcert.pem

10. Copy client key to correct directory

10.1 # mkdir -p /etc/pki/libvirt/private

10.2 # cp clientkey.pem /etc/pki/libvirt/private

10.3 # cp clientcert.pem /etc/pki/libvirt/

11. Conect to server hypervisor

# virsh -c qemu+tls://dhcp-66-70-159.nay.redhat.com/system
	
Expected Results:

11. Connect sucessfully, output

The authenticity of host 'dhcp-66-70-159.nay.redhat.com (10.66.70.159)' can't be established.
RSA key fingerprint is 24:fb:07:de:0d:b3:4c:6d:a3:e4:99:d2:a3:e3:a3:18.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'dhcp-66-70-159.nay.redhat.com' (RSA) to the list of known hosts.
root@dhcp-66-70-159.nay.redhat.com's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #
Notes:
Comments:

		177456 	[remote access] Connect to the hypervisor using TLS with SASL BZ#726568 	whuang 	None 	Manual 		Regression 	P1 	2090 	Edit
Setup:

1.

- 2 host installed with kvm kernel

- libvirtd service is running on both system

-require package "gnutls-utils" installed

2.

Make sure 2 hosts UTC time was same.

#date -U

if not please set it.

3.

Add server hostname and ip to client /etc/hosts

10.66.70.159 dhcp-66-70-159.nay.redhat.com
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access
    Regression

bug:

    No bug found

Actions:

On server (dhcp-66.70.159.nay.redhat.com)

1. Set up a Certificate Authority (CA)

1.1 # certtool --generate-privkey > cakey.pem

1.2 self-sign cakey.pem by creating a file with the signature details called ca.info containing:

cn = dhcp-66.70.159.nay.redhat.com
ca
cert_signing_key

1.3 # certtool --generate-self-signed --load-privkey cakey.pem --template ca.info --outfile cacert.pem

2. Create server certificates

2.1 certtool --generate-privkey > serverkey.pem

2.2 sign that key with the CA's private key by first creating a template file called server.info
    
organization = Red Hat
cn = dhcp-66.70.159.nay.redhat.com
tls_www_server
encryption_key
signing_key

2.3 # certtool --generate-certificate --load-privkey serverkey.pem --load-ca-certificate cacert.pem \
--load-ca-privkey cakey.pem --template server.info --outfile servercert.pem

3. Copy CA key and server key to correct directory

3.1 # cp cakey.pem cacert.pem /etc/pki/CA

3.2 # mkdir -p /etc/pki/libvirt/private

3.3 # cp serverkey.pem /etc/pki/libvirt/private

3.4 # cp servercert.pem /etc/pki/libvirt

4. Copy CA key to client(10.66.70.162) into correct directory

4.1 # scp cakey.pem cacert.pem root@10.66.70.162:/etc/pki/CA

5. Turn on libvird monitor listening in /etc/sysconfig/libvirtd
  -- uncomment LIBVIRTD_ARGS="--listen"

6. Edit /etc/libvirt/libvirtd.conf
  -- enbale listen_tls = 1

auth_tls = "sasl"

#echo redhat |saslpasswd2 -p -a libvirt redhat

7. # service libvirtd restart

8. # service iptables stop


On client (dhcp-66.70.162.nay.redhat.com)

9.  Create client certificates

9.1 # certtool --generate-privkey > clientkey.pem

9.2 Act as CA and sign the certificate.  Create client.info containing:

country = GB
state = London
locality = London
organization = Red Hat
cn = dhcp-66.70.162.nay.redhat.com
tls_www_client
encryption_key
signing_key

9.3 # certtool --generate-certificate  --load-privkey clientkey.pem --load-ca-certificate /etc/pki/CA/cacert.pem \
--load-ca-privkey /etc/pki/CA/cakey.pem --template client.info --outfile clientcert.pem

10. Copy client key to correct directory

10.1 # mkdir -p /etc/pki/libvirt/private

10.2 # cp clientkey.pem /etc/pki/libvirt/private

10.3 # cp clientcert.pem /etc/pki/libvirt/

11. Conect to server hypervisor

# virsh -c qemu+tls://dhcp-66-70-159.nay.redhat.com/system

input ID: redhat  passwd:redhat 
	
Expected Results:

Can access successfully 
Notes:
Comments:

		177457 	[Remote access] Connect to the local 'default' hypervisor running on host 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	2100 	Edit
Setup:

- 1 host installed with kvm kernel
- libvirt service is running
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    remote access
    virsh-rail

bug:

    No bug found

Actions:

1. Connect to local hypervisor via ssh

# virsh -c qemu+ssh:///system

2. Connect to local hypervisor via unix socket

# virsh -c qemu+unix:///system

OR # virsh -c qemu:///system

3. Connect to local hypervisor via plain tcp (refer to test case [Remote access] Connect to the hypervisor on host using an unsecured TCP connection)

 # virsh -c qemu+tcp:///system

4. Connect to local hypervisor via plain tls (refer to test case [Remote access] Connect to the hypervisor running on host using TLS with SASL)

  # virsh -c qemu+tls://localhost/system(OR if you use the hostname pls use your hostname)
	
Expected Results:

1. Output

The authenticity of host 'localhost (127.0.0.1)' can't be established.
RSA key fingerprint is 9c:34:f9:a8:c2:04:6d:67:81:b0:00:7d:7d:87:db:40.
Are you sure you want to continue connecting (yes/no)? yes
root@localhost's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
 17 win2008              running
 18 rhel6                running

virsh #

2. Output

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #

3. Output

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

4. Output

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh # quit
Notes:
Comments:

		177458 	[remote access] Customizable x509 certificate paths for client - tls 	yoyzhang 	None 	Manual 		Feature 	P1 	2110 	Edit
Setup:

- 2 host installed with kvm kernel

- libvirtd service is running on both system

-require package "gnutls-utils" installed

-add two PC's hostname in both /etc/hosts file
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access

bug:

    No bug found

Actions:

On server (10.66.93.59)

1. Set up a Certificate Authority (CA)

 1) # certtool --generate-privkey > cakey.pem

 2) self-sign cakey.pem by creating a file with the signature details called
ca.info containing:

cn = 10.66.93.59
ca
cert_signing_key

 3) # certtool --generate-self-signed --load-privkey cakey.pem --template
ca.info --outfile cacert.pem

2. Create server certificates

 1) certtool --generate-privkey > serverkey.pem

 2) sign that key with the CA's private key by first creating a template file
called server.info

organization = Red Hat
cn = 10.66.93.59
tls_www_server
encryption_key
signing_key

 3) # certtool --generate-certificate --load-privkey serverkey.pem
--load-ca-certificate cacert.pem --load-ca-privkey cakey.pem --template
server.info --outfile servercert.pem

3. Copy CA key and server key to correct directory

 1) # cp cakey.pem cacert.pem /mnt/ca/

 2) # mkdir -p /mnt/ca/libvirt/private

 3) # cp serverkey.pem /mnt/ca/libvirt/private

 4) # cp servercert.pem /mnt/ca/libvirt

4. Copy CA key to client(10.66.65.85) into correct directory

  # scp cakey.pem cacert.pem root@10.66.65.85:/mnt/pki/client

5. Turn on libvird monitor listening in /etc/sysconfig/libvirtd
  -- uncomment LIBVIRTD_ARGS="--listen"

6. Edit /etc/libvirt/libvirtd.conf
  -- enbale listen_tls = 1   and specify the cert file  

vim /etc/libvirt/libvirtd.conf

# This is enabled by default, uncomment this to disable it
listen_tls = 1

 
#
# TLS x509 certificate configuration
#


# Override the default server key file path
#
key_file = "/mnt/ca/libvirt/private/serverkey.pem"

# Override the default server certificate file path
#
cert_file = "/mnt/ca/libvirt/servercert.pem"

# Override the default CA certificate path

ca_file = "/mnt/ca/cacert.pem"

# Specify a certificate revocation list.

# Defaults to not using a CRL, uncomment to enable it
crl_file = "/mnt/ca/crl.pem"

 

 

 


7. # service libvirtd restart

8. # service iptables stop


On client (10.66.65.85), make sure the current directory is /mnt/pki/client

9.  Create client certificates

 1) # certtool --generate-privkey > clientkey.pem

 2) Act as CA and sign the certificate.  Create client.info containing:

country = GB
state = London
locality = London
organization = Red Hat
cn = 10.66.65.85
tls_www_client
encryption_key
signing_key

 3) # certtool --generate-certificate  --load-privkey clientkey.pem --load-ca-certificate /mnt/pki/client/cacert.pem 
--load-ca-privkey /mnt/pki/client/cakey.pem  --template client.info --outfile clientcert.pem
 

 

Note: if you are in the ï»¿/mnt/pki/client/ , you can do not do  4) and 5) 

4)#cp clientcert.pem /mnt/pki/client/
 5)#cp clientkey.pem /mnt/pki/client/

10. Conect to server hypervisor

# virsh -c qemu+tls://10.66.93.59/system?pkipath=/mnt/pki/client

11. If no cacert.pem in /mnt/pki/client, an error will output:

# virsh -c qemu+tls://10.66.93.59/system?pkipath=/mnt/pki/client

	
Expected Results:

10.

# virsh -c qemu+tls://10.66.93.59/system?pkipath=/mnt/pki/client
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # 

11. 

# virsh -c qemu+tls://10.66.93.59/system?pkipath=/mnt/pki/client
error: Cannot access CA certificate '/mnt/pki/client/cacert.pem': No such file
or directory
error: failed to connect to the hypervisor

Notes:
Comments:

		177459 	[Remote access] Libvirtd output proper error for TLS failure 	yoyzhang 	None 	Auto 		Negative test 	P2 	2120 	Edit
Setup:

Make sure package "gnutls-utils" is installed.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access
    virsh-rail

bug:

    No bug found

Actions:

1. Create a private key for CA 
# certtool --generate-privkey > cakey.pem

2. self-sign it by creating a file with the signature details called ca.info
containing:
cn=Redhat QA,CN=10.66.70.128
ca
cert_signing_key

# certtool --generate-self-signed --load-privkey cakey.pem --template ca.info
--outfile cacert.pem

3. copy certificate to right position 
# cp cacert.pem cakey.pem /etc/pki/CA

4. issuing server certificates
1> Make a private key for the server: # certtool --generate-privkey >
serverkey.pem

2> and sign that key with the CA's private key by first creating a template
file called server.info:

organization = redhat-china
cn = 10.66.70.128
tls_www_server
encryption_key
signing_key

3> sign

# certtool --generate-certificate --load-privkey serverkey.pem \

   --load-ca-certificate cacert.pem --load-ca-privkey cakey.pem \

   --template server.info --outfile servercert.pem

5. copy server certificates to right position 
# mkdir -p /etc/pki/libvirt/private
# cp serverkey.pem /etc/pki/libvirt/private 
# cp servercert.pem /etc/pki/libvirt

6.scp cacert.pem and cakey.pem to client /etc/pki/CA 

#scp ï»¿cacert.pem cakey.pem  root@client:/etc/pki/CA

 


6. issuing client certificates
1>. Make a private key 
# certtool --generate-privkey > clientkey.pem

2> Create client.info and sign the certificate

client.info:
country = China
state = Beijign
locality = Beijing
organization = Red Hat
cn = 10.66.65.132
tls_www_client
encryption_key
signing_key

3> sign

# certtool --generate-certificate --load-privkey clientkey.pem \

   --load-ca-certificate /etc/pki/CA/cacert.pem --load-ca-privkey /etc/pki/CA/cakey.pem \

   --template client.info --outfile clientcert.pem

7. copy client certificates to right position 

#mkdir -p /etc/pki/libvirt/private

# cp clientkey.pem /etc/pki/libvirt/private 
# cp clientcert.pem /etc/pki/libvirt

In the server host 


8. enable "listen_tls" in "libvirtd.conf"

9. Setup of libvirt server:
vi /etc/libvirt/libvirtd.conf # By default, no DN's are checked
tls_allowed_dn_list = ["C=China, O=Red Hat, L=Beijing, ST=Beijign, CN=10.66.65.132"]

10. Enable logging.
vi /etc/libvirt/libvirtd.conf # Logging level: 0 none, 4 errors, 3 warnings, 2
informations, 1 debug # basically 1 will log everything possible
log_level = 1

11. stop libvirtd 
# service libvirtd stop

12. start libvirtd with option "--listen" 
# libvirtd -d --listen

In the client 


13. #virsh -c qemu+tls://10.66.70.128/system list --all


	
Expected Results:

13. On server, # tail -f /var/log/libvirt/libvirtd.log

"remoteCheckCertificate: client's Distinguished Name is not on the list of
allowed clients (tls_allowed_dn_list).Run 'certtool -i --infile
/etc/pki/libvirt/clientcert.pem' to view the Distinguished Name field in the
client certificate, or run this daemon with --verbose option."

Notes:
Comments:

		177460 	[remote access] listen_addr custom 	whuang 	None 	Manual 		Feature 	P1 	2130 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access

bug:

    No bug found

Actions:




In Server     listen addr
edit  /etc/libvirt/libvirtd.conf

# Override the default configuration which binds to all network
# interfaces. This can be a numeric IPv4/6 address, or hostname
listen_addr = "your host IP or hostname"

log_level = 1


#service libvirtd restart

setup a guest

#tail -f   /var/log/libvirt/libvirtd.log
 
	
Expected Results:

run command "service libvirtd status", make sure the libvirtd is still in running status.

check the libvirtd.log to make sure there isn't any error message.
Notes:
Comments:

		177461 	[remote access] mDSN custom 	whuang 	None 	Manual 		Feature 	P1 	2140 	Edit
Setup:
do Case: 91150: [Connection] View libvirt host lists via avahi first

in Server 
edit  /etc/libvirt/libvirtd.conf

# Flag toggling mDNS advertizement of the libvirt service.
#
# Alternatively can disable for all services on a host by
# stopping the Avahi daemon
#
# This is enabled by default, uncomment this to disable it
#mdns_adv = 0

# Override the default mDNS advertizement name. This must be
# unique on the immediate broadcast network.
#
# The default is "Virtualization Host HOSTNAME", where HOSTNAME
# is subsituted for the short hostname of the machine (without domain)
#
mdns_name = "Virtualization Host test-Demo"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access

bug:

    No bug found

Actions:
1.  find a client which is in the same network with the server 
2.then Launch virt-manger in your client
3. Click File-> Add Connection, Select hypervisor as QEMU/KVM, Tick Connect to remote host, Method as SSH, Username as root
4. Check hostname dropdown list
you will find the âVirtualization test-Demoâ


5. set mdns_adv = 0 then do the step 3 and 4 , you can not find the âVirtualization test-Demoâ anymore.
	
Expected Results:

4) you will find the âVirtualization test-Demoâ

 

5. set mdns_adv = 0 then do the step 3 and 4 , you can not find the âVirtualization test-Demoâ anymore.
Notes:
Comments:

		177462 	[Remote Access] memory leak on virNetTLSContextCheckCertificate and testTLSSessionInit BZ#735650 	whuang 	None 	Manual 		Regression 	P1 	2150 	Edit
Setup:

 add this repo

cat /etc/yum.repos.d/rhel6.4-autotest.repo

[RHEL-Server-6.4]
name=rhel6.4
baseurl=http://porkchop.devel.redhat.com/brewroot/repos/RHEL-6.4-build/latest/x86_64/
enabled=1
gpgcheck=0

 

#yum install valgrind -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    alsa-lib

Tag:

    remote access
    Regression

bug:

    No bug found

Actions:

1) download the libvirt src pkg 
like:

 libvirt-0.10.2-10.el6.src.rpm



2) try to rebuild the src pkg you should resolve all the dep you get    , you can use yum or brewweb  get these pkgs
#rpmbuild --rebuild   /root/libvirt-0.10.2-10.el6.src.rpm

error: Failed build dependencies:
        libblkid-devel >= 2.17 is needed by libvirt-0.10.2-10.el6.x86_64
        autoconf is needed by libvirt-0.10.2-10.el6.x86_64
        automake is needed by libvirt-0.10.2-10.el6.x86_64
        gettext-devel is needed by libvirt-0.10.2-10.el6.x86_64
        libtool is needed by libvirt-0.10.2-10.el6.x86_64
        python-devel is needed by libvirt-0.10.2-10.el6.x86_64
        libxml2-devel is needed by libvirt-0.10.2-10.el6.x86_64
        xhtml1-dtds is needed by libvirt-0.10.2-10.el6.x86_64
        readline-devel is needed by libvirt-0.10.2-10.el6.x86_64
        ncurses-devel is needed by libvirt-0.10.2-10.el6.x86_64
        libtasn1-devel is needed by libvirt-0.10.2-10.el6.x86_64
        gnutls-devel is needed by libvirt-0.10.2-10.el6.x86_64
        augeas is needed by libvirt-0.10.2-10.el6.x86_64
        libudev-devel >= 145 is needed by libvirt-0.10.2-10.el6.x86_64
        libpciaccess-devel >= 0.10.9 is needed by libvirt-0.10.2-10.el6.x86_64
        yajl-devel is needed by libvirt-0.10.2-10.el6.x86_64
        sanlock-devel >= 2.4 is needed by libvirt-0.10.2-10.el6.x86_64
        libpcap-devel is needed by libvirt-0.10.2-10.el6.x86_64
        libnl-devel is needed by libvirt-0.10.2-10.el6.x86_64
        avahi-devel is needed by libvirt-0.10.2-10.el6.x86_64
        libselinux-devel is needed by libvirt-0.10.2-10.el6.x86_64
        cyrus-sasl-devel is needed by libvirt-0.10.2-10.el6.x86_64
        parted-devel is needed by libvirt-0.10.2-10.el6.x86_64
        device-mapper-devel is needed by libvirt-0.10.2-10.el6.x86_64
        numactl-devel is needed by libvirt-0.10.2-10.el6.x86_64
        libcap-ng-devel >= 0.5.0 is needed by libvirt-0.10.2-10.el6.x86_64
        libssh2-devel >= 1.3.0 is needed by libvirt-0.10.2-10.el6.x86_64
        netcf-devel >= 0.1.8 is needed by libvirt-0.10.2-10.el6.x86_64
        libcurl-devel is needed by libvirt-0.10.2-10.el6.x86_64
        audit-libs-devel is needed by libvirt-0.10.2-10.el6.x86_64
        systemtap-sdt-devel is needed by libvirt-0.10.2-10.el6.x86_64





2. install src libvirt rpm  
name like this : 
libvirt-0.9.13-3.el6.src.rpm 

rpm-ivh libvirt-0.9.13-3.el6.src.rpm 


#rpmbuild -bb /root/rpmbuild/SPEC/libvirt.spec       (if there are some deps pls install them , you can find them in brewweb or yum repo)

wait libvirt compiling finish 

 3 cd libvirt, compile and run tests:
#cd  /root/rpmbuild/BUILD/libvirt/

 
#valgrind -v --leak-check=full ./tests/virnettlscontexttest

	
Expected Results:

no mem leak 

==12367== LEAK SUMMARY:
==12367==    definitely lost: 0 bytes in 0 blocks
==12367==    indirectly lost: 0 bytes in 0 blocks
==12367==      possibly lost: 349 bytes in 18 blocks
==12367==    still reachable: 0 bytes in 0 blocks
==12367==         suppressed: 0 bytes in 0 blocks

Notes:
Comments:

		177463 	[remote access] missing functions on remote libvirt give wrong error BZ#738439BZ#781895 	whuang 	None 	Manual 		Regression 	P1 	2160 	Edit
Setup:

Server: 
libvirt-0.9.3-2.el6.x86_64  

Client: 
libvirt-0.9.4-12.el6.x86_64  or newer

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    regression
    remote access

bug:

    No bug found

Actions:

On the client 

 

#yum install libvirt-devel
# cat foo.c

 

#include <stdio.h>
#include <libvirt/libvirt.h>
#include <libvirt/virterror.h>
int main(int argc, char **argv) {
    if (argc < 3) {
        fprintf(stderr, "usage: %s uri file\n", argv[0]);
        return 1;
    }
    virConnectPtr conn = virConnectOpenAuth(argv[1], virConnectAuthPtrDefault,
                                            0);
    if (!conn) {
        fprintf(stderr, "unable to open connection to %s\n", argv[1]);
        return 1;
    }
    char *xml = virDomainSaveImageGetXMLDesc(conn, argv[2], 0);
    if (xml) {
        fprintf(stderr, "server is too new\n");
        return 1;
    }
    virErrorPtr err = virGetLastError();
    if (!err) {
        fprintf(stderr, "missing error\n");
        return 1;
    }
    if (err->code != VIR_ERR_NO_SUPPORT) {
        fprintf(stderr, "wrong error code %d\n", err->code);
        return 1;
    }
    puts("success");
    virConnectClose(conn);
    return 0;
}

ï»¿

# gcc -o foo foo.c -lvirt

ï»¿
# ./foo qemu+ssh://$Server/system /tmp/foo.log

	
Expected Results:

libvir: RPC error : unknown procedure: 235
success

Notes:
Comments:

		177464 	[remote access] RunVmOnce often fails to start the VM, VM in wait_for_launchBZ#746689 	whuang 	whuang 	Manual 		Regression 	P1 	2170 	Edit
Setup:

1. Have 1  hosts, 1 vm on nfs, ISO domain
2. RunVmOnce with some CDROM of floppy attached and paused
enabled

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    regression
    remote access

bug:

    No bug found

Actions:

1) start the VM via rhevm  with  RunOnce and enable pause 

 

2) test  5 times 

 
	
Expected Results:

the guest start up soon. the status should be  not   "Wait For Launch"  and Uptime > 3 min
Notes:
Comments:

		177465 	[remote access] sasl_allowed_username_list custom 	whuang 	None 	Auto 		Feature 	P1 	2180 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access
    virsh-rail

bug:

    No bug found

Actions:
at server host :

1]  edit /etc/libvirt/libvirtd.conf

sasl_allowed_username_list = ["test" ]
listen_tls = 1
auth_tls = "sasl"

2]  adduser fred

 #  saslpasswd2 -a libvirt fred

do case " 39342 [Remote access] Connect to the hypervisor running on host using TLS with SASL via ipv4"first for setting up tls environment (do not edit /etc/libvirt/libvirtd.conf again, just generate key and copy to the right position)
at the client host  :

1]  #virsh -c qemu+tls://$server/system list
username fred

result :
# virsh -c qemu+tls://7-142-r6/system list
Please enter your authentication name: fred
Please enter your password:
error: authentication failed: authentication failed
error: failed to connect to the hypervisor
Note:
If edit  server  sasl_allowed_username_list = ["fred" ] then  run cmd   at client ,it will show the domain list 

# virsh -c qemu+tcp://10.66.7.142/system list
Please enter your authentication name: fred
Please enter your password: 
 Id Name                 State
----------------------------------
  3 r6                   running

ï»¿
	
Expected Results:

 

if  sasl_allowed_username_list = ["test" ]  then :

error: authentication failed: authentication failed
error: failed to connect to the hypervisor 

 

 

if sasl_allowed_username_list = ["fred" ] then :

# virsh -c qemu+tcp://10.66.7.142/system list
Please enter your authentication name: fred
Please enter your password: 
 Id Name                 State
----------------------------------
  3 r6                   running


Notes:
Comments:

		177466 	[remote access] TCP ports custom 	whuang 	None 	Manual 		Feature 	P1 	2190 	Edit
Setup:
In Server 
edit /etc/sysconfig/libvirtd uncomment LIBVIRTD_ARGS

LIBVIRTD_ARGS="--listen"


edit  /etc/libvirt/libvirtd.conf
listen_tls = 0
listen_tcp =1
tcp_port = â16510â
auth_tcp = "none"

#service libvirtd restart
#iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access

bug:

    No bug found

Actions:
in the client host
#virsh -c qemu+tcp://$IP:16510/system list
	
Expected Results:

it will show the host domain list
Notes:
Comments:

		176885 	[Floppy disk support] Test floppy media insert/eject Bug - 625319 	yimwang 	None 	Manual (Autoproposed) 		Feature 	P2 	2200 	Edit
Setup:

1.a healthy guest, which is shutoff

2.if the guest os is rhel6, pls do modprobe first when login into the guest.

#modprobe floppy

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual disks

bug:

    No bug found

Actions:

1.  dump the xml of guest

   # virsh dumpxml toy > toy.xml

2.  insert following xml into the "<device>" node of toy.xml

    <disk type='block' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source dev='/var/lib/libvirt/images/floppy1.img'/>
      <target dev='fda' bus='fdc'/>
      <readonly/>
    </disk>


3. undefine guest
 # virsh undefine toy

4. create floppy disks, and format them in ext3 format
# qemu-img create floppy1.img 10M
# qemu-img create floppy2.img 10M

# mkfs.ext3 floppy1.img
# mkfs.ext3 floppy2.img

5. define guest with "toy.xml"
  # virsh define toy.xml

6.Create a floppy eject XML.
 # cat eject.xml

   <disk type='block' device='floppy'>
     <target dev='fda' bus='fdc'/>
   </disk>


7.Create a floppy insert XML.

  # cat insert.xml
   <disk type='block' device='floppy'>
     <source dev='/var/lib/libvirt/images/floppy2.img'/>
     <target dev='fda' bus='fdc'/>
   </disk>



8. Eject floppy.

 # virsh update-device toy eject.xml

9.Inset floppy.

# virsh update-device toy insert.xml  

# mount /dev/fd0 /mnt
# touch /mnt/test.txt

	
Expected Results:

7. Check the floppy1.img is disconnected from guest

8. Check floppy2.img is attached to guest successfully and could be mounted in guest, read and write in guest
Notes:
Comments:

		177467 	[remote access] TLS ports custom 	whuang 	None 	Manual 		Feature 	P1 	2200 	Edit
Setup:
add two hostname in  two hostsâ hosts files

Server:
edit   /etc/sysconfig/libvirtd uncomment LIBVIRTD_ARGS

LIBVIRTD_ARGS="--listen"


edit  /etc/libvirt/libvirtd.conf

listen_tls =1
tls_port = â16515â
#iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access

bug:

    No bug found

Actions:
In the Server host

1.setup certification

1.1 # certtool --generate-privkey > cakey.pem

1.2 self-sign cakey.pem by creating a file with the signature details called ca.info containing:

cn = $server_hostname

ca

cert_signing_key

1.3 # certtool --generate-self-signed --load-privkey cakey.pem --template ca.info --outfile cacert.pem

2. Create server certificates

2.1 certtool --generate-privkey > serverkey.pem

2.2 sign that key with the CA's private key by first creating a template file called server.info

   

organization = Red Hat

cn = $server_hostname

tls_www_server

encryption_key

signing_key

2.3 # certtool --generate-certificate --load-privkey serverkey.pem --load-ca-certificate cacert.pem \

--load-ca-privkey cakey.pem --template server.info --outfile servercert.pem

3. Copy CA key and server key to correct directory

3.1 # cp cakey.pem cacert.pem /etc/pki/CA

3.2 # mkdir -p /etc/pki/libvirt/private

3.3 # cp serverkey.pem /etc/pki/libvirt/private

3.4 # cp servercert.pem /etc/pki/libvirt

4. Copy CA key to client($client_hostname) into correct directory

4.1 # scp cakey.pem cacert.pem root@$client_hostname:/etc/pki/CA

}

#service libvirtd restart
In the client host

{

9.  Create client certificates

9.1 # certtool --generate-privkey > clientkey.pem

9.2 Act as CA and sign the certificate.  Create client.info containing:

country = GB

state = London

locality = London

organization = Red Hat

cn = $client_hostname

tls_www_client

encryption_key

signing_key

9.3 # certtool --generate-certificate  --load-privkey clientkey.pem --load-ca-certificate /etc/pki/CA/cacert.pem \

--load-ca-privkey /etc/pki/CA/cakey.pem --template client.info --outfile clientcert.pem

10. Copy client key to correct directory

10.1 # mkdir -p /etc/pki/libvirt/private

10.2 # cp clientkey.pem /etc/pki/libvirt/private

10.3 # cp clientcert.pem /etc/pki/libvirt/
}

#virsh -c qemu+tls://$server_hostname:16515/system list


	
Expected Results:

It will show the host domian list  without error 
Notes:
Comments:

		177468 	[Remote access] UNIX socket access controls 	whuang 	None 	Auto 		Feature 	P1 	2210 	Edit
Setup:

Just need one Host 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    QE consumption
    remote access

bug:

    No bug found

Actions:

1ï¼  edit /etc/libvirt/libvirtd.conf 

auth_unix_ro = "none"
auth_unix_rw = "none"
unix_sock_group = "wheel"
unix_sock_ro_perms = "0777"
unix_sock_rw_perms = "0770"

2ï¼add a test1 user then add it in wheel group

#adduser test1
#passwd test1
#usermod -g wheel test1


3ï¼#service libvirtd restart

login with test1
4ï¼#su - test 1

5ï¼$virsh -c qemu+unix:///system list
Id Name                 State
----------------------------------

6)edit /etc/libvirt/libvirtd.conf


unix_sock_group = "root"

7)#service libvirtd restart


login with test1
8)#su - test 1

9)$virsh -c qemu+unix:///system list
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Permission denied
error: failed to connect to the hypervisor
	
Expected Results:

step 5:  it will show domain list successfully 

 

step 9 : it will get permission denied
Notes:
Comments:

		177469 	[remote access] UNIX socket dir custom 	whuang 	None 	Manual 		Feature 	P1 	2220 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access

bug:

    No bug found

Actions:

1)
edit /etc/libvirt/libvirtd.conf

unix_sock_dir = "/var/run/tmp/libvirt"

2)
#mkdir -p /var/run/tmp/libvirt

3)
#service libvirtd restart

# ll /var/run/tmp/libvirt
total 0

srwxrwxrwx. 1 root root 0 12æ 26 16:41 libvirt-sock
srwxrwxrwx. 1 root root 0 12æ 26 16:41 libvirt-sock-ro



check the new socket
4)
#virsh -c qemu+unix:///system?socket=/var/run/tmp/libvirt/libvirt-sock
	
Expected Results:

 4)  no error  pop up 
Notes:
Comments:

		177471 	[remote access]Check if the guest in WaitForLaunch status after run once with "paused" mode - 748025 	yupzhang 	None 	Manual 		Regression 	P1 	2230 	Edit
Setup:

1.Refer to the following link to setup RHEVM.

https://docspace.corp.redhat.com/docs/DOC-101056

2.Register a RHEL to RHEVM

3.Install a guest with ISO

 

Set the status to "Need_Update" as the bug 746689 is not fixed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access
    Regression

bug:

    No bug found

Actions:

1.Login RHEVM,start the guest with click "Run Once"(Select the guest,and right click)

2.Select Attach CD,then select a iso.Then click "Start in Pause Mode",click "OK"

3.Stop the guest.

4.Repeat many times of step 2 and 3.(at lease 20 times)
	
Expected Results:

1."Run Virtual Machines(s)" menu pop up.

Step 2 and 4:

The guest will be "Paused" status,the guest will not be in "Wait For Launch" status.
Notes:
Comments:

		177474 	[Scalability] 256 autostarted storage pools reloading 	jiachen 	None 	Auto 		Stress 	P2 	2240 	Edit
Setup:

 cat pool.xml
<pool type="dir">
    <name>#pool-name#</name>
    <target>
       <path>#pool-path#</path>
    </target>
</pool>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

 

Save the following script into pool-testing.sh , then

# sh pool-testing.sh pool.xml

#!/bin/sh

#make sure there is no more than 256 pools exist
existing_pool_num=$(virsh pool-list --all|awk 'NR>2'|wc -l)
if [[ $existing_pool_num -gt 1 ]];then
    for i in $(virsh pool-list --all|awk 'NR>2'|awk -F' ' '{ if($1 != "default") print $1}')
    do
        virsh pool-destroy $i
        virsh pool-undefine $i
    done
fi

name="pool"
parent_path="/var/lib/libvirt/images"
for i in {1..255}
do
    new_path=$parent_path/$name$i
    sed -i -e "s,#pool-name#,$name$i,g" pool.xml
    sed -i -e "s,#pool-path#,$new_path,g" pool.xml
    virsh pool-define pool.xml
    mkdir -p $new_path
    sed -i -e "s,$new_path,#pool-path#,g" pool.xml
    sed -i -e "s,$name$i,#pool-name#,g" pool.xml
done

for k in {1..255}
do
    virsh pool-start pool$k
done

virsh pool-start default

#reload 256 pool
service libvirtd restart
wait
running_pool_num=$(virsh pool-list --all |grep active |wc -l)
if [[ $running_pool_num -lt 256 ]];then
    echo "pools reloading failed"
else
    echo "succeed"
fi
	
Expected Results:

Make sure you get Succeed return value after performing the script
Notes:
Comments:

		176954 	[Guest network driver] Illegal interface type or NIC model 	xhu 	None 	Manual (Autoproposed) 		Negative test 	P3 	2250 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    virtual networks

bug:

    864773 - From Run 46177
    864773 - From Run 51204
    864773 - From Run 53753
    864773 - From Run 54159

Actions:

1. Starting with the guest domain, remove the interface definition and replace it with:
    <interface type='test'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>

OR
  
    <interface type='network'>
        <mac address='54:52:00:54:9e:f4'/>     
        <source network='default'/>
        <model type='test'/>  
    </interface>

	
Expected Results:

1. Verify that a virtual network with an illegal interface type or NIC model
   does not operate. Libvirt should prompt the wrong information accordingly,
   such as, deny NIC models which the hypervisor does not support.

Notes:
Comments:

		177416 	[PCI and USB device assignment] Hot unplug normal PCI device with 'managed=yes' - bug 781985 	yimwang 	None 	Auto 		--default-- 	P1 	2250 	Edit
Setup:

- Host supports VT-d

- Could lose connection with host after dettach network device from host

- Have finished test case '[PCI device] Hot plug normal PCI device with 'managed='yes''' '
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.0
    regression

bug:

    No bug found

Actions:

1. # virsh detach-device <guest> nodedev.xml

2. On guest, run # lspci
	
Expected Results:

 

1. The device could be hot-unplug from the guest

2. Could not see the unpluged device on guest

and  PCI device auto reassign to  host
Notes:
Comments:

		177417 	[PCI and USB device assignment] Hot unplug normal PCI device without 'managed=yes' 	jialiu 	None 	Auto 		--default-- 	P1 	2260 	Edit
Setup:

- Host supports VT-d

- Could lose connection with host after dettach network device from host

- Have finished test case [PCI and USB device assignment] Hot plug normal PCI device without 'managed=yes'
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. # virsh detach-device <guest> nodedev.xml

# virsh nodedev-reset pci_0000_00_19_0

2. On guest, run # lspci

3. Re-attach this network device to host

# virsh nodedev-reattach pci_0000_00_19_0

4. Shutdown guest and start the guest again.
	
Expected Results:

1. The device could be hot-unplug from the guest

2. Could not see the unpluged device on guest

3. The network is working fine now in host.

4. The guest is working fine, and no any hot-plug nic from host is seen.
Notes:
Comments:

		176959 	[Guest network driver] NIC virtio driver with and without vhost-net 	xhu 	None 	Manual (Autoproposed) 		Feature 	P3 	2270 	Edit
Setup:

Firstly, you must run "124156 [Guest network driver] NIC virtio driver"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    virtual networks

bug:

    No bug found

Actions:

1. Using the same guest configured for the virtio NIC driver test;
   with the guest shut down, verify that /dev/vhost-net exists.
   
   (Note: BZ#596891 - vhost-net module was not being loaded automatically).
   If not, (1) that is a bug, and (2) load it manually with:
   
   # modprobe -i vhost_net

2. Start the guest.

3. Stop the guest from the previous test, then remove the vhost-net module:   
    # rmmod vhost_net
4. Restart the guest.

	
Expected Results:

1. verify that /dev/vhost-net exists
2. Look at the qemu commandline in /var/log/libvirt/qemu/guestname.log and verify that it contains "vhost=on", and that networking on the guest works.
3. Verify that /dev/vhost-net no longer exists.
4. Verify that the new qemu commandline does *not* contain "vhost=on", and that  networking on the guest still works.

Notes:
Comments:

		177420 	[PCI and USB device assignment] Hotplug PCI device to guest via virt-manager - bug 555309 	yoyzhang 	yoyzhang 	Auto 		Regression 	P2 	2270 	Edit
Setup:

Test on a machine supporting VT-d

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. Check installed the driver for the  pci device.

#lspci

...

00:19.0 Ethernet controller: Intel Corporation 82567LM-3 Gigabit Network Connection (rev 02)

# virsh nodedev-list

...

pci_0000_00_19_0

...

# readlink -f /sys/bus/pci/devices/0000\:00\:19.0/driver/

/sys/bus/pci/drivers/pci-stub

 

2. Start the guest using virt-manager or virsh

3. Attach this pci device to guest in virt-manager

Open Virt-Manager->"Add Hardware"->"Physical Host Device" to assign the hidden PCI device to a guest

4. log in the guest and issue command

# lspci

# service network restart

# ifconfig

# ping {host_ip}

 

5. click 'shutdown the virtual machine' 

then click 'power on the virtual machine'

6. log in guest

run # lspci ,#ping host
	
Expected Results:

4. In guest, assigned PCI device info is seen. The device could get ip and guest could ping to host

5. boot successfully

6. Could see the assigned device and ping successfully
Notes:
Comments:

		177421 	[PCI and USB device assignment] Hotplug/Hotunplug normal USB device with bus + device 	jialiu 	None 	Auto 		--default-- 	P1 	2280 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. Define and start a domain

2. Plug a USB disk to host.

# lsusb
Bus 008 Device 003: ID 413c:2106 Dell Computer Corp. Dell QuietKey Keyboard
Bus 008 Device 002: ID 413c:3012 Dell Computer Corp. Optical Wheel Mouse
Bus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 002 Device 004: ID 090c:1000 Feiya Technology Corp. Flash Drive
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub

3. Dump xml for the usb device.

# virsh nodedev-list --tree

...

 +- pci_0000_00_1d_7
  |   |
  |   +- usb_usb2
  |       |
  |       +- usb_2_0_1_0
  |       +- usb_2_2
  |           |
  |           +- usb_2_2_1_0
  |               |
  |               +- scsi_host6
  |                   |
  |                   +- scsi_target6_0_0
  |                       |
  |                       +- scsi_6_0_0_0
  |                           |
  |                           +- block_sdb_USB_Flash_Disk_AA72013000022095_0_0

...

# virsh nodedev-dumpxml usb_2_2
<device>
  <name>usb_2_2</name>
  <parent>usb_usb2</parent>
  <capability type='usb_device'>
    <bus>2</bus>
    <device>4</device>
    <product id='0x1000'>USB DISK</product>
    <vendor id='0x090c'>SMI Corporation</vendor>
  </capability>
</device>

4. Prepare the following xml file

# cat hostdev-usb-address.xml
    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='2' device='4'/>
      </source>
    </hostdev>

5. Hotplug the usb device to guest.

# virsh attach-device <guestname> hostdev-usb-address.xml

6. In guest, check the device is seen.

# lsusb

7. Hotunplug the usb device from guest.

# virsh detach-device <guestname> hostdev-usb-address.xml

8. In guest, check the device is not seen.

# lsusb

9. Shutdown the guest and start it again.

10. In guest, check the usb device again via lsusb
	
Expected Results:

5. Output:

# virsh detach-device <guestname> hostdev-usb-address.xml
Device detached successfully

6. The usb device is seen in guest.

7.  Output:

# virsh detach-device <guestname> hostdev-usb-address.xml
Device detached successfully

8. The usb device is not existing now.

9. guest is shutdown and started successfully

10. The device is not existing.
Notes:
Comments:

		177422 	[PCI and USB device assignment] Hotplug/Hotunplug normal USB device with product + vendor-bug798838 	jialiu 	None 	Auto 		--default-- 	P2 	2290 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. Define and start a domain

2. Plug a USB disk to host.

# lsusb
Bus 008 Device 003: ID 413c:2106 Dell Computer Corp. Dell QuietKey Keyboard
Bus 008 Device 002: ID 413c:3012 Dell Computer Corp. Optical Wheel Mouse
Bus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 002 Device 004: ID 090c:1000 Feiya Technology Corp. Flash Drive
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub

3. Dump xml for the usb device.

# virsh nodedev-list --tree

...

 +- pci_0000_00_1d_7
  |   |
  |   +- usb_usb2
  |       |
  |       +- usb_2_0_1_0
  |       +- usb_2_2
  |           |
  |           +- usb_2_2_1_0
  |               |
  |               +- scsi_host6
  |                   |
  |                   +- scsi_target6_0_0
  |                       |
  |                       +- scsi_6_0_0_0
  |                           |
  |                           +- block_sdb_USB_Flash_Disk_AA72013000022095_0_0

...

# virsh nodedev-dumpxml usb_2_2
<device>
  <name>usb_2_2</name>
  <parent>usb_usb2</parent>
  <capability type='usb_device'>
    <bus>2</bus>
    <device>4</device>
    <product id='0x1000'>USB DISK</product>
    <vendor id='0x090c'>SMI Corporation</vendor>
  </capability>
</device>

4. Prepare the following xml file

# cat hostdev-usb-product.xml
    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <product id='0x1000'/>
        <vendor id='0x090c'/>
      </source>
    </hostdev>

5. Hotplug the usb device to guest.

# virsh attach-device <guestname> hostdev-usb-product.xml

6. In guest, check the device is seen.

# lsusb

7. Shutdown the guest

8. Start the guest

9. Hotplug the usb device to guest again.

# virsh attach-device <guestname> hostdev-usb-product.xml
10. Hotunplug the usb device from guest.

# virsh detach-device <guestname> hostdev-usb-product.xml

11. In guest, check the device is not seen.

# lsusb

12. Shutdown the guest and start it again.

13. In guest, check the usb device again via lsusb
	
Expected Results:

5. Output:

# virsh attach-device <guestname> hostdev-usb-product.xml
Device attached successfully

6. The usb device is seen in guest.

8. Successfully start .No error occurs.
10. Output:

# virsh detach-device <guestname> hostdev-usb-product.xml
Device detached successfully

11. The usb device is not existing now.

12. guest is shutdown and started successfully

13. The device is not existing.
Notes:
Comments:

		177423 	[PCI and USB device assignment] Need an xml switch to disable rombar of assigned devices 	ajia 	None 	Auto 		--default-- 	P1 	2300 	Edit
Setup:

- Host supports VT-d

- Could lose connection with host after dettach network device from host

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. # lspci | grep -i network
00:19.0 Ethernet controller: Intel Corporation 82566DM-2 Gigabit Network Connection (rev 02)

2. # virsh edit the following xml into guest XML configuration:

    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x00' slot='0x19' function='0x0'/>
      </source>
      <alias name='hostdev0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06'
function='0x0'/>
      <rom bar='off'/>
    </hostdev>

or <rom bar='on'/>

3. virsh start the guest

4. check qemu-kvm command line
	
Expected Results:

Step 4:

You should see information like this:

# ps -ef|grep qemu-kvm|grep -v grep
qemu      3950     1 58 16:25 ?        00:00:09 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name
vr-rhel5u4-x86_64-kvm -uuid 894d5b0f-42c3-c13d-ee81-f55a5313cecc -nodefconfig
-nodefaults -chardev
socket,id=charmonitor,path=/var/lib/libvirt/qemu/vr-rhel5u4-x86_64-kvm.monitor,server,nowait
-mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown
-drive
file=/var/lib/libvirt/images/vr-rhel5u4-x86_64-kvm,if=none,id=drive-ide0-0-0,format=qcow2
-device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1
-netdev tap,fd=23,id=hostnet0 -device
rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:7b:91:a4,bus=pci.0,addr=0x3
-chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0
-usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device
AC97,id=sound0,bus=pci.0,addr=0x4 -device
pci-assign,host=00:19.0,id=hostdev0,configfd=24,bus=pci.0,addr=0x6,rombar=0
-device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

 Notes, if you config <rom bar='on'/>, you should see rombar=1 in qemu-kvm command line.

 
Notes:
Comments:

		177425 	[PCI and USB device assignment] passthrough multifunction PCI devices to KVM guests 	ajia 	None 	Auto 		--default-- 	P1 	2310 	Edit
Setup:

- Host supports VT-d

- Host a dual port (at least) NICs such as dual port -Broadcom
Corporation NetXtreme II BCM5709 Gigabit Ethernet

- Could lose connection with host after dettach network device from host

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. # lspci

lspci output from host
--------------------
02:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit
Ethernet (rev 20)
02:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit
Ethernet (rev 20)

2. # virsh edit the following xml into guest XML configuration:

    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x02' slot='0x00' function='0x0'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0' multifunction='on'/>
    </hostdev>
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x02' slot='0x00' function='0x1'/>
      </source>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x1'/>
    </hostdev>

3. virsh start the guest

4. check qemu-kvm command line

or

5. login guest then check the PCI device's slot
	
Expected Results:

Step 4:

You should see information like this:

# ps -ef|grep qemu-kvm
qemu     12186     1 31 10:42 ?        00:00:01 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name
vr-rhel6u2-x86_64-kvm -uuid 4b2bab0e-2b67-b743-49c8-bc2b9a551b0e -nodefconfig
-nodefaults -chardev
socket,id=charmonitor,path=/var/lib/libvirt/qemu/vr-rhel6u2-x86_64-kvm.monitor,server,nowait
-mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown
-drive
file=/var/lib/libvirt/images/vr-rhel6u2-x86_64-kvm,if=none,id=drive-ide0-0-0,format=raw
-device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1
-netdev tap,fd=23,id=hostnet0 -device
rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:80:4d:ac,bus=pci.0,addr=0x3
-chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0
-usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device
AC97,id=sound0,bus=pci.0,addr=0x4 -device
pci-assign,host=02:00.0,id=hostdev0,configfd=24,bus=pci.0,multifunction=on,addr=0x6
-device pci-assign,host=02:00.1,id=hostdev1,configfd=25,bus=pci.0,addr=0x6.0x1
-device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

Notes, the first addr is '0x6', and the second addr is '0x6.0x1' in pci-assign
section.

Step 5:

You should see information like this:

# lspci
......
00:06.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit
Ethernet (rev 20)
00:06.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit
Ethernet (rev 20)

 
Notes:
Comments:

		189583 	[PCI and USB device assignment] libvirt Support ivshmem (Inter-VM Shared Memory) - bug787536 	ajia 	ajia 	Manual 		--default-- 	P1 	2310 	Edit
Setup:

- Host supports VT-d

- Host a dual port (at least) NICs such as dual port -Broadcom
Corporation NetXtreme II BCM5709 Gigabit Ethernet

- Could lose connection with host after dettach network device from host

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL7.0
    PCI and USB device assignment

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177426 	[PCI and USB device assignment] PCI passthrough normal device with manage='yes' and then hot-unplug 	weizhan 	None 	Manual 		Function 	P1 	2320 	Edit
Setup:

Host supports VT-d

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree

2. # virsh nodedev-dumpxml pci_8086_10bd

3. Add the following code to <guest> xml file and save this xml part into nodedev.xml

...
          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='0' slot='25' function='0'/>
            </source>
          </hostdev>
          ...

4. # virsh define <guest>.xml

5. # virsh start <guest>

6. In guest, run # lspci

# service network restart

# ifconfig

# ping {host_ip}

7. Hot-unplug pci device from guest

# virsh detach-device <guest> nodedev.xml

8. On guest, run # lspci

9.Shutdown guest and start the guest again.

10.Destroy the guest

 
	
Expected Results:

1. Output:

computer
  |
  +-pci_8086_10bd
  |   |
  |   +-net_00_1e_4f_a8_a7_dc

........

2. Output

<device>
  <name>pci_8086_10bd</name>
  <parent>computer</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>0</bus>
    <slot>25</slot>
    <function>0</function>
    <product id='0x10bd'>82566DM-2 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

5.Output

Domain rh6.1 started

6. The assigned network should be seen in guest. The device could get ip in guest and guest could ping to host successfully

7.The device could be hot-unplug from the guest

8. Could not see the unpluged device on guest

9. The device wil be back

10.The network is working fine now in host.
Notes:
Comments:

		177427 	[PCI and USB device assignment] PCI passthrough normal device without manage='yes' and then hot-unplug 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	2330 	Edit
Setup:

- Host supports VT-d

- Could lose connection with host after dettach network device from host

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree

2. # virsh nodedev-dumpxml pci_8086_10bd

3. Dettach this network device from host

# virsh nodedev-dettach pci_8086_10bd

4. Add the following code to <guest> xml file and save this xml part info nodedev.xml

...
          <hostdev mode='subsystem' type='pci'>
            <source>
              <address bus='0' slot='25' function='0'/>
            </source>
          </hostdev>
          ...

5. # virsh define <guest>.xml

6. # virsh start <guest>

7. In guest, run # lspci

# service network restart

# ifconfig

# ping {host_ip}

8. Hot-unplug pci device from guest

# virsh detach-device <guest> nodedev.xml

# virsh nodedev-reset pci_0000_00_19_0

9. On guest, run # lspci

10.Shutdown guest and start the guest again.

11.Destroy the guest and re-attach this network device to host

# virsh nodedev-reattach pci_0000_00_19_0

 
	
Expected Results:

1. Output:

computer
  |
  +-pci_8086_10bd
  |   |
  |   +-net_00_1e_4f_a8_a7_dc

........

2. Output

<device>
  <name>pci_8086_10bd</name>
  <parent>computer</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>0</bus>
    <slot>25</slot>
    <function>0</function>
    <product id='0x10bd'>82566DM-2 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

3. Output

Device pci_8086_10bd dettached

6.Output

Domain rh6.1 started

7. The assigned network should be seen in guest. The device could get ip in guest and guest could ping to host successfully

8.The device could be hot-unplug from the guest

9. Could not see the unpluged device on guest

10. The device wil be back

11.The network is working fine now in host.

 

 
Notes:
Comments:

		177428 	[PCI and USB device assignment] Prevent PCI coldplug/hotplug on non-IOMMU host 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	2340 	Edit
Setup:

- Host doesn't support VT-d

- Turn OFF VT-d in  BOIS

NOTE: After the bug https://bugzilla.redhat.com/show_bug.cgi?id=616415 QMP: does not report the real cause of PCI device assignment failure fixed, we should change the expect result step 8 output
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. virsh nodedev-list --tree

........................

+-pci_1022_9606
  |   |
  |   +-pci_14e4_167a
  |       |
  |       +-net_00_24_21_0e_d2_10
  |         
  +-pci_103c_9602
      |
      +-pci_1002_9611

..........................

2. Select a network pci device

# virsh nodedev-dumpxml pci_14e4_167a

3. Dettach this pci device from host

# virsh nodedev-dettach pci_14e4_167a
Device pci_14e4_167a dettached

4. Add this pci device node to <guest> xml file and also save these in nodedev.xml

<hostdev mode='subsystem' type='pci'>
        <source>
          <address bus='63' slot='0' function='0'/>
        </source>
    </hostdev>

5. # virsh define <guest>.xml

6 # virsh start <guest>

7. Remove above xml from guest and then start guest

8. Hotplug device with

# virsh attach-device <guest> nodedev.xml
	
Expected Results:

2. Output:

<device>
  <name>pci_14e4_167a</name>
  <parent>pci_1022_9606</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>63</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x167a'>NetXtreme BCM5754 Gigabit Ethernet PCI Express</product>
    <vendor id='0x14e4'>Broadcom Corporation</vendor>
  </capability>
</device>

6. Libvirt should prevent assign this pci device to guest as this host doesn't support VT-d . Output:

# virsh start rh6.1
error: Failed to start domain rh6.1
error: internal error Process exited while reading console log output: char device redirected to /dev/pts/1
Using CPU model "cpu64-rhel6"
No IOMMU found.  Unable to assign device "hostdev0"
qemu-kvm: -device pci-assign,host=00:19.0,id=hostdev0,configfd=30,bus=pci.0,addr=0x7: Device 'pci-assign' could not be initialized

8. Will report

error: Failed to attach device from nodedev.xml
error: internal error unable to execute QEMU command 'device_add': Device 'pci-assign' could not be initialized
Notes:
Comments:

		177436 	[PCI device] attach-detach one vnet for 1000 rounds 	jiachen 	None 	Auto 		Stress 	P2 	2350 	Edit
Setup:

have no auto script on virsh-rail
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177544 	[Snapshot] Create a domain snapshot of a running domain with a specified name and description 	jyang 	None 	Auto 		Feature 	P2 	2350 	Edit
Setup:

1. a running healthy guest,  with only qcow2 disks. because qemu doesn't support snapshot of other format disks.

 

this case will create a disk snapshot

Make sure your host installed qemu-kvm-rhev for --disk-only (disk snapshot)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    RHEL6.0
    QE consumption

bug:

    No bug found

Actions:

1. create a xml like following:

<domainsnapshot>
  <name>s1</name>
 <description>Snapshot of OS install and updates</description>
</domainsnapshot>

 

2. create snapshot of the domain

    # virsh snapshot-create foo snap1.xml --disk-only

 

3. wait till step 2 is finished

 

4. check state of domain

   # virsh domstate foo

 

5. check whether snapshot file image exists

   # ls -l /var/lib/libvirt/qemu/snapshot/foo

 

6. check whether the snapshot is defined in libvirt

    # virsh snapshot-list foo
	
Expected Results:


step 4:

# virsh domstate foo
running

step 5:

# ll /var/lib/libvirt/qemu/snapshot/foo/
total 4
-rw-------. 1 root root 221 Mar 17 16:10 s1.xml

step 6:

# virsh snapshot-list foo
 Name                 Creation Time             State
---------------------------------------------------
 s1                2011-03-17 16:09:52 -0400 disk-snapshot
Notes:
Comments:

		177440 	[Installation] serial file path is converted to lower case -- bug 731115 	vbian 	vbian 	Manual 		Function 	P1 	2360 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    Installation

bug:

    No bug found

Actions:

1.Create a folder named as upper cases

#mkdir /UPPER

2. try to create a new guest with following command

#virt-install --name test --location http://tree.englab.nay.redhat.com/pub/rhel/rel-eng/RHEL6.4/RHEL6.4-20130109.1/6.4/Server/x86_64/os
 -r 1024 -f /var/lib/libvirt/images/test.img   --serial file,path=/UPPER/console.log

 3. during the installation of guest, start a new terminal

# virsh dumpxml test  |grep serial -A 5 

 
	
Expected Results:

Make sure you can get following output

    <serial type='file'>
      <source path='/UPPER/console.log'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='file'>
      <source path='/UPPER/console.log'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>

 The higher letters are not converted to lower letter
Notes:
Comments:

		177441 	[Remote access] Configuration TLS x509 server key file path 	whuang 	None 	Auto 		Regression 	P1 	2370 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access
    virsh-rail

bug:

    No bug found

Actions:

On server 

1. Set up a Certificate Authority (CA)

 1) # certtool --generate-privkey > cakey.pem

 2) self-sign cakey.pem by creating a file with the signature details called
ca.info containing:

cn = server host IP
ca
cert_signing_key

 3) # certtool --generate-self-signed --load-privkey cakey.pem --template
ca.info --outfile cacert.pem

2. Create server certificates

 1) certtool --generate-privkey > serverkey.pem

 2) sign that key with the CA's private key by first creating a template file
called server.info

organization = Red Hat
cn = server host IP
tls_www_server
encryption_key
signing_key

 3) # certtool --generate-certificate --load-privkey serverkey.pem
--load-ca-certificate cacert.pem --load-ca-privkey cakey.pem --template
server.info --outfile servercert.pem

3. Copy CA key and server key to correct directory

 1) # cp cakey.pem cacert.pem /mnt/ca/

 2) # mkdir -p /mnt/ca/libvirt/private

 3) # cp serverkey.pem /mnt/ca/libvirt/private

 4) # cp servercert.pem /mnt/ca/libvirt

4. Copy CA key to client into correct directory

  # scp cakey.pem cacert.pem root@{client host IP}:/etc/pki/CA

5. Turn on libvird monitor listening in /etc/sysconfig/libvirtd
  -- uncomment LIBVIRTD_ARGS="--listen"

6. Edit /etc/libvirt/libvirtd.conf
  -- enbale listen_tls = 1   and specify the cert file  

vim /etc/libvirt/libvirtd.conf

listen_tls = 1
key_file = "/mnt/ca/libvirt/private/serverkey.pem"

cert_file = "/mnt/ca/libvirt/servercert.pem"

ca_file = "/mnt/ca/cacert.pem"

crl_file = "/mnt/ca/crl.pem"

7. # service libvirtd restart

8. # service iptables stop
On client 

9.  Create client certificates

9.1 # certtool --generate-privkey > clientkey.pem

9.2 Act as CA and sign the certificate.  Create client.info containing:

country = GB

state = London

locality = London

organization = Red Hat

cn = $client_hostname

tls_www_client

encryption_key

signing_key

9.3 # certtool --generate-certificate  --load-privkey clientkey.pem --load-ca-certificate /etc/pki/CA/cacert.pem \

--load-ca-privkey /etc/pki/CA/cakey.pem --template client.info --outfile clientcert.pem

10. Copy client key to correct directory

10.1 # mkdir -p /etc/pki/libvirt/private

10.2 # cp clientkey.pem /etc/pki/libvirt/private

10.3 # cp clientcert.pem /etc/pki/libvirt/

11. #virsh -c qemu+tls://$server_hostname:16515/system list

 

 
	
Expected Results:

step 2 :

while the specified the server key file path in the libvirtd.conf file, make sure the remote access is working well in step11.
Notes:
Comments:

		177552 	[Snapshot] Dump the XML of a domain snapshot 	jyang 	None 	Auto 		Feature 	P1 	2370 	Edit
Setup:

1. a running healthy guest, with only qcow2 disks.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. create the snapshot of guest with following xml (snap1.xml):

    <domainsnapshot>

        <name>snap1</name>

        <description>Create a live snapshot</description>

    </domainsnapshot>

 

   # virsh snapshot-create foo snap1.xml

 

2. get current domain state

   # virsh domstate foo

 

3. dump the xml of snapshot

   # virsh snapshot-dumpxml foo snap1

 

4. # virsh snapshot-list foo

 

5. check whether the value of "name", "description", "creationTime", "state", "domain" is correct.

    checkpoints:

      1> snapshot name must be "snap1"

      2> description must be "Create a live snapshot"

      3> createTime must be consistent with what we got from "snapshot-list" in step 4

      4> domain state must be equal to we got in step 2

      5> value of "<domain>" must be the UUID or name of domain "foo".
	
Expected Results:

step 5:

      all checkpoints are passed
Notes:
Comments:

		177442 	[remote access] connect to hypervisor with unconfigured tls/tcp connection BZ#723442 	whuang 	None 	Manual 		Regression 	P1 	2380 	Edit
Setup:

service libvirtd restart

 

connect the hypersior without configure tls/tcp environment, just do

# virsh -c qemu+tls:///system

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access
    Regression

bug:

    No bug found

Actions:

 #virsh -c qemu+tls:///system

	
Expected Results:

report some error  like:

ï»¿# virsh -c qemu+tls:///system
error: Cannot read CA certificate '/etc/pki/CA/cacert.pem': No such file or directory
error: failed to connect to the hypervisor

ï»¿ï»¿but not 

Segmentation fault (core dumped)

Notes:
Comments:

		177443 	[remote access] SSH GSSAPI login broken BZ#737176 	whuang 	None 	Manual 		Regression 	P1 	2390 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    alsa-lib

Tag:

    remote access
    Regression

bug:

    No bug found

Actions:

test steps : 
1. export KRB5CCNAME=libvirt_krb_test  
2. export LIBVIRT_DEBUG=1
3. virsh -c qemu+ssh://10.66.5.12/system
...
 virCommandRunAsync:2042 : About to run LC_ALL=C
PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin
HOME=/root USER=root LOGNAME=root KRB5CCNAME=libvirt_krb_test
SSH_ASKPASS=/usr/libexec/openssh/gnome-ssh-askpass DISPLAY=localhost:11.0 ssh
10.66.5.12 nc -U /var/run/libvirt/libvirt-sock
...

	
Expected Results:

check   debug log

there is  message like this:

 KRB5CCNAME=libvirt_krb_test

 
Notes:
Comments:

		177554 	[Snapshot] Get the current snapshot 	jyang 	None 	Auto 		Feature 	P1 	2390 	Edit
Setup:

1. a running healthy guest, with only qcow2 disks. suppose its name is "foo".

 

2. make sure the guest has no snapshot before.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. create a xml named snap1.xml with following contents:

    <domainsnapshot>

        <name>snap1</name>

        <description>snapshot API testing - get currrent snapshot</description>

    </domainsnapshot>

 

2. creating snapshot

   # virsh snapshot-create foo snap1.xml

 

3. wait till step 2 is finished

 

4. get current snapshot

   # virsh snapshot-current foo

 

5. dump the xml of current snapshot

   # virsh snapshot-dumpxml foo snap1

 

6. delete current snapshot

   # virsh snapshot-delete foo snap1

 

7. get current snapshot

   # virsh snapshot-current foo
	
Expected Results:

step 4:

     the current snapshot's name  will be what we just created in step 2

     and its xml will be dumped

 

step 5:

     the xml will be same as we got in step 4

 

step 7:

     return none, and an error raised.
Notes:
Comments:

		177444 	[Remote access] access test driver - bug 684848 	jyang 	yoyzhang 	Auto 		Feature 	P2 	2400 	Edit
Setup:

1.make sure your libvirtd is running.

2.login host with a non-root user.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access
    RHEL6.0
    virsh-rail

bug:

    837466 - From Run 42300

Actions:

Do make sure execute all the following operation with # virsh --connect URI instead of # virsh -c URI for another cycle.

1. connect to test driver via unix socket.

  # virsh -c test:///default

 

2. connect to test driver via plain tcp (refer to case "[libvirtd] remote access via plain tcp" to get how setup libvirtd listens on tcp connection) 

 # virsh -c test+tcp:///default

OR

 # virsh -c test+tcp://${IP}/default

 

3. connect to test driver via ssh

  # virsh -c test+ssh://${IP}/default

 

4. connect to test driver via tls (refer to case "[libvirtd] remote access via tls" to get how to setup libvirtd so that it listens on tls connection)

  # virsh -c test+tls://${IP}/default

OR

 # virsh -c test+tls:///default
	
Expected Results:

step 1:

[root@dhcp-66-70-128 autostart]# virsh -c test:///default
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh # quit

 

step 2:

[root@dhcp-66-70-128 autostart]# virsh -c test+tcp:///default
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh # quit

 

step 3:

[root@dhcp-66-70-128 autostart]# virsh -c test+ssh://10.66.70.128/default
root@10.66.70.128's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh #

 

step 4:

[root@dhcp-66-70-128 autostart]# virsh -c test+tls:///default
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh # quit
Notes:
Comments:

		177445 	[Remote access] access test driver - bug 684848 	jyang 	jyang 	Both 		Feature 	P2 	2410 	Edit
Setup:

1.make sure your libvirtd is running.

2.login host with a non-root user.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    remote access
    Regression

bug:

    No bug found

Actions:

Do make sure execute all the following operation with # virsh --connect URI instead of # virsh -c URI for another cycle.

1. connect to test driver via unix socket.

  # virsh -c test:///default

 

2. connect to test driver via plain tcp (refer to case "[libvirtd] remote access via plain tcp" to get how setup libvirtd listens on tcp connection) 

 # virsh -c test+tcp:///default

OR

 # virsh -c test+tcp://${IP}/default

 

3. connect to test driver via ssh

  # virsh -c test+ssh://${IP}/default

 

4. connect to test driver via tls (refer to case "[libvirtd] remote access via tls" to get how to setup libvirtd so that it listens on tls connection)

  # virsh -c test+tls://${IP}/default

OR

 # virsh -c test+tls:///default
	
Expected Results:

step 1:

[root@dhcp-66-70-128 autostart]# virsh -c test:///default
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh # quit

 

step 2:

[root@dhcp-66-70-128 autostart]# virsh -c test+tcp:///default
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh # quit

 

step 3:

[root@dhcp-66-70-128 autostart]# virsh -c test+ssh://10.66.70.128/default
root@10.66.70.128's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh #

 

step 4:

[root@dhcp-66-70-128 autostart]# virsh -c test+tls:///default
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh # quit
Notes:
Comments:

		177560 	[Snapshot] Revert a running domain to a running snapshot 	jyang 	yoyzhang 	Auto 		Feature 	P2 	2410 	Edit
Setup:

1. a running healthy guest, with only qcow2 disks
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    RHEL6.0

bug:

    No bug found

Actions:

1. create a snapshot

   # virsh snapshot-create foo snap1.xml

 

    toy-snapshot.xml is a file with contents:

    <domainsnapshot>

         <name>snap1</name>

         <description>snapshot API test - revert domain to a snapshot</description>

   </domainsnapshot>

 

2. wait till step 1 is finished

 

3. check if the snapshot is defined

   # virsh snapshot-list foo

 

4.  login into the guest, create two files in "/tmp"

    # cd /tmp

    # > aircraft

    # > airplane

 

5. add some strings to file "/tmp/airplane" and "/tmp/aircraft"

    # for i in {1..10000}; do echo "hello, girl" >> /tmp/airplane; echo "hi, man" >> /tmp/aircraft; done

 

6. create snapshot same as step1

 

7. get current snapshot name

    # virsh snapshot-current foo

 

8. revert to current snapshot

    # virsh snapshot-revert foo ${current_snapshot}

 

9. login into the guest, check whether file "/tmp/airplane" and "/tmp/aircraft" exists

    # ls -l /tmp/airplane /tmp/aircraft

 

10. create another snapshot, same as step 1. just need modify the value of "name" and description.

 

11. login into guest, do some operations. such as create a directory

    # cd /tmp

    # mkdir snapshot-test

    # cd snapshot

    # > I_like_the_girl

 

12. create a another snapshot, same as step 9

 

13. revert to snapshot we created in step 9.

    # virsh snapshot-revert foo ${snapshot_created_in_step9}

 

14. login into guest, check whether directory "/tmp/snapshot-test" exists

 

15. revert to snapshot we created in step 11

    # virsh snapshot-revert ${snapshot_created_in_step11}

 

16. login into the guest, check whether directory "/tmp/snapshot-test" exists, also "I_like_the_girl", :)
	
Expected Results:

step 8:

       exists

       lines: 10001

       line of "airplane": hello, girl

       line of "aircraft": hi, man

 

step 13:

       not exist

 

step 14:

       exists. also "I_like_the_girl".  (right, I like the girl very much. |0|)
Notes:
after disk-only snapshot can revert , we will update this case to case running snapshot revert
Comments:

		177446 	[Remote access] Check connect with UNIX sockets with polkit 	yoyzhang 	None 	Auto 		Feature 	P2 	2420 	Edit
Setup:

Install package polkit
# rpm -q polkit
polkit-0.96-2.el6.x86_64

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access

bug:

    No bug found

Actions:

1. Edit /etc/libvirt/libvirtd.conf
auth_unix_rw = "polkit"
unix_sock_rw_perms = "0777"

2. Create 2 test users

   # useradd test1
   # useradd test2

3. change the password of "test1" and "test2" to "redhat"
   # passwd test1
   # passwd test2

4. Config polkit auth to 'test1' as rw and 'test2' as ro

# cd /etc/polkit-1/localauthority/50-local.d/
# vim polkit.pkla
[Allow test1 libvirt management permissions]
Identity=unix-user:test1
Action=org.libvirt.unix.manage
ResultAny=yes
ResultInactive=yes
ResultActive=yes

[Allow test2 libvirt monitor permissions]
Identity=unix-user:test2
Action=org.libvirt.unix.monitor
ResultAny=yes
ResultInactive=yes
ResultActive=yes

5. # service libvirtd restart

6. Check test1 has rw permission

[test1@dhcp-65-132 root]$ virsh -c qemu+unix:///system

7. Check test2 doesn't has rw permssion

[test2@dhcp-65-132 root]$ virsh -c qemu+unix:///system

8. Check test2 has ro permission
[test2@dhcp-65-132 root]$ virsh -c qemu+unix:///system --readonly

	
Expected Results:

6. test1 has rw permission

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # 

7. test2 doesn't have rw perssion 

error: authentication failed
error: failed to connect to the hypervisor


8. test2 has ro permission

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh >

Notes:
Comments:

		177447 	[Remote access] Check connect with UNIX sockets with SASL 	yoyzhang 	None 	Auto 		Feature 	P2 	2430 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access
    QE consumption

bug:

    No bug found

Actions:

1. Edit /etc/libvirt/libvirtd.conf

auth_unix_rw = "sasl" 
auth_unix_ro = "none"

2. Restart libvirtd service

# service libvirtd restart

3. Generate a username/passwd

# echo redhat | saslpasswd2 -p -a libvirt tester

4. Connect to read-write Unix socket

# virsh -c qemu+unix:///system

5. Connect to read-only Unix socket

# virsh -c qemu+unix:///system --readonly

 

	
Expected Results:

4. SASL auth is asked and after confirmed correct, connection is established successfully

# virsh -c qemu+unix:///system
Please enter your authentication name: tester
Please enter your password: 
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # 

5. No SASL auth is required and connection can be established directly

# virsh -c qemu+unix:///system --readonly
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh > 

Notes:
Comments:

		177448 	[Remote access] Check connect with UNIX sockets without auth check 	yoyzhang 	None 	Auto 		Feature 	P2 	2440 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access
    virsh-rail

bug:

    No bug found

Actions:

1. Make sure unix socket is set as none on  host

auth_unix_ro = "none"
 auth_unix_rw = "none"

2. Restart libvirtd server to make it effect

# service libvirtd restart

3. On host, connect to  read write unix socket

# virsh -c qemu+unix:///system

4. On host, connect to  read only unix socket

# virsh -c qemu+unix:///system --readonly

	
Expected Results:

3. Could connect to server host without auth required

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
 17 win2008              running
 18 rhel6                running

virsh # shutdown win2008
Domain win2008 is being shutdown

virsh #

4. Could connect to server host without auth required

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh > list --all
 Id Name                 State
----------------------------------
 18 rhel6                running
  - win2008              shut off

virsh >

Notes:
Comments:

		177449 	[Remote access] Connect to a non-kvm hypervisor 	yoyzhang 	None 	Manual 		Feature 	P2 	2450 	Edit
Setup:

- 1 host installed with kvm kernel

- libvirtd service is running

2.prepare one machine with XEN and one machine with ESX.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    remote access

bug:

    No bug found

Actions:

1. onnect to local test driver

# virsh -c test+ssh:///default

2. connect to a remote xen driver

# virsh -c xen+ssh://10.66.72.23/

3. connect to a remote ESX driver

# virsh -c esx://10.66.6.211/?no_verify=1 

(root   123qweP)
	
Expected Results:

1.should connect successfully

root@localhost's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  1 test                 running

virsh #

2. should connect successfully


The authenticity of host '10.66.93.220 (10.66.93.220)' can't be established.
RSA key fingerprint is 57:02:24:ab:64:13:d7:58:ec:4b:2b:c4:1e:9a:0c:56.
Are you sure you want to continue connecting (yes/no)? yes
root@10.66.93.220's password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  0 Domain-0             running
  - rhel56-64-encryptsystem shut off

virsh #

3. Should connect to ESX successfully
Notes:
Comments:

		177450 	[Remote access] Connect to the hypervisor on host using a plain TCP connection without SASL via ipv4 	yoyzhang 	None 	Auto 		Feature 	P1 	2460 	Edit
Setup:

- 2 host installed with kvm kernel
- libvirtd service is running

Bakc up config file

 # cp /etc/sysconfig/libvirtd /tmp

# cp /etc/libvirt/libvirtd.conf /tmp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virsh-rail
    RHEL6.0
    remote access

bug:

    No bug found

Actions:

On host A
1. Edit /etc/sysconfig/libvirtd
       LIBVIRTD_ARGS="--listen"

2. Edit /etc/libvirt/libvirtd.conf
       listen_tls = 0
       listen_tcp=1
       auth_tcp="none"

3. # service libvirtd restart

 On host B
4. # virsh -c qemu+tcp://10.66.70.159/system
          (10.66.70.159 is host A's ip)
	
Expected Results:

4. Connect to host B successfully without auth require

Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #
Notes:
Comments:

		177631 	[Stable guest ABI] Update lower version qemu-kvm with Linux domain 	jialiu 	None 	Manual 		Feature 	P3 	2480 	Edit
Setup:

The work required is limited to QEMU and libvirt.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Ensure that a little lower version qemu-kvm is installed.

2. Install a linux guest using virt-install or virt-manager.

3. Check the XML configuration of the guest using virsh dumpxml

4.  Note all the pci address allocation using lspci command, then shutdown the guest.

5. Upgrade qemu-kvm to the latest version.

6. Start the guest again.

7. Check the XML configuration of the guest using virsh dumpxml again.

8. Note all the pci address allocation using lspci command again, and compare the pci address with step 4.

	
Expected Results:

1.

2. Guest is installed successfully.

3.  Verify the machine property of the <os><type> element is rhel6.2.0.

4.

5.

6. The guest is working fine.

7. The machine property of the <os><type> element is still rhel6.2.0.

8. All the pci address allocation is not chaged.
Notes:
Comments:

		177628 	[Stable guest ABI] Stable PCI addresses when adding devie for linux guest 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P3 	2500 	Edit
Setup:

The work required is limited to QEMU and libvirt.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Start a linux guest with one PCI NICs (such as: 8139 model)

2. Note the PCI slot numbers of the NIC in guest using lspci command.

3. Shutdown the guest

4. Add the 2th NIC (such as: e1000 model) and start the guest
	
Expected Results:

1.

2.

3.

4. Check that the slot number of the 1th NIC hasn't changed, and guest is working fine.
Notes:
Comments:

		177685 	[Supported hypervisors] KVM Xen VMware (LXC) 	nzhang 	None 	Manual 		Feature 	P1 	2520 	Edit
Setup:

 *  ESX SSL certificate:
    Generate new SSL certificates signed by a CA known to your client computer and replace the original ones on your ESX server. See the section Replace a Default Certificate with a CA-Signed Certificate  in the ESX Configuration Guide

http://libvirt.org/drvesx.html

http://pubs.vmware.com/vsphere-50/topic/com.vmware.vsphere.security.doc_50/GUID-A261E6D8-03E4-48ED-ADB6-473C2DAAB7AD.html

http://pubs.vmware.com/vsphere-50/index.jsp?topic=/com.vmware.vsphere.solutions.doc_50/GUID-37AAEDFE-EF2E-45FC-B0C6-44841E4FB302.html

Our virsh use libcurl:

http://curl.haxx.se/docs/sslcerts.html

* ESX server IP:

10.66.91.100 or 10.66.72.124 or 10.66.72.149(pass: 123qwe!@#)

ESX ENV:
# cd /etc/pki/CA/
# mkdir {certs,crl,newcerts}
# touch index.txt
# echo 00 > serial

create private key:
[root@zheng CA]# openssl req -new -x509 -extensions v3_ca -keyout myroot.key -out myroot.crt -days 3650
Generating a 2048 bit RSA private key
................................................................+++
...............................................+++
writing new private key to 'myroot.key'
Enter PEM pass phrase:
Verifying - Enter PEM pass phrase:
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:CN
State or Province Name (full name) []:BEIJING
Locality Name (eg, city) [Default City]:BEIJING
Organization Name (eg, company) [Default Company Ltd]:REDHAT
Organizational Unit Name (eg, section) []:QE
Common Name (eg, your name or your server's hostname) []:10.66.6.209
Email Address []:

[root@zheng CA]# mv myroot.key private/cakey.pem
[root@zheng CA]# mv myroot.crt cacert.pem

2, create private key and certificate request file for ESXi5.0 server.
# openssl req -new -nodes -out mycsr.csr
Generating a 2048 bit RSA private key
........+++
...............+++
writing new private key to 'privkey.pem'
-----
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [XX]:CN
State or Province Name (full name) []:BEIJING
Locality Name (eg, city) [Default City]:BEIJING
Organization Name (eg, company) [Default Company Ltd]:REDHAT
Organizational Unit Name (eg, section) []:QE
Common Name (eg, your name or your server's hostname) []:10.66.6.211
Email Address []:

Please enter the following 'extra' attributes
to be sent with your certificate request
A challenge password []:
An optional company name []:


# mv privkey.pem rui.key
3,scp the certificate request file to CA and certificate it.
[root@zheng CA]# openssl ca -out rui.crt  -infiles mycsr.csr
Using configuration from /etc/pki/tls/openssl.cnf
Enter pass phrase for /etc/pki/CA/private/cakey.pem:
Check that the request matches the signature
Signature ok
Certificate Details:
        Serial Number: 0 (0x0)
        Validity
            Not Before: Mar  5 06:53:52 2012 GMT
            Not After : Mar  5 06:53:52 2013 GMT
        Subject:
            countryName               = CN
            stateOrProvinceName       = BEIJING
            organizationName          = REDHAT
            organizationalUnitName    = QE
            commonName                = 10.66.6.211
        X509v3 extensions:
            X509v3 Basic Constraints:
                CA:FALSE
            Netscape Comment:
                OpenSSL Generated Certificate
            X509v3 Subject Key Identifier:
                84:ED:53:00:56:7B:F3:AD:69:70:44:8C3:09:A0:6E:9D:69:30:0A
            X509v3 Authority Key Identifier:
                keyid:E5:FC:AC:8B:C4:6EDF:32:19:E3:C1:17:3E:08:5B:7D:0D:79D

Certificate is to be certified until Mar  5 06:53:52 2013 GMT (365 days)
Sign the certificate? [y/n]:y


1 out of 1 certificate requests certified, commit? [y/n]y
Write out database with 1 new entries
Data Base Updated

4, change the ESXi to maintance mode and scp ssl keys(rui.crt rui.key) to /etc/vmware/ssl. restart hostd server
   then quit the maintance mode.

5, import cacert in client:

certutil -d sql:/etc/pki/nssdb -A -t TC -n "esx" -i /root/cacert.pem

6, check if it works well:
curl https://10.66.6.211/ --cacert cacert.pem  ---> will show https web page
curl https://10.66.6.211/sdk --cacert cacert.pem ---> will not show error info
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    supported hypervisors
    RHEL6.0

bug:

    864384 - From Run 47366
    873538 - From Run 49806

Actions:

It's important to conduct the define/create/start/destroy on the local and remote connection in this case.

1. Connect KVM hypervisor and issue basic operation (remote host os: RHEL5 and RHEL6).

# virsh -c qemu:///system

# virsh -c qemu:///system list

# virsh -c qemu:///system define [guestxml]

# virsh -c qemu:///system create [guestxml]

# virsh -c qemu:///system start [guestname]

# virsh -c qemu:///system destroy [guestname]

AND

# virsh -c qemu+ssh://remote_hostip/system

# virsh -c qemu+ssh://remote_hostip/system list

# virsh -c qemu+ssh://remote_hostip/system define [guestxml]

# virsh -c qemu+ssh://remote_hostip/system create [guestxml]

# virsh -c qemu+ssh://remote_hostip/system start [guestname]

# virsh -c qemu+ssh://remote_hostip/system destroy [guestname]

 

2. Connect Xen hypervisor and issue basic operation (remote host os: RHEL5).

# virsh -c xen+ssh://remote_hostip

# virsh -c xen+ssh://remote_hostip list

# virsh -c xen+ssh://remote_hostip define [guestxml]

# virsh -c xen+ssh://remote_hostip create [guestxml]

# virsh -c xen+ssh://remote_hostip start [guestname]

# virsh -c xen+ssh://remote_hostip destroy [guestname]

 

3. Connect Vmware ESX and issue basic operation. (remote host os: windows).

# virsh -c esx://10.66.91.100/?no_verify=1 

[passwd: 123qwe!@#]

# virsh -c esx://10.66.91.100/?no_verify=1  list

[passwd: 123qwe!@#]

# virsh -c esx://10.66.91.100/?no_verify=1  define [guestxml]

[passwd: 123qwe!@#]

# virsh -c esx://10.66.91.100/?no_verify=1  create [guestxml]

[passwd: 123qwe!@#]

# virsh -c esx://10.66.91.100/?no_verify=1  start [guestname]

[passwd: 123qwe!@#]

# virsh -c esx://10.66.91.100/?no_verify=1  stop [guestname]
[passwd: 123qwe!@#]

AND

Connection to ESX with a valid SSL certificate

 # virsh -c esx://10.66.91.100/

# virsh -c esx://10.66.91.100/ list

# virsh -c esx://10.66.91.100/ define [guestxml]

# virsh -c esx://10.66.91.100/ create [guestxml]

# virsh -c esx://10.66.91.100/ start [guestname]

# virsh -c esx://10.66.91.100/ stop [guestname]

 

4.

Connect  LXC and issue basic operation (remote host os: RHEL5 and RHEL6).

# virsh -c lxc:///

# virsh -c lxc:/// list

# virsh -c lxc:/// define [guestxml]

# virsh -c lxc:/// create [guestxml]

# virsh -c lxc:/// start [guestname]

# virsh -c lxc:/// destroy [guestname]

AND

# virsh -c lxc+ssh://remote_hostip/

# virsh -c lxc+ssh://remote_hostip/ list

# virsh -c lxc+ssh://remote_hostip/ define [guestxml]

# virsh -c lxc+ssh://remote_hostip/ create [guestxml]

# virsh -c lxc+ssh://remote_hostip/ start [guestname]

# virsh -c lxc+ssh://remote_hostip/ destroy [guestname]
	
Expected Results:

Verify all local and remote connections and basic operations is successful without no error.
Notes:
Comments:

		177698 	[sVirt] Guest on OS with selinux disabled 	jialiu 	None 	Manual 		Feature 	P2 	2540 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt
    RHEL6.0

bug:

    No bug found

Actions:

1. Edit /etc/selinux/config and change the SELINUX line to SELINUX=disabled;

2. Reboot the OS.

3. Check the selinux is disabled:

# getenforce

4. Start a VM using virsh start command.

5. Check the vm image and qemu-kvm process
	
Expected Results:

1.

2.

3.

# getenforce
Disabled

4. VM is started successfully.

5. qemu-kvm process has no any security context, it is empty

# ps -efZ|grep kvm
-                               qemu      2050     1 55 22:38 ?        00:00:10 /usr/libexec/qemu-kvm -S -M pc-0.12 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name xx -uuid 92bc0bea-f3d5-1def-f662-b37e62449ce0 -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/xx.monitor,server,nowait -mon chardev=monitor,mode=control -boot c -drive file=/mnt/jialiu/rhel5u4_kvm.img,if=none,id=drive-ide0-0-0,boot=on -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -device rtl8139,vlan=0,id=net0,mac=52:54:00:27:29:58,bus=pci.0,addr=0x4 -net tap,fd=18,vlan=0,name=hostnet0 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device i6300esb,id=watchdog0,bus=pci.0,addr=0x5 -watchdog-action pause -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3
Notes:
Comments:

		177720 	[sVirt] shared disks with <shareable/> usability 	jialiu 	yoyzhang 	Auto 		Feature 	P2 	2560 	Edit
Setup:


Make sure the selinux is "Enforcing"
# getenforce 
Enforcing


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Use qemu-img or virt-manager to create an image file named "c.img"

2. Prepare two vm, named a and b.

3. Attach the image file to the two vm as the second block disk.

    Seperately add the following into domain config xml:

    <disk type='file' device='disk'>
      <source file='/var/lib/libvirt/images/c.img'/>
      <target dev='hdb' bus='ide'/>
      <shareable/>
    </disk>

   NOTE: '/var/lib/libvirt/images/c.img' is my created image file.

shared disks with the <shareable/> flag can be shared between multiple
guests, this flag has a label allowing multiple guest access.

4. Start the VM a.

5. In VM a, fdisk, mkfs, and mount the newly added disk

# fdisk /dev/hdb

# partprobe

# mkfs.ext3 /dev/hdb1

# mount /dev/hdb1 /mnt

6. start VMb, and log into VM b, do the following command:

# partprobe

# ls /dev/hdb1

(Make sure the hdb1 is seen)

# mount /dev/hdb1 /mnt

Mount the second disk seperately in the two guest.

7. Check the label of the second disk
	
Expected Results:

1.
# ll -Z /var/lib/libvirt/images/
-rw-------. root root system_u:object_r:virt_image_t:s0 c.img
(Here, I create it via virt-manager)

2.

3. The second disk is attached successfully.

4. The two VMs are started successfully.

5. In guest a, the second disk is mounted successfully.

6. In guest b, the second disk is also mounted successfully.

7. The image file is labeled with "svirt_image_t", and owner is "qemu"

# ll -Z /var/lib/libvirt/images/
-rw-------. qemu qemu system_u:object_r:svirt_image_t:s0 c.img
Notes:
Comments:

		177753 	[Virtio-serial] Create a channel with pty-virtserialport type 	nzhang 	None 	Manual (Autoproposed) 		Feature 	P2 	2600 	Edit
Setup:

Both the host and guest os should be rhel6.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtio-serial

bug:

    No bug found

Actions:

1, Add the following XML into <devices> tag for a domain:

    <controller type='virtio-serial' index='0'/>
        <channel type='pty'>
          <target type='virtio' name='org.linux-kvm.port.1'/>
    </channel>

Note:    The sippnet corresponds to the following qemu command line.

    -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x6
    -chardev pty,id=channel0
    -device virtserialport,bus=virtio-serial0.0,nr=0,chardev=channel0,name=org.linux-kvm.port.1



2, Start the guest, using "virsh dumpxml $domain-name" to determine the which pty the domain use,
    like the the example, the pty is /dev/pts/6 in the host.
      
    # virsh dumpxml rhel6

    <channel type='pty'>
      <source path='/dev/pts/6'/>
      <target type='virtio' name='org.linux-kvm.port.1'/>
          <alias name='channel0'/>
          <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>

    In the host to receive the data:
    
        # cat /dev/pts/6
    
     In the guest, send data:
        
        # echo a > /dev/vport0p0   
     and the host received the data.

    In the guest, receive the data:
        
        # cat /dev/vport0p0
  In the host to send out a data:
  
        # echo "This is from host" > /dev/pts/6
       the guest can receive the data

 
	
Expected Results:

Check all the steps are correct.

Notes:
Comments:

		177005 	[Host network interface management] Define a bond interface that contains two ethernets 	yimwang 	None 	Manual 		Feature 	P2 	2620 	Edit
Setup:

A host with 2 ethernet, eth0 and eth1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. Prepare the following xml:

<interface type='bond' name='bond0'>
   <start mode='none'/>
   <protocol family='ipv4'>
    <dhcp/>
   </protocol>
  <bond mode='active-backup'>
    <miimon freq='100' updelay='10' carrier='ioctl'/>
    <interface type='ethernet' name='eth0'>

</interface>
    <interface type='ethernet' name='eth1'>
    </interface>
   </bond>
</interface>

2. Define new bond0

# virsh iface-define bond0.xml

# virsh iface-start bond0

3. Check the new bond0

3.1 # virsh iface-dumpxml bond0

3.2 # cat /etc/syconfig/network-script/ifcfg-bond0

3.3 # ifconfig bond0

3.4. # ping google.com -I bond0
	
Expected Results:

3. 1

<interface type='bond' name='bond0'>
  <protocol family='ipv4'>
    <ip address='10.66.93.39' prefix='23'/>
  </protocol>
  <protocol family='ipv6'>
    <ip address='fe80::21b:21ff:fe39:8b18' prefix='64'/>
  </protocol>
  <bond>
    <interface type='ethernet' name='eth0'>
      <mac address='00:1b:21:39:8b:18'/>
    </interface>
    <interface type='ethernet' name='eth1'>
      <mac address='00:1b:21:39:8b:18'/>
    </interface>
  </bond>
</interface>

3.2

# cat /etc/sysconfig/network-scripts/ifcfg-bond0
DEVICE=eth0
DEVICE=eth1
ONBOOT=no
BOOTPROTO=dhcp

3.3 # ifconfig bond0
bond0     Link encap:Ethernet  HWaddr 00:1B:21:39:8B:18  
          inet addr:10.66.93.39  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: fe80::21b:21ff:fe39:8b18/64 Scope:Link
          UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
          RX packets:2088 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1393 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:1086688 (1.0 MiB)  TX bytes:241970 (236.2 KiB)

3.4 # ping google.com -I bond0
PING google.com (72.14.204.103) from 10.66.93.39 bond0: 56(84) bytes of data.
64 bytes from iad04s01-in-f103.1e100.net (72.14.204.103): icmp_seq=1 ttl=43 time=319 ms
64 bytes from iad04s01-in-f103.1e100.net (72.14.204.103): icmp_seq=2 ttl=43 time=289 ms
Notes:
Comments:

		177756 	[Virtio-serial] Open multiple channels at same time 	nzhang 	yoyzhang 	Manual 		Feature 	P2 	2630 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtio-serial

bug:

    No bug found

Actions:

1. Add the following XML to guest domain.

    <controller type='virtio-serial' index='0'/>
    <controller type='virtio-serial' index='1'/>
    <channel type='pty'>
      <target type='virtio' name='org.linux-kvm.port.0'/>
    </channel>
    <channel type='pty'>
      <target type='virtio' name='org.linux-kvm.port.1'/>
    </channel>

2. Boot the guest domain.

3. Use"virsh dumpxml $domain-name" to determine the which pty the domain use,

    like the the example, the pty is /dev/pts/2 and /dev/pts/3 in the host.
      
    # virsh dumpxml rhel6

    <channel type='pty'>
      <source path='/dev/pts/2'/>
      <target type='virtio' name='org.linux-kvm.port.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <channel type='pty'>
      <source path='/dev/pts/3'/>
      <target type='virtio' name='org.linux-kvm.port.1'/>
      <alias name='channel1'/>
      <address type='virtio-serial' controller='0' bus='0' port='2'/>
    </channel>

 4.   In the host to receive the data:
    
        # cat /dev/pts/2

        # cat /dev/pts/3
  


 5.    In the guest, send data:
        
        # echo a > /dev/vport0p1
        # echo b > /dev/vport0p2
       step4 will recieve "a" and "b"
    

6.    In the guest, receive the data:
        
        # cat /dev/vport0p1
        This a is from host
        # cat /dev/vport0p2
        This b is from host


7.    In the host to send out a data:
        
        # echo "This a is from host" > /dev/pts/2
        # echo "This b is from host" > /dev/pts/3

     In the guest, receive the data
        This a is from host

        This b is from host
	
Expected Results:
Notes:
Comments:

		177012 	[Host network interface management] define a bridge that uses an ethernet interface 	yimwang 	None 	Manual 		Feature 	P2 	2640 	Edit
Setup:




	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    host network interface management

bug:

    No bug found

Actions:

A) Define a bridge that uses an ethernet interface that is already defined & up

1.Create bridge XML.
#cat bridge.xml
<interface type='bridge' name='br0'>
  <start mode='onboot'/>
  <mtu size='1500'/>
  <protocol family='ipv4'>
    <dhcp/>
  </protocol>
  <bridge stp='off' delay='0.01'>
    <interface type='ethernet' name='eth0'>
    </interface>
  </bridge>
</interface>

2.Define the bridge
# virsh iface-define bridge.xml
Interface br0 defined from bridge.xml

# virsh iface-list --inactive
Name                 State      MAC Address
--------------------------------------------
br0                  inactive    

3.start the bridge.
# virsh iface-start br0


 B):Define a bridge that uses an ethernet that *isn't* already defined & up.
Steps:

1.  .Create bridge XML.
#cat bridge1.xml
<interface type='bridge' name='br1'>
  <start mode='onboot'/>
  <mtu size='1500'/>
  <protocol family='ipv4'>
    <dhcp/>
  </protocol>
  <bridge stp='off' delay='0.01'>
    <interface type='ethernet' name='eth1'>
    </interface>
  </bridge>
</interface>

2.Define the bridge

# virsh iface-define bridge1.xml
Interface br1 defined from bridge1.xml

3.start and define bridge.
# virsh iface-start br1

	
Expected Results:

A):

step3

bridge is defined and started successfully

Verify the bridge could get dhcp ip via eth0 and ping to google.com

B:)

bridge is defined and started successfully

Verify the bridge could get dhcp ip via eth0 and ping to google.com

 

 

should not see the following error while start the bridge interface.

error: Failed to start interface br0
error: internal error failed to create (start) interface br0: failed to execute external program - Running 'ifup br0' failed with exit code 1:
Determining IP information for br0...PING 10.66.7.254 (10.66.7.254) from 10.66.7.230 br0: 56(84) bytes of data.


Notes:
Comments:

		177434 	[PCI and USB devices assignment] Hot/Cold-plug a PCI device without FLR capability to guest -- bug 773667 	ajia 	ajia 	Manual 		Feature 	P1 	2640 	Edit
Setup:

Host supports VT-d

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

As usual, you require a Broadcom NetExtreme II with dual ports, for example:
# lspci
01:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
01:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
02:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
02:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)

You may find this kind of NICs in virtlab such as intel-e5620-12-2.englab.nay.redhat.com, intel-e5530-8-2.englab.nay.redhat.com etc.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

The following is a hot-plug the PCI example, you may try cold-plug case, they are similar:

1. make sure the guest foo is running, if you're trying hot-plug.

# virsh list --all
 Id Name                 State
----------------------------------
 11 foo                  running

2. check PCI devices which will be assigned to guest and orignial device driver.

# lspci

01:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
01:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
02:00.0 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)
02:00.1 Ethernet controller: Broadcom Corporation NetXtreme II BCM5709 Gigabit Ethernet (rev 20)


# readlink -f /sys/bus/pci/devices/0000\:02\:00.0/driver
/sys/bus/pci/drivers/bnx2


# readlink -f /sys/bus/pci/devices/0000\:02\:00.1/driver
/sys/bus/pci/drivers/bnx2

3. construct device assignment xml.
# cat pci_dev.xml

    <hostdev mode="subsystem" type="pci" managed="yes">
        <source>
            <address domain="0x0000" bus="0x02" slot="0x00" function="0x0"/>
        </source>
    </hostdev>

Notes, assign "02:00.0" to guest.

4.  need to detach second pci device on the same bus due to the NICs lacks FLR capability.
# virsh nodedev-dettach pci_0000_02_00_1
Device pci_0000_02_00_1 dettached

5. check device driver change
# readlink -f /sys/bus/pci/devices/0000\:02\:00.1/driver
/sys/bus/pci/drivers/pci-stub

Notes, the change is introduced by step 4 operation, and make sure the pci is hided by pci-stub drvier.

# readlink -f /sys/bus/pci/devices/0000\:02\:00.0/driver
/sys/bus/pci/drivers/bnx2

6. hot-plug the pci device to guest
# virsh attach-device foo pci_dev.xml
Device attached successfully

7. check device driver change
# readlink -f /sys/bus/pci/devices/0000\:02\:00.0/driver
/sys/bus/pci/drivers/pci-stub

Notes, make sure the pci is hided by pci-stub drvier.

8. check <hostdev>...</hostdev> element block in guest xml
# virsh dumpxml foo
<domain type='kvm' id='2'>
  <name>foo</name>
......
    <hostdev mode='subsystem' type='pci' managed='yes'>
      <source>
        <address domain='0x0000' bus='0x02' slot='0x00' function='0x0'/>
      </source>
      <alias name='hostdev0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </hostdev>
......
</domain>

9. log in guest and make sure the pci device works well, for example:

# ping g.cn

10. destroy the guest and make sure the pci device with 'managed' mode is automatically returned to host

virsh destroy foo
Domain foo destroyed

# readlink -f /sys/bus/pci/devices/0000\:02\:00.0/driver
/sys/bus/pci/drivers/bnx2

Notes, the guest has original device driver.

11. need to manually return the second pci deivce to host.

# readlink -f /sys/bus/pci/devices/0000\:02\:00.1/driver
/sys/bus/pci/drivers/pci-stub

# virsh nodedev-reattach pci_0000_02_00_1
Device pci_0000_02_00_1 re-attached

# readlink -f /sys/bus/pci/devices/0000\:02\:00.1/driver
/sys/bus/pci/drivers/bnx2


	
Expected Results:

All of steps are successfully run and without resource leaks(using valgrind to check each steps).

Notes:

This NetExtreme card does not play very nice with virtualization. One one hand, a since PCI card
adds 2 PCI devices onto slot (each with different PCI function number). But on the other hand, it 
completely lacks FLReset; Therefore if one wants to add the first PCI device, he has to 'virsh nodedev-detach' 
the second one and vice versa in order to allow secondary bus reset.

 
Notes:
Comments:

		177418 	[PCI and USB device assignment] Hot/cold-plug a PCI device to guest on Intel VT/AMD-V + non-VT-d/non-IOMMU host-bug818055 	ajia 	None 	Auto 		Feature 	P1 	2650 	Edit
Setup:

This bug is close as a NOTABUG now, but I still need to test this kind of negative scenario.

 

Intel host supports VT-d

On kernel option of boot list, do not add "intel_iommu=on"

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

NOTE: After the bug https://bugzilla.redhat.com/show_bug.cgi?id=616415 QMP: does not report the real cause of PCI device assignment failure fixed, we should change the expect result step 6 output
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    818055

Actions:

1. # virsh nodedev-list --tree

2. # virsh nodedev-dumpxml pci_0000_00_19_0

3. Edit the following code as nodedev.xml


          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='0' slot='25' function='0'/>
            </source>
          </hostdev>
       

4. # virsh define <guest>.xml

5. # virsh start <guest>

6. # virsh attach-device <guest> nodedev.xml

7. Destroy the guest and add the above xml into guest xml

8. for i in {1..100}; do virsh start <guest>; done
	
Expected Results:

Step 6:

will report error

error: Failed to attach device from nodedev.xml
error: internal error unable to execute QEMU command 'device_add': Device 'pci-assign' could not be initialized

Step 8:

error: Failed to start domain manu-install
error: internal error process exited while connecting to monitor: char device redirected to /dev/pts/1
No IOMMU found.  Unable to assign device "hostdev0"
qemu-kvm: -device pci-assign,host=00:19.0,id=hostdev0,configfd=33,bus=pci.0,addr=0x7: Device 'pci-assign' could not be initialized

In addition, PCI device should reattach to host automatically if attach-device failed with "managed=yes"

 OR the result is the following error:

error: Failed to start domain  guest

error: internal error process exited while reading console log output: char device redirected to /dev/pts/3 No IOMMU found. Unable to assign device "hostdev0" qemu-kvm: -device pci-assign,host=00:19.0,id=hostdev0,configfd=33,bus=pci.0,addr=0x7: Device 'pci-assign' could not be initialized

 
Notes:
Comments:

		177017 	[Host network interface management] Define an ethernet with both static IPv4 and IPv6 address 	jialiu 	None 	Manual 		Feature 	P2 	2660 	Edit
Setup:

1. save the config file of eth0

  # cp /etc/sysconfig/network-scritps/ifcfg-eth0 /tmp

2. Before destroy eth0, should stop NetworkManager

 # service NetworkManger stop

3. After the following test step is finished, please copy the ifcfg-eth0 back, and restart network to restore netowrk.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. Prepare the following xml:

<interface type='ethernet' name='eth0'>
  <start mode='onboot'/>
  <protocol family='ipv4'>
    <ip address='10.66.4.54' prefix='23'/>
  </protocol>
  <protocol family='ipv6'>
    <ip address='3ffe::200' prefix='64'/>
  </protocol>
</interface>

2. Destroy and undefine the orignal interface.

# virsh iface-destroy eth0

# virsh iface-undefine eth0

3. Define new eth0 with static IPV4 and IPv6 address

# virsh iface-define new-eth0-ipv6.xml

# virsh iface-start eth0

4. Check the new eth0

# virsh iface-dumpxml eth0

# cat /etc/syconfig/network-script/ifcfg-eth0

# ifconfig eth0
	
Expected Results:

3. Command is run successfully.

4. ping other host in the same sub-net successfully, and ifcfg-eth0 show that the eth0 is using both static IPV4 and IPv6 address

DEVICE=eth0
ONBOOT=yes
BOOTPROTO=none
IPADDR=10.66.4.54
NETMASK=255.255.254.0
IPV6INIT=yes
IPV6_AUTOCONF=no
DHCPV6=no
IPV6ADDR=3ffe::200/64

 

# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A7:1F:4D  
          inet addr:10.66.4.54  Bcast:10.66.5.255  Mask:255.255.254.0
          inet6 addr: 3ffe::200/64 Scope:Global
          inet6 addr: fe80::225:64ff:fea7:1f4d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:572928 errors:0 dropped:0 overruns:0 frame:0
          TX packets:107933 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:819328581 (781.3 MiB)  TX bytes:8142591 (7.7 MiB)
          Interrupt:21 Memory:febe0000-fec00000

# ping 10.66.4.205
PING 10.66.4.205 (10.66.4.205) 56(84) bytes of data.
64 bytes from 10.66.4.205: icmp_seq=1 ttl=64 time=1.34 ms
64 bytes from 10.66.4.205: icmp_seq=2 ttl=64 time=0.210 ms

....

 

# ping6 3ffe::201
PING 3ffe::201(3ffe::201) 56 data bytes
64 bytes from 3ffe::201: icmp_seq=1 ttl=64 time=2.02 ms
64 bytes from 3ffe::201: icmp_seq=2 ttl=64 time=0.202 ms

...

 
Notes:
Comments:

		177419 	[PCI and USB device assignment] Hot/Cold-plug multiple PCI devices to guest 	ajia 	None 	Auto 		Feature 	P1 	2660 	Edit
Setup:

- find a machine with 2 or more NICs

- VT-d is enabled

- Could lose connection with host after dettach network device from host

 For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. check NICs

# lspci |grep -i eth

01:00.0 Ethernet controller: Broadcom Corporation NetXtreme BCM5764M Gigabit Ethernet PCIe (rev 10)
03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection (rev 01)

2. # virsh nodedev-list --tree

  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
......

  |   +- pci_0000_01_00_0
  |       |
  |       +- net_eth2_d8_d3_85_7e_61_9b

3. Find the bus and slot and function number of NICs

# virsh nodedev-dumpxml pci_0000_03_00_0

# virsh nodedev-dumpxml pci_0000_01_00_0

4. # cat nodedev-1.xml

          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='3' slot='0' function='0'/>
            </source>
          </hostdev>

# cat nodedev-2.xml        

  <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='1' slot='0' function='0'/>
            </source>
 </hostdev>

5. Define and start a normal guest

6. Do

# virsh attach-device guest nodedev-1.xml

# virsh attach-device guest nodedev-2.xml

7. login to guest, and check if this 2 nic can all work well

# lspci | grep -i eth

After config the ifcfg file and restart network, chek

# ping -I eth1 <host ip>

# ping -I eth2 <host ip>

8. Detach multi pci device from guest

# virsh detach-device guest nodedev-1.xml

# virsh detach-device guest nodedev-2.xml

then check the guest

9. Destroy the guest

10. Attach the above 2 parts of xml to the guest xml

11. Start the gust

12. login to guest, and check if this 2 nic can all work well
	
Expected Results:

Step 6

Devices attached successfully with no error

Step 7

Can see the other 2 nics info

All these 2 nic can get ip and can ping host successfully

Step 8

Devices detached successfully and on guest there is no these 2 nic info

Step 11

Guest can be started successfully with no error

Step 12

the same as step 7 shows
Notes:
Comments:

		177430 	[PCI and USB device assignment] tight loop of hotplug/unplug NIC to guest on normal host-- BZ#619455, BZ#822373, BZ#877095 	vbian 	None 	Auto 		Regression 	P1 	2670 	Edit
Setup:

1. Enable VT-d on your host
2. edit the /boot/grub/grub.conf like this
Add the kernel option 'intel_iommu=on'

default=0
timeout=0
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu
title Fedora (2.6.31.5-127.fc12.x86_64)
        root (hd0,0)
        kernel /vmlinuz-2.6.31.5-127.fc12.x86_64 ro
root=/dev/mapper/vg_intelx5550121-lv_root  LANG=en_US.UTF-8
SYSFONT=latarcyrheb-sun16 KEYBOARDTYPE=pc KEYTABLE=us intel_iommu=on rhgb
quiet
        initrd /initramfs-2.6.31.5-127.fc12.x86_64.img
3.Power off your testing machine ,then power on

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    868098 - From Run 47646
    868098 - From Run 49841
    868098 - From Run 51765
    868098 - From Run 54140

Actions:

1.Get your host NIC info
  # virsh nodedev-list --tree (For example , you can see the following info)
    +- pci_0000_00_19_0
  |   |
  |   +- net_eth0_00_1f_16_1e_27_f3

  # virsh nodedev-dumpxml pci_0000_00_19_0 (remember the bus, slot and function number , for example you got 0,25,0)

2.edit the vtd.xml for hotplug
  # cat vtd.xml
  <hostdev mode='subsystem' type='pci' managed='yes'>
    <source>
      <address bus='0x00' slot='0x19' function='0x00'/>
    </source>
  </hostdev>
3.run the following script:
  # cat script.sh
  #!/bin/sh
  for ((i=0; i < 300; i++))
  do
  echo $i
  virsh attach-device vm vtd.xml
  sleep 5
  virsh detach-device vm vtd.xml
  sleep 5
  done
  # sh script.sh

4. Meanwhile, open a new terminal then check if libvirtd leaks FDs:

# ll /proc/`pidof libvirtd`/fd

5.  Edit  the vtd.xml  and remove the " managed='yes'"

  # cat vtd.xml
  <hostdev mode='subsystem' type='pci' >
    <source>
      <address bus='0x00' slot='0x19' function='0x00'/>
    </source>
  </hostdev>

6.  Repeat  step3.
	
Expected Results:

1. you won't get Call Trace , or kernel panic during 300 times hotplug/unplug NIC

2. After running serveral loop of the step3 such as 20 times then you don't should see many

/sys/devices/pci0000:00/0000:00:19.0/config is refered on step4(see the following notes)

3.  After step6 ,  No the following error occurs.

error: Failed to detach device from vtd.xml
error: End of file while reading data: Input/output error

error: Failed to reconnect to the hypervisor
error: no valid connection
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': Connection refused


Notes, the following is a libvirtd FD leaks example:

# ll /proc/`pidof libvirtd`/fd
total 0
lr-x------. 1 root root 64 Nov 27 16:25 0 -> /dev/null
l-wx------. 1 root root 64 Nov 27 16:25 1 -> /dev/null
l-wx------. 1 root root 64 Nov 27 16:25 10 -> pipe:[118817327]
lrwx------. 1 root root 64 Nov 27 16:25 11 -> socket:[118817328]
lrwx------. 1 root root 64 Nov 27 16:25 12 -> socket:[118817349]
lrwx------. 1 root root 64 Nov 27 16:25 13 -> socket:[118817358]
lrwx------. 1 root root 64 Nov 27 16:25 14 -> socket:[118817490]
lrwx------. 1 root root 64 Nov 27 16:25 15 -> socket:[118817420]
l-wx------. 1 root root 64 Nov 27 16:25 16 -> /proc/mtrr
lrwx------. 1 root root 64 Nov 27 16:25 17 -> socket:[118817875]
lrwx------. 1 root root 64 Nov 27 16:25 18 -> /var/run/libvirt/network/nwfilter.leases
lrwx------. 1 root root 64 Nov 27 16:28 19 -> socket:[119746477]
l-wx------. 1 root root 64 Nov 27 16:25 2 -> /dev/null
lrwx------. 1 root root 64 Nov 27 16:28 20 -> socket:[119746478]
lrwx------. 1 root root 64 Nov 27 16:28 21 -> socket:[119746479]
lrwx------. 1 root root 64 Nov 27 16:28 22 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:28 24 -> socket:[119742094]
lrwx------. 1 root root 64 Nov 27 16:28 25 -> socket:[119741984]
lrwx------. 1 root root 64 Nov 27 16:28 28 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 29 -> /sys/devices/pci0000:00/0000:00:19.0/config
lr-x------. 1 root root 64 Nov 27 16:25 3 -> /dev/urandom
lrwx------. 1 root root 64 Nov 27 16:34 30 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 31 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 32 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 33 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 34 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 35 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 36 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 37 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 38 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 39 -> /sys/devices/pci0000:00/0000:00:19.0/config
l-wx------. 1 root root 64 Nov 27 16:25 4 -> /var/log/libvirt/libvirtd.log
lrwx------. 1 root root 64 Nov 27 16:34 40 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 41 -> /sys/devices/pci0000:00/0000:00:19.0/config
lrwx------. 1 root root 64 Nov 27 16:34 42 -> /sys/devices/pci0000:00/0000:00:19.0/config
l-wx------. 1 root root 64 Nov 27 16:25 5 -> /var/run/libvirtd.pid
lrwx------. 1 root root 64 Nov 27 16:25 6 -> socket:[118817451]
lr-x------. 1 root root 64 Nov 27 16:25 7 -> pipe:[118817326]
l-wx------. 1 root root 64 Nov 27 16:25 8 -> pipe:[118817326]
lr-x------. 1 root root 64 Nov 27 16:25 9 -> pipe:[118817327]

 

 
Notes:
Comments:

		177857 	[Watchdog device]Watchdog device "i6300esb" - pause 	jialiu 	yoyzhang 	Auto 		Feature 	P2 	2670 	Edit
Setup:

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    RHEL6.0
    virsh-rail
    watchdog device

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='i6300esb' action='pause'/> to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, check the watchdog dirvier is installed:

# dmesg | grep i6300

# /sbin/lsmod | grep i6300esb

# lspci

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm
17108 ?        Sl     0:19 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name rhel6 -uuid 2292db8e-9f74-ea3a-5fc4-2240a3f8534e -nodefconfig -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/rhel6.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -device virtio-serial-pci,id=virtio-serial0,max_ports=16,vectors=4,bus=pci.0,addr=0x5 -device virtio-serial-pci,id=virtio-serial1,bus=pci.0,addr=0x6 -drive file=/var/lib/libvirt/images/nfs_test.img,if=none,id=drive-ide0-0-0,boot=on,format=raw,cache=none -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=20,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:61:a6:9e,bus=pci.0,addr=0x3 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -chardev pty,id=channel0 -device virtserialport,bus=virtio-serial0.0,nr=0,chardev=channel0,name=org.linux-kvm.port.0 -chardev pty,id=channel1 -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=channel1,name=org.linux-kvm.port.1 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device i6300esb,id=watchdog0,bus=pci.0,addr=0x7 -watchdog-action pause  -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4
17255 pts/11   S+     0:00 grep qemu-kvm

4.The oupt is similar to the following:

# dmesg | grep i6300
i6300ESB timer: initialized (0xffffc20000036000). heartbeat=30 sec (nowayout=0)
# /sbin/lsmod | grep i6300esb
i6300esb     40557     0

5.watchdong software is installed successfully.


8. After about 2 minutes, the guest is paused automatically.
Notes:
Comments:

		177020 	[Host network interface management] Define an ethernet with static IPv4 address 	jialiu 	None 	Manual 		Feature 	P2 	2680 	Edit
Setup:

1. save the config file of eth0

  # cp /etc/sysconfig/network-scritps/ifcfg-eth0 /tmp

2. Before destroy eth0, should stop NetworkManager

 # service NetworkManger stop

3. After the following test step is finished, please copy the ifcfg-eth0 back, and restart network to restore netowrk.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. Prepare the following xml:

<interface type='ethernet' name='eth0'>
  <start mode='onboot'/>
  <mac address='xx:xx:xx:xx:xx:xx'/>
  <protocol family='ipv4'>
    <ip address='xx.xx.xx.xx' prefix='23'/>
  </protocol>
</interface>

NOTE;

In order to avoid conflict with other host in the network, please use the orignal IP address from dhcp in xml.

2. Destroy and undefine the orignal interface.

# virsh iface-destroy eth0

# virsh iface-undefine eth0

3. Define new eth0 with static IP

# virsh iface-define new-eth0.xml

# virsh iface-start eth0

4. Check the new eth0

# virsh iface-dumpxml eth0

# cat /etc/syconfig/network-script/ifcfg-eth0

ping a host in the same sub-network.

5. stop NetworkManager and add no-exist device eth1 profile

# cat /etc/sysconfig/network-scripts/ifcfg-eth1
DEVICE=eth1
ONBOOT=yes
BOOTPROTO=none
IPADDR=192.168.0.5
NETMASK=255.255.255.0
GATEWAY=192.168.0.1
6.
#virsh iface-list --all
Name                 State      MAC Address
--------------------------------------------
eth0                  active     78:2b:cb:9a:73:9b
lo                   active     00:00:00:00:00:00
eth1                 inactive   
# virsh iface-start eth1
 error: Failed to start interface eth1 error: internal error failed to create (start) interface eth1: failed to execute external program - Running 'ifup eth1' failed with exit code 1: Device eth1 does not seem to be present, delaying initialization.

 
	
Expected Results:

3. Command is run successfully.

4. ping successfully, and ifcfg-eth0 show that the eth0 is using static IP.

# cat /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0
HWADDR=xx:xx:xx:xx:xx:xx
ONBOOT=yes
BOOTPROTO=none
IPADDR=xx.xx.xx.xx
NETMASK=255.255.254.0
Notes:
Comments:

		177400 	[PCI and USB device assignment] attach-detach-reattach node device for 1000 rounds 	jiachen 	None 	Auto 		Stress 	P2 	2680 	Edit
Setup:

- Could lose connection with host after dettach network device from host
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. virsh nodedev-list --tree

  +- pci_0000_00_19_0
  |   |
  |   +- net_eth0_b8_ac_6f_3b_0b_bc

2. Run the following command

# for i in {1..1000}; do virsh nodedev-dettach pci_0000_00_19_0 ; virsh nodedev-reattach pci_0000_00_19_0 ; done
	
Expected Results:

Step 2:

Check that after step 2 the pci device can be reattached to host
Notes:
Comments:

		177402 	[PCI and USB device assignment] Block assignment of devices below non-ACS switch - bug 526713 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	2690 	Edit
Setup:

- Host supports VT-d

- Host inserted with a non-ACS device

make sure not set relaxed_acs_check=1 in /etc/libvirt/qemu.conf if you want to and understand the risk
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment
    Regression

bug:

    No bug found

Actions:

1. # lspci

2. Detach all 4 pci devices

# virsh nodedev-dettach pci_8086_10e8

# virsh nodedev-dettach pci_8086_10e8_0

# virsh nodedev-dettach pci_8086_10e8_1

# virsh nodedev-dettach pci_8086_10e8_2

3. Add the following device info in <guest> xml file

..........

...
    <hostdev mode='subsystem' type='pci'>
      <source>
        <address bus='5' slot='0' function='0'/>
       </source>
    </hostdev>
...

4. # virsh start <guest>

	
Expected Results:

1. Output:

...
05:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
05:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
06:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
06:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
...

4. Prevention error info pops up as expected

error: Failed to start domain rhel6
error: Requested operation is not valid: PCI device 0000:06:00.0 is not assignable

Notes:
Comments:

		177854 	[Watchdog device] Watchdog device "i6300esb" - poweroff 	jialiu 	yoyzhang 	Auto 		Feature 	P2 	2690 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    RHEL6.0
    virsh-rail
    watchdog device

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='i6300esb' action='poweroff'/> to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, check the watchdog dirvier is installed:

# dmesg | grep i6300

# /sbin/lsmod | grep i6300esb

# lspci

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm
17108 ?        Sl     0:19 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name rhel6 -uuid 2292db8e-9f74-ea3a-5fc4-2240a3f8534e -nodefconfig -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/rhel6.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -device virtio-serial-pci,id=virtio-serial0,max_ports=16,vectors=4,bus=pci.0,addr=0x5 -device virtio-serial-pci,id=virtio-serial1,bus=pci.0,addr=0x6 -drive file=/var/lib/libvirt/images/nfs_test.img,if=none,id=drive-ide0-0-0,boot=on,format=raw,cache=none -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=20,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:61:a6:9e,bus=pci.0,addr=0x3 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -chardev pty,id=channel0 -device virtserialport,bus=virtio-serial0.0,nr=0,chardev=channel0,name=org.linux-kvm.port.0 -chardev pty,id=channel1 -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=channel1,name=org.linux-kvm.port.1 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device i6300esb,id=watchdog0,bus=pci.0,addr=0x7 -watchdog-action poweroff  -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4
17255 pts/11   S+     0:00 grep qemu-kvm

4.The oupt is similar to the following:

# dmesg | grep i6300
i6300ESB timer: initialized (0xffffc20000036000). heartbeat=30 sec (nowayout=0)
# /sbin/lsmod | grep i6300esb
i6300esb     40557     0

5.watchdong software is installed successfully.

8. After about 2 minutes, the guest is power off automatically.
Notes:
Comments:

		177021 	[Host network interface management] Define an ethernet with static IPv6 address 	jialiu 	None 	Manual 		Feature 	P2 	2700 	Edit
Setup:

1. save the config file of eth0

  # cp /etc/sysconfig/network-scritps/ifcfg-eth0 /tmp

2. Before destroy eth0, should stop NetworkManager

 # service NetworkManger stop

3. After the following test step is finished, please copy the ifcfg-eth0 back, and restart network to restore netowrk.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. Prepare the following xml:

<interface type='ethernet' name='eth0'>
  <start mode='onboot'/>
  <protocol family='ipv6'>
    <ip address='3ffe::200' prefix='64'/>
  </protocol>
</interface>

2. Destroy and undefine the orignal interface.

# virsh iface-destroy eth0

# virsh iface-undefine eth0

3. Define new eth0 with static IPv6 address

# virsh iface-define new-eth0-ipv6.xml

# virsh iface-start eth0

4. Check the new eth0

# virsh iface-dumpxml eth0

# cat /etc/syconfig/network-script/ifcfg-eth0

# ifconfig eth0
	
Expected Results:

3. Command is run successfully.

4. ping other host in the same sub-net successfully, and ifcfg-eth0 show that the eth0 is using static IPv6 address

# ping6 3ffe::201
PING 3ffe::201(3ffe::201) 56 data bytes
64 bytes from 3ffe::201: icmp_seq=1 ttl=64 time=0.728 ms
64 bytes from 3ffe::201: icmp_seq=2 ttl=64 time=0.212 ms

...

DEVICE=eth0
HWADDR=00:25:64:a6:fb:cc
ONBOOT=yes
IPV6INIT=yes
IPV6_AUTOCONF=no
DHCPV6=no
IPV6ADDR=3ffe::200/64

# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A6:FB:CC  
          inet6 addr: 3ffe::200/64 Scope:Global
          inet6 addr: fe80::225:64ff:fea6:fbcc/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:19531 errors:0 dropped:0 overruns:0 frame:0
          TX packets:7675 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:15895192 (15.1 MiB)  TX bytes:641197 (626.1 KiB)
          Memory:febe0000-fec00000
Notes:
Comments:

		177403 	[PCI and USB device assignment] Block assignment of devices below non-ACS switch - bug 526713,706869 	yoyzhang 	yoyzhang 	Auto 		Regression 	P2 	2700 	Edit
Setup:

- Host supports VT-d

- Host inserted with a non-ACS device

intel-e5530-24-1.englab.nay.redhat.com 10.66.72.125 or 10.66.72.11
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. # lspci

2. Detach all 4 pci devices

# virsh nodedev-dettach pci_0000_05_00_0

# virsh nodedev-dettach pci_0000_05_00_1

# virsh nodedev-dettach pci_0000_06_00_0

# virsh nodedev-dettach pci_0000_06_00_1

3. Add the following device info in <guest> xml file

..........

...
    <hostdev mode='subsystem' type='pci'>
      <source>
        <address bus='6' slot='0' function='1'/>
       </source>
    </hostdev>
...

4. # virsh start <guest>

	
Expected Results:

1. Output:

...
05:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
05:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
06:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
06:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
...

4. Prevention error info pops up as expected

error: Failed to start domain rhel6
error: Requested operation is not valid: PCI device 0000:06:00.0 is not assignable

Notes:
Comments:

		177414 	[PCI and USB device assignment] Hot plug normal PCI device with 'managed=yes' 	yimwang 	None 	Auto 		--default-- 	P1 	2710 	Edit
Setup:
Setup:

- VT-d is enabled

- Could lose connection with host after dettach network device from host

 For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    PCI and USB device assignment

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree

2. # virsh nodedev-dumpxml pci_0000_00_19_0

3. Edit the following code as nodedev.xml


          <hostdev mode='subsystem' type='pci' managed='yes'>
            <source>
              <address bus='0' slot='25' function='0'/>
            </source>
          </hostdev>
       

4. # virsh define <guest>.xml

5. # virsh start <guest>

(NOTE)For the rhel4 and rhel5 guest, please load acpiphp module, if it is rhel6, ignore this step
# modprobe acpiphp

6. # virsh attach-device <guest> nodedev.xml

7. In Windows guest

Meed download & install Network Adapter Driver to use the PCI NIC, give window 2008 for example

<http://downloadcenter.intel.com/Detail_Desc.aspx?agr=Y&ProdId=3003&DwnldID=18720&ProductFamily=Ethernet+Components&ProductLine=Ethernet+Controllers&ProductProduct=Intel%C2%AE+82567+Gigabit+Ethernet+Controller&lang=eng>

In Linux guest

Run # lspci ,#ping www.google.com
	
Expected Results:

1. Output:

computer
  |
  +-pci_8086_10bd
  |   |
  |   +-net_00_1e_4f_a8_a7_dc

........

2. Output

<device>
  <name>pci_0000_00_19_0</name>
  <parent>computer</parent>
  <driver>
    <name>e1000e</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>0</bus>
    <slot>25</slot>
    <function>0</function>
    <product id='0x10bd'>82566DM-2 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

5.  The guest is running

6. The device is hot-plug to guest sucessfully

7. Could see the assigned device and ping to google.com successfully
Notes:
Comments:

		177858 	[Watchdog device]Watchdog device "i6300esb" - reset 	jialiu 	None 	Auto 		Feature 	P2 	2710 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    RHEL6.0
    virsh-rail
    watchdog device

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='i6300esb' action='reset'/> to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, check the watchdog dirvier is installed:

# dmesg | grep i6300

# /sbin/lsmod | grep i6300esb

# lspci

5. In guest, install watchdog software.

download the latest version of watchdog from brewweb.

https://brewweb.devel.redhat.com/packageinfo?packageID=3829


# rpm -ivh watchdog*

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60. **

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm
17108 ?        Sl     0:19 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name rhel6 -uuid 2292db8e-9f74-ea3a-5fc4-2240a3f8534e -nodefconfig -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/rhel6.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -device virtio-serial-pci,id=virtio-serial0,max_ports=16,vectors=4,bus=pci.0,addr=0x5 -device virtio-serial-pci,id=virtio-serial1,bus=pci.0,addr=0x6 -drive file=/var/lib/libvirt/images/nfs_test.img,if=none,id=drive-ide0-0-0,boot=on,format=raw,cache=none -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=20,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:61:a6:9e,bus=pci.0,addr=0x3 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -chardev pty,id=channel0 -device virtserialport,bus=virtio-serial0.0,nr=0,chardev=channel0,name=org.linux-kvm.port.0 -chardev pty,id=channel1 -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=channel1,name=org.linux-kvm.port.1 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device i6300esb,id=watchdog0,bus=pci.0,addr=0x7 -watchdog-action reset  -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x4
17255 pts/11   S+     0:00 grep qemu-kvm

4.The oupt is similar to the following:

# dmesg | grep i6300
i6300ESB timer: initialized (0xffffc20000036000). heartbeat=30 sec (nowayout=0)
# /sbin/lsmod | grep i6300esb
i6300esb     40557     0

5.watchdong software is installed successfully.

8. After about 2 minutes, the guest is restart automatically.
Notes:
Comments:

		177028 	[Host network interface management] Error handlING for invalid IP address 	jialiu 	None 	Manual 		Feature 	P2 	2720 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. Prpeare the following xml:

# cat new-eth0-ipv4.xml
<interface type='ethernet' name='eth0'>
  <start mode='onboot'/>
  <mac address='00:25:64:a6:fb:cc'/>
  <protocol family='ipv4'>
    <ip address='10.66.9a.91' prefix='23'/>
  </protocol>
</interface>

# cat new-eth0-ipv6.xml
<interface type='ethernet' name='eth0'>
  <start mode='onboot'/>
  <mac address='00:25:64:a6:fb:cc'/>
  <protocol family='ipv6'>
    <ip address='3ffe::20x' prefix='64'/>
  </protocol>
</interface>

2. Try to define the new interface.

# virsh iface-define new-eth0-ipv4.xml

# virsh iface-define new-eth0-ipv6.xml
	
Expected Results:

2. Error message is seen:

# virsh iface-define new-eth0-ipv4.xml

error: Failed to define interface from net.xml
error: XML error: could not get interface XML description: XML invalid - Element interface has extra content: mac

# virsh iface-define new-eth0-ipv6.xml
error: Failed to define interface from net.xml
error: XML error: could not get interface XML description: XML invalid - Element interface has extra content: mac

Notes:
Comments:

		177415 	[PCI and USB device assignment] Hot plug normal PCI device without 'managed=yes' 	jialiu 	None 	Auto 		--default-- 	P1 	2720 	Edit
Setup:

- VT-d is enabled

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    PCI and USB device assignment
    RHEL6.0

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree

2. # virsh nodedev-dumpxml pci_0000_00_19_0

3. Edit the following code as nodedev.xml


          <hostdev mode='subsystem' type='pci'>
            <source>
              <address bus='0' slot='25' function='0'/>
            </source>
          </hostdev>
       

4. # virsh define <guest>.xml

5. # virsh start <guest>

6. Dettach this network device from host

# virsh nodedev-dettach pci_0000_00_19_0

# virsh nodedev-reset pci_0000_00_19_0

7. Attach the pci interface to guest.

# virsh attach-device <guest> nodedev.xml

8. In guest, run # lspci ,#ping host
	
Expected Results:

1. Output:

computer
  |    
  +- pci_0000_00_19_0
  |   |
  |   +- net_eth0_b8_ac_6f_3b_0d_56
  |    

........

2. Output

<device>
  <name>pci_0000_00_19_0</name>
  <parent>computer</parent>
  <driver>
    <name>e1000e</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>0</bus>
    <slot>25</slot>
    <function>0</function>
    <product id='0x10bd'>82566DM-2 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

5.  The guest is running

6. The device is dettached successfully.

7. The device is hot-plug to guest sucessfully

8. Could see the assigned device and ping successfully
Notes:
Comments:

		177046 	[Interface hotplug] Attach 32 NIC to guest 	jialiu 	None 	Manual 		--default-- 	P1 	2750 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks

bug:

    No bug found

Actions:

1. Define and start a domain

2. Prepare the following file:

# cat nic_templ.xml
    <interface type='network'>
      <source network='default'/>
      <target dev='#TARGET_DEV#'/>
    </interface>

# cat hotplug_nic.sh
 for i in `seq 32`; do
    echo "------${i}-------"
    sed "s/#TARGET_DEV#/vnet$i/" nic_templ.xml > hotplug_nic.xml
    virsh attach-device "$1" hotplug_nic.xml

    sleep 3
done

# cat hotunplug_nic.sh
for i in `seq 32`; do
    echo "------${i}-------"
    macaddr=$(virsh dumpxml "$1" | grep "'vnet$i'" -B 3|grep mac)
    sed -e "s/#TARGET_DEV#/vnet$i/" \
        -e "/<source/i $macaddr" nic_templ.xml > hotplug_nic.xml

    virsh detach-device "$1" hotplug_nic.xml
    sleep 3
done

3. Attach 32 nic to guest

# sh hotplug_nic.sh <guestname>

In guest, cd to /etc/sysconfig/network-scripts/, run the follwoing command

# lspci |grep -i eth |wc -l

27

# for i in `seq 26`; do  sed -e "s/eth0/eth$i/g" -e "/HWADDR/d" ifcfg-eth0 > ifcfg-eth$i; done

# service network restart

4. Dettach all the hotplg nic

# sh hotunplug_nic.sh <guestname>

5. check if the hotplug disks are not existing via "lspci |grep -i eth"
	
Expected Results:

3. virsh command working fine without any error.

NOTE:

Currently qemu-kvm support 32 pci slot, so if the following error is seen, it is not an issue.

# sh hotplug_nic.sh vr-migration-tls-tcp
------1-------
Device attached successfully

...
...

------26-------
Device attached successfully

------27-------
error: Failed to attach device from hotplug_nic.xml
error: internal error No more available PCI addresses

# virsh dumpxml vr-migration-tls-tcp|grep "<address type='pci' "| wc -l
31

In guest,

all the nic get ipaddress successfully.

4. virsh command is working fine.

5. The hotplug nic is not existing now.
Notes:
Comments:

		177048 	[Interface hotplug] attach interface via xml after libvirtd restart - bug 623877 	xhu 	None 	Manual 		Regression 	P1 	2760 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual networks

bug:

    No bug found

Actions:

1. start a guest

2. restart libvirtd

3. attach network device to the guest via interface.xml
# cat interface.xml
<interface type='bridge'>
<source bridge='virbr0'/>
<model type='virtio'/>
</interface>

# virsh attach-device rhel6 interface.xml


	
Expected Results:

1.

# virsh start rhel6

Domain rhel6 started

2.

# service libvirtd restart

Stopping libvirtd daemon: [ OK ]

Starting libvirtd daemon: [ OK ]

3.

# virsh attach-device rhel6 interface.xml

Device attached successfully
Notes:
Comments:

		177050 	[Interface hotplug] Attach/Detach e1000 NIC to guest 	jialiu 	None 	Auto 		--default-- 	P1 	2770 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    virsh-rail
    virtual networks

bug:

    No bug found

Actions:

1. Create and start a domain.
2. Prepare a xml as following:
    <interface type='network'>
      <mac address='52:54:00:80:d9:bb'/>
      <source network='default'/>
      <model type='e1000'/>
    </interface>
3. Hotplug the NIC to guest.
#  virsh attach-device <guest name> <xml file name>

4. In guest, check the NIC device is works fine.
# lspci | grep -i eth
# ip link
# ifconfig eth[N]
# ping www.google.com -I eth[N]

Note: 
if the new eth does NOT get ip address, please write a file - /ets/sysconfig/network-script/ifcfg-eth[N], 
here is a sample:
# cat /etc/sysconfig/network-scripts/ifcfg-eth4 
DEVICE="eth4"
BOOTPROTO="dhcp"
NM_CONTROLLED="yes"
ONBOOT="yes"
Then restart network service.

5. Hot unplug the NIC from guest.
# virsh detach-device <guest name> <xml file name>

6. In guest, check the NIC in step 4 is not seen.

# lspci | grep -i eth
# ip link

	
Expected Results:

3. hotplug the NIC device successfully.

# virsh attach-device <guest name> <xml file name>

Device attached successfully

4. Here, u will found a new eth is seen.

# lspci |grep -i eth
00:03.0 Ethernet controller: Qumranet, Inc. Virtio network device
00:17.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 03)

# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 1000
    link/ether 52:54:00:80:d9:6a brd ff:ff:ff:ff:ff:ff
36: eth4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:80:d9:bb brd ff:ff:ff:ff:ff:ff

# ifconfig eth4
eth4      Link encap:Ethernet  HWaddr 52:54:00:80:D9:BB  
          inet addr:192.168.122.221  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fe80:d9bb/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:15 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:1348 (1.3 KiB)  TX bytes:1390 (1.3 KiB)
          Interrupt:11 Base address:0x8000

# ping www.google.com -I eth4
PING www.l.google.com (72.14.204.147) from 192.168.122.221 eth4: 56(84) bytes of data.
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=1 ttl=42 time=303 ms
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=2 ttl=42 time=277 ms

5. hot unplug the NIC device successfully.

# virsh detach-device test nic-hostput.xml
Device detached successfully

6. The NIC in step 4 is not seen.
Notes:
Comments:

		177053 	[Interface hotplug] Attach/Detach virtio NIC to guest 	jialiu 	None 	Auto 		--default-- 	P1 	2780 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual networks

bug:

    No bug found

Actions:

1. Create and start a domain.
2. Prepare a xml as following:
    <interface type='network'>
      <mac address='52:54:00:80:d9:bb'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
3. Hotplug the NIC to guest.
#  virsh attach-device <guest name> <xml file name>

4. In guest, check the NIC device is works fine.
# lspci | grep -i eth
# ip link
# ifconfig eth[N]
# ping www.google.com -I eth[N]

Note: 
if the new eth does NOT get ip address, please write a file - /ets/sysconfig/network-script/ifcfg-eth[N], 
here is a sample:
# cat /etc/sysconfig/network-scripts/ifcfg-eth4 
DEVICE="eth4"
BOOTPROTO="dhcp"
NM_CONTROLLED="yes"
ONBOOT="yes"
Then restart network service.

5. Hot unplug the NIC from guest.
# virsh detach-device <guest name> <xml file name>

6. In guest, check the NIC in step 4 is not seen.

# lspci | grep -i eth
# ip link

	
Expected Results:

3. hotplug the NIC device successfully.

# virsh attach-device <guest name> <xml file name>

Device attached successfully

4. Here, u will found a new eth is seen.

# lspci |grep -i eth
00:03.0 Ethernet controller: Qumranet, Inc. Virtio network device
00:18.0 Ethernet controller: Qumranet, Inc. Virtio network device

# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 1000
    link/ether 52:54:00:80:d9:6a brd ff:ff:ff:ff:ff:ff
36: eth4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:80:d9:bb brd ff:ff:ff:ff:ff:ff

# ifconfig eth4
eth4      Link encap:Ethernet  HWaddr 52:54:00:80:D9:BB  
          inet addr:192.168.122.221  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fe80:d9bb/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:15 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:1348 (1.3 KiB)  TX bytes:1390 (1.3 KiB)
          Interrupt:11 Base address:0x8000

# ping www.google.com -I eth4
PING www.l.google.com (72.14.204.147) from 192.168.122.221 eth4: 56(84) bytes of data.
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=1 ttl=42 time=303 ms
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=2 ttl=42 time=277 ms

5. hot unplug the NIC device successfully.

# virsh detach-device test nic-hostput.xml
Device detached successfully

6. The NIC in step 4 is not seen.
Notes:
Comments:

		177051 	[Interface hotplug] Attach/Detach NIC to guest for 500 times - stress test - Bug 618484 	jialiu 	None 	Manual 		Regression 	P1 	2790 	Edit
Setup:
if the guest is rhel5, execute the following command in guest before attach-device:
# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    virtual networks

bug:

    No bug found

Actions:

1. Create and start a domain.
2. Prepare a xml as following:
    <interface type='network'>
      <mac address='52:54:00:80:d9:bb'/>
      <source network='default'/>
    </interface>
3. Hotplug the NIC to guest.
#  virsh attach-device <guest name> <xml file name>

4. In guest, check the NIC device is works fine.
# lspci | grep -i eth
# ip link
# ifconfig eth[N]
# ping www.google.com -I eth[N]

Note:
if the new eth does NOT get ip address, please write a file - /ets/sysconfig/network-script/ifcfg-eth[N],
here is a sample:
# cat /etc/sysconfig/network-scripts/ifcfg-eth4
DEVICE="eth4"
BOOTPROTO="dhcp"
NM_CONTROLLED="yes"
ONBOOT="yes"
Then restart network service.

5. Hot unplug the NIC from guest.
# virsh detach-device <guest name> <xml file name>

6. In guest, check the NIC in step 4 is not seen.
# lspci | grep -i eth
# ip link

7. Hotplug / hot-unplug the nic device for 500 times.
# for i in `seq 500`; \
do echo "---${i}---"; virsh attach-device test nic-hostput.xml; sleep 3; virsh detach-device test nic-hostput.xml; sleep 3; \
done
	
Expected Results:

3. hotplug the NIC device successfully.

# virsh attach-device <guest name> <xml file name>

Device attached successfully

4. Here, u will found a new eth is seen.

# lspci |grep -i eth
00:03.0 Ethernet controller: Qumranet, Inc. Virtio network device
00:16.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL-8139/8139C/8139C+ (rev 20)

# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 1000
    link/ether 52:54:00:80:d9:6a brd ff:ff:ff:ff:ff:ff
36: eth4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:80:d9:bb brd ff:ff:ff:ff:ff:ff

# ifconfig eth4
eth4      Link encap:Ethernet  HWaddr 52:54:00:80:D9:BB  
          inet addr:192.168.122.221  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fe80:d9bb/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:15 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:1348 (1.3 KiB)  TX bytes:1390 (1.3 KiB)
          Interrupt:11 Base address:0x8000

# ping www.google.com -I eth4
PING www.l.google.com (72.14.204.147) from 192.168.122.221 eth4: 56(84) bytes of data.
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=1 ttl=42 time=303 ms
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=2 ttl=42 time=277 ms

5. hot unplug the NIC device successfully.

# virsh detach-device test nic-hostput.xml
Device detached successfully

6. The NIC in step 4 is not seen.

7. all the virsh command is finished without any error
Notes:
Comments:

		177052 	[Interface hotplug] Attach/Detach rtl8139 NIC to guest 	jialiu 	None 	Auto 		--default-- 	P1 	2800 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    virsh-rail
    virtual networks

bug:

    No bug found

Actions:

1. Create and start a domain.
2. Prepare a xml as following:
    <interface type='network'>
      <mac address='52:54:00:80:d9:bb'/>
      <source network='default'/>
      <model type='rtl8139'/>
    </interface>
3. Hotplug the NIC to guest.
#  virsh attach-device <guest name> <xml file name>

4. In guest, check the NIC device is works fine.
# lspci | grep -i eth
# ip link
# ifconfig eth[N]
# ping www.google.com -I eth[N]

Note: 
if the new eth does NOT get ip address, please write a file - /ets/sysconfig/network-script/ifcfg-eth[N], 
here is a sample:
# cat /etc/sysconfig/network-scripts/ifcfg-eth4 
DEVICE="eth4"
BOOTPROTO="dhcp"
NM_CONTROLLED="yes"
ONBOOT="yes"
Then restart network service.

5. Hot unplug the NIC from guest.
# virsh detach-device <guest name> <xml file name>

6. In guest, check the NIC in step 4 is not seen.

# lspci | grep -i eth
# ip link

	
Expected Results:

3. hotplug the NIC device successfully.

# virsh attach-device <guest name> <xml file name>

Device attached successfully

4. Here, u will found a new eth is seen.

# lspci |grep -i eth
00:03.0 Ethernet controller: Qumranet, Inc. Virtio network device
00:16.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL-8139/8139C/8139C+ (rev 20)

# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UNKNOWN qlen 1000
    link/ether 52:54:00:80:d9:6a brd ff:ff:ff:ff:ff:ff
36: eth4: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
    link/ether 52:54:00:80:d9:bb brd ff:ff:ff:ff:ff:ff

# ifconfig eth4
eth4      Link encap:Ethernet  HWaddr 52:54:00:80:D9:BB  
          inet addr:192.168.122.221  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fe80:d9bb/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:15 errors:0 dropped:0 overruns:0 frame:0
          TX packets:11 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:1348 (1.3 KiB)  TX bytes:1390 (1.3 KiB)
          Interrupt:11 Base address:0x8000

# ping www.google.com -I eth4
PING www.l.google.com (72.14.204.147) from 192.168.122.221 eth4: 56(84) bytes of data.
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=1 ttl=42 time=303 ms
64 bytes from iad04s01-in-f147.1e100.net (72.14.204.147): icmp_seq=2 ttl=42 time=277 ms

5. hot unplug the NIC device successfully.

# virsh detach-device test nic-hostput.xml
Device detached successfully

6. The NIC in step 4 is not seen.
Notes:
Comments:

		177407 	[PCI and USB device assignment] Cold-plug a USB device to guest with bus + device and then hot-unplug 	ajia 	None 	Auto 		Feature 	P1 	2850 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Define a domain

2. Plug a USB disk to host.

# lsusb
Bus 008 Device 003: ID 413c:2106 Dell Computer Corp. Dell QuietKey Keyboard
Bus 008 Device 002: ID 413c:3012 Dell Computer Corp. Optical Wheel Mouse
Bus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 002 Device 004: ID 090c:1000 Feiya Technology Corp. Flash Drive
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub

3. Dump xml for the usb device.

# virsh nodedev-list --tree

...

 +- pci_0000_00_1d_7
  |   |
  |   +- usb_usb2
  |       |
  |       +- usb_2_0_1_0
  |       +- usb_2_2
  |           |
  |           +- usb_2_2_1_0
  |               |
  |               +- scsi_host6
  |                   |
  |                   +- scsi_target6_0_0
  |                       |
  |                       +- scsi_6_0_0_0
  |                           |
  |                           +- block_sdb_USB_Flash_Disk_AA72013000022095_0_0

...

# virsh nodedev-dumpxml usb_2_2
<device>
  <name>usb_2_2</name>
  <parent>usb_usb2</parent>
  <capability type='usb_device'>
    <bus>2</bus>
    <device>4</device>
    <product id='0x1000'>USB DISK</product>
    <vendor id='0x090c'>SMI Corporation</vendor>
  </capability>
</device>

4. Add the following xml to guest xml (also save the following xml as hostdev-usb-address.xml)

<hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='2' device='4'/>
      </source>
    </hostdev>

5. Start the guest

6. In guest, check the device is seen.

# lsusb

7. Hot-unplug the usb device from guest.

# virsh detach-device <guestname> hostdev-usb-address.xml

8. In guest, check the device is seen.

# lsusb

9. Shutdown the guest and start it again.

10.  In guest, check the usb device again via lsusb

 

 
	
Expected Results:

6. The usb device is seen in guest.

7.  Output:

# virsh detach-device <guestname> hostdev-usb-address.xml
Device detached successfully

8. The usb device is not existing now.

9. guest is shutdown and started successfully

10. The device still exist.
Notes:
Comments:

		177198 	[Managed save] Domains are *not* automatically restored via initscript when host boots up 	yimwang 	None 	Auto 		--default-- 	P2 	2870 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Start up 2 domains and shutdown 1 domain:
   # virsh start <domain1>
   # virsh start <domain2>
   # virsh shutdown <domain3>

Note: make sure the autostart should be disabled for the 3 domains, or they will start by libvirtd on host bootup.

2. keep all configure as default in /etc/sysconfig/libvirt-guests
ON_SHUTDOWN=suspend
ON_BOOT=start

3. Make sure the libvirt-guests script is on:
   # chkconfig libvirt-guests on

4. reboot the *host*:
   # reboot




	
Expected Results:

1. During boot, the libvirt-guests script should NOT restore the domain that was *inactive* on shutdown

domain1 running
domain2 running
domain3 shutoff

Notes:
Comments:

		177408 	[PCI and USB device assignment] Cold-plug a USB device to guest with product + vendor and the hot-unplug-bug798838 	ajia 	None 	Auto 		Feature 	P1 	2870 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Define a domain

2. Plug a USB disk to host.

# lsusb
Bus 008 Device 003: ID 413c:2106 Dell Computer Corp. Dell QuietKey Keyboard
Bus 008 Device 002: ID 413c:3012 Dell Computer Corp. Optical Wheel Mouse
Bus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 002 Device 004: ID 090c:1000 Feiya Technology Corp. Flash Drive
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub

3. Dump xml for the usb device.

# virsh nodedev-list --tree

...

 +- pci_0000_00_1d_7
  |   |
  |   +- usb_usb2
  |       |
  |       +- usb_2_0_1_0
  |       +- usb_2_2
  |           |
  |           +- usb_2_2_1_0
  |               |
  |               +- scsi_host6
  |                   |
  |                   +- scsi_target6_0_0
  |                       |
  |                       +- scsi_6_0_0_0
  |                           |
  |                           +- block_sdb_USB_Flash_Disk_AA72013000022095_0_0

...

# virsh nodedev-dumpxml usb_2_2
<device>
  <name>usb_2_2</name>
  <parent>usb_usb2</parent>
  <capability type='usb_device'>
    <bus>2</bus>
    <device>4</device>
    <product id='0x1000'>USB DISK</product>
    <vendor id='0x090c'>SMI Corporation</vendor>
  </capability>
</device>

4. Add the following xml to guest xml (also save the following xml as hostdev-usb-address.xml)

<hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <product id='0x1000'/>
        <vendor id='0x090c'/>
      </source>
    </hostdev>

5. Start the guest

6. In guest, check the device is seen.

# lsusb

7. Shutdown the guest

8. Start the guest

9. Virsh edit  guest and add the following xml to guest xml

<hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <product id='0x1000'/>
        <vendor id='0x090c'/>
      </source>
    </hostdev>

10. Destroy the guest.

11. Start the guest.

12. Hot-unplug the usb device from guest.

# virsh detach-device <guestname> hostdev-usb-address.xml

13. In guest, check the device is seen.

# lsusb

14. Shutdown the guest and start it again.

15.  In guest, check the usb device again via lsusb

 

 
	
Expected Results:

6. The usb device is seen in guest.

8. Successfully start. No error occurs.

12.  Output:

# virsh detach-device <guestname> hostdev-usb-address.xml
Device detached successfully

13. The usb device is not existing now.

14. guest is shutdown and started successfully

15. The device still exist.
Notes:
Comments:

		177213 	[Managed save] Same domain cannot be started with the same saved image again. 	yimwang 	None 	Auto 		--default-- 	P2 	2880 	Edit
Setup:

First do case '[Managed save]Stopped domain with managed save file can be manually restored via virsh'
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Run a domain:
   # virsh start <domain>

2. In a terminal, run:
   # virsh managedsave <domain>
   
3. start the domain:

# virsh start <domain>

4.Check the .save file
#ll /var/lib/libvirt/qemu/save/<domain>.save

 5. Shut down the domain manually
 # virsh shutdown <domain>

 6. Start the domain again:
 # virsh start <domain>


	
Expected Results:

2. The domain should be successfully saved.

4.ls: cannot access /var/lib/libvirt/qemu/save/<domain>.save: No such file or directory

6.The domain should start successfully, but should boot up nornally, not restore to the previous state.

Notes:
Comments:

		177200 	[Managed save] Domains are automatically saved via initscript when host shuts down 	yimwang 	None 	Auto 		--default-- 	P2 	2890 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Start up three domains:
   # virsh start <domain1>
   # virsh start <domain2>
   # virsh start <domain3>

2. keep all configure as default in /etc/sysconfig/libvirt-guests
ON_SHUTDOWN=suspend


3. Make sure the libvirt-guests script is off:
   # chkconfig libvirt-guests off

4. Make sure all the domians autostart status is disabled, take <domain1> as instance:

   #virsh dominfo <domain1>

Id:             1
Name:           <domain1>
UUID:           941e2c29-ea3b-840f-0b0a-09900c5bbbde
OS Type:        hvm
State:          running
CPU(s):         1
Max memory:     1048576 KiB
Used memory:    1048576 KiB
Persistent:     yes
Autostart: disable
Managed save:   no
Security model: selinux
Security DOI:   0

 

If the autostart is not disabled, please set it as disable with the following command:

#virsh autostart <domain1> --disable



5. shutdown the *host*:
   # shutdown -h 0

 

6. start the *host*

 

7. start these domains

for i in {<domain1>, <domain2>,<domain3>}; do virsh start $i;done

	
Expected Results:

6. After the host booting up, check the status of these domains:

  6.1 the 3 domains should be in shutdown status:

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     <domain1>                   shut off
 -     <domain2>                   shut off
 -     <domain3>                   shut off

 

  6.2 make sure the save files are exist in path /var/lib/libvirt/qemu/save/

# ll /var/lib/libvirt/qemu/save/
total 883032
-rw-------. 1 root root 297409569 Dec  5 17:19 <domain1>.save
-rw-------. 1 root root 295215351 Dec  5 17:19 <domain2>.save
-rw-------. 1 root root 311587016 Dec  5 17:19 <domain3>.save

 

7. the domains should be resumed to the same place which is before shutdown.

Make sure the save files are removed:

# ll /var/lib/libvirt/qemu/save/

total 0

 

 

 
Notes:
Comments:

		177409 	[PCI and USB device assignment] Cold-plug multiple USB devices to guest 	ajia 	None 	Auto 		Feature 	P1 	2890 	Edit
Setup:

For now, the default usb controller is usb1.x , it means whether or not you use usb1.x or usb2.x, you will get usb1.x speed, for usb1.x device, you may test use keyboard, mouse, etc.

 

Prepare 2 usb device and plug into host.

This case should test both USB 1.1 and USB 2.0 controller.

We will use the usb keyboard for the usb 1.x testing, and usb disk for the usb 2.0 testing.

 

The video device model type should be qxl:

<video>
      <model type='qxl' vram='65536' heads='1'/>
      <alias name='video0'/>
</video>

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    841202 - From Run 51765

Actions:

For usb 1.x

1. insert an usb keyboard to the host.

# lsusb
Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 002 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub
Bus 003 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 004 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 005 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 006 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 007 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 008 Device 001: ID 1d6b:0001 Linux Foundation 1.1 root hub
Bus 003 Device 002: ID 04b3:3025 IBM Corp. NetVista Full Width Keyboard
Bus 003 Device 003: ID 413c:3012 Dell Computer Corp. Optical Wheel Mouse


# virsh nodedev-list --tree

+- pci_0000_00_1a_0
  |   |
  |   +- usb_usb3
  |       |
  |       +- usb_3_0_1_0
  |       +- usb_3_1
  |           |
  |           +- usb_3_1_1_0

# virsh nodedev-dumpxml usb_3_1

<device>
  <name>usb_3_1</name>
  <parent>usb_usb3</parent>
  <capability type='usb_device'>
    <bus>3</bus>
    <device>2</device>
    <product id='0x3025'>USB NetVista Full Width Keyboard.</product>
    <vendor id='0x04b3'>LITE-ON Technology</vendor>
  </capability>
</device>

or

<device>
  <name>usb_3_1</name>
  <parent>usb_usb3</parent>
  <capability type='usb_device'>
    <bus>3</bus>
    <device>3</device>
    <product id='0x3012'>Dell USB Optical Mouse</product>
    <vendor id='0x413c'>Dell</vendor>
  </capability>
</device>

2.    Add the following xml to the guest

# virsh edit guest

...
<hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='2' device='3'/>
      </source>
</hostdev>

or

<hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='3' device='3'/>
      </source>
</hostdev>
...

3.  start the guest

# virsh start guest

4. Check the usb keyboard works well in the guest

4.1 # virsh dumpxml guest

...
    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='3' device='2'/>
      </source>
      <alias name='hostdev0'/>
    </hostdev>

or

<hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='3' device='3'/>
      </source>
</hostdev>

...

4.2 log into the guest, then issue

# lsusb

...
Bus 001 Device 002: ID 04b3:3025 IBM Corp.

Bus 001 Device 002: ID 413C:3012 Dell Computer Corp, Optical Wheel Mouse

4.3 check that the keyboard and mouse can take affect in guest(move the mouse and click some items, fill some chars in a file).

Bug 841202 - Usb keyboard can't work after pass through it into guest [please check the bug status during your testing.]

==================================

For usb 2.0

1. virsh nodedev-list --tree

  |   +- usb_usb2
  |       |
  |       +- usb_2_0_1_0
  |       +- usb_2_1
  |       |   |
  |       |   +- usb_2_1_1_0
  |       |       |
  |       |       +- scsi_host10
  |       |           |
  |       |           +- scsi_target10_0_0
  |       |               |
  |       |               +- scsi_10_0_0_0
  |       |                   |
  |       |                   +- block_sdb_Kingston_DT_101_II_001CC0EC345AF9C126920DED_0_0
  |       |                    
  |       +- usb_2_2
  |           |
  |           +- usb_2_2_1_0
  |               |
  |               +- scsi_host1
  |                   |
  |                   +- scsi_target1_0_0
  |                       |
  |                       +- scsi_1_0_0_0
  |                           |
  |                           +- block_sdc_2_0_Flash_Disk_18202000F483C300_0_0

2. # virsh nodedev-dumpxml usb_2_1

# virsh nodedev-dumpxml usb_2_2

3. Attach the following xml to the guest

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='2' device='3'/>
      </source>
    </hostdev>

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='2' device='4'/>
      </source>
    </hostdev>

4. Start the guest

5. login to guest and check if the usb can work well

# fdisk -l

# mount /dev/sda1

# mount /dev/sdb1

 
	
Expected Results:

Step 4

guest can be started successfully

Step 5

All the usb can be mounted and can be read and writen
Notes:
Comments:

		177199 	[Managed save] Domains are automatically restored via initscript when host boots up 	yimwang 	None 	Auto 		--default-- 	P2 	2900 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Start up three domains:
   # virsh start <domain1>
   # virsh start <domain2>
   # virsh start <domain3>

2. keep all configure as default in /etc/sysconfig/libvirt-guests
ON_SHUTDOWN=suspend
ON_BOOT=start

3. Make sure the libvirt-guests script is on:
   # chkconfig libvirt-guests on

4. reboot the *host*:
   # reboot

	
Expected Results:

4. During boot, the libvirt-guests script should restore the domains to the running state.
Both domains should be at the same place they were when the host shutdown.

Notes:
Comments:

		177410 	[PCI and USB device assignment] Cold/hot-plug USB/PCI devices to guest and then shutdown or reboot the guest 	ajia 	None 	Auto 		Feature 	P1 	2900 	Edit
Setup:

Host supports VT-d

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

At lease have 2 pci devices and 2 usb flash disks
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Prepare 2 guest

2. Guest1: cold-plug a pci device and a usb device and then start

    Guest2: start first and then hot-plug a pci device and a usb device

3. Shutdown and start these 2 guests

 

NOTE: The detail steps about hot/cold-plug pci/usb device please consult cases 95765  ,  95769 , 103575  , 111010
	
Expected Results:

The cold pluged pci and usb device still exist on the first guest and can work well, the hotpluged pci and usb device will not exist on the second guest
Notes:
Comments:

		177216 	[Managed save] Stopped domain fails to be saved via virsh. 	yimwang 	None 	Auto 		--default-- 	P2 	2910 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Stop a domain:
   # virsh shutdown <domain>

2. In a terminal, run:
   # virsh managedsave <domain>
   

	
Expected Results:

2. The domain should fail to be saved since it is not running.

# virsh  managedsave rhel6
error: Failed to save domain rhel6 state
error: Requested operation is not valid: domain is not running

Notes:
Comments:

		177412 	[PCI and USB device assignment] Dynamically switch a PCI/USB devices between different guests without shutdown 	ajia 	None 	Auto 		Feature 	P1 	2910 	Edit
Setup:

- VT-d is enabled

- Could lose connection with host after dettach network device from host

 For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

Detail steps please consult  95765  ,  95769

1. Prepare 2 guests and start both

2. Hot-plug PCI device to guest1

3. Hot-plug USB device to guest1

4. Hot-unplug PCI device from guest1

5. Hot-unplug USB device from guest1

6. Hot-plug PCI device to guest2 and check if device is available and usable

7. Hot-plug USB device to guest2 and check if device is available and usable

8. Hot-unplug PCI device from guest2 and check if device is removed

9. Hot-unplug USB device from guest2 and check if device is removed
	
Expected Results:
Notes:
Comments:

		177217 	[Managed save] Stopped domain with managed save file can be manually restored via virsh 	yimwang 	None 	Auto 		--default-- 	P2 	2920 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Run a domain:
   # virsh start <domain>

2. In a terminal, run:
   # virsh managedsave <domain>

3. The domain should be successfully saved.

4. In a terminal, run:
   # virsh start <domain>

	
Expected Results:

4. The domain should be restored to the state before the managed save.
Notes:
Comments:

		177413 	[PCI and USB device assignment] Dynamically switch one PCI/USB device between different guests 	ajia 	ajia 	Auto 		Feature 	P1 	2920 	Edit
Setup:

- VT-d is enabled

- Could lose connection with host after dettach network device from host

 For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Have a USB device on the host:
Bus 006 Device 002: ID 03f0:1027 Hewlett-Packard Virtual keyboard and mouse

2. Create two separate xmls describing the virtual machine

3. Check the permissions/SELinux label of the USB device:
# ls -Z /dev/bus/usb/006/
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 001
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 002

4. Create the first VM and check the USB device:
# virsh create guest1.xml
Domain RHEL6-1 created from guest1.xml

# ls -Z /dev/bus/usb/006/
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 001
crw-rw-r--. qemu qemu system_u:object_r:svirt_image_t:s0:c741,c999 002

5. Create the second VM and check the permissions again:

# virsh create guest2.xml
Domain RHEL6-2 created from guest2.xml

# ls -Z /dev/bus/usb/006/
crw-rw-r--. root root system_u:object_r:usb_device_t:s0 001
crw-rw-r--. qemu qemu system_u:object_r:svirt_image_t:s0:c484,c564 002

	
Expected Results:

Libvirt should prevent assigning one USB device to two separate virtual machines.
Notes:
Comments:

		177384 	[NPIV] Restart the libvirtd to see if the wwpn is same as before 	nzhang 	None 	Manual 		--default-- 	P1 	3000 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NPIV

bug:

    No bug found

Actions:

1. Dumpxml vHBA XML.
    # virsh nodedev-dumpxml scsi_host9
    <device>
      <name>scsi_host9</name>
      <parent>scsi_host0</parent>
      <capability type='scsi_host'>
        <host>9</host>
        <capability type='fc_host'>
          <wwnn>2000001b3289f25b</wwnn>
          <wwpn>2100001b32890001</wwpn>
        </capability>
      </capability>
    </device>

2. Restart libvirtd daemon.
    # service libvirtd restart
    Stopping libvirtd daemon:                                  [  OK  ]
    Starting libvirtd daemon:                                  [  OK  ]

3. Confirm wwpn is consistent before libvirtd restart.
    # virsh nodedev-dumpxml scsi_host9
	
Expected Results:

output:

# virsh nodedev-dumpxml scsi_host9
    <device>
      <name>scsi_host9</name>
      <parent>scsi_host0</parent>
      <capability type='scsi_host'>
        <host>9</host>
        <capability type='fc_host'>
          <wwnn>2000001b3289f25b</wwnn>
          <wwpn>2100001b32890001</wwpn>
        </capability>
      </capability>
    </device>
Notes:
Comments:

		177390 	[NUMA] libvirtd drops client connection when querying capabilities on NUMA machines with sparse topology 	gren 	gren 	Manual 		Regression 	P1 	3020 	Edit
Setup:

There are two machines qualified for the regression test on beaker system https://beaker.engineering.redhat.com/

sgi-xe270-01.rhts.eng.bos.redhat.comï¼broken 02/24/2012ï¼
sun-x4440-01.rhts.eng.bos.redhat.com
intel-e78850-512-1.englab.nay.redhat.comï¼broken 02/24/2012ï¼
intel-e78850-512-2.englab.nay.redhat.comï¼broken 02/24/2012ï¼

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NUMA
    Regression

bug:

    No bug found

Actions:

bugzilla : https://bugzilla.redhat.com/show_bug.cgi?id=620847#c12

The bug was fixed on libvirt-0.8.1-27.el6 version

issure

[root@sgi-xe270-01 ~]# virsh capabilities
error: failed to get capabilities
error: server closed connection: 

[root@sgi-xe270-01 ~]# virt-install
ERROR    server closed connection: 
Traceback (most recent call last):
  File "/usr/sbin/virt-install", line 1054, in <module>
    main()
  File "/usr/sbin/virt-install", line 812, in main
    capsguest, capsdomain = get_virt_type(conn, options)
  File "/usr/sbin/virt-install", line 473, in get_virt_type
    capabilities = virtinst.CapabilitiesParser.parse(conn.getCapabilities())
  File "/usr/lib64/python2.6/site-packages/libvirt.py", line 1329, in
getCapabilities
    if ret is None: raise libvirtError ('virConnectGetCapabilities() failed',
conn=self)
libvirtError: server closed connection:

ensure, the function of "virsh capabilities" work well and virt-install could install a new guest 
on these two machine as follows:

[root@sgi-xe270-01 ~]# virsh hostname
sgi-xe270-01.rhts.bos.redhat.com

[root@sgi-xe270-01 ~]# rpm -q libvirt qemu-kvm kernel seabios python-virtinst
libvirt-0.8.1-27.el6.x86_64
qemu-kvm-0.12.1.2-2.112.el6.x86_64
kernel-2.6.32-66.el6.x86_64
seabios-0.5.1-3.el6.x86_64
python-virtinst-0.500.3-7.el6.noarch

[root@sgi-xe270-01 ~]# virsh capabilities
<capabilities>

  <host>
    <uuid>00020003-0004-0005-0006-000700080009</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Nehalem</model>
      <vendor>Intel</vendor>
      <topology sockets='2' cores='4' threads='2'/>
      <feature name='rdtscp'/>
      <feature name='dca'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>
    <migration_features>
      <live/>
      <uri_transports>
        <uri_transport>tcp</uri_transport>
      </uri_transports>
    </migration_features>
    <topology>
      <cells num='2'>
        <cell id='0'>
          <cpus num='16'>
            <cpu id='0'/>
            <cpu id='1'/>
            <cpu id='2'/>
            <cpu id='3'/>
            <cpu id='4'/>
            <cpu id='5'/>
            <cpu id='6'/>
            <cpu id='7'/>
            <cpu id='8'/>
            <cpu id='9'/>
            <cpu id='10'/>
            <cpu id='11'/>
            <cpu id='12'/>
            <cpu id='13'/>
            <cpu id='14'/>
            <cpu id='15'/>
          </cpus>
        </cell>
        <cell id='2'>
          <cpus num='0'>
          </cpus>
        </cell>
      </cells>
    </topology>
    <secmodel>
      <model>selinux</model>
      <doi>0</doi>
    </secmodel>
  </host>

  <guest>
    <os_type>hvm</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.0.0</machine>
      <machine canonical='rhel6.0.0'>pc</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <pae/>
      <nonpae/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.0.0</machine>
      <machine canonical='rhel6.0.0'>pc</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

</capabilities>

[root@sgi-xe270-01 ~]# virt-install --hvm --name vm642 --ram 1024 --vcpus=2
--vnc --location
http://download.lab.bos.redhat.com/redhat/rel-eng/RHEL6.0-20100818.0/6/Server/x86_64/os
--disk path=/var/lib/libvirt/images/vm642.img.raw,size=6,device=disk
--os-variant=rhel6 --debug --extra-args
ks=http://jenner.bos.redhat.com/ks/virtio.cfg --network network=default




 
	
Expected Results:
Notes:
Comments:

		177405 	[PCI and USB device assignment] Check owner and selinux label after hot-unplugging a USB device from guest with managed mode-bug825068 	ajia 	ajia 	Auto 		Feature 	P1 	3030 	Edit
Setup:

prepare 1 usb flash and plug into host
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. virsh nodedev-list --tree

  |   +- usb_usb2
  |       |
  |       +- usb_2_0_1_0
  |       +- usb_2_1
  |       |   |
  |       |   +- usb_2_1_1_0
  |       |       |
  |       |       +- scsi_host10
  |       |           |
  |       |           +- scsi_target10_0_0
  |       |               |
  |       |               +- scsi_10_0_0_0
  |       |                   |
  |       |                   +- block_sdb_Kingston_DT_101_II_001CC0EC345AF9C126920DED_0_0
  |       |                    
  |       +- usb_2_2
  |           |
  |           +- usb_2_2_1_0
  |               |
  |               +- scsi_host1
  |                   |
  |                   +- scsi_target1_0_0
  |                       |
  |                       +- scsi_1_0_0_0
  |                           |
  |                           +- block_sdc_2_0_Flash_Disk_18202000F483C300_0_0

2. # virsh nodedev-dumpxml usb_2_1

3. check usb device owner and selinux context:

# ll -Z /dev/bus/usb/002/003

4. Start the guest

5. Attach the following xml to the guest by virsh attach-device.

    <hostdev mode='subsystem' type='usb' managed='yes'>
      <source>
        <address bus='2' device='3'/>
      </source>
    </hostdev>

6. Detach the usb device from guest by virsh detach-device.

7. Repeat step 3

8.  # virsh attach-device guest usb.xml --persistent

# virsh start guest

Domain guest started

9.  Start second guest with assigned same usb device, check the label of usb

# virsh attach-device guest2 usb.xml --persistent

# virsh start  guest2

error: Failed to start domain guest2

error: Requested operation is not valid: USB device 002:003 is in use by domain guest.

10. check usb device owner and selinux context:

# ll -Z /dev/bus/usb/002/003

 

 
	
Expected Results:

Make sure you can got the same result between step 3 and step 7. step10.
Notes:
Comments:

		177406 	[PCI and USB device assignment] Check whether return device to host when destroying guest with managed mode after stopping libvirtd service 	ajia 	ajia 	Auto 		Feature 	P1 	3040 	Edit
Setup:

- VT-d is enabled

- Could lose connection with host after dettach network device from host

 For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment

bug:

    No bug found

Actions:

1. Start a guest 

# virsh start domain

2. Attach assiginable PCI device to guest with  'managed' mode

# virsh attach-device dom hostpci.xml

3. Stop libvirtd service

# service libvirtd restart

4. Destroy the guest

	
Expected Results:

The device should be returned to host after destroying or shutting down the guest.

ï»¿
Notes:
Comments:

		177364 	[Node devices] Dump usb device xml info 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	3110 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree

2. Select one of usb pci device on the host

# virsh nodedev-dumpxml pci_1002_4398_0
	
Expected Results:

1. Output:

computer
 |
  +- net_66_ba_7b_14_a5_08
  +- net_computer_loopback
  +- net_e2_d8_18_80_29_04
  +- pci_1002_4383
  +- pci_1002_4384

.................

2. Output:

<device>
  <name>pci_1002_4398_0</name>
  <parent>computer</parent>
  <driver>
    <name>ohci_hcd</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>0</bus>
    <slot>19</slot>
    <function>1</function>
    <product id='0x4398'>SB700 USB OHCI1 Controller</product>
    <vendor id='0x1002'>ATI Technologies Inc</vendor>
  </capability>
</device>
Notes:
Comments:

		177366 	[Node devices] node driver should be back after reattach node device - bug 713697 	mzhan 	mzhan 	Manual 		Regression 	P1 	3120 	Edit
Setup:

Please try with 82576 card to test
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    node devices

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree
...
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |       |
  |       +- net_eth1_00_1b_21_39_8b_19

...
# ifconfig -a
eth0      Link encap:Ethernet  HWaddr 00:1B:21:39:8B:18  
          inet addr:10.66.4.218  Bcast:10.66.7.255  Mask:255.255.252.0
          inet6 addr: fe80::21b:21ff:fe39:8b18/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:7524 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1724 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:2762743 (2.6 MiB)  TX bytes:205807 (200.9 KiB)
          Memory:f4800000-f4820000 

eth1      Link encap:Ethernet  HWaddr 00:1B:21:39:8B:19  
          inet addr:10.66.4.219  Bcast:10.66.7.255  Mask:255.255.252.0
          inet6 addr: fe80::21b:21ff:fe39:8b19/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:6317 errors:0 dropped:0 overruns:0 frame:0
          TX packets:1355 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:1566626 (1.4 MiB)  TX bytes:188221 (183.8 KiB)
          Memory:f4820000-f4840000

# readlink /sys/bus/pci/devices/0000\:03\:00.1/driver/ -f
/sys/bus/pci/drivers/igb

2. # virsh nodedev-dettach pci_0000_03_00_1 
Device pci_0000_03_00_1 dettached 

# readlink /sys/bus/pci/devices/0000\:03\:00.1/driver/ -f 
/sys/bus/pci/drivers/pci-stub 

# virsh nodedev-reset pci_0000_03_00_1 
Device pci_0000_03_00_1 reset 

# readlink /sys/bus/pci/devices/0000\:03\:00.1/driver/ -f 
/sys/bus/pci/drivers/pci-stub 

 3. # virsh nodedev-reattach pci_0000_03_00_1

Device pci_0000_03_00_1 re-attached

# readlink /sys/bus/pci/devices/0000\:03\:00.1/driver/ -f

/sys/bus/pci/drivers/igb

# virsh nodedev-reset pci_0000_03_00_1

Device pci_0000_03_00_1 reset

# readlink /sys/bus/pci/devices/0000\:03\:00.1/driver/ -f

/sys/bus/pci/drivers/igb

4. # ip link
	
Expected Results:

From step 3 and step 4 pci-igb is added back  and eth1 is back.

Step 3. # readlink /sys/bus/pci/devices/0000\:03\:00.1/driver/ -f

/sys/bus/pci/drivers/igb

Step 4. # ip link

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 00:1b:21:39:8b:18 brd ff:ff:ff:ff:ff:ff

4: eth2: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN qlen 1000 link/ether d8:d3:85:7e:61:9b brd ff:ff:ff:ff:ff:ff

5: virbr0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN link/ether 52:54:00:b7:9a:4e brd ff:ff:ff:ff:ff:ff

6: virbr0-nic: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN qlen 500 link/ether 52:54:00:b7:9a:4e brd ff:ff:ff:ff:ff:ff

8: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000 link/ether 00:1b:21:39:8b:19 brd ff:ff:ff:ff:ff:ff

 
Notes:
Comments:

		177321 	[Migration]During live migration, fail gracefully on network error BZ 822839 	yimwang 	None 	Manual 		--default-- 	P2 	3130 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    regression
    RHEL6.5

bug:

    No bug found

Actions:

1. Start a domain;
   # virsh start <domain>

2. Start a live migration;
   # virsh migrate --live <domain> qemu+ssh://host2/system

3. While the migration is happening, pull the network cable out of the destination system.

4. While the migration is happening, stop the network

 

	
Expected Results:

3.Pass criteria: The migration should fail, and the guest should still  be running on the source.
4.Pass criteria: The migration should fail, and the guest should still  be running on the source.

BUG 822839 is not fixed, so keep the case NEED_UPDATE.

Notes:
Comments:

		177367 	[Node devices] Reattach a pci device which is using by guest to host - bug 603039 	mzhan 	mzhan 	Manual 		Negative test 	P1 	3130 	Edit
Setup:

1.# lsmod | grep kvm
   kvm_intel              52890    0 
   kvm                       315104  1    kvm_intel
2.#modprobe -r kvm_intel
3.#modprobe -r kvm
4.#modprobe kvm allow_unsafe_assigned_interrupts=1"

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    node devices

bug:

    No bug found

Actions:

1. Select a network pci device from host
   # virsh nodedev-dumpxml pci_0000_03_02_0
 <device>
  <name>pci_0000_03_02_0</name>
  <parent>pci_0000_00_1e_0</parent>
  <driver>
    <name>e1000</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>3</bus>
    <slot>2</slot>
    <function>0</function>
    <product id='0x107c'>82541PI Gigabit Ethernet Controller</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
    <capability type='virt_functions'>
    </capability>
  </capability>
</device>
2. Dettach a network pci device from host
   # virsh nodedev-dettach pci_0000_03_02_0
   Device pci_0000_03_02_0 dettached
   # virsh nodedev-reset pci_0000_03_02_0
   Device pci_0000_03_02_0 reset
3. Add this pci device info into guest xml config file
    <hostdev mode='subsystem' type='pci'>
      <source>
      <address bus='3' slot='2' function='0'/>
      </source>
    </hostdev>
4. Run the guest
   # virsh define rhel6u2_x86_64_kvm.xml 
   Domain rhe6u2_x86_64_kvm defined from rhel6u2_x86_64_kvm.xml
   # virsh start rhel6u42_x86_64_kvm
   Domain rhel6u2_x86_64_kvm started
   The pci device works well in the guest
5. In host, try to reattach the assigned pci device
    # virsh nodedev-reattach pci_0000_03_02_0

error: Failed to reset device pci_0000_03_00_0
error: internal error Not reattaching active device 0000:03:02.0

# readlink /sys/bus/pci/devices/0000\:03\:02.0/driver
../../../../bus/pci/drivers/pci-stub 

 

 
	
Expected Results:

All checkpoints are passed.
Notes:
Comments:

		177322 	[Migration]During live migration, restart destination libvirtd 	yimwang 	None 	Manual (Autoproposed) 		--default-- 	P2 	3140 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Start a domain;
   # virsh start <domain>

2. Start a live migration;
   # virsh migrate --live <domain> qemu+ssh://host2/system

3. While the migration is happening, reboot libvirtd on the destination of the migration;
   # service libvirtd restart


	
Expected Results:

3.The migration should fail and the guest still running on source machine.

Error like:

# virsh migrate --live $guest qemu+ssh://$targetMachine/system

error: internal error client socket is closed

 

Notes:
Comments:

		177368 	[Node devices] Reattach node device on host 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	3140 	Edit
Setup:

One node device such as pci_0000_00_19_0 has been dettached from host
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices

bug:

    No bug found

Actions:

1. # virsh nodedev-reattach pci_0000_00_19_0

2. Check whether reattached device works

# ifconfig

3. # service network restart

  # ifconfig
	
Expected Results:

1. Output:

Device pci_0000_00_19_0 re-attached

Also check this pci driver:

# readlink /sys/bus/pci/devices/0000:\00:\19.0/driver/ -f

   /sys/bus/pci/drivers/e1000e

2. eth0 re-appear

eth0      Link encap:Ethernet  HWaddr 00:24:21:7F:B7:3A  
          UP BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)
          Interrupt:18

.........................

3. Output

eth0      Link encap:Ethernet  HWaddr 00:24:21:7F:B7:3A  
          inet addr:10.66.70.159  Bcast:10.66.70.255  Mask:255.255.255.0
          inet6 addr: fe80::224:21ff:fe7f:b73a/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:2 errors:0 dropped:0 overruns:0 frame:0
          TX packets:18 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:440 (440.0 b)  TX bytes:3874 (3.7 KiB)
          Interrupt:18

.............................
Notes:
Comments:

		177323 	[Migration]During live migration, restart source libvirtd - bug 690175,634065,728603 	yimwang 	None 	Manual (Autoproposed) 		Regression 	P2 	3150 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

 2. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Start a domain;
   # virsh start <domain>

2. Start a live migration;
   # virsh migrate --live <domain> qemu+ssh://host2/system

3. While the migration is happening, reboot libvirtd on the source of the migration;
   # service libvirtd restart

4. Restart domain
# virsh destroy <domain>
# virsh start <domain>

5. Start a live migration;

# virsh migrate --persistent --undefinesource --tunnelled --p2p --live <domain> qemu+ssh://{host2 ip}/system 

6. While the migration is happening, reboot libvirtd on the source of the migration;
   # service libvirtd restart

 




	
Expected Results:

3.Pass criteria:

If it fails, it should fail gracefully with reasonable error message, and on target host, the domain should be destroyed.

6. Pass criteria:

The migration is cancelled and the guest is running on the src host and does not exist on the dst host.

 

 

Notes:
Comments:

		177369 	[Node devices] Reset node device from host 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	3150 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices

bug:

    No bug found

Actions:

1. # virsh nodedev-list

2.Select one of pci device on the hsot

# virsh nodedev-dettach pci_14e4_167a

3. # virsh nodedev-reset pci_14e4_167a
	
Expected Results:

3. Output

Device pci_14e4_167a reset
Notes:
Comments:

		177324 	[Migration]Live migration of a domain with a PCI device hotplugged 	yimwang 	None 	Manual 		--default-- 	P2 	3160 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort"

if pci device is nic, need to do it on dual nic machine

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Start a domain with no PCI devices passed-through;
   # virsh start <domain>

2. Hotplug a PCI device to the domain;
   # virsh attach-device <domain> <device.xml>

3. Live migrate the domain;
   # virsh migrate --live <domain> qemu+ssh://host2/system


	
Expected Results:

3.The live migration should fail with
error: Requested operation is not valid: Domain with assigned non-USB host devices cannot be migrated.

The domain should continue to run on the source host.

Notes:
Comments:

		177370 	[Node devices] Support for wireless NIC node device 	mzhan 	None 	Manual 		Feature 	P2 	3160 	Edit
Setup:

Choose a notebook which support wireless nic
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    node devices

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree

2. Dump this node information

1) # virsh nodedev-dumpxml net_wlan0_58_94_6b_66_0e_60

2) # virsh nodedev-dumpxml pci_0000_02_00_0

3. Check this wireless nic device can also be dettached and reattached

        # virsh nodedev-dettach pci_0000_02_00_0

          Device pci_0000_02_00_0 dettached

       # ifconfig -a

         wlan0 is disable

     # virsh nodedev-reset pci_0000_02_00_0

        Device pci_0000_02_00_0 reset

     # virsh nodedev-reattach pci_0000_02_00_0

       Device pci_0000_02_00_0 re-attached

     # ifconfig -a

       wlan0 is re-appear and works well
	
Expected Results:

1........  

  +- pci_0000_00_1c_4
  |   |
  |   +- pci_0000_02_00_0
  |       |
  |       +- net_wlan0_58_94_6b_66_0e_60

.....

2. 1) # virsh nodedev-dumpxml net_wlan0_58_94_6b_66_0e_60
<device>
  <name>net_wlan0_58_94_6b_66_0e_60</name>
  <parent>pci_0000_02_00_0</parent>
  <capability type='net'>
    <interface>wlan0</interface>
    <address>58:94:6b:66:0e:60</address>
    <capability type='80211'/>
  </capability>
</device>

2) # virsh nodedev-dumpxml pci_0000_02_00_0
<device>
  <name>pci_0000_02_00_0</name>
  <parent>pci_0000_00_1c_4</parent>
  <driver>
    <name>iwlagn</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>2</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x4239'>Centrino Advanced-N 6200</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

3. Check the result is as expected
Notes:
Comments:

		177325 	[Migration]Live migration of a domain with hotplugged NICs 	yimwang 	None 	Manual 		--default-- 	P2 	3170 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Start a domain;
   # virsh start <domain>

2. Hotplug an interface to the domain;
   # virsh attach-interface <domain> bridge virbr0

3. Live migrate the domain;
   # virsh migrate --live <domain> qemu+ssh://host2/system


	
Expected Results:

Pass criteria: The live migration should successfully finish and the hotplugged interface should be accessible in the domain.

Notes:
Comments:

		177371 	[Node devices] Try to create node device which is not a fibre channel HBA- bug 510426 	yoyzhang 	yoyzhang 	Manual 		Regression 	P3 	3170 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices

bug:

    No bug found

Actions:

1. Prepare a xml as following:

<device>
  <name>pci_0000_00_19_0</name>
  <parent>computer</parent>
  <driver>
    <name>e1000e</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>0</bus>
    <slot>19</slot>
    <function>0</function>
    <product id='0x10de'>82567LM-3 Gigabit Network Connection</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

2. Try to create this device using this xml file

# virsh nodedev-create my_device.xml

	
Expected Results:

2.

Error message is proper as following:
# virsh nodedev-create my_device.xml
error: Failed to create node device from cc.xml
error: this function is not supported by the hypervisor: Device is not a fibre channel HBA

The following message is not correct:
libvir: Domain Config error : internal error incorrect root element



Notes:
Comments:

		177330 	[Migration]Offline migration 	yimwang 	None 	Auto 		--default-- 	P1 	3180 	Edit
Setup:

same as case "[domain async job handling] live coredump & domjobabort "
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Start up a domain;
   # virsh start <domain>

2. Migrate the domain to the second system;
   # virsh migrate <domain> qemu+ssh://host2/system

Note : no --live is means offline mode

	
Expected Results:

2.Pass criteria: The domain should be paused and get migrated to host2.

 

if you use offline migration, the guest will be paused on source machine then...migration start


if you don't use offline, just like live migration, the guest will be running then... migration start , the guest should be available the entire time
Notes:
Comments:

		177372 	[Node devices]Virtio Net/Disk block devices get correct parent in node device info - bug 570130 	mzhan 	None 	Manual 		Regression 	P2 	3180 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    node devices

bug:

    No bug found

Actions:

1. Prepare a rhel6 guest installed with libvirt. XML is like following:

<domain type='kvm' id='26'>
  <name>test</name>
  <uuid>d51c83b8-3e25-277a-3019-17789e61d4b8</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>2</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.1.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/mnt/rhel61_x86_64.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/data.img'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/data1.img'/>
      <target dev='vdc' bus='virtio'/>
      <alias name='virtio-disk2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </disk>
    <interface type='network'>
      <mac address='52:54:00:d9:aa:07'/>
      <source network='default'/>
      <target dev='vnet1'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:bb:e2:e9'/>
      <source network='default'/>
      <target dev='vnet2'/>
      <model type='virtio'/>
      <alias name='net1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/6'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/6'>
      <source path='/dev/pts/6'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='tablet' bus='usb'>
      <alias name='input0'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5901' autoport='yes'/>
    <sound model='ac97'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c152,c156</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c152,c156</imagelabel>
  </seclabel>
</domain>

2. Make sure the guest kernel is no earlier than 2.6.32-112 

3. Start the guest and check virtio nic/storage should get the correct parent

# virsh start <guest>

In guest :

# virsh nodedev-list --tree
	
Expected Results:

2. Guest start successfully and In guest the tree should like these:

The virtio pci should be similiar with this:
...
  +- pci_0000_00_19_0
  |   |
  |   +- net_eth0_f0_de_f1_30_a3_49

...

The virtio storage should be similiar with this:
...
  +- pci_0000_00_1f_2
  |   |
  |   +- scsi_host0
  |   |   |
  |   |   +- scsi_target0_0_0
  |   |       |
  |   |       +- scsi_0_0_0_0
  |   |           |
  |   |           +- block_sda_WDC_WD2500BEVT_08A23T1_WD_WX31AB0J9035

...

 

Note: This format virtio nic/storage directly connect to computer is not correct.

computer
 |
  +- block_vda
  +- net_eth0_54_52_00_1a_ee_20
  +- net_eth1_54_52_00_1a_ee_21
  +- net_eth2_54_52_00_1a_ee_22
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
.......

Notes:
Comments:

		177331 	[Migration]Tunnelled migration 	yimwang 	None 	Manual 		--default-- 	P2 	3190 	Edit
Setup:
1. same as case "[domain async job handling] migrate & domjobabort"

2. dispatch ssh publick key of source host to target host.  so that we don't need input the passphrase.

- Creating your local public key pair (by running "ssh-keygen -t rsa ", just give the default answer to the request questions.)
- Copying the public key to a remote host by "$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@somehost "
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Tunnelled migration

   # virsh migrate --tunnelled--p2p --live toy qemu+ssh://${target_source_host}/system --verbose

2. if step 1 was successful, check the domain state on both source host, target host

    # virsh domstate toy

3. connect to migrated guest on target host. check if it's live, execute some commands in it. e.g:

    # virt-viewer -c qemu+ssh://${target_host_ip}/system toy

    In the guest, open the terminal, then

    [guest] # cat /var/log/messages
	
Expected Results:

step 1

     can be migrated successfully, can see the progress change during migration

 

step 2:

      on source host, domain toy is shutoff. if toy is not defined on source host before, it will not exist any more.

      on target host, it's running

 

step 3:

       the guest is live, and runs normally
Notes:
Comments:

		177373 	[Node devices]Virtio Net/Disk block devices get correct parent in node device info - bug 570130 	mzhan 	mzhan 	Manual 		Regression 	P2 	3190 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    node devices

bug:

    No bug found

Actions:

1. Prepare a rhel6 guest installed with libvirt. XML is like following:

<domain type='kvm' id='26'>
  <name>test</name>
  <uuid>d51c83b8-3e25-277a-3019-17789e61d4b8</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>2</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.1.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/mnt/rhel61_x86_64.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/data.img'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/data1.img'/>
      <target dev='vdc' bus='virtio'/>
      <alias name='virtio-disk2'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08' function='0x0'/>
    </disk>
    <interface type='network'>
      <mac address='52:54:00:d9:aa:07'/>
      <source network='default'/>
      <target dev='vnet1'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:bb:e2:e9'/>
      <source network='default'/>
      <target dev='vnet2'/>
      <model type='virtio'/>
      <alias name='net1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x09' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/6'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/6'>
      <source path='/dev/pts/6'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='tablet' bus='usb'>
      <alias name='input0'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5901' autoport='yes'/>
    <sound model='ac97'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c152,c156</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c152,c156</imagelabel>
  </seclabel>
</domain>

2. Make sure the guest kernel is no earlier than 2.6.32-112 

3. Start the guest and check virtio nic/storage should get the correct parent

# virsh start <guest>

In guest :

# virsh nodedev-list --tree
	
Expected Results:

2. Guest start successfully and In guest the tree should like these:

The virtio pci should be similiar with this:
...
  +- pci_0000_00_19_0
  |   |
  |   +- net_eth0_f0_de_f1_30_a3_49

...

The virtio storage should be similiar with this:
...
  +- pci_0000_00_1f_2
  |   |
  |   +- scsi_host0
  |   |   |
  |   |   +- scsi_target0_0_0
  |   |       |
  |   |       +- scsi_0_0_0_0
  |   |           |
  |   |           +- block_sda_WDC_WD2500BEVT_08A23T1_WD_WX31AB0J9035

...

 

Note: This format virtio nic/storage directly connect to computer is not correct.

computer
 |
  +- block_vda
  +- net_eth0_54_52_00_1a_ee_20
  +- net_eth1_54_52_00_1a_ee_21
  +- net_eth2_54_52_00_1a_ee_22
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
.......

Notes:
this case is duplicated with the case https://tcms.engineering.redhat.com/case/177372/?from_plan=6578
Comments:

		177332 	[Migration]Use an alternate migration URI to do the migration 	yimwang 	None 	Manual 		--default-- 	P2 	3200 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort"

For TLS config, refer to case 39342 [Remote access] Connect to the hypervisor running on host using TLS.

For TCP config, refer to case 59342 [libvirtd] remote access via plain tcp.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Use an alternate migration URI to do the migration

     # virsh migrate --live  vm1 qemu+ssh://${target_source_host}/system

    # virsh migrate --live  vm2 qemu+tcp://${target_source_host}/system

    # virsh migrate --live  vm3 qemu+tls://${target_source_host}/system

2. if step 1 was successful, check the domain state on both source host, target host

    # virsh domstate toy

3. connect to migrated guest on target host. check if it's live, execute some commands in it. e.g:

    # virt-viewer  toy

    In the guest, open the terminal, then

    [guest] # cat /var/log/messages
	
Expected Results:

step 1

    All  can be migrated successfully

 

step 2:

      on source host, domain toy is shutoff. if toy is not defined on source host before, it will not exist any more.

      on target host, it's running

 

step 3:

       the guest is live, and runs normally
Notes:
Comments:

		177374 	[NPIV] Create a virtual HBA with NPIV 	nzhang 	None 	Manual 		Feature 	P1 	3200 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NPIV
    RHEL6.0

bug:

    No bug found

Actions:

# ls /sys/class/fc_host/
host0  host1 

# cd host1

# echo '2001001b32a9f001:2101001b32a9f25b' > /sys/class/fc_host/host5/vport_create

# ls /sys/class/fc_host/
host0  host1  host9

# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host2
scsi_host3
scsi_host4
scsi_host5
scsi_host6
scsi_host7
scsi_host8
scsi_host9

# virsh nodedev-dumpxml scsi_host9
<device>
  <name>scsi_host9</name>
  <parent>scsi_host1</parent>
  <capability type='scsi_host'>
    <host>9</host>
    <capability type='fc_host'>
      <wwnn>2101001b32a9f25b</wwnn>
      <wwpn>2001001b32a9f001</wwpn>
    </capability>
  </capability>
</device>



Once you know the node device name of the parent HBA, create a file called newhba.xmlcontaining XML describing the virtual HBA to create:

<device>
  <parent>scsi_host1</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
      <wwnn>2101001b32a9f25b</wwnn>
      <wwpn>2001001b32a9f001</wwpn>
    </capability>
  </capability>
</device>

 

 

# virsh nodedev-create newhba.xml
Node device scsi_host10 created from newhba.xml



	
Expected Results:

Verify that virsh nodedev-create [xml file] can create a virtual HBA.

Verify that a virtual HBA can be created successfully using virsh command to check the details of the virtual HBA.
Notes:
Comments:

		177375 	[NPIV] Create and destroy vport frequently - bug595490 	nzhang 	None 	Auto 		Stress 	P1 	3210 	Edit
Setup:

This test case takes 8 hours
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NPIV

bug:

    No bug found

Actions:

1. Create new hba on the host

# cat virtualhba.xml
<device>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9f012</wwpn>
    </capability>
  </capability>
</device>

2.# virsh nodedev-create virtualhba.xml
Node device scsi_host15 created from virtualhba.xml

3.# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host15
scsi_host2
scsi_host3
scsi_host4
scsi_host5



4. # cat test.sh

#!/bin/bash

hba_xml=$1
timeout=28800    # 8h
interval=2
log="/var/log/mem_monitor.log"

#Output memory usage of libvirt into log
output_log(){
  sleep $interval
  date "+%F %T" >> $log
  ps -C libvirtd -o rss,size,vsize >> $log
  echo >> $log
}

#Get new creation node device name
get_latest_node_name(){
  node_name=scsi_$(ls --time=ctime /sys/class/fc_host/|head -1)
  echo $node_name
}


if [ $# -ne 1 ]; then
  echo "Usage: <mem_monitor> <virtual_hba.xml>"
  exit 1
fi

lspci | grep -i hba
virsh nodedev-list --cap=scsi_host
ls -l /sys/class/fc_host/
ls -l /sys/class/scsi_host/


while [ $timeout -gt 0 ]
do
  output_log
  virsh nodedev-create $hba_xml
  output_log
  virsh nodedev-destroy $(get_latest_node_name)
  output_log
  let timeout=$timeout-3*$interval
done

echo "log info: $log"

5. # ./test.sh virtualhba.xml

	
Expected Results:

5. The RSS range similar from 9468 to 11560, and SZ=457100, VSZ=609080.

The log /var/log/mem_monitor.log should be looks like

.........
2010-07-12 22:31:35
  RSS    SZ    VSZ
10120 457100 609080

2010-07-12 22:31:37
  RSS    SZ    VSZ
10508 457100 609080

2010-07-12 22:31:39
  RSS    SZ    VSZ
10120 457100 609080

2010-07-12 22:31:41
  RSS    SZ    VSZ
10120 457100 609080

2010-07-12 22:31:43
  RSS    SZ    VSZ
 9856 457100 609080

2010-07-12 22:31:45
  RSS    SZ    VSZ
 9468 457100 609080

2010-07-12 22:31:47
  RSS    SZ    VSZ
 9468 457100 609080

2010-07-12 22:31:49

ï»¿

......
Notes:
Comments:

		177376 	[NPIV] Create and destroy vport frequently - bug595490 	nzhang 	nzhang 	Auto 		Stress 	P1 	3220 	Edit
Setup:

This test case takes 8 hours
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NPIV
    Regression

bug:

    No bug found

Actions:

1. Create new hba on the host

# cat virtualhba.xml
<device>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9f012</wwpn>
    </capability>
  </capability>
</device>

2.# virsh nodedev-create virtualhba.xml
Node device scsi_host15 created from virtualhba.xml

3.# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host15
scsi_host2
scsi_host3
scsi_host4
scsi_host5



4. # cat test.sh

#!/bin/bash

hba_xml=$1
timeout=28800    # 8h
interval=2
log="/var/log/mem_monitor.log"

#Output memory usage of libvirt into log
output_log(){
  sleep $interval
  date "+%F %T" >> $log
  ps -C libvirtd -o rss,size,vsize >> $log
  echo >> $log
}

#Get new creation node device name
get_latest_node_name(){
  node_name=scsi_$(ls --time=ctime /sys/class/fc_host/|head -1)
  echo $node_name
}


if [ $# -ne 1 ]; then
  echo "Usage: <mem_monitor> <virtual_hba.xml>"
  exit 1
fi

lspci | grep -i hba
virsh nodedev-list --cap=scsi_host
ls -l /sys/class/fc_host/
ls -l /sys/class/scsi_host/


while [ $timeout -gt 0 ]
do
  output_log
  virsh nodedev-create $hba_xml
  output_log
  virsh nodedev-destroy $(get_latest_node_name)
  output_log
  let timeout=$timeout-3*$interval
done

echo "log info: $log"

5. # ./test.sh virtualhba.xml

	
Expected Results:

5. The RSS range similar from 9468 to 11560, and SZ=457100, VSZ=609080.

The log /var/log/mem_monitor.log should be looks like

.........
2010-07-12 22:31:35
  RSS    SZ    VSZ
10120 457100 609080

2010-07-12 22:31:37
  RSS    SZ    VSZ
10508 457100 609080

2010-07-12 22:31:39
  RSS    SZ    VSZ
10120 457100 609080

2010-07-12 22:31:41
  RSS    SZ    VSZ
10120 457100 609080

2010-07-12 22:31:43
  RSS    SZ    VSZ
 9856 457100 609080

2010-07-12 22:31:45
  RSS    SZ    VSZ
 9468 457100 609080

2010-07-12 22:31:47
  RSS    SZ    VSZ
 9468 457100 609080

2010-07-12 22:31:49

ï»¿

......
Notes:
Comments:

		177377 	[NPIV] Create more than 2 vports and destroy one to see if others can still work 	nzhang 	None 	Manual 		--default-- 	P1 	3230 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NPIV

bug:

    No bug found

Actions:

1. Create 2 vHBAs.
   Refer to case 39814 [NPIV] Create a virtual HBA with NPIV

2. Connect 1 lun for each of vHBAs.
   Refer to case 39817 [NPIV] Discover SAN Storage

3. Destroy 1 vHBA.

   # virsh nodedev-destroy ${vhba1}

4. Create a file on the disk.

----------If your stroage is sigle-path---------------

   #mkfs.ext3 /dev/sdb1
   # mount /dev/sdX /mnt
   # cd /mnt
   # dd if=/dev/zero of=./write_test bs=1024 count=100
   # ls -lh write_test

----------If your stroage is multi-path---------------

   #mkfs.ext3 mkfs.ext3 /dev/dm-X
   # mount /dev/dm-X /mnt
   # cd /mnt
   # dd if=/dev/zero of=./write_test bs=1024 count=100
   # ls -lh write_test
	
Expected Results:

Expectd results:

3. Confirm only 1 lun is connected by one vHBA.
   # fdisk -l

4. 


# ls -lh write_test
-rw-r--r--. 1 root root 1.0K Mar 15 08:44 write_test

Notes:
Comments:

		177393 	[NUMA] VM CPU Runtime pinning 	xhu 	None 	Auto 		--default-- 	P1 	3230 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    cpu

bug:

    No bug found

Actions:

1 Start a guest demo(vcpu=2)

2 issue the below command to see the pinning configuration for a running domain demo:

   # virsh vcpuinfo demo

3 issue the below command to change cpu pinning:

   # virsh vcpupin $vmname $vcpu_number $cpuset (commas ONLY)

   e.g. virsh vcpupin demo 0 1

4 verify that the VM is only running on the pinned host VCPUs.

4.1 issue the below command to see the pinning configuration for a running domain demo

   # virsh vcpuinfo demo

4.2 issue the below command to get Guest process num

   # ps aux|grep kvm

4.3 issue the below command to check wchich cpu are allowed

   # grep Cpus_allowed /proc/4283/task/*/status
	
Expected Results:

2

VCPU:          0
CPU:            1
State:          running
CPU time:       10.8s
CPU Affinity:   yyyy

VCPU:          1
CPU:            3
State:          running
CPU time:       6.7s
CPU Affinity:   yyyy

3    
#virsh vcpupin demo 0 1
#virsh vcpupin demo 1 1

4
4.1
# virsh vcpuinfo demo
VCPU:           0
CPU:            1
State:          running
CPU time:       11.5s
CPU Affinity:   -y--

VCPU:           1
CPU:            1
State:          running
CPU time:       7.5s
CPU Affinity:   -y--

4.2
# ps aux|grep kvm
  760 ?        S      0:00 [kvm-irqfd-clean]
20517 ?        Sl     0:29 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 1024 -smp 2,sockets=2,cores=1,threads=1 -name demo -uuid 28d25a2a-38ba-8d54-38e9-8708f071556c -nodefconfig -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/rhel6.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -drive file=/var/lib/libvirt/images/rhel6.img,if=none,id=drive-virtio-disk0,boot=on,format=raw,cache=none -device virtio-blk-pci,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0 -netdev tap,fd=20,id=hostnet0,vhost=on,vhostfd=21 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:6f:80:64,bus=pci.0,addr=0x3 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -device usb-tablet,id=input0 -vnc 127.0.0.1:0 -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x4 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6
20524 ?        S      0:00 [kvm-pit-wq]
20886 pts/0    S+     0:00 grep kvm
4.3
# grep Cpus_allowed /proc/20517/task/*/status
/proc/20517/task/20517/status:Cpus_allowed:    0f
/proc/20517/task/20517/status:Cpus_allowed_list:    0-3
/proc/20517/task/20528/status:Cpus_allowed:    02
/proc/20517/task/20528/status:Cpus_allowed_list:    1
/proc/20517/task/20529/status:Cpus_allowed:    02
/proc/20517/task/20529/status:Cpus_allowed_list:    1
/proc/20517/task/20955/status:Cpus_allowed:    02
/proc/20517/task/20955/status:Cpus_allowed_list:    1



Notes:
Comments:

		177378 	[NPIV] Create up to 127 vport 	nzhang 	None 	Manual 		--default-- 	P1 	3240 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NPIV

bug:

    No bug found

Actions:

1. Save the following with newhba.xml

    <device>
      <parent>#PARENT_SCSI_HOST#</parent>
      <capability type='scsi_host'>
        <capability type='fc_host'>
          <wwnn>#WWNN#</wwnn>
          <wwpn>#WWPN#</wwpn>
        </capability>
      </capability>
    </device>

2. Save the below script with create_vhba_max.sh

#!/bin/bash

get_vport_parent() {
    local node_dev

    for node_dev in $(virsh nodedev-list --cap=scsi_host); do
        if virsh nodedev-dumpxml $node_dev | grep 'vport_ops' > /dev/null; then
            PARENT_HBA=$node_dev
            echo "The parent HBA is '$node_dev'."
            return 0
        fi
    done

    if [ -z $PARENT_HBA ]; then
        echo "No any HBA has NPIV capability."
        return 1
    fi
}

get_vport_parent
parent_hba=$PARENT_HBA
parent_wwnn=$(virsh nodedev-dumpxml $parent_hba | grep wwnn | cut -c13-28)
parent_wwpn=$(virsh nodedev-dumpxml $parent_hba | grep wwpn | cut -c13-28)
wwpn_prefix=$(echo $parent_wwpn | awk '{print substr($0,1,12)}')

for i in {001..127}; do
    child_wwpn="${wwpn_prefix}0${i}"
    wwpn_tmp=$(mktemp)
    cp -f newhba.xml $wwpn_tmp
    sed -i -e "s/#PARENT_SCSI_HOST#/${parent_hba}/" \
           -e "s/#WWNN#/${parent_wwnn}/" \
           -e "s/#WWPN#/${child_wwpn}/" $wwpn_tmp
    virsh nodedev-create $wwpn_tmp
done

3. Run the script.
   # sh create_vhba_max.sh

	
Expected Results:

Totally 127 vHBAs are created successfully.
Notes:
Comments:

		177379 	[NPIV] Destroy a virtual HBA with NPIV 	nzhang 	None 	Manual 		Feature 	P1 	3250 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NPIV
    RHEL6.0

bug:

    No bug found

Actions:

First, do test case "39814 [NPIV] Create a virtual HBA with NPIV"

# echo '101001b32a90001:2001001b32a90550' > /sys/class/fc_host/host5/vport_delete

# virsh nodedev-destroy pci_1077_2432_scsi_host_scsi_host
Destroyed node device 'pci_1077_2432_scsi_host_scsi_host'

	
Expected Results:

Verify that the virtual HBA can be destroyed.

Verify that virsh nodedev-destroy destroys the device.
Notes:
Comments:

		177380 	[NPIV] Discover SAN Storage 	nzhang 	None 	Manual 		Feature 	P1 	3260 	Edit
Setup:

For discover storage, we need the fixed wwpn to create node

on amd-1352-8-2, it should be

# cat newhba.xml

<device>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9f004</wwpn>
    </capability>
  </capability>
</device>

"2001001b32a9da4e" here is the scsi_host5 node name

# cat /sys/class/fc_host/host5/node_name
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NPIV
    RHEL6.0

bug:

    No bug found

Actions:

First, do test case "124548 [NPIV] Create a virtual HBA with NPIV".


----------If your stroage is sigle-path---------------
Prepare the following xml file:

<pool type="scsi">
	<name>host6</name>
	<source>
		<adapter name="host6"/>
	</source>
	<target>
		<path>/dev/disk/by-id</path>
	</target>
</pool>

# virsh pool-create hbapool.xml
Pool host6 created from hbapool.xml




# virsh pool-list --all 
Name      State      Autostart 
------------------------------
host6     active     no




Confirm that all the appropriate logical units are visible as volumes:




# virsh vol-list host6 
Name       Path                                    
------------------------------
6.0.2.0    /dev/disk/by-id/scsi-3600a0b80005ad1d700002ea04b54d18b




After creating the pool, add a new logical unit on a target that is visible on that host, and then refresh the pool:




# virsh pool-refresh host6
Pool host6 refreshed




Confirm that the new storage is visible.





----------If your stroage is multi-path--------------- 
1.First installed "device-mapper-multipath-0.4.9-31.el6.x86_64"  package .

 # cp /usr/share/doc/device-mapper-multipath-0.4.9/multipath.conf /etc/multipath.conf

2.Edit /etc/multipath.conf:

#blacklist {
#        devnode "*"
#}

Start multipath:
/etc/init.d/multipathd start

Prepare the following xml file:

<pool type="mpath">
  <name>mpath</name>
  <target>
    <path>/dev/mapper</path>
  </target>
</pool>



# virsh pool-create mpath.xml 
Pool mpath created from mpath.xml

# virsh vol-list mpath
Name                 Path                                    
-----------------------------------------
dm-0                 /dev/mapper/mpath2            

Then create a guest on the new volume - /dev/mapper/mpath2 to make sure it is available.





	
Expected Results:

Verify that the storage pool can be created after the creation of virtual HBA using NPIV.
Notes:
Comments:

		177381 	[NPIV] Each guest writes data into SAN storage via different vHBA at the same time. 	xhu 	None 	Manual 		--default-- 	P2 	3270 	Edit
Setup:

If you use "10.66.72.14" host for test,  you can set value wwpn to 2101001b32a90001, 2101001b32a90002, 2101001b32a90003, 2101001b32a90004 or  2101001b32a90005


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NPIV
    RHEL6.0

bug:

    No bug found

Actions:

1. Create 4 vHBAs at least by using e.g. as follows:
   # virsh nodedev-create newhba.xml

   where newhba.xml contains: 
   <device>
     <parent>scsi_host0</parent>
     <capability type='scsi_host'>
       <capability type='fc_host'>
         <wwnn>2001001b32a9f25b</wwnn>
         <wwpn>2101001b32a90001</wwpn>
       </capability>
     </capability>
   </device>

----------If your stroage is sigle-path---------------
2. Create different pool for guest install via each of vHBAs, and confirm each
of the pool will point to different LUNs.
   # virsh pool-create hbapool.xml

   where hbapool.xml contains:
   <pool type="scsi">
     <name>hostX</name>
     <source>
       <adapter name="hostX"/>
     </source>
     <target>
       <path>/dev/disk/by-path</path>
     </target>
   </pool> 

3. Using each of created pool to install guests, and confirm no any conflicthappens between guests. 

----------If your stroage is multi-path--------------- 
2.Create a multipath pool for guest install via each of vHBAs.
# cat mpath.xml
<pool type="mpath">
  <name>mpath</name>
  <target>
    <path>/dev/mapper</path>
  </target>
</pool>

# virsh pool-create mpath.xml

3. Using mutilpath pool to install guests, and confirm no any conflicthappens between guests. 

	
Expected Results:

3. All guests could be installed successfully

no error output

3.1 Should ping to host successfully

3.2 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

3.3 firefox application should be launched
Notes:
Comments:

		177382 	[NPIV] Install a guest into SAN storage via vHBA. 	xhu 	None 	Manual 		--default-- 	P1 	3280 	Edit
Setup:

Fisrt, do "39817 [NPIV] Discover SAN Storageï»¿" case
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NPIV
    RHEL6.0

bug:

    No bug found

Actions:

1. Run virt-install, specify a volume path as the guest disk.
   # virt-install -n foo -r 1024 --vcpus=1 --disk
path=/dev/disk/by-path/pci-0000:03:00.0-fc-0x203500a0b85ad1d7:0x0000000000000000
--network network:default --accelerate --vnc --location
http://download.englab.nay.redhat.com/pub/rhel/rel-eng/latest-RHEL-6/6.0/Server/x86_64/os/

2, Confirm the guest is installed successfully into SAN storage via vHBA.

	
Expected Results:

1. The guest is installed successfully and can be started with no error.

2. 

Before install into LUNs:
   # fdisk -l
   ...
   Disk /dev/sdd: 42.9 GB, 42949672960 bytes
   64 heads, 32 sectors/track, 40960 cylinders
   Units = cylinders of 2048 * 512 = 1048576 bytes
   Sector size (logical/physical): 512 bytes / 512 bytes
   I/O size (minimum/optimal): 512 bytes / 512 bytes
   Disk identifier: 0x0005211e

   Device Boot      Start         End      Blocks   Id  System

   After install into LUNs:
   # fdisk -l
   ...
   Disk /dev/sdd: 42.9 GB, 42949672960 bytes
   64 heads, 32 sectors/track, 40960 cylinders
   Units = cylinders of 2048 * 512 = 1048576 bytes
   Sector size (logical/physical): 512 bytes / 512 bytes
   I/O size (minimum/optimal): 512 bytes / 512 bytes
   Disk identifier: 0x0005211e

   Device Boot      Start         End      Blocks   Id  System
   /dev/sdd1               1       20480    20971504   83  Linux
   /dev/sdd2   *       20481       20980      512000   83  Linux
   Partition 2 does not end on cylinder boundary.
   /dev/sdd3           20981       40960    20459520   8e  Linux LVM
   Partition 3 does not end on cylinder boundary.

Notes:
Comments:

		177383 	[NPIV] Install a guest on the storage which connect to the vport of host1 and reconnect it to host2 with the same vport 	nzhang 	None 	Manual 		Feature 	P1 	3290 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. Install a guest via vHBA with assigned WWPN on host1.
    Refer to 55529 [NPIV] Install a guest into SAN storage via vHBA.

2. Destroy the vHBA and connected pool on host1.

3. Create a vHBA with the fixed WWPN on host2.

4. Build a pool to connect the same LUNs on host2.

5. Boot a guest with the same LUNs on host2.
	
Expected Results:

5.Confirm the installed guest can still be booted up on host2.

5.0 All guests could be installed successfully no error output

5.1 Should ping to host successfully

5.2 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

5.3 firefox application should be launched
Notes:
Comments:

		177339 	[Network filter] - edit XML configuration for a network filter 	jyang 	yoyzhang 	Manual 		Feature 	P1 	3300 	Edit
Setup:

1.  ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    network filter
    RHEL6.0

bug:

    No bug found

Actions:

1.  # virsh nwfilter-list

2. choose a network filter name or uuid in list for edit, eg: change the priority 500->600

 # virsh  nwfilter-edit allow-arp
Network filter allow-arp XML configuration edited.

or

# virsh  nwfilter-edit 4f620a1f-70ca-9c57-4de9-e4a30d60419a
Network filter allow-arp XML configuration edited.

3. check if the xml has been edited

# virsh  nwfilter-dumpxml 4f620a1f-70ca-9c57-4de9-e4a30d60419a

	
Expected Results:

step 1:

# virsh  nwfilter-list
UUID                                  Name                 
----------------------------------------------------------------
4f620a1f-70ca-9c57-4de9-e4a30d60419a  allow-arp           
1627e113-ed48-6a4c-7ed7-073f96dcf293  allow-dhcp          
1708fb60-c455-ca25-de47-765ac2fdf30c  allow-dhcp-server   
ae08c396-0024-0728-1c59-de45fada6394  allow-incoming-ipv4
34e9a66c-ebff-3ced-f2b8-22c27458217c  allow-ipv4          
1a50f92b-c9d8-d3e8-bdd1-53370892d189  clean-traffic       
f88f1932-debf-4aa1-9fbe-f10d3aa4bc95  no-arp-spoofing     
7cad5cc9-fe5e-9ada-c79f-e044ea704f7b  no-ip-multicast     
65dbb595-d18f-cf99-292f-c4bb2dce1c45  no-ip-spoofing      
376acc92-dba0-137b-8f81-fbd66b68825f  no-mac-broadcast    
7aa3d8f2-97c5-98d5-9b50-7b0fe3589170  no-mac-spoofing     
b61d7d27-de52-647a-a10b-79b939ecd6be  no-other-l2-traffic
3901ba05-57d3-07c9-84ce-1309e262bb0d  no-other-rarp-traffic
68e12969-ca52-baca-8684-57127ad77d52  qemu-announce-self  
dfce6f9d-8fc0-ea1b-9842-3423f25a8e39  qemu-announce-self-rarp

step 2:

# virsh  nwfilter-edit allow-arp
Network filter allow-arp XML configuration edited.

# virsh  nwfilter-edit 4f620a1f-70ca-9c57-4de9-e4a30d60419a
Network filter allow-arp XML configuration edited.


step 3:

# virsh  nwfilter-dumpxml 4f620a1f-70ca-9c57-4de9-e4a30d60419a

<filter name='allow-arp' chain='arp'>
  <uuid>4f620a1f-70ca-9c57-4de9-e4a30d60419a</uuid>
  <rule action='accept' direction='inout' priority='600'/>
</filter>
Notes:
Comments:

		177340 	[Network filter] - libvirt should initialize nwfilter when /tmp is mounted with noexec option - bug 752255 	xhu 	None 	Manual 		Regression 	P1 	3310 	Edit
Setup:

1.Check if ebtables is installed, if not, yum install it.

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter
    Regression

bug:

    No bug found

Actions:

1. Create a lv(vg_intelw352081-tmp) or a new partition(sda7)  .mkfs it and mount it 

2. #mount /dev/mapper/vg_intelw352081-tmp /mnt/tmp

3. #cp -ar /tmp /mnt/ 

4. #umount /mnt/tmp 

5. #mount /dev/mapper/vg_intelw352081-tmp  /tmp -o noexec,nosuid 

6. #service libvirtd restart 
7. Need add a nwfilter in test's xml ,like this : 

 <interface type='bridge'>
      <mac address='52:54:00:61:cd:ed'/>
      <source bridge='breth0'/>
      <filterref filter='clean-traffic'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>


8. #virsh start test   

	
Expected Results:

8. the guest can be started successfully.
Notes:
Comments:

		177341 	[Network filter] - list network filters 	jyang 	yoyzhang 	Auto 		Feature 	P1 	3320 	Edit
Setup:

1.  ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    network filter
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1.  # virsh nwfilter-list
	
Expected Results:

step 1:

# virsh  nwfilter-list
UUID                                  Name                 
----------------------------------------------------------------
4f620a1f-70ca-9c57-4de9-e4a30d60419a  allow-arp           
1627e113-ed48-6a4c-7ed7-073f96dcf293  allow-dhcp          
1708fb60-c455-ca25-de47-765ac2fdf30c  allow-dhcp-server   
ae08c396-0024-0728-1c59-de45fada6394  allow-incoming-ipv4
34e9a66c-ebff-3ced-f2b8-22c27458217c  allow-ipv4          
1a50f92b-c9d8-d3e8-bdd1-53370892d189  clean-traffic       
f88f1932-debf-4aa1-9fbe-f10d3aa4bc95  no-arp-spoofing     
7cad5cc9-fe5e-9ada-c79f-e044ea704f7b  no-ip-multicast     
65dbb595-d18f-cf99-292f-c4bb2dce1c45  no-ip-spoofing      
376acc92-dba0-137b-8f81-fbd66b68825f  no-mac-broadcast    
7aa3d8f2-97c5-98d5-9b50-7b0fe3589170  no-mac-spoofing     
b61d7d27-de52-647a-a10b-79b939ecd6be  no-other-l2-traffic
3901ba05-57d3-07c9-84ce-1309e262bb0d  no-other-rarp-traffic
68e12969-ca52-baca-8684-57127ad77d52  qemu-announce-self  
dfce6f9d-8fc0-ea1b-9842-3423f25a8e39  qemu-announce-self-rarp
Notes:
Comments:

		177342 	[Network filter] - network filter information in XML 	jyang 	yoyzhang 	Auto 		Feature 	P1 	3330 	Edit
Setup:

1.  ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    network filter
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1.  # virsh nwfilter-list

2. choose a network filter name or uuid in list for dumpxml

# virsh nwfilter-dumpxml allow-arp

or

# virsh  nwfilter-dumpxml 4f620a1f-70ca-9c57-4de9-e4a30d60419a



	
Expected Results:

step 1:

# virsh  nwfilter-list
UUID                                  Name                 
----------------------------------------------------------------
4f620a1f-70ca-9c57-4de9-e4a30d60419a  allow-arp           
1627e113-ed48-6a4c-7ed7-073f96dcf293  allow-dhcp          
1708fb60-c455-ca25-de47-765ac2fdf30c  allow-dhcp-server   
ae08c396-0024-0728-1c59-de45fada6394  allow-incoming-ipv4
34e9a66c-ebff-3ced-f2b8-22c27458217c  allow-ipv4          
1a50f92b-c9d8-d3e8-bdd1-53370892d189  clean-traffic       
f88f1932-debf-4aa1-9fbe-f10d3aa4bc95  no-arp-spoofing     
7cad5cc9-fe5e-9ada-c79f-e044ea704f7b  no-ip-multicast     
65dbb595-d18f-cf99-292f-c4bb2dce1c45  no-ip-spoofing      
376acc92-dba0-137b-8f81-fbd66b68825f  no-mac-broadcast    
7aa3d8f2-97c5-98d5-9b50-7b0fe3589170  no-mac-spoofing     
b61d7d27-de52-647a-a10b-79b939ecd6be  no-other-l2-traffic
3901ba05-57d3-07c9-84ce-1309e262bb0d  no-other-rarp-traffic
68e12969-ca52-baca-8684-57127ad77d52  qemu-announce-self  
dfce6f9d-8fc0-ea1b-9842-3423f25a8e39  qemu-announce-self-rarp

step 2:

# virsh nwfilter-dumpxml allow-arp
<filter name='allow-arp' chain='arp'>
  <uuid>4f620a1f-70ca-9c57-4de9-e4a30d60419a</uuid>
  <rule action='accept' direction='inout' priority='500'/>
</filter>

 

# virsh  nwfilter-dumpxml 4f620a1f-70ca-9c57-4de9-e4a30d60419a
<filter name='allow-arp' chain='arp'>
  <uuid>4f620a1f-70ca-9c57-4de9-e4a30d60419a</uuid>
  <rule action='accept' direction='inout' priority='500'/>
</filter>
Notes:
Comments:

		177343 	[Network filter] - undefine a network filter 	jyang 	yoyzhang 	Auto 		Feature 	P1 	3340 	Edit
Setup:

1. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    network filter
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

First, do the case "49929 [nwfilter-define] - define or update a network filter from an XML file"

1. undefine "disallow-arp"

  # virsh nwfilter-undefine disallow-arp

 

2. check if network filter "disallow-arp" is undefined

  # virsh nwfilter-list
	
Expected Results:

step 2:

      "disallow-arp" is undefined
Notes:
Comments:

		177344 	[Network filter] allow arp 	jyang 	jyang 	Auto 		Feature 	P2 	3350 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    network filter
    virsh-rail

bug:

    No bug found

Actions:

 

1. edit xml of "toy"

  # virsh edit toy

  In the node "<interface>", insert a "<filterref>" child node. e,g

    <interface type='network'>
      <mac address='52:54:00:14:e0:97'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <filterref filter='allow-arp'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </interface>

 

5. restart toy

   # virsh destroy toy

   # virsh start toy

 

6. get the name of virtual network device that "toy" uses.

   # virsh dumpxml toy

 

   e.g.

         <target dev='vnet0'/>

   the name of virtual network device of "toy" is "vnet0"

 

7. check the tables of ebtables, ${virtual_network_device_of_guest} is what we get in step 6, such as vnet0.

    7.1 # ebtables -t nat -L

 

 

8. login into guest, arping host

    [guest] # arping -I eth0 -c 3 ${host_ip}

 

9. ping guest from host via the bridge which the guest's virtual network device attached to. e.g.

   [host] #  arping -I virbr0 -c 3 ${guest_ip}

10. restart libvirtd and start the guest, then loop step 7,8,9
	
Expected Results:

step 7:

      7.1 contains following outputs:

       [root@dhcp-66-70-131 nwfilterxml2xmlin]# ebtables -t nat -L I-vnet0-arp
Bridge table: nat

Bridge chain: I-vnet0-arp, entries: 1, policy: ACCEPT
-j ACCEPT
[root@dhcp-66-70-131 nwfilterxml2xmlin]# ebtables -t nat -L
Bridge table: nat

Bridge chain: PREROUTING, entries: 1, policy: ACCEPT
-i vnet0 -j libvirt-I-vnet0

Bridge chain: OUTPUT, entries: 0, policy: ACCEPT

Bridge chain: POSTROUTING, entries: 1, policy: ACCEPT
-o vnet0 -j libvirt-O-vnet0

Bridge chain: libvirt-I-vnet0, entries: 1, policy: ACCEPT
-p ARP -j I-vnet0-arp

Bridge chain: libvirt-O-vnet0, entries: 1, policy: ACCEPT
-p ARP -j O-vnet0-arp

Bridge chain: I-vnet0-arp, entries: 1, policy: ACCEPT
-j ACCEPT

Bridge chain: O-vnet0-arp, entries: 1, policy: ACCEPT
-j ACCEPT

   from the outputs, you will see get a conclusion:

   chain applied on incoming arp packet:  libvirt-I-vnet0 -> I-vnet0-arp -> ACCEPT

   chain applied on outcoming arp packet:  libvirt-O-vnet0 -> O-vnet0-arp -> ACCEPT

 

    it means all the incoming and outcoming ARP packet are allowed.

 

step 8:

       success

       ARPING 10.66.70.131 from 192.168.122.211 eth0
Unicast reply from 10.66.70.131 [72:E9:60:A7:D5:75]  0.713ms
Unicast reply from 10.66.70.131 [72:E9:60:A7:D5:75]  0.806ms
Unicast reply from 10.66.70.131 [72:E9:60:A7:D5:75]  0.832ms
Sent 3 probes (1 broadcast(s))
Received 3 response(s)

 

step 9:

       success, the output will be like

       ARPING 192.168.122.211 from 192.168.122.1 virbr0
Unicast reply from 192.168.122.211 [52:54:00:14:E0:97]  0.827ms
Unicast reply from 192.168.122.211 [52:54:00:14:E0:97]  0.743ms
Unicast reply from 192.168.122.211 [52:54:00:14:E0:97]  0.689ms
Sent 3 probes (1 broadcast(s))
Received 3 response(s)

step 10:

        libvirtd will not crash and all the check can be pass
Notes:
Comments:

		177345 	[Network filter] allow incoming ipv4 	jyang 	yoyzhang 	Auto 		Feature 	P2 	3360 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    network filter
    virsh-rail

bug:

    No bug found

Actions:

 

1. edit xml of "toy"

  # virsh edit toy

  In the node "<interface>", insert a "<filterref>" child node. e,g

    <interface type='network'>
      <mac address='52:54:00:14:e0:97'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <filterref filter='allow-incoming-ipv4'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </interface>

 

5. restart toy

   # virsh destroy toy

   # virsh start toy

 

6. get the name of virtual network device that "toy" uses.

   # virsh dumpxml toy

 

   e.g.

         <target dev='vnet0'/>

   the name of virtual network device of "toy" is "vnet0"

 

7. check the tables of ebtables, ${virtual_network_device_of_guest} is what we get in step 6, such as vnet0.

    7.1 # ebtables -t nat -L POSTROUTING

    7.2 # ebtales -t nat -L libvirt-O-${virtual_network_device_of_guest}

    7.3 # ebtables -t nat -L O-${virtual_network_device_of_guest}-ipv4

 

8. login into guest, ping host using ipv4.

    [guest] # ping ${host_ip}

 

9. ping guest from host

   [host] # ping ${guest_ip} 

 

10. login into guest, ping google.com

   [guest] # ping google.com
	
Expected Results:

step 7:

      7.1 contains following outputs:

      [root@dhcp-66-70-131 nwfilterxml2xmlin]# ebtables -t nat -L POSTROUTING
Bridge table: nat

Bridge chain: POSTROUTING, entries: 1, policy: ACCEPT
-o vnet0 -j libvirt-O-vnet0

      7.2 contains following outputs:

       [root@dhcp-66-70-131 nwfilterxml2xmlin]# ebtables -t nat -L libvirt-O-vnet0
Bridge table: nat

Bridge chain: libvirt-O-vnet0, entries: 1, policy: ACCEPT
-p IPv4 -j O-vnet0-ipv4

      7.3 contains following outputs

        [root@dhcp-66-70-131 nwfilterxml2xmlin]# ebtables -t nat -L O-vnet0-ipv4
Bridge table: nat

Bridge chain: O-vnet0-ipv4, entries: 1, policy: ACCEPT
-j ACCEPT

 

      the meaning of upper outputs is to allow the ipv4 packet in and out.

 

step 8:

       ping success

 

step 9:

       ping success

 

step 10:

       ping success
Notes:
Comments:

		177346 	[Network filter] allow ipv4 	jyang 	None 	Auto 		Feature 	P1 	3370 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    network filter
    virsh-rail

bug:

    No bug found

Actions:

 

1. destroy default net

# virsh net-destroy default
Network default destroyed

2. start default net

# virsh net-start default
Network default started

3. restart libvirtd

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

4. restart toy

   # virsh destroy toy

5. edit xml of "toy"

  # virsh edit toy

  In the node "<interface>", insert a "<filterref>" child node. e,g

    <interface type='network'>
      <mac address='52:54:00:14:e0:97'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <filterref filter='allow-ipv4'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </interface>

 

   # virsh start toy

 

6. get the name of virtual network device that "toy" uses.

   # virsh dumpxml toy

 

   e.g.

         <target dev='vnet0'/>

   the name of virtual network device of "toy" is "vnet0"

 

7. check the tables of ebtables, ${virtual_network_device_of_guest} is what we get in step 6, such as vnet0.

    7.1 # ebtables -t nat -L PREROUTING

    7.2 # ebtales -t nat -L libvirt-I-${virtual_network_device_of_guest}

    7.3 # ebtables -t nat -L I-${virtual_network_device_of_guest}-ipv4

 

8. login into guest, ping host using ipv4.

    [guest] # ping ${host_ip}

 

9. ping guest from host

   [host] # ping ${guest_ip} 

 

10. login into guest, ping google.com

   [guest] # ping google.com
	
Expected Results:

step 7:

      7.1 contains following outputs:

       [root@dhcp-66-70-131 nwfilterxml2xmlin]# ebtables -t nat -L PREROUTING
Bridge table: nat

Bridge chain: PREROUTING, entries: 1, policy: ACCEPT
-i vnet0 -j libvirt-I-vnet0

      7.2 contains following outputs:

        [root@dhcp-66-70-131 nwfilterxml2xmlin]# ebtables -t nat -L libvirt-I-vnet0
Bridge table: nat

Bridge chain: libvirt-I-vnet0, entries: 1, policy: ACCEPT
-p IPv4 -j I-vnet0-ipv4

      7.3 contains following outputs

        [root@dhcp-66-70-131 nwfilterxml2xmlin]# ebtables -t nat -L I-vnet0-ipv4
Bridge table: nat

Bridge chain: I-vnet0-ipv4, entries: 1, policy: ACCEPT
-j ACCEPT

 

      the meaning of upper outputs is to allow the ipv4 packet in and out.

 

step 8:

       ping success

 

step 9:

       ping success

 

step 10:

       ping success
Notes:
Comments:

		177347 	[Network filter] allow-dhcp 	xhu 	None 	Manual (Autoproposed) 		Feature 	P2 	3380 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

 1. Refer filter "allow-dhcp" to the domain.

# virsh edit toy

add "<filterref filter='allow-dhcp'/>" inside one of the "interface" node. 

2. Start or restart domain.

# virsh destroy toy
# virsh start toy 

3. Inside toy, reget ip via "dhclient", expect it to be SUCCESS.

# dhclient

# ping www.baidu.com

	
Expected Results:

3 The guest can gain IP and can ping public network successfully
Notes:
Comments:

		177348 	[Network filter] allow-incoming-arp 	xhu 	None 	Manual 		Feature 	P1 	3390 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. Make sure you have a effective domain, (suppose it's named 'toy').

2. Refer filter "allow-incoming-arp" to the domain.

# virsh edit toy

add "<filterref filter='allow-incoming-arp'/>" inside one of the "interface" node.

3. Restart toy

4. Check the rules of ebtables 

	
Expected Results:

4 

# ebtables -t nat -L 

will output information like following:

Bridge table: nat
Bridge chain: PREROUTING, entries: 0, policy: ACCEPT
Bridge chain: OUTPUT, entries: 0, policy: ACCEPT
Bridge chain: POSTROUTING, entries: 1, policy: ACCEPT -o vnet0 -j libvirt-O-vnet0
Bridge chain: libvirt-O-vnet0, entries: 1, policy: ACCEPT -p ARP -j O-vnet0-arp
Bridge chain: O-vnet0-arp, entries: 1, policy: ACCEPT -j ACCEPT 

It means "vnet0" is the tap device for toy, and for "vnet0", the ebtables on host
allows the arp packet go out via "vnet0". but not rule for "the packet come in"

Notes:
Comments:

		177349 	[Network filter] allow-ipv6 	xhu 	None 	Auto 		Feature 	P1 	3400 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter
    virsh-rail

bug:

    No bug found

Actions:

1 define allow-ipv6.xml as follows:
# cat allow-ipv6.xml
<filter name='allow-ipv6' chain='ipv6'>
  <rule action='accept' direction='inout' priority='500'/>
</filter>
# virsh nwfilter-define allow-ipv6.xml

2 define a guest with "<filterref filter='allow-ipv6'/>" in interface section
# virsh dumpxml rhel6
<interface type='network'>
      <mac address='52:54:00:0a:ba:19'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <model type='virtio'/>
      <filterref filter='allow-ipv6'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</interface>

3 check ebtables rule

4. # sysctl net.ipv6.conf.virbr0.disable_ipv6=0
net.ipv6.conf.virbr0.disable_ipv6 = 0

5 log into guest and ping virbr0 ipv6 link local address

	
Expected Results:

3 # ebtables -t nat -L
Bridge table: nat

Bridge chain: PREROUTING, entries: 1, policy: ACCEPT
-i vnet0 -j libvirt-I-vnet0

Bridge chain: OUTPUT, entries: 0, policy: ACCEPT

Bridge chain: POSTROUTING, entries: 1, policy: ACCEPT
-o vnet0 -j libvirt-O-vnet0

Bridge chain: libvirt-I-vnet0, entries: 1, policy: ACCEPT
-p IPv6 -j I-vnet0-ipv6

Bridge chain: libvirt-O-vnet0, entries: 1, policy: ACCEPT
-p IPv6 -j O-vnet0-ipv6

Bridge chain: I-vnet0-ipv6, entries: 1, policy: ACCEPT
-j ACCEPT

Bridge chain: O-vnet0-ipv6, entries: 1, policy: ACCEPT
-j ACCEPT

4. # ifconfig virbr0
virbr0    Link encap:Ethernet  HWaddr 52:54:00:FA:A2:5C  
          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fefa:a25c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:84571 errors:0 dropped:0 overruns:0 frame:0
          TX packets:511136 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:4831112 (4.6 MiB)  TX bytes:740107888 (705.8 MiB)

5 # cat test
# ping6 fe80::5054:ff:fefa:a25c -I eth0
PING fe80::5054:ff:fefa:a25c(fe80::5054:ff:fefa:a25c) from fe80::5054:ff:feae:d4f eth0: 56 data bytes
64 bytes from fe80::5054:ff:fefa:a25c: icmp_seq=1 ttl=64 time=0.673 ms
64 bytes from fe80::5054:ff:fefa:a25c: icmp_seq=2 ttl=64 time=0.110 ms
64 bytes from fe80::5054:ff:fefa:a25c: icmp_seq=3 ttl=64 time=0.117 ms

Notes:
Comments:

		177350 	[Network filter] clean-traffic 	xhu 	None 	Manual (Autoproposed) 		Feature 	P2 	3410 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1 define a guest with "<filterref filter='clean-traffic'/>" in interface section  
# virsh dumpxml demo
<interface type='network'>
      <mac address='52:54:00:0a:ba:19'/>
      <source network='default'/>
      <model type='virtio'/>
      <filterref filter='clean-traffic'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</interface>

2 start the guest
# virsh start demo
Domain demo started

3 check ebtables rule

4 login into guest, ping google.com

5 ping guest from host

6 login into the guest, and bring eth0(probly other interface) down, change IP
    and try to ping 'virbr0'. e.g.
 
7 restart the network

8 login into toy, and bring eth0(probly other interface) down, change MAC
    and try to ping 'virbr0'. e.g.
# /sbin/ifconfig eth0
# /sbin/ifconfig eth0 down
# /sbin/ifconfig eth0 hw ether ${new_mac}
# /sbin/ifconfig eth0 up
# /sbin/ifconfig eth0
# ping -c 10 192.168.122.1


9 restore the guest mac address
# /sbin/ifconfig eth0 down
# /sbin/ifconfig eth0 hw ether ${new_mac}
# /sbin/ifconfig eth0 up
# service network restart
	
Expected Results:

3 # ebtables -t nat -L
Bridge table: nat

Bridge chain: PREROUTING, entries: 1, policy: ACCEPT
-i vnet0 -j libvirt-I-vnet0

Bridge chain: OUTPUT, entries: 0, policy: ACCEPT

Bridge chain: POSTROUTING, entries: 0, policy: ACCEPT

Bridge chain: libvirt-I-vnet0, entries: 4, policy: ACCEPT
-s ! 52:54:0:a:ba:19 -j DROP
-p IPv4 -j ACCEPT
-p ARP -j ACCEPT
-j DROP

4 # ping google.com
expect it to be SUCCESS

5 # ping ${toy_ip}
expect it to be SUCCESS

6 # /sbin/ifconfig eth0 down
# /sbin/ifconfig eth0 192.168.122.183 netmask 255.255.255.0 up
# /bin/ping -c 1 192.168.122.1
ping is expected to FAILED with "100% packet loss"

7 # service network restart
the network is restart successful and the guest can gain ip

8 # ping -c 10 192.168.122.1
ping is expected to FAILED with "100% packet loss"

9 # service network restart
the network is restarted successfully
Notes:
Comments:

		177515 	[Scalability] migrate a guest for 1024 rounds through SSH connection --live 	gsun 	None 	Auto 		Stress 	P2 	3410 	Edit
Setup:

dispatch ssh publick key of source host to target host.  so that we don't need input the passphrase.

- Creating your local public key pair (by running "ssh-keygen -t rsa ", just give the default answer to the request questions.)
- Copying the public key to a remote host by "$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@somehost "

Do this on both source and target host, make sure no password need from both side. Also do on localhost and dispatch the pub key to both source host and target host.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define and start a domain on SOURCE host

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live"
#TRANSPORT="tcp"
#TRANSPORT="tls"
TRANSPORT="ssh"

date
for i in `seq 1 1024`;
do
    echo "Loop ${i}: Migrating ${GUEST} from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system
done
date

3. Start to migrate domain for 1024 rounds

# sh migrate.sh <guestname> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 1024 round should be fininshed successfully.

Output:

.....


Loop 1: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.512s
user    0m0.000s
sys    0m0.005s
Loop 1: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.445s
user    0m0.002s
sys    0m0.007s
Loop 2: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.235s
user    0m0.003s
sys    0m0.003s
Loop 2: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.201s
user    0m0.001s
sys    0m0.004s

......

4. The migration time in the above output should be acceptable, range should be in 0 ~ 5 sec.

no big degradation is seen.
Notes:
Comments:

		177351 	[Network filter] disallow arp 	jyang 	yoyzhang 	Auto 		Feature 	P1 	3420 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    network filter
    virsh-rail

bug:

    No bug found

Actions:

1. create a network filter named "disallow-arp.xml", with contents as following:

<filter name='disallow-arp' chain='arp'>
  <rule action='drop' direction='inout' priority='500'/>
</filter>

 

2. define "disallow-arp"

  # virsh nwfilter-define disallow-arp.xml

 

3. check if network filter "disallow-arp" is defined

  # virsh nwfilter-list

 

4. edit xml of "toy"

  # virsh edit toy

  In the node "<interface>", insert a "<filterref>" child node. e,g

    <interface type='network'>
      <mac address='52:54:00:14:e0:97'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <filterref filter='disallow-arp'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </interface>

 

5. restart toy

   # virsh destroy toy

   # virsh start toy

 

6. get the name of virtual network device that "toy" uses.

   # virsh dumpxml toy

 

   e.g.

         <target dev='vnet0'/>

   the name of virtual network device of "toy" is "vnet0"

 

7. check the tables of ebtables, ${virtual_network_device_of_guest} is what we get in step 6, such as vnet0.

    # ebtables -t nat -L

 

 

8. login into guest, arping host

    8.1 [guest] # arping -I eth0 -c 3 ${host_ip}

    8.2 [guest] # echo $?

9. ping guest from host via the bridge which the guest's virtual network device attached to. e.g.

   [host] #  arping -I virbr0 -c 3 ${guest_ip}

   [host] # echo $?
	
Expected Results:

step 3:

      "disallow-arp" is defined

 

step 7:

      contains following outputs:

[root@dhcp-66-70-131 libvirt_test]# ebtables -t nat -L
Bridge table: nat

Bridge chain: PREROUTING, entries: 1, policy: ACCEPT
-i vnet0 -j libvirt-I-vnet0

Bridge chain: OUTPUT, entries: 0, policy: ACCEPT

Bridge chain: POSTROUTING, entries: 1, policy: ACCEPT
-o vnet0 -j libvirt-O-vnet0

Bridge chain: libvirt-I-vnet0, entries: 1, policy: ACCEPT
-p ARP -j I-vnet0-arp

Bridge chain: libvirt-O-vnet0, entries: 1, policy: ACCEPT
-p ARP -j O-vnet0-arp

Bridge chain: I-vnet0-arp, entries: 1, policy: ACCEPT
-j DROP

Bridge chain: O-vnet0-arp, entries: 1, policy: ACCEPT
-j DROP

   from the outputs, you will see get a conclusion:

   chain applied on incoming arp packet:  libvirt-I-vnet0 -> I-vnet0-arp -> DROP

   chain applied on outcoming arp packet:  libvirt-O-vnet0 -> O-vnet0-arp -> DROP

 

    it means all the incoming and outcoming ARP packets will be dropped.

 

step 8:

      8.1  failed, with outputs like:

Sent 3 probes (3 broadcast(s))
Received 0 response(s)

      8.2 output "1"

 

step 9:

      9.1  failed, with outputs like:

Sent 3 probes (3 broadcast(s))
Received 0 response(s)

      9.2 output "1"
Notes:
Comments:

		177516 	[Scalability] migrate a guest for 1024 rounds through SSH connection --tunnelled 	gsun 	gsun 	Auto 		Stress 	P2 	3420 	Edit
Setup:

dispatch ssh publick key of source host to target host.  so that we don't need input the passphrase.

- Creating your local public key pair (by running "ssh-keygen -t rsa ", just give the default answer to the request questions.)
- Copying the public key to a remote host by "$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@somehost "

Do this on both source and target host, make sure no password need from both side. Also do on localhost and dispatch the pub key to both source host and target host.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define and start a domain on SOURCE host

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live --p2p --tunnelled"
#TRANSPORT="tcp"
#TRANSPORT="tls"
TRANSPORT="ssh"

date
for i in `seq 1 1024`;
do
    echo "Loop ${i}: Migrating ${GUEST} from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system
done
date

3. Start to migrate domain for 1024 rounds

# sh migrate.sh <guestname> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 1024 round should be fininshed successfully.

Output:

.....


Loop 1: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.512s
user    0m0.000s
sys    0m0.005s
Loop 1: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.445s
user    0m0.002s
sys    0m0.007s
Loop 2: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.235s
user    0m0.003s
sys    0m0.003s
Loop 2: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.201s
user    0m0.001s
sys    0m0.004s

......

4. The migration time in the above output should be acceptable, range should be in 0 ~ 5 sec.

no big degradation is seen.
Notes:
Comments:

		177352 	[Network filter] disallow dhcp 	jyang 	yoyzhang 	Auto 		Feature 	P2 	3430 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    network filter
    virsh-rail

bug:

    No bug found

Actions:

1. Make sure you have a effective domain, (suppose it's named 'toy').

2. Refer filter "disallow-dhcp" to the domain.


<filter name='disallow-dhcp' chain='ipv4'>
   <rule action='drop' direction='out' priority='100'/>
 </filter>





# virsh edit toy

add "<filterref filter='disallow-dhcp'/>" inside one of the "interface" node.

3. Start or restart domain.

# virsh destroy toy
# virsh start toy

4. Inside toy, reget ip via "dhclient", expect it to be FAIL.

# dhclient
# ping www.baidu.com

	
Expected Results:

4 the guest can't gain IP and failed to ping public network
Notes:
Comments:

		177517 	[Scalability] migrate a guest for 1024 rounds through TCP connection --live 	jialiu 	None 	Manual 		Stress 	P2 	3430 	Edit
Setup:

Following case:

[Remote access] Connect to the hypervisor on host using a plain TCP connection without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124649/?from_plan=5066>

Run the case on both source host and target host to enable tcp listen for libvirtd.

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define and start a domain on SOURCE host

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live"
TRANSPORT="tcp"
#TRANSPORT="tls"
#TRANSPORT="ssh"

date
for i in `seq 1 1024`;
do
    echo "Loop ${i}: Migrating ${GUEST} from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system
done
date

3. Start to migrate domain for 1024 rounds

# sh migrate.sh <guestname> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 1024 round should be fininshed successfully.

Output:

.....


Loop 1: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.512s
user    0m0.000s
sys    0m0.005s
Loop 1: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.445s
user    0m0.002s
sys    0m0.007s
Loop 2: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.235s
user    0m0.003s
sys    0m0.003s
Loop 2: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.201s
user    0m0.001s
sys    0m0.004s

......

4. The migration time in the above output should be acceptable, range should be in 0 ~ 5 sec.

no big degradation is seen.
Notes:
Comments:

		177353 	[Network filter] disallow-dhcp-server 	xhu 	None 	Manual (Autoproposed) 		--default-- 	P1 	3440 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

2. ebtables is installed, if not:

# yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1. prepare nwfilter xml as follows:
# cat disallow-dhcp-server.xml
<filter name='disallow-dhcp-server' chain='ipv4'>
  <rule action='accept' direction='out' priority='100'>
    <ip srcipaddr='0.0.0.0' dstipaddr='255.255.255.255' protocol='udp' srcportstart='68' dstportstart='67'/>
  </rule>
  <rule action='drop' direction='in' priority='100'>
    <ip srcipaddr='$DHCPSERVER' protocol='udp' srcportstart='67' dstportstart='68'/>
  </rule>
</filter>

2. define the disallow-dhcp-server nwfilter rule:
# virsh nwfilter-define disallow-dhcp-server.xml
Network filter disallow-dhcp-server defined from disallow-dhcp-server.xml

3. insert guest toy xml with "<filterref filter='disallow-dhcp-server'/>" in
interface section
# virsh dumpxml toy
  ...
    <interface type='network'>
      <mac address='52:54:00:6c:73:8c'/>
      <source network='default'/>
      <model type='virtio'/>
      <filterref filter='disallow-dhcp-server'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
function='0x0'/>
    </interface>
...

4.  start the domain
# virsh start toy
error: Failed to start domain rhel6
error: internal error Cannot instantiate filter due to unresolvable variables:
DHCPSERVER

5. re-edit guest toy xml and insert  the following xml into interface section:
<interface type='network'>>
...
<filterref filter='disallow-dhcp-server'>
        <parameter name='DHCPSERVER' value='192.168.122.1'/>
</filterref>
...
</interface>

6.  start the guest
# virsh start toy

7. use dhclient to achieve ip
# dhclient

8 ping www.baidu.com
# ping www.baidu.com


	
Expected Results:

4 Fail to start the guest

# virsh start toy

error: Failed to start domain rhel6

error: internal error Cannot instantiate filter due to unresolvable variables: DHCPSERVER

6 the guest toy should be started successfully

# virsh start toy
Domain toy started

7 Fail to get guest ip

8 Fail to ping www.baidu.com
Notes:
Comments:

		177519 	[Scalability] migrate a guest for 1024 rounds through TCP connection --tunnelled 	jialiu 	jialiu 	Manual 		Stress 	P2 	3440 	Edit
Setup:

Following case:

[Remote access] Connect to the hypervisor on host using a plain TCP connection without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124649/?from_plan=5066>

Run the case on both source host and target host to enable tcp listen for libvirtd.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define and start a domain on SOURCE host

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live  --p2p --tunnelled"
TRANSPORT="tcp"
#TRANSPORT="tls"
#TRANSPORT="ssh"

date
for i in `seq 1 1024`;
do
    echo "Loop ${i}: Migrating ${GUEST} from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system
done
date

3. Start to migrate domain for 1024 rounds

# sh migrate.sh <guestname> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 1024 round should be fininshed successfully.

Output:

.....


Loop 1: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.512s
user    0m0.000s
sys    0m0.005s
Loop 1: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.445s
user    0m0.002s
sys    0m0.007s
Loop 2: Migrating from 10.66.93.91 to 10.66.93.84


real    0m9.235s
user    0m0.003s
sys    0m0.003s
Loop 2: Migrating back from 10.66.93.84 to 10.66.93.91


real    0m9.201s
user    0m0.001s
sys    0m0.004s

......

4. The migration time in the above output should be acceptable, range should be in 0 ~ 5 sec.

no big degradation is seen.
Notes:
Comments:

		177354 	[Network filter] disallow-ipv6 	xhu 	None 	Auto 		Feature 	P1 	3450 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter
    virsh-rail

bug:

    No bug found

Actions:

1 define disallow-ipv6.xml as follows:
# cat disallow-ipv6.xml
<filter name='disallow-ipv6' chain='ipv6'>
  <rule action='drop' direction='inout' priority='500'/>
</filter>

# virsh nwfilter-define disallow-ipv6.xml

2 define a guest with "<filterref filter='disallow-ipv6'/>" in interface section
# virsh dumpxml rhel6
<interface type='network'>
      <mac address='52:54:00:0a:ba:19'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <model type='virtio'/>
      <filterref filter='disallow-ipv6'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</interface>

3 check ebtables rule

4. # sysctl net.ipv6.conf.virbr0.disable_ipv6=0
net.ipv6.conf.virbr0.disable_ipv6 = 0

5 log into guest and ping virbr0 ipv6 link local address

	
Expected Results:

3 # ebtables -t nat -L
Bridge table: nat

Bridge chain: PREROUTING, entries: 1, policy: ACCEPT
-i vnet0 -j libvirt-I-vnet0

Bridge chain: OUTPUT, entries: 0, policy: ACCEPT

Bridge chain: POSTROUTING, entries: 1, policy: ACCEPT
-o vnet0 -j libvirt-O-vnet0

Bridge chain: libvirt-I-vnet0, entries: 1, policy: ACCEPT
-p IPv6 -j I-vnet0-ipv6

Bridge chain: libvirt-O-vnet0, entries: 1, policy: ACCEPT
-p IPv6 -j O-vnet0-ipv6

Bridge chain: I-vnet0-ipv6, entries: 1, policy: ACCEPT
-j DROP

Bridge chain: O-vnet0-ipv6, entries: 1, policy: ACCEPT
-j DROP

4. # ifconfig virbr0
virbr0    Link encap:Ethernet  HWaddr 52:54:00:FA:A2:5C  
          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:fefa:a25c/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:84571 errors:0 dropped:0 overruns:0 frame:0
          TX packets:511136 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:4831112 (4.6 MiB)  TX bytes:740107888 (705.8 MiB)

5 # ping6 fe80::5054:ff:fefa:a25c -I eth0
PING fe80::5054:ff:fefa:a25c(fe80::5054:ff:fefa:a25c) from fe80::5054:ff:feae:d4f eth0: 56 data bytes

From fe80::5054:ff:feae:d4f icmp_seq=2 Destination unreachable: Address unreachable
From fe80::5054:ff:feae:d4f icmp_seq=3 Destination unreachable: Address unreachable

Notes:
Comments:

		177520 	[Scalability] migrate 3 guests for 512 rounds through TLS connection --live 	gsun 	None 	Auto 		Stress 	P2 	3450 	Edit
Setup:
Finished case:

[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124654/?from_plan=5066>

Run the case on both source host and target host to enable tls listen for libvirtd.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define and start 3 guests on SOURCE host named test1 test2 test3

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live"
TRANSPORT="tls"


date
for i in `seq 1 512`;
do
    echo "Loop ${i}: Migrating ${GUEST}123 from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}123 qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}1 qemu+${TRANSPORT}://root@${HOST2}/system & pid1=$!
   time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}2 qemu+${TRANSPORT}://root@${HOST2}/system & pid2=$!
   time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}3 qemu+${TRANSPORT}://root@${HOST2}/system & pid3=$!
    wait $pid1
    wait $pid2
    wait $pid3

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}123 qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}1 qemu+${TRANSPORT}://root@${HOST1}/system & pid1=$!
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}2 qemu+${TRANSPORT}://root@${HOST1}/system & pid2=$!
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}3 qemu+${TRANSPORT}://root@${HOST1}/system & pid3=$!
    wait $pid1
    wait $pid2
    wait $pid3
   
done
date

3. Start to migrate domain for 512 rounds

# sh migrate.sh <guest_prefix> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 512 round should be fininshed successfully.

4. The migration time in the above output should be acceptable, range should be in 0 ~ 5 sec.

no big degradation is seen.
Notes:
Comments:

		177355 	[Network filter] no-arp-spoofing 	xhu 	None 	Manual (Autoproposed) 		Feature 	P2 	3460 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1. Make sure you have a effective domain, (suppose it's named 'toy').

2. Refer filter "no-arp-spoofing" to the domain.

# virsh edit toy

add "<filterref filter='no-arp-spoofing'/>" inside one of the "interface" node.

3. Restart toy.

4. Start tcpdump.

# /usr/sbin/tcpdump -v -i virbr0 not ip  > /tmp/tcpdump.log &

5. Login into toy, and generate arp spoofing packet.

# yum -y install dsniff

you should confige the right repo, and get libnet and libnids pkg before you install.
   http://www.rpmfind.net/linux/rpm2html/search.php?query=&submit=Search+...&system=&arch=

# arpspoof 192.168.122.183 &
# sleep 10
# kill -15 `pidof arpspoof`

6. Stop tcpdump.

# kill -15 `pidof tcpdump`

7. Confirm no arp reply packets is caputured by tcpdump with "192.168.122.183 is-at"

# cat /tmp/tcpdump.log

	
Expected Results:

7. request packages like:

18:20:59.021457 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.122.229 tell 192.168.122.1, length 28
18:21:00.020466 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.122.229 tell 192.168.122.1, length 28
18:21:01.020466 ARP, Ethernet (len 6), IPv4 (len 4), Request who-has 192.168.122.229 tell 192.168.122.1, length 28

 and no reply packages.
Notes:
Comments:

		177522 	[Scalability] migrate 3 guests for 512 rounds through TLS connection --tunnelled 	gsun 	gsun 	Auto 		Stress 	P2 	3460 	Edit
Setup:
Finished case:

[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124654/?from_plan=5066>

Run the case on both source host and target host to enable tls listen for libvirtd.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0
    migration

bug:

    No bug found

Actions:

1.1. Define and start 3 guests on SOURCE host named test1 test2 test3

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live --p2p --tunnelled"
TRANSPORT="tls"


date
for i in `seq 1 512`;
do
    echo "Loop ${i}: Migrating ${GUEST}123 from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}123 qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}1 qemu+${TRANSPORT}://root@${HOST2}/system & pid1=$!
   time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}2 qemu+${TRANSPORT}://root@${HOST2}/system & pid2=$!
   time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST}3 qemu+${TRANSPORT}://root@${HOST2}/system & pid3=$!
    wait $pid1
    wait $pid2
    wait $pid3

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}123 qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}1 qemu+${TRANSPORT}://root@${HOST1}/system & pid1=$!
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}2 qemu+${TRANSPORT}://root@${HOST1}/system & pid2=$!
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST}3 qemu+${TRANSPORT}://root@${HOST1}/system & pid3=$!
    wait $pid1
    wait $pid2
    wait $pid3
   
done
date

3. Start to migrate domain for 512 rounds

# sh migrate.sh <guest_prefix> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 512 round should be fininshed successfully.

4. The migration time in the above output should be acceptable, range should be in 0 ~ 5 sec.

no big degradation is seen.
Notes:
Comments:

		177523 	[Scalability] migrate a guest for 1024 rounds with undefinesource and persistent option -- bug 757635 	gsun 	gsun 	Manual 		Stress 	P2 	3460 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    scalability
    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define and start a domain on SOURCE host with iscsi disk

define iscsi-pool on both hosts

# virsh pool-define pool.xml

# virsh pool-start pool-iscsi-testing

please change to your own iscsi path and name

cat pool.xml

<pool type='iscsi'>
  <name>pool-iscsi-testing</name>
  <uuid>7439bdcb-d47d-8147-2f4e-d7d5e8917b29</uuid>
  <capacity>21485322240</capacity>
  <allocation>21485322240</allocation>
  <available>0</available>
  <source>
    <host name='10.66.90.100'/>
    <device path='iqn.2001-05.com.equallogic:0-8a0906-12a1f7d03-0daf49b25a84ee02-s3-kyla-131842'/>
  </source>
  <target>
    <path>/dev/disk/by-path</path>
    <permissions>
      <mode>0700</mode>
      <owner>-1</owner>
      <group>-1</group>
    </permissions>
  </target>
</pool>

# fdisk -l

....

Disk /dev/sdb: 21.5 GB, 21485322240 bytes
64 heads, 32 sectors/track, 20490 cylinders
Units = cylinders of 2048 * 512 = 1048576 bytes
Sector size (logical/physical): 512 bytes / 512 bytes

....

define and start guest with following xml on source host

# virsh dumpxml guest

...

    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='native'/>
      <source dev='/dev/sdb'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
...

# virsh start guest

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS="--live --undefinesource --persistent"
TRANSPORT="ssh"

date
for i in `seq 1 1024`;
do
    echo "Loop ${i}: Migrating ${GUEST} from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system
done
date

3. Start to migrate domain for 1024 rounds

# sh migrate.sh <guestname> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:

3. migration for 1024 round should be fininshed successfully.

no error message like "permission denied" or "cannot acquire state change lock" exist on /var/log/libvirt/qemu/guest.log and /var/log/libvirt/libvirtd.log
Notes:
Comments:

		177356 	[Network filter] no-ip-spoofing 	xhu 	None 	Manual (Autoproposed) 		Feature 	P2 	3470 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1. make sure you have a effective domain, (suppose it's named 'toy').

2.  refer filter "no-ip-spoofing" to the domain.

# virsh edit toy

add "<filterref filter='no-ip-spoofing'/>" inside one of the "interface" node.

3. Restart toy

4. Login into toy, and bring eth0(probly other interface) down, change IP 
   and try to ping 'virbr0'. e.g.

# /sbin/ifconfig eth0 down
# /sbin/ifconfig eth0 192.168.122.183 netmask 255.255.255.0 up
# /bin/ping -c 1 192.168.122.1 

	
Expected Results:

4 ping is expected to FAILED with "100% packet loss"
Notes:
Comments:

		177543 	[Snapshot] Create a domain snapshot of a running domain 	xhu 	None 	Auto 		--default-- 	P1 	3470 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    snapshot
    virsh-rail

bug:

    No bug found

Actions:

1. Create a XML file:
   # cat snap1.xml
   <domainsnapshot>
     <name>snap1</name>
   </domainsnapshot>

2. Start a domain:
   # virsh start <domain>

3. Take a snapshot of a domain:
   # virsh snapshot-create <domain> snap1.xml

4. Verify the snapshot was taken:
   # virsh snapshot-list <domain>
   # virsh snapshot-dumpxml <domain> snap1

	
Expected Results:

Confirm the outputs of step 4:

# virsh snapshot-list <domain>
 Name                 Creation Time             State
---------------------------------------------------
 snap1                2011-03-17 16:09:52 -0400 running

# virsh snapshot-dumpxml <domain> snap1
<domainsnapshot>
  <name>snap1</name>
  <state>running</state>
  <creationTime>1300392592</creationTime>
  <domain>
    <uuid>f403a96b-e521-fedf-d615-4a42b928a79b</uuid>
  </domain>
</domainsnapshot>
Notes:
case 177544 cover it
Comments:

		177357 	[Network filter] no-mac-broadcast 	xhu 	None 	Manual (Autoproposed) 		Feature 	P2 	3480 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1. Make sure you have a effective domain, (suppose it's named 'toy').

2. Refer filter "no-mac-broadcast" to the domain.

# virsh edit toy

add "<filterref filter='no-mac-broadcast'/>" inside one of the "interface" node.

3. Restart toy.

4. Make sure no other guest is using network via "virbr0".

5. On host, start tcpdump like following:

# /usr/sbin/tcpdump -v -i virbr0 -n host 255.255.255.255 2> /tmp/tcpdump.log &

6. Login into toy, generate  mac broadcast

# /bin/ping -c 5 192.168.122.255 -b

7. Stop tcpdump.

# kill -15 `/sbin/pidof tcpdump`

8. Confirm tcpdump captured "0 packet". 

# cat /tmp/tcpdump.log 

	
Expected Results:

8 expect to see "0 packets captured".
Notes:
Comments:

		177545 	[Snapshot] Create a domain snapshot of a shutdown domain 	xhu 	None 	Auto 		--default-- 	P1 	3480 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    snapshot
    virsh-rail

bug:

    No bug found

Actions:

1. Make sue domain is shut off status:
   # virsh domstate <domain>
   shut off

2. Take a snapshot of a domain:
   # virsh snapshot-create <domain>

3. Verify the snapshot was taken:
   # virsh snapshot-list <domain>
   # virsh snapshot-dumpxml <domain> snapname

4.virsh heap check
#valgrind virsh -c test:///default  snapshot-create-as test --print-xml --diskspec vda,file=a,,b

...
==29018== HEAP SUMMARY:
==29018==     in use at exit: 127,906 bytes in 1,362 blocks
==29018==   total heap usage: 6,496 allocs, 5,134 frees, 850,278 bytes allocated
==29018== 
==29018== LEAK SUMMARY:
==29018==    definitely lost: 0 bytes in 0 blocks
==29018==    indirectly lost: 0 bytes in 0 blocks
==29018==      possibly lost: 0 bytes in 0 blocks
==29018==    still reachable: 127,906 bytes in 1,362 blocks
==29018==         suppressed: 0 bytes in 0 blocks
==29018== Rerun with --leak-check=full to see details of leaked memory
==29018== 
==29018== For counts of detected and suppressed errors, rerun with: -v
==29018== Use --track-origins=yes to see where uninitialised values come from
==29018== ERROR SUMMARY: 45 errors from 10 contexts (suppressed: 8 from 6)


	
Expected Results:

Verify step 2:

# virsh snapshot-create <domain>
Domain snapshot 1300462367 created

Verify step 3:

# virsh snapshot-list <domain>
 Name                 Creation Time             State
---------------------------------------------------
 1300462367           2011-03-18 11:32:47 -0400 shutoff

# virsh snapshot-dumpxml <domain> 1300462367
<domainsnapshot>
  <name>1340851047</name>
  <state>shutoff</state>
  <creationTime>1340851047</creationTime>
  <domain type='kvm'>
    <name>sss</name>
    <uuid>1cab4318-2dff-9fec-bf17-b8c5f794a536</uuid>
    <memory unit='KiB'>524288</memory>
    <currentMemory unit='KiB'>524288</currentMemory>
    <vcpu placement='static'>1</vcpu>
    <os>
      <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
      <boot dev='hd'/>
    </os>
    <features>
      <acpi/>
      <apic/>
      <pae/>
    </features>
    <clock offset='utc'/>
    <on_poweroff>destroy</on_poweroff>
    <on_reboot>restart</on_reboot>
    <on_crash>restart</on_crash>
    <devices>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <disk type='file' device='disk'>
        <driver name='qemu' type='qcow2' cache='none'/>
        <source file='/mnt/save/save.img'/>
        <target dev='hda' bus='ide'/>
        <address type='drive' controller='0' bus='0' target='0' unit='0'/>
      </disk>
      <controller type='ide' index='0'>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
      </controller>
      <controller type='usb' index='0'/>
      <interface type='network'>
        <mac address='52:54:00:62:ae:64'/>
        <source network='default'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
      </interface>
      <serial type='pty'>
        <target port='0'/>
      </serial>
      <console type='pty'>
        <target type='serial' port='0'/>
      </console>
      <input type='mouse' bus='ps2'/>
      <graphics type='vnc' port='-1' autoport='yes'/>
      <sound model='ich6'>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
      </sound>
      <video>
        <model type='cirrus' vram='9216' heads='1'/>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
      </video>
      <memballoon model='virtio'>
        <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
      </memballoon>
    </devices>
  </domain>
</domainsnapshot>


Step4:

there is not  "invalid write of size 1" error

 
Notes:
Comments:

		177358 	[Network filter] no-mac-spoofing 	xhu 	None 	Manual (Autoproposed) 		Feature 	P2 	3490 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1. Make sure you have a effective domain, (suppose it's named 'toy').

2. Refer filter "no-mac-spoofing" to the domain.

# virsh edit toy

add "<filterref filter='no-mac-spoofing'/>" inside one of the "interface" node.

3. Restart toy.

4. Login into toy, and bring eth0(probly other interface) down, change MAC 
   and try to ping 'virbr0'. e.g.

# /sbin/ifconfig eth0
# /sbin/ifconfig eth0 down
# /sbin/ifconfig eth0 hw ether ${new_mac}
# /sbin/ifconfig eth0 up
# /sbin/ifconfig eth0
# ping -c 10 192.168.122.1 

	
Expected Results:

4 ping is expected to FAILED with "100% packet loss"
Notes:
Comments:

		177548 	[Snapshot] Delete a snapshot and all children snapshots 	xhu 	None 	Manual (Autoproposed) 		--default-- 	P2 	3490 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    snapshot

bug:

    837544 - From Run 42300

Actions:

1. Start a domain:
   # virsh start <domain>

2. Create a snapshot:
   # virsh snapshot-create <domain>

3. List all of the snapshots:
   # virsh snapshot-list <domain>

4. Create another snapshot:
   # virsh snapshot-create <domain>

5. List all of the snapshots (the new snapshot should be a child of the snapshot in step 2:
   # virsh snapshot-list <domain>

6. Delete the snapshot from step 2), and all of its children:
   # virsh snapshot-delete --children <domain> <snapshot-name>

7. List all of the snapshots, <snapshot-name> from step 2 and
   <snapshot-child> from step 4) should be gone:
   # virsh snapshot-list <domain>

	
Expected Results:

1 # virsh start rhel6
Domain rhel6 started

2 # virsh snapshot-create rhel6

3 # virsh snapshot-list rhel6
 Name                 Creation Time             State
---------------------------------------------------
 1281535709           2010-08-11 22:08:29 +0800 nostate

4 # virsh snapshot-create rhel6

5 # virsh snapshot-list rhel6
 Name                 Creation Time             State
---------------------------------------------------
 1281535709           2010-08-11 22:08:29 +0800 nostate
 1281535958           2010-08-11 22:12:38 +0800 nostate

6 # virsh snapshot-delete --children rhel6 1281535709

7 # virsh snapshot-list rhel6
 Name                 Creation Time             State
---------------------------------------------------
Notes:
Comments:

		177359 	[Network filter] no-other-l2-traffic 	xhu 	None 	Manual (Autoproposed) 		Feature 	P2 	3500 	Edit
Setup:

1. a healthy running guest. suppose its name is "toy"

 

2. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    network filter

bug:

    No bug found

Actions:

1 define a guest with "<filterref filter='no-other-l2-traffic'/>" in interface section
<interface type='network'>
      <mac address='52:54:00:0a:ba:19'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <model type='virtio'/>
      <filterref filter='no-other-l2-traffic'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
</interface>

2 start the guest
# virsh start demo
Domain demo started

3 check ebtables rule


4 log into the guest to check network connectivity

	
Expected Results:

3 # ebtables -t nat -L
Bridge table: nat

Bridge chain: PREROUTING, entries: 1, policy: ACCEPT
-i vnet0 -j libvirt-I-vnet0

Bridge chain: OUTPUT, entries: 0, policy: ACCEPT

Bridge chain: POSTROUTING, entries: 1, policy: ACCEPT
-o vnet0 -j libvirt-O-vnet0

Bridge chain: libvirt-I-vnet0, entries: 1, policy: ACCEPT
-j DROP

Bridge chain: libvirt-O-vnet0, entries: 1, policy: ACCEPT
-j DROP

4 # ifconfig
no ip address is expected
# ping google.com
expect it to be fail
Notes:
Comments:

		177561 	[Snapshot] Revert a running domain to a shutdown snapshot 	xhu 	None 	Auto 		--default-- 	P2 	3500 	Edit
Setup:

First, You must have a shutoff snapshot
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    snapshot
    virsh-rail

bug:

    No bug found

Actions:

1. Start a domain:
   # virsh start <domain>

2. List all of the snapshots:
   # virsh snapshot-list <domain>

3. Choose one of the snapshots in the "stopped" state, and revert to it:
   # virsh snapshot-revert <domain> <snapshot-name>

4. After the revert process is complete, the domain should be shutdown.

5. The domain should be reverted to it's previous state, verify by starting the domain and checking:
   # virsh start <domain>

	
Expected Results:

1 # virsh start rhel6
Domain rhel6 started

2 # virsh snapshot-list rhel6
 Name                 Creation Time             State
---------------------------------------------------
 1281538923           2010-08-11 23:02:03 +0800 shutoff

3 # virsh snapshot-revert rhel6  1281538923

4 # virsh list --all
 Id Name                 State
----------------------------------
  - rhel6                shutoff

5 # virsh start rhel6
Domain rhel6 started

# virsh list --all
 Id Name                 State
----------------------------------
  6 rhel6                running
Notes:
Comments:

		177360 	[Node devices] Allow PCI PM reset on multi-function devices - bug 517460 	yoyzhang 	yoyzhang 	Manual 		--default-- 	P2 	3510 	Edit
Setup:

- Host supports VT-d

- Host has a multi-function device which supports PM (power management) feature but doesn't support FLR feature

(In this test case, use Zephyr-X LightPulse Fibre Channel Host Adapter and Lenovo S20)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices

bug:

    No bug found

Actions:

1. # lspci | grep Fibre

2. # lspci -n | grep 08:00.0

3. # virsh nodedev-list

4. # virsh nodedev-dettach pci_0000_08_00_0

5. # virsh nodedev-reset pci_0000_08_00_0
	
Expected Results:

1. Output:

08:00.0 Fibre Channel: Emulex Corporation Zephyr-X LightPulse Fibre Channel Host Adapter (rev 02)
08:00.1 Fibre Channel: Emulex Corporation Zephyr-X LightPulse Fibre Channel Host Adapter (rev 02)

2. Output:

08:00.0 0c04: 10df:fe00 (rev 02)

3. Output:

computer
net_eth0_00_22_19_b8_c4_e7
net_lo_00_00_00_00_00_00
.....................
pci_0000_08_00_0
pci_0000_08_00_1

.........................

4. Output:

Device pci_0000_08_00_0 dettached

5. Output:

Device pci_0000_08_00_0 reset
Notes:
Comments:

		177562 	[Snapshot] Revert a shutdown domain to a running snapshot 	xhu 	None 	Auto 		--default-- 	P2 	3510 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    snapshot

bug:

    No bug found

Actions:

prepare a running snapshot (system checkpoint ) , before disk snapshot fix the revert bug , pls test it with system checkpoint 
 
virsh snapshot-create-as  doamin  s1 



1. Stop a domain:      
   # virsh shutdown <domain>

2. List all of the snapshots:
   # virsh snapshot-list <domain>

3. Choose one of the snapshots in the "running" state, and revert to it:
   # virsh snapshot-revert <domain> <snapshot-name>

4. After the revert process is complete, the domain should be running, but in the state saved in <snapshot-name>

	
Expected Results:

1. # virsh snapshot-list <domain>
 Name                 Creation Time             State
---------------------------------------------------
 snap1                2011-03-17 16:09:52 -0400 running

2. # virsh list --all
 Id Name                 State
----------------------------------
  - foo                  shut off

3. # virsh snapshot-revert foo snap1

4. # virsh list --all
 Id Name                 State
----------------------------------
  4 foo                  running
Notes:
Comments:

		177361 	[Node devices] Allow PCI PM reset on multi-function devices - bug 517460 	yoyzhang 	yoyzhang 	Manual 		--default-- 	P2 	3520 	Edit
Setup:

- Host supports VT-d

- Host has a multi-function device which supports PM (power management) feature but doesn't support FLR feature

(In this test case, use Zephyr-X LightPulse Fibre Channel Host Adapter and Lenovo S20)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices
    Regression

bug:

    No bug found

Actions:

1. # lspci | grep Fibre

2. # lspci -n | grep 01:00.0

3. # virsh nodedev-list

4. # virsh nodedev-dettach pci_10df_fe00

5. # virsh nodedev-reset pci_10df_fe00
	
Expected Results:

1. Output:

01:00.0 Fibre Channel: Emulex Corporation Zephyr-X LightPulse Fibre Channel Host Adapter (rev 02)
01:00.1 Fibre Channel: Emulex Corporation Zephyr-X LightPulse Fibre Channel Host Adapter (rev 02)

2. Output:

01:00.0 0c04: 10df:fe00 (rev 02)

3. Output:

computer
net_00_21_86_93_f6_fd
pci_10de_658
pci_10df_fe00

.........................

4. Output:

Device pci_10df_fe00 dettached

5. Output:

Device pci_10df_fe00 reset
Notes:
Comments:

		177563 	[Snapshot] Revert a shutdown domain to a shutdown snapshot 	xhu 	None 	Auto 		--default-- 	P2 	3520 	Edit
Setup:

First, make sure you have a shutoff snapshot
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    snapshot
    virsh-rail

bug:

    No bug found

Actions:

1. Stop a domain:
   # virsh shutdown <domain>

2. List all of the snapshots:
   # virsh snapshot-list <domain>

3. Choose one of the snapshots in the "stopped" state, and revert to it:
   # virsh snapshot-revert <domain> <snapshot-name>

4. After the revert process is complete, the domain should be shutdown.

5. The domain should be reverted to it's previous state, verify by starting the domain and checking:
   # virsh start <domain>

	
Expected Results:

1 # virsh shutdown rhel6
Domain rhel6 is being shutdown

# virsh list --all
 Id Name                 State
----------------------------------
  - rhel6                shut off

2 # virsh snapshot-list rhel6
 Name                 Creation Time             State
---------------------------------------------------
 1281537421           2010-08-11 22:37:01 +0800 shutoff

3 #virsh snapshot-revert rhel6  1281537421

4 # virsh list --all
 Id Name                 State
----------------------------------
  - rhel6                shut off

5 # virsh start rhel6
Domain rhel6 started

# virsh list --all
 Id Name                 State
----------------------------------
 12 rhel6                running
Notes:
Comments:

		177362 	[Node devices] Dettach node device from host 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	3530 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree

2. Dettach network device

# virsh nodedev-dettach pci_0000_00_19_0

3. Check network deivice no longer work

# ifconfig
	
Expected Results:

1. Output

.................

  |     
  +- pci_0000_00_19_0
  |   |
  |   +- net_eth0_b8_ac_6f_3e_4b_e0

........

2. Output

Device pci_0000_00_19_0 dettached

Also check this pci driver

# readlink /sys/bus/pci/devices/0000:\00:\19.0/driver/ -f

   /sys/bus/pci/drivers/pci-stub

3. eth0 is disabled

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:64 errors:0 dropped:0 overruns:0 frame:0
          TX packets:64 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:6192 (6.0 KiB)  TX bytes:6192 (6.0 KiB)

virbr0    Link encap:Ethernet  HWaddr 1E:55:D6:3B:1C:9D  
          inet addr:192.168.122.1  Bcast:192.168.122.255  Mask:255.255.255.0
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:46 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 b)  TX bytes:7898 (7.7 KiB)
Notes:
Comments:

		177363 	[Node devices] Dump network device xml info 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P2 	3540 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices

bug:

    No bug found

Actions:

1. # virsh nodedev-list --tree

 

2. Select one of networkr pci device on host

# virsh nodedev-dumpxml pci_14e4_167a
	
Expected Results:

1. Output:

computer
 |
  +- net_66_ba_7b_14_a5_08
  +- net_computer_loopback
  +- net_e2_d8_18_80_29_04
  +- pci_1002_4383
  +- pci_1002_4384

.......................

2. Output:

<device>
  <name>pci_14e4_167a</name>
  <parent>pci_1022_9606</parent>
  <driver>
    <name>tg3</name>
  </driver>
  <capability type='pci'>
    <domain>0</domain>
    <bus>63</bus>
    <slot>0</slot>
    <function>0</function>
    <product id='0x167a'>NetXtreme BCM5754 Gigabit Ethernet PCI Express</product>
    <vendor id='0x14e4'>Broadcom Corporation</vendor>
  </capability>
</device>
Notes:
Comments:

		177651 	[Storage] error handle for overcommit to storage - scenario 1 	jialiu 	None 	Manual 		--default-- 	P2 	3540 	Edit
Setup:

Prepare a small partition for this tesing.

such as:

# fdisk -l /dev/sda11

Disk /dev/sda11: 11 MB, 11517952 bytes
64 heads, 32 sectors/track, 10 cylinders
Units = cylinders of 2048 * 512 = 1048576 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

# mkfs.ext3 /dev/sda11
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    storage

bug:

    No bug found

Actions:

1. Prepare the followng pool xml:

<pool type='fs'>
  <name>mypool</name>
  <source>
    <device path='/dev/sda11'/>
    <format type='auto'/>
  </source>
  <target>
    <path>/var/lib/libvirt/images/mypool</path>
  </target>
</pool>

2. Define, build and start the pool

# virsh pool-define fs-pool.xml

# virsh pool-build mypool

# virsh pool-start mypool

3. Check the pool is working fine.

# df -h

4. Prepare the following xml to create volume in the pool.

# cat vol-disk-template.xml
<volume>
  <name>disk#NUMBER#.img</name>
  <capacity unit='M'>5</capacity>
  <allocation unit='M'>5</allocation>
  <target>
    <path>/var/lib/libvirt/images/mypool/disk#NUMBER#.img</path>
    <format type='raw'/>
  </target>
</volume>

5. Create volume untill the allocation is exceed the disk capability.

# for i in `seq 3`; do sed "s/#NUMBER#/$i/g" vol-disk-template.xml > vol-disk.xml; virsh vol-create mypool vol-disk.xml; done
	
Expected Results:

2. All the command is working fine.

3. Output

# df -h
...
/dev/sda11              11M  1.1M  9.0M  11% /var/lib/libvirt/images/mypool
....

5. When allocation exceed the disk avalable capability, the foolwing error message should be seen:

# for i in `seq 3`; do sed "s/#NUMBER#/$i/g" vol-disk-template.xml > vol-disk.xml; virsh vol-create mypool vol-disk.xml; done
Vol disk1.img created from vol-disk.xml

error: Failed to create vol from vol-disk.xml
error: cannot create path '/var/lib/libvirt/images/mypool/disk2.img': No space left on device

error: Failed to create vol from vol-disk.xml
error: cannot create path '/var/lib/libvirt/images/mypool/disk3.img': No space left on device

# ls /var/lib/libvirt/images/mypool/
disk1.img

Notes:
Comments:

		177335 	[Miscellanea] QEMU driver does not notice when a guest OS ejects CDROM/Floppy device media - Bug 575160 	gren 	None 	Manual 		Regression 	P1 	3550 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Miscellanea

bug:

    No bug found

Actions:

 QEMU driver does not notice when a guest OS ejects CDROM/Floppy device media
https://bugzilla.redhat.com/show_bug.cgi?id=575160

1, Editing the domain XML directly or using virt-manage to add IDE cdrom

...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/root/Desktop/boot.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>

...

2, Start the guest, in the guest, ensure we can see that the boot.iso has been exposed.

3, run "eject" command in the guest to eject the cdrom, then run mount /dev/sr0 /mnt again,
   it will report error which indicats no cdrom in the guest.

4, execute domain save and restore commands like:

# virsh save rhel6 /tmp/rhel6.save
Domain rhel6 saved to /tmp/rhel6.save

# virsh restore rhel6.save
Domain restored from rhel6.save

5, then, login the guest, try the mount command again "mount /dev/sr0 /mnt", it succeeded.
  this is the bug, it shouldn't be mounted successfully as we have ejected it already.
	
Expected Results:
Notes:
Comments:

		177652 	[Storage] error handle for overcmmit to storage - scenario 2 	jialiu 	None 	Manual 		--default-- 	P2 	3550 	Edit
Setup:

Prepare a small partition for this tesing.

such as:

# fdisk -l /dev/sda11

Disk /dev/sda11: 11 MB, 11517952 bytes
64 heads, 32 sectors/track, 10 cylinders
Units = cylinders of 2048 * 512 = 1048576 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

# mkfs.ext3 /dev/sda11

 

Bug 720903 - The guest cannot be resumed without any error info when there is overcommit to storage [CLOSED NOTABUG]
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    storage
    QE consumption

bug:

    878400 - From Run 51444
    878400 - From Run 51959

Actions:

1. Prepare the followng pool xml:

<pool type='fs'>
  <name>mypool</name>
  <source>
    <device path='/dev/sda11'/>
    <format type='auto'/>
  </source>
  <target>
    <path>/var/lib/libvirt/images/mypool</path>
  </target>
</pool>

2. Define, build and start the pool

# virsh pool-define fs-pool.xml

# virsh pool-build mypool

# virsh pool-start mypool

3. Check the pool is working fine.

# df -h

4. Prepare the following xml to create volume in the pool.

# cat vol-disk-template.xml
<volume>
  <name>disk1.img</name>
  <capacity unit='M'>100</capacity>
  <allocation unit='M'>0</allocation>
  <target>
    <path>/var/lib/libvirt/images/mypool/disk1.img</path>
    <format type='raw'/>
  </target>
</volume>

5. Create volume untill the allocation is exceed the disk capability.

# virsh vol-create mypool vol-disk-template.xml

6. Attach the volume to an existing guest as secondary disk, then start the guest.

7. In guest, try to write some staf (which size is bigger than 50M ) in the secondary disk.

# fdisk /dev/vdb

In fdisk prompt, print "n, p, 1, enter, enter, w" in order.

(it will be the vda if the first disk is not virtio driver, use "# fdisk -l" to determine the 2rd virtual disk for the guest)

# mkfs.ext3 /dev/vdb1

# mount /dev/vdb1 /mnt

# dd if=/dev/zero of=/mnt/test.img bs=1M count=50

8. # virsh resume guest
	
Expected Results:

2. All the command is working fine.

3. Output

# df -h
...
/dev/sda11              11M  1.1M  9.0M  11% /var/lib/libvirt/images/mypool
....

4. NOTE:

the volume's capability is 100M, which exceed the whole disk capability, but its allocation is "0"

5. volume is created successfully.

6. Guest is started successfully.

7. guest is paused, there should be some message that prompt user no availabe space.

On host, issue the following command:

# virsh list --all
 Id Name                 State
----------------------------------
  9 <guestname>    paused

# df -h
Filesystem            Size  Used Avail Use% Mounted on
...
/dev/sda11              11M   11M     0 100% /var/lib/libvirt/images/mypool
...

8. the guest can NOT be resumed successfully unless you extend the underlying storage.

 
Notes:
Comments:

		177334 	[Miscellanea] QEMU driver does not notice when a guest OS ejects CDROM/Floppy device media 	gren 	gren 	Manual 		Regression 	P1 	3560 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Miscellanea
    Regression

bug:

    No bug found

Actions:

 QEMU driver does not notice when a guest OS ejects CDROM/Floppy device media
https://bugzilla.redhat.com/show_bug.cgi?id=575160

1, Editing the domain XML directly or using virt-manage to add IDE cdrom

...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/root/Desktop/boot.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>

...

2, Start the guest, in the guest, ensure we can see that the boot.iso has been exposed.

3, run "eject" command in the guest to eject the cdrom, then run mount /dev/sr0 /mnt again,
   it will report error which indicats no cdrom in the guest.

4, execute domain save and restore commands like:

# virsh save rhel6 /tmp/rhel6.save
Domain rhel6 saved to /tmp/rhel6.save

# virsh restore rhel6.save
Domain restored from rhel6.save

5, then, login the guest, try the mount command again "mount /dev/sr0 /mnt", it succeeded.
  this is the bug, it shouldn't be mounted successfully as we have ejected it already.
	
Expected Results:
Notes:
Comments:

		177669 	[Storage] SCSI based storage pool 	xhu 	None 	Manual 		--default-- 	P2 	3560 	Edit
Setup:

You must have a host with HBA card connected to at least one storage

Check with the following command:

# virsh nodedev-list --cap=scsi_host

scsi_host0
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    storage

bug:

    No bug found

Actions:

1. Create an XML file, save as pool-scsi.xml.

<pool type="scsi">
  <name>scsipool</name>
  <uuid>e9392370-2917-565e-692b-d057f46512d6</uuid>
  <source>
    <adapter name="host0"/>
  </source>
  <target>
    <path>/dev/disk/by-path</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>


# virsh pool-define pool-scsi.xml 

2. Start the inactive pool, verify the pool state is active.
# virsh pool-start scsipool

# virsh pool-list --all   

3.  Destroy the pool, verify the pool state is inactive.

# virsh pool-destroy scsipool
Pool scsipool destroyed

# virsh pool-list --all
Name                 State      Autostart 
-----------------------------------------       
scsipool             inactive   no     

4. Undefine the inactive pool.

#virsh pool-undefine scsipool
Pool scsipool has been undefined

	
Expected Results:

1

# virsh pool-define pool-scsi.xml

Pool scsipool defined from pool-scsi.xml

2

# virsh pool-start scsipool

Pool scsipool started

# virsh pool-list --all

Name State Autostart

-----------------------------------------

scsipool active no

3

# virsh pool-destroy scsipool

Pool scsipool destroyed

# virsh pool-list --all

Name State Autostart

-----------------------------------------

scsipool inactive no

4

#virsh pool-undefine scsipool

Pool scsipool has been undefined
Notes:
Comments:

		177336 	[Miscellanea] Upgrade all related pkgs on RHEL6.0 release os tree and test basic operations 	mzhan 	mzhan 	Manual 		Regression 	P1 	3570 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    miscellanea

bug:

    No bug found

Actions:

1. Prepare a host with RHEL6.0 released OS.

2. Setup the repo file, such as:

# cat /etc/yum.repos.d/rhel61.repo
[RHEL6.1]
name=rhel6-1
baseurl = http://fileshare.englab.nay.redhat.com/pub/redhat/rhel/released/RHEL-6/U1/x86_64/os/
enabled=1
gpgcheck=0

3. Upgrade all related packages to make this OS upgrade to RHEL6.1

# yum -y update kernel

# yum -y update qemu-kvm

# yum -y update libvirt

# yum -y update virt-manager

# yum -y update selinux-policy

# yum -y update seabios

# yum -y install spice-client

# yum -y install tigervnc

4. Reboot the host

5. Use virt-manager to create a new guest
	
Expected Results:

4. After the reboot, make sure

1). Check host ip can get
2). Check firefox can works well
3). Create file/folder successfully

#mkdir testfolder
# touch testfolder/testfile
# vim /testfolder/testfile
# cat /testfolder/testfile
# cp testfolder testfolder.bak
# dd if=/dev/zero of=/var/lib/libvirt/images/fd.img bs=1 count=1 seek=1474K
# ll -lhs /var/lib/libvirt/images/fd.img

5. Check guest can be installed successfully. Also login the guest and check basic operation

1). # ping ${host_ip}

2). memory,cpu info check
# cat /proc/meminfo | grep MemTotal
# cat /proc/cpuinfo

3). guest save/shutdown/reboot/start/pause successfully

4). write files to guest
#mkdir testfolder
# touch testfolder/testfile
# vim /testfolder/testfile
# cat /testfolder/testfile
# cp testfolder testfolder.bak
# dd if=/dev/zero of=/root/a.img bs=1 count=100
# ls /root/a.img

5). guest support interface hotplug,check the nic can also get ip

6). # lspci
sound can be heard under spice graphic

Notes:
Comments:

		177337 	[Miscellanea] Upgrade all related pkgs on RHEL6.0 release os tree and test basic operations 	mzhan 	mzhan 	Manual 		Regression 	P1 	3580 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Miscellanea
    Regression

bug:

    No bug found

Actions:

1. Prepare a host with RHEL6.0 released OS.

2. Setup the repo file, such as:

# cat /etc/yum.repos.d/rhel61.repo
[RHEL6.1]
name=rhel6-1
baseurl = http://download.englab.nay.redhat.com/pub/rhel/rel-eng/RHEL6.1-20110330.2/6.1/Server/x86_64/os/
enabled=1
gpgcheck=0

3. Upgrade all related packages to make this OS upgrade to RHEL6.1

# yum -y update kernel

# yum -y update qemu-kvm

# yum -y update libvirt

# yum -y update virt-manager

# yum -y update selinux-policy

# yum -y update seabios

# yum -y install spice-client

# yum -y install tigervnc

4. Reboot the host

5. Use virt-manager to create a new guest
	
Expected Results:

4. After the reboot, make sure

1). Check host ip can get
2). Check firefox can works well
3). Create file/folder successfully

#mkdir testfolder
# touch testfolder/testfile
# vim /testfolder/testfile
# cat /testfolder/testfile
# cp testfolder testfolder.bak
# dd if=/dev/zero of=/var/lib/libvirt/images/fd.img bs=1 count=1 seek=1474K
# ll -lhs /var/lib/libvirt/images/fd.img

5. Check guest can be installed successfully. Also login the guest and check basic operation

1). # ping ${host_ip}

2). memory,cpu info check
# cat /proc/meminfo | grep MemTotal
# cat /proc/cpuinfo

3). guest save/shutdown/reboot/start/pause successfully

4). write files to guest
#mkdir testfolder
# touch testfolder/testfile
# vim /testfolder/testfile
# cat /testfolder/testfile
# cp testfolder testfolder.bak
# dd if=/dev/zero of=/root/a.img bs=1 count=100
# ls /root/a.img

5). guest support interface hotplug,check the nic can also get ip

6). # lspci
sound can be heard under spice graphic

Notes:
Comments:

		177783 	[Virtual disks] create guest with disk have error_policy - bug 730909 	zpeng 	zpeng 	Manual 		Function 	P1 	3590 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    python-virtinst

Tag:

    virtual disks

bug:

    816849 - From Run 44256
    816849 - From Run 45064
    816849 - From Run 46172
    816849 - From Run 49499
    816849 - From Run 50422
    816849 - From Run 54154

Actions:

run command line:(only on linux guest)

1:# virt-install -n demo -r 1024 --disk path=/tmp/demo.img,size=4 --disk path=/dev/sda2,error_policy=stop -c /tmp/RHEL6.1-20110510.1-Server-x86_64-DVD1.iso --vnc

2:# virt-install -n demo -r 1024 --disk path=/tmp/demo.img,size=4 --disk path=/dev/sda2,error_policy=enospace -c /tmp/RHEL6.1-20110510.1-Server-x86_64-DVD1.iso --vnc

3:#virt-install -n demo -r 1024 --disk path=/tmp/demo.img,size=4 --disk path=/dev/sda2,error_policy=none -c /dev/cdrom --vnc --debug --force
	
Expected Results:

1:verify guest xml

#virsh dumpxml demo

.....

<disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' error_policy='stop' io='native'/>
      <source dev='/dev/sda2'/>
      <target dev='hdb' bus='ide'/>
      <alias name='ide0-0-1'/>
      <address type='drive' controller='0' bus='0' unit='1'/>
    </disk>

.....

start the guest and verify qemu-kvm cmd line have elements:

 # ps -ef | grep $guest_name

.......,werror=stop,rerror=stop....

2:virsh dumpxml demo
.......
 <disk type='block' device='disk'>
   <driver name='qemu' type='raw' cache='none' error_policy='enospace' io='native'/>
 <source dev='/dev/sda2'/> 
...... 
start the guest and verify qemu-kvm cmd line have elements:

 # ps -ef | grep $guest_name

.......werror=enospc....


3:for virt-install, both error_policy= none and ignore have error, so test this manually ,
edit xml for existing vm.
#virsh dumpxml demo
.....
<disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none' error_policy='ignore'
io='native'/>
      <source dev='/dev/sda2'/>
....

start the guest and verify qemu-kvm cmd line have elements:

 # ps -ef | grep $guest_name

 ......werror=ignore,rerror=ignore....

 

Notes:
Comments:

		177338 	[Network filter] - define or update a network filter from an XML file 	jyang 	yoyzhang 	Auto 		Feature 	P1 	3600 	Edit
Setup:

1. ebtables is installed, if not:

   # yum install ebtables -y
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    network filter
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. create a network filter named "disallow-arp.xml", with contents as following:

<filter name='disallow-arp' chain='arp'>
  <rule action='drop' direction='inout' priority='500'/>
</filter>

 

2. define "disallow-arp"

  # virsh nwfilter-define disallow-arp.xml

 

3. check if network filter "disallow-arp" is defined

  # virsh nwfilter-list
	
Expected Results:

step 3:

      "disallow-arp" is defined
Notes:
Comments:

		177309 	[Migration] Stopping src libvirtd during migration - bug 728341 	weizhan 	None 	Manual 		Regression 	P1 	3610 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    rhel7
    migration
    Regression

bug:

    No bug found

Actions:

1. Start a domain;
   # virsh start <domain>

2. Start a live migration;
   # virsh migrate --live <domain> qemu+ssh://host2/system

3. While the migration is happening, stop libvirtd on the source of the migration;
   # service libvirtd stop

4. Check the guest status with QMP
  # nc -U /var/lib/libvirt/qemu/<domain>.monitor
  {"QMP": {"version": {"qemu": {"micro": 1, "minor": 12, "major": 0}, "package": "(qemu-kvm-0.12.1.2)"}, "capabilities": []}}
then input
  {"execute": "qmp_capabilities"}
  {"execute": "query-status"}

5. Check the guest status with vncviewer
 # vncviewer 127.0.0.1:0

	
Expected Results:

step 3. Migration is interrupted

step 4. Shows the guest status

{"return": {"singlestep": false, "running": true}}

which means domain is running.

steps 5. Can see that guest can be operated.

 

Bug 728341 - stopping src libvirtd during migration pauses the guest being migrated is not fixed.
Notes:
Comments:

		177289 	[Migration] migration with large RAM 	gren 	yoyzhang 	Auto 		--default-- 	P2 	3640 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  prepare a guest virtual machine with large memory on source box, the xml description is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.
Notes:
Comments:

		177290 	[Migration] migration with multi guests 	gren 	yoyzhang 	Auto 		--default-- 	P2 	3650 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  prepare multiple guest virtual machines on source box, the xml description for one of them is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.

6, Then, do the migration for these vmssimultaneously.
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.
3, Ensuse all the migration are successful.
Notes:
Comments:

		177291 	[Migration] migration with network load on both source and target host in high speed(1000Mb/s) network 	xhu 	None 	Manual 		Stress 	P2 	3660 	Edit
Setup:

source host: 10.66.93.14

target host:  10.66.93.12
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1 download and install netperf
# wget http://download.devel.redhat.com/brewroot/packages/netperf/2.4.5/1.el6/x86_64/netperf-2.4.5-1.el6.x86_64.rpm
# rpm -ivh netperf-2.4.5-1.el6.x86_64.rpm
2 prepare migration environment
#setenforce 0
#iptables -F
#mount -t nfs 10.66.90.113:/vol/xenimage /mnt
3 start netperf server service
#netserver

on target host:
4 start bulk data tranfer from target to source

#netperf -H <soure_host_ip> -l <time>

eg.

#netperf -H 10.66.93.14 -l 600


on source host:
5 start bulk data tranger from source to target
#netperf -H <target_host_ip> -l <time>

eg.

#netperf -H 10.66.93.12 -l 600


6 define and start a guest with image located on nfs dir(/mnt)
#virsh start mig
7 start migration from source to target
#virsh migrate mig --live qemu+ssh://<target_host_ip>/system
	
Expected Results:

7 the migration can be done successfully

# virsh migrate --live mig qemu+ssh://10.66.93.12/system
root@10.66.93.12's password:
Notes:
Comments:

		177292 	[Migration] migration with network load on both source and target host in slow(128Kb/s) network 	xhu 	None 	Manual 		--default-- 	P2 	3670 	Edit
Setup:

source host: 10.66.93.14

target host:  10.66.93.12
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1 download and install netperf
# wget http://download.devel.redhat.com/brewroot/packages/netperf/2.4.5/1.el6/x86_64/netperf-2.4.5-1.el6.x86_64.rpm
# rpm -ivh netperf-2.4.5-1.el6.x86_64.rpm
2 prepare migration environment
#setenforce 0
#iptables -F
#mount -t nfs 10.66.90.113:/vol/xenimage /mnt
3 start netperf server service
#netserver

on target host:
4 start bulk data tranfer from target to source
#netperf -H <soure_host_ip> -l <time>

eg.

#netperf -H 10.66.93.14 -l 600000000


on source host:

5 do limit for traffic

#tc qdisc add dev eth0 root tbf rate 128kbit burst 1024kbit latency 50ms

6 start bulk data tranger from source to target
#netperf -H <target_host_ip> -l <time>

eg.

#netperf -H 10.66.93.12 -l 600000000


7 define and start a guest with image located on nfs dir(/mnt)
#virsh start mig
8 start migration from source to target
#virsh migrate mig --live qemu+ssh://<target_host_ip>/system

Note:

when finishing the test,you can issue the following command to clear environment:

#tc qdisc del dev eth0 root
	
Expected Results:

8 The migration can be done successfully

# virsh migrate --live mig qemu+ssh://10.66.93.12/system
root@10.66.93.12's password:
Notes:
Comments:

		177295 	[Migration] Migration with not start libvirtd on target host - bug 736590 	weizhan 	weizhan 	Manual (Autoproposed) 		Regression 	P1 	3680 	Edit
Setup:

Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    845893 - From Run 43450
    845893 - From Run 44174
    736590 - From Run 47503
    736590 - From Run 49843
    736590 - From Run 53304
    736590 - From Run 54138

Actions:

1. Start a guest located on nfs on source host

2. On target host, do

# service libvirtd stop

# service libvirtd status

libvirtd is stopped

3. Do migration from hostA to hostB

# virsh migrate --live guest qemu+ssh://{hostB ip}/system
	
Expected Results:

will report error like:

error: failed to connect to the hypervisor 

Notes:
Comments:

		177296 	[Migration] migration with SMP enabled 	gren 	yoyzhang 	Manual 		--default-- 	P2 	3690 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  prepare a guest virtual machine with SMP enabled on source box, the xml description is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.
Notes:
Comments:

		177298 	[Migration] p2p migration 	jyang 	yoyzhang 	Auto 		Feature 	P2 	3700 	Edit
Setup:

1. same as case "[domain async job handling] migrate & domjobabort"

 

2. dispatch ssh publick key of source host to target host.  so that we don't need input the passphrase.

- Creating your local public key pair (by running "ssh-keygen -t rsa ", just give the default answer to the request questions.)
- Copying the public key to a remote host by "$ ssh-copy-id -i ~/.ssh/id_rsa.pub root@somehost "
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. p2p migrate

   # virsh migrate --p2p toy qemu+ssh://${target_source_host}/system

 

2. if step 1 was successful, check the domain state on both source host, target host

    # virsh domstate toy

 

3. connect to migrated guest on target host. check if it's live, execute some commands in it. e.g:

    # virt-viewer -c qemu+ssh://${target_host_ip}/system toy

    In the guest, open the terminal, then

    [guest] # cat /var/log/messages

 

4. destroy migrate guest on target host

    # virsh destroy toy
	
Expected Results:

step 1

     can be migrated successfully

 

step 2:

      on source host, domain toy is shutoff. if toy is not defined on source host before, it will not exist any more.

      on target host, it's running

 

step 3:

       the guest is live, and runs normally

 

step 4:

       domain toy will not exists any more.  but not in shutoff state.
Notes:
Comments:

		177302 	[Migration] Reboot the host durning live migration 	nzhang 	None 	Manual 		--default-- 	P2 	3710 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Start a VM.
   # virsh start <domain>

2. Live migrate the domain.
   # virsh migrate --live <domain> qemu+ssh://dest_host/system

3. Execute reboot host on another terminal during migration.
   # reboot
	
Expected Results:

Check the errors can be handled by libvirt. The guest be not damaged and can be normally boot
Notes:
Comments:

		176888 	[Floppy disk support] readonly 	jyang 	yoyzhang 	Auto 		Feature 	P2 	3720 	Edit
Setup:

1.a healthy guest, which is shutoff

2.if the guest os is rhel6, pls do modprobe first when login into the guest.

#modprobe floppy

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1.  dump the xml of guest

   # virsh dumpxml toy > toy.xml

 

2.  insert following xml into the "<device>" node of toy.xml

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
      <readonly/>
    </disk>


3. undefine guest
 # virsh undefine toy

4. define guest with "toy.xml"
  # virsh define toy.xml

5. create fd.img
  # dd if=/dev/zero of=/var/lib/libvirt/images/fd.img bs=1 count=1 seek=1474K


6. format the floppy disk in ext3 format.
  # mkfs.ext3 /var/lib/libvirt/images/fd.img

7. start the guest
  # virsh start toy

8. login into the guest,  check if the floppy disk exists by:
   # ls /dev/fd*
   # dmesg | grep -i floppy
 
9. mount   the floppy disk under /mnt, e.g.
   # mount /dev/fd0 /mnt

10. # create a large file in the floppy, (NOTE: don't exceed the compacity of floppy disk)
   # cd /mnt
   # touch test.txt

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

1.  dump the xml of guest

   # virsh dumpxml toy > toy.xml

 

2.  insert following xml into the "<device>" node of toy.xml

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
    </disk>


3. undefine guest
 # virsh undefine toy

4. define guest with "toy.xml"
  # virsh define toy.xml

5. create fd.img
  # dd if=/dev/zero of=/var/lib/libvirt/images/fd.img bs=1024 count=10240


6. format the floppy disk in ext3 format.
  # mkfs.ext3 /var/lib/libvirt/images/fd.img

7. start the guest
  # virsh start toy

8. login into the guest,  check if the floppy disk exists by:
   # ls /dev/fd*
   # dmesg | grep -i floppy
 
9. mount   the floppy disk under /mnt, e.g.
   # mount /dev/fd0 /mnt

10. # create a large file in the floppy, (NOTE: don't exceed the compacity of floppy disk)
   # cd /mnt
   # dd if =/dev/zero of=./write_test bs=1024 size=100
   # ls -lh write_test
   # cat write_test

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

1.  dump the xml of guest

   # virsh dumpxml toy > toy.xml

 

2.  insert following xml into the "<device>" node of toy.xml

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
    </disk>


3. undefine guest
 # virsh undefine toy

4. define guest with "toy.xml"
  # virsh define toy.xml

5. create fd.img
  # dd if=/dev/zero of=/var/lib/libvirt/images/fd.img bs=1024 count=10240


6. format the floppy disk in ext3 format.
  # mkfs.ext3 /var/lib/libvirt/images/fd.img

7. start the guest
  # virsh start toy

8. login into the guest,  check if the floppy disk exists by:
   # ls /dev/fd*
   # dmesg | grep -i floppy
 
9. mount   the floppy disk under /mnt, e.g.
   # mount /dev/fd0 /mnt

10. # create a large file in the floppy, (NOTE: don't exceed the compacity of floppy disk)
   # cd /mnt
   # dd if =/dev/zero of=./write_test bs=1024 size=100
   # ls -lh write_test
   # cat write_test

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

	
Expected Results:

10. fail to write
Notes:
Comments:

		177304 	[Migration] seemless SPICE migration - Bug 589989 	jialiu 	None 	Auto 		Regression 	P2 	3720 	Edit
Setup:

About how to define a domain with spice, please refer to 39137 [Graphical framebuffers] spice
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define a domain with spice graphic.
...
<graphics type='spice' port='5900'  autoport='no' keymap='en-us'/>
...

2. Edit the following line in /etc/libvirt/qemu.conf both in source host and target host:
from:
# spice_listen = "0.0.0.0"
to 
spice_listen = "0.0.0.0"

3. Restart libvirtd service

4. Start the domain on source host (10.66.93.84)
# virsh start spice

5. Use spice client to connect the domain on source host.
# /usr/libexec/spicec -h 10.66.93.84 -p 5900

6. Migrate the domain to destination host (10.66.92.185)
# virsh migrate --live spice qemu+ssh://10.66.92.185/system

NOTE:

During migration, when the following warning is seen:
WARN    : [12952:12954] RedChannel::run: cannot resolve host addressdhcp-92-185.nay.redhat.com    

It is because the destination host name - dhcp-92-185.nay.redhat.com, can not be resolved. 
I guess need add a entry in /etc/hosts on source host, such as:

10.66.92.185    dhcp-92-185.nay.redhat.com

BTW, if failed due to connection is refused, please "iptables -F" try again.
7. Check the spice client on the source host during migration.

	
Expected Results:

7. Found that the spice client remains connected & active throughout migration & after completion.
Notes:
Comments:

		176889 	[Floppy disk support] shareable 	jyang 	yoyzhang 	Auto 		Feature 	P2 	3730 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1. create two guests with the following xml in node "<device>"

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
      <shareable/>
    </disk>

2. create fd.img 
  # dd if=/dev/zero of=/var/lib/libvirt/images/fd.img bs=1 count=1 seek=1474K

3. format fd.img (better to proceed in guest)
   # mkfs.ext3 /var/lib/libvirt/images/fd.img

4. start up the two guests
   # virsh start guest1
   # virsh start guest2
5.

. login into the guest,  check if the floppy disk exists by:
   # ls /dev/fd*
   # dmesg | grep -i floppy

6. login into guest 1, mount the floopy disk, create a file
 # for i in {1..10000}; do echo $i >> /mnt/haha.txt; done

7. login into guest 2, mount the floppy disk, check the content of file "haha.txt"
 # cd /mnt; cat haha.txt; cd -

8. create a file on the floppy disk in guest 2
 # for i in {1..10000}; do echo $i >> /mnt/heihei.txt; done

9. in guest 1, umount floppy disk. then mount again, check the content of file "heihei.txt"
 # cd /mnt/; cat heihei.txt; cd -

	
Expected Results:

step 7:

      the result is what we wrote during step 6

 

step 9:

      the result is what we wrote during step 8
Notes:
Comments:

		177305 	[Migration] Set bandwidth of migration 	weizhan 	None 	Manual 		Regression 	P1 	3730 	Edit
Setup:

Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Start a guest on source host

2. Do live migration and recording the time

#time virsh migrate domain_name --live qemu+ssh://{target ip}/system --verbose

3. During the migration, on another console do

#virsh migrate-setspeed domain_name 1

4. Shutdown the guest on target host and start again on source

5. Do live migration and recording the time

#time virsh migrate domain_name --live qemu+ssh://{target ip}/system --verbose

6. During the migration, on another console do

#virsh migrate-setspeed domain_name 1000

7. Compare the time of these 2 migration

 

 
	
Expected Results:

Compare the migration speed of step 3 and step 5, step 5 migration will faster

Migration will succeed with no error
Notes:
Comments:

		176890 	[Floppy disk support] write & read floppy 	jyang 	None 	Auto 		Feature 	P2 	3740 	Edit
Setup:

a healthy guest, which is shutoff

if the guest os is rhel6, pls do modprobe first when log into the guest.
#modprobe floppy

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1.  dump the xml of guest

   # virsh dumpxml toy > toy.xml

 

2.  insert following xml into the "<device>" node of toy.xml

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
    </disk>


3. undefine guest
 # virsh undefine toy

4. define guest with "toy.xml"
  # virsh define toy.xml

5. create fd.img ((1.44M fd.img))
# dd if=/dev/zero of=/var/lib/libvirt/images/fd.img bs=1  count=1474560


6. format the floppy disk in ext3 format.
  # mkfs.ext3 /var/lib/libvirt/images/fd.img

7. start the guest
  # virsh start toy

Only for Linux:
a. log into the guest,  check if the floppy disk exists by:
   # ls /dev/fd*
   # dmesg | grep -i floppy
 
b. mount   the floppy disk under /mnt, e.g.
   # mount /dev/fd0 /mnt

8. # create a large file in the floppy, (NOTE: don't exceed the compacity of floppy disk)
 For linux:
   # cd /mnt
   # touch test.txt
   # echo hello > test.txt
   # cat tet.txt
hello

 For Windows:
Need format the floppy in guest before creating files
  Create a 'txt' or 'doc' and input "hello".
 
 

    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/fd.img'/>
      <target dev='fda' bus='fdc'/>
      <alias name='fdc0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>

	
Expected Results:

step 7:

        1. there is block device named such as "fd*", which is our floppy disk

         2. the output will be like:

          ide floppy driver 0.99.newhide

          Floppy drive(s-): fd0 is 1.44M

 

step 8:

        the file is created successfully, and with correct size.

        and can be read.
Notes:
Comments:

		177306 	[Migration] Set bandwidth of migration with virt-manager 	weizhan 	weizhan 	Manual 		Regression 	P1 	3740 	Edit
Setup:

with migrate_set_speed, because libvirt does not give virsh interface, so need to test on virt-manager

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. Run virt-manager

2. Click File->Add Connection, choose QEMU/KVM on Hypervisor, choose Remote tunnel over SSH on Connection, and write down the hostname of target host, then click Connect

3. Right click 'vm1' and select migrate, select new host with dest ip, click the "Advance", choose checkbox 'Bandwidth' and input valid number "1", then click 'migrate' button.

4. Migration back from target host, and do step 3 again with Bandwidth = 1000

5. See the log info on /var/log/libvirt/libvirtd.log (with setting log_filters="1:qemu_monitor_json" log_outputs="1:file:/var/log/libvirt/libvirtd.log" in the libvirtd.conf file)
	
Expected Results:

Compare the migration speed of step 3 and step 4, step 4 migration will faster

on /var/log/libvirt/libvirtd.log

...

10:24:16.915: 28315: debug : qemuMonitorJSONCommandWithFd:217 : Send command '{"execute":"migrate_set_speed","arguments":{"value":1048576000}}' for write with FD -1 10:24:16.916: 28310: debug : qemuMonitorJSONIOProcessLine:115 : Line [{"return": {}}] 10:24:16.916: 28310: debug : qemuMonitorJSONIOProcess:188 : Total used 16 bytes out of 16 available in buffer 10:24:16.916: 28315: debug : qemuMonitorJSONCommandWithFd:222 : Receive command reply ret=0 errno=0 14 bytes '{"return": {}}' and no error occurs

...
Notes:
Comments:

		177307 	[Migration] Set max downtime during migration 	weizhan 	None 	Manual 		Regression 	P1 	3750 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Start domain 'vm1' on host A

   # virsh start vm1

2. In domain vm1 load high i/o with iozone

   # ./iozone -a

3. Live migrate vm1 to host B WITHOUT setting MaxShutdown time

  # virsh migrate --live vm1 qemu+ssh://{target ip}/system

4. Migrate the guest back from destination host to source host

5. Live migrate vm1 to host B WITH setting MaxShutdown time

  # virsh migrate --live vm1 qemu+ssh://{target ip}/system

6. Before step6 is finished, open another terminal, set maximum tolerable downtime of the domain.

 # virsh migrate-setmaxdowntime --domain vm1 --downtime 1000

7. Check the /var/log/libvirt/libvirtd.log file on source host

	
Expected Results:

Step3
Migration is slow and lasts about 6 seconds.Guest is changed from running ->
shutoff status on source host.

Step 6
Migration is faster than in step4. Guest is changed from running -> paused ->
shutoff status on source host.

Step 7
Qemu command 'migrate_set_downtime' should be executed successfully as below
in the /var/log/libvirt/libvirtd.log(setting log_filters="1:qemu_monitor_json" log_outputs="1:file:/var/log/libvirt/libvirtd.log" in the libvirtd.conf file)

.....
21:39:14.068: debug : qemuMonitorJSONCommandWithFd:217 : Send command
'{"execute":"migrate_set_downtime","arguments":{"value":1.000000}}' for write
with FD -1
21:39:14.068: debug : qemuMonitorJSONIOProcessLine:115 : Line [{"return": {}}]
21:39:14.068: debug : qemuMonitorJSONIOProcess:188 : Total used 16 bytes out of
16 available in buffer
21:39:14.068: debug : qemuMonitorJSONCommandWithFd:222 : Receive command reply
ret=0 errno=0 14 bytes '{"return": {}}'
......
Notes:
Comments:

		177266 	[Migration] migrate a guest based on iscsi storage 	jialiu 	None 	Manual 		--default-- 	P2 	3760 	Edit
Setup:

On both source and target host:

# setenforce 0

# iptables -F

 

update the initiatorname to access the storage:
# cat /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.1994-05.com.redhat:libvirt

# service iscsid restart
Stopping iscsid:
Starting iscsid:                                    [  OK  ]

# iscsiadm --mode discovery --type sendtargets --portal 10.66.90.100
10.66.90.100:3260,1 iqn.2001-05.com.equallogic:0-8a0906-6eb1f7d03-30cf49b25f24f94d-libvirt-1-150313
10.66.90.100:3260,1 iqn.2001-05.com.equallogic:0-8a0906-9951f7d03-34cf49b25f04f94b-libvirt-2-150313
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. Prepare a iscsi pool on source host

<pool type='iscsi'>
  <name>test-iscsi</name>
            <source>
                <host name='10.66.90.100'/>
                <device path='iqn.2001-05.com.equallogic:0-8a0906-6eb1f7d03-30cf49b25f24f94d-libvirt-1-150313'/>
             </source>
             <target>
             <path>/dev/disk/by-path</path>
             </target>
</pool>


2. Define and start the iscsi pool on source host

# virsh pool-define iscsi_pool.xml

# virsh pool-start test-iscsi

3. Check the iscsi disk is connected on source host

# iscsiadm -m session

# fdisk -l

4. On source host, create a lv.

# cat lvm_pool.xml
     <pool type='logical'>
       <name>HostVG</name>
       <source>
         <name>HostVG</name>
         <format type='lvm2'/>
         <device path='/dev/sdb'/>
       </source>
       <target>
         <path>/dev/HostVG</path>
       </target>
     </pool>

# virsh pool-define lvm_pool.xml

# virsh pool-build HostVG

# virsh pool-start HostVG

# cat vol.xml
<volume>
  <name>kvmguest</name>
  <source>
    <device path='/dev/sdb'>
    </device>
  </source>
  <capacity>4194304000</capacity>
  <target>
    <path>/dev/HostVG/kvmguest</path>
  </target>
</volume>

# virsh vol-create HostVG vol.xml

5. Check the lvm volume is created on source host.

# lvdisplay

# lvscan

6. Repeate step 1,2,3 on target host.

7. Repeate step 5 on target host.

8. Install a guest on /dev/HostVG/kvmguest which is on  iscsi disk.

# virt-install -n guestname -r 1024 --disk path=/dev/HostVG/kvmguest -l http://download.englab.nay.redhat.com/pub/rhel/rel-eng/latest-RHEL-6/6.2/Server/x86_64/os/

9. Migrate the guest to target host

# virsh migrate --live <guestname> qemu+ssh://<target_host_ip>/system

10. Check the guest, in guest, try to create a dir.Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # pool-define iscsi.xml
Pool test-iscsi defined from iscsi.xml

virsh # pool-start test-iscsi
Pool test-iscsi started

virsh # migrate --live guestname  qemu+ssh://10.66.6.209/system
root@10.66.6.209's password:

	
Expected Results:

2. pool is defined and started successfully.

3. Output:

# iscsiadm -m session
tcp: [1] 10.66.90.100:3260,1 iqn.2001-05.com.equallogic:0-8a0906-6eb1f7d03-30cf49b25f24f94d-libvirt-1-150313

# fdisk -l

Disk /dev/sda: 500.1 GB, 500107862016 bytes
...
...

Disk /dev/sdb: 21.5 GB, 21485322240 bytes
64 heads, 32 sectors/track, 20490 cylinders
Units = cylinders of 2048 * 512 = 1048576 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x00000000

Disk /dev/sdb doesn't contain a valid partition table

NOTE: the new disk - /dev/sdb is seen, and it is the iscsi storage.

5. Output:

# lvdisplay
  --- Logical volume ---
  LV Name                /dev/HostVG/kvmguest
  VG Name                HostVG
  LV UUID                JOO9z1-AHZT-TJCW-cfcm-j3VV-frNK-occiyo
  LV Write Access        read/write
  LV Status              available
  # open                 0
  LV Size                3.91 GiB
  Current LE             1000
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# lvscan
  ACTIVE            '/dev/HostVG/kvmguest' [3.91 GiB] inherit

NOTE:

if the lv is inactive in output of lvscan command, run the command to activiate it.

# lvchange -ay /dev/HostVG/kvmguest

6. the command output should be the same as step 1,2,3.

7. Output:

# lvdisplay
  --- Logical volume ---
  LV Name                /dev/HostVG/kvmguest
  VG Name                HostVG
  LV UUID                JOO9z1-AHZT-TJCW-cfcm-j3VV-frNK-occiyo
  LV Write Access        read/write
  LV Status              NOT available
  LV Size                3.91 GiB
  Current LE             1000
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto


NOTE: 

If the output of lvdisplay is empty, then run "pvscan", then "lvdisplay" again.

 
# lvscan
  inactive          '/dev/HostVG/kvmguest' [3.91 GiB] inherit

NOTE:

if the lv is inactive in output of lvscan command, run the command to activiate it.

# lvchange -ay /dev/HostVG/kvmguest

8. Installation is done successfully.

9. Migration is done successfully.

10. In guest, dir is created successfully.
Notes:
Comments:

		176956 	[Guest network driver] NIC e1000 driver 	gren 	gren 	Auto 		Feature 	P3 	3770 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual networks

bug:

    No bug found

Actions:

1, Install a new vm , then, we specify a e1000 model of NIC into network device container

<snip>

<interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='e1000'/>
    </interface>

</snip>

2, Define the new xml description again and boot it up

3, try to ping the vm in the host to verify the function of the emulated NIC
	
Expected Results:

The output of command "lspci" insides guest vm:

00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)
00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]
00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]
00:01.2 USB Controller: Intel Corporation 82371SB PIIX3 USB [Natoma/Triton II] (rev 01)
00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)
00:02.0 VGA compatible controller: Cirrus Logic GD 5446
00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 03)
00:04.0 RAM memory: Qumranet, Inc. Virtio memory balloon
Notes:
Comments:

		177268 	[Migration] migrate a guest for 1024 rounds through TCP connection --offline 	ajia 	None 	Auto 		Stress 	P2 	3770 	Edit
Setup:

Following case:

[Remote access] Connect to the hypervisor on host using a plain TCP connection without SASL via ipv4

<https://tcms.engineering.redhat.com/case/124649/?from_plan=5066>

Run the case on both source host and target host to enable tcp listen for libvirtd.

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:
 

1. Define and start a domain on SOURCE host

2. Prepare the follwing shell script:
#!/bin/bash

# Migrate a guest back and forth between two hosts, printing progress as it goes
GUEST=$1
HOST1=$2
HOST2=$3
OPTIONS=""
TRANSPORT="tcp"
#TRANSPORT="tls"
#TRANSPORT="ssh"

date
for i in `seq 1 1024`;
do
    echo "Loop ${i}: Migrating ${GUEST} from ${HOST1} to ${HOST2}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST1}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST2}/system

    echo "Loop ${i}: Migrating ${GUEST} back from ${HOST2} to ${HOST1}"
    echo "COMMAND: virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system"
    time virsh -c qemu+${TRANSPORT}://root@${HOST2}/system migrate ${OPTIONS} ${GUEST} qemu+${TRANSPORT}://root@${HOST1}/system
done
date

3. Start to migrate domain for 1024 rounds

# sh migrate.sh <guestname> <source_host_ip> <target_host_ip>

4. Take note the migration performance.
	
Expected Results:
Notes:
Comments:

		176957 	[Guest network driver] NIC rtl8139 driver 	gren 	yoyzhang 	Auto 		Feature 	P3 	3780 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual networks

bug:

    No bug found

Actions:

1, Install a new vm , then, we specify a rtl8139 model of NIC into network device container

<snip>

<interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='rtl8139'/>
    </interface>

</snip>

2, Define the new xml description again and boot it up

3, try to ping the vm in the host to verify the function of the emulated NIC
	
Expected Results:

The output of command "lspci" from guest vm shell:

00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)
00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]
00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]
00:01.2 USB Controller: Intel Corporation 82371SB PIIX3 USB [Natoma/Triton II] (rev 01)
00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)
00:02.0 VGA compatible controller: Cirrus Logic GD 5446
00:03.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL-8139/8139C/8139C+ (rev 20)
00:04.0 RAM memory: Qumranet, Inc. Virtio memory balloon
Notes:
Comments:

		176958 	[Guest network driver] NIC virtio driver 	gren 	yoyzhang 	Auto 		Feature 	P3 	3790 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual networks

bug:

    No bug found

Actions:

1, Install a new vm , then, we specify a virtio model of NIC into network device container

<snip>

<interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>

</snip>

2, Define the new xml description again and boot it up

3, try to ping the vm in the host to verify the function of the emulated NIC
	
Expected Results:

the output of command "lspci" insides guest vm:

00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC [Natoma] (rev 02)
00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA [Natoma/Triton II]
00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE [Natoma/Triton II]
00:01.2 USB Controller: Intel Corporation 82371SB PIIX3 USB [Natoma/Triton II] (rev 01)
00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI (rev 03)
00:02.0 VGA compatible controller: Cirrus Logic GD 5446
00:03.0 Ethernet controller: Qumranet, Inc. Virtio network device
00:04.0 RAM memory: Qumranet, Inc. Virtio memory balloon
Notes:
Comments:

		177009 	[Host network interface management] define & undefine interface 	jyang 	None 	Auto 		Feature 	P2 	3800 	Edit
Setup:

1. Before destroy eth0, should stop NetworkManager

 # service NetworkManger stop
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. save the config file of eth0

  # cp /etc/sysconfig/network-scritps/ifcfg-eth0 /tmp

 

2. save the output of "iface-edit eth0" as eth0.xml

 

3. undefine eth0

   # virsh iface-destroy eth0

   # virsh iface-undefine eth0

 

4. check if the config file of eth0 exist.

   # ls -l /etc/sysconfig/network-scripts/ifcfg-eth0

 

5. check if eth0 has IP address via ifconfig

   # /sbin/ifconfig

 

6. check if eth0 is still defined

   # virsh iface-list --all

 

7. check if the network if good

   # ping google.com

    # ping 10.66.70.?

 

8. restore eth0, using the xml file we saved at step 1.

   # virsh iface-define eth0.xml

 

9. check if the config file of eth0 exist.

   # ls -l /etc/sysconfig/network-scripts/ifcfg-eth0

 

10. check if eth0 is still defined

   # virsh iface-list --all

 

11. if step 10 passed, start eth0

   # virsh iface-start eth0

 

 

12. check if eth0 has IP address via ifconfig

   # /sbin/ifconfig

 

 

13. check if the network is good

   # ping google.com

    # ping 10.66.70.?

 

14. loop upper steps with passing MAC address but not interface name to "iface-destroy", "iface-undefine" and 'iface-start'.

 

14. loop upper steps on all other physical network interaces.  step 7 and step 13 should be skipped on loopback device.
	
Expected Results:

step 4:

        not exist

 

step 5:

        no eth0 exist

 

step 6:

        not defined

 

step 7:

        not good

 

step 8:

         successfully restored.

 

step 9:

         exist

 

step 10:

         is defined

 

step 11:

         successfully started

 

step 12:

          has IP address

 

step 13:

          good
Notes:
Comments:

		177269 	[Migration] migrate a guest with disk I/O load 	ajia 	None 	Manual (Autoproposed) 		Stress 	P2 	3800 	Edit
Setup:

1. setup a migration environment according to previous migration cases.

2. install a guest and make sure gcc package is installed on the guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. start a existed guest

# virsh start demo

2. download and install iozone on the guest (demo)

# wget http://home.englab.nay.redhat.com/~ajia/benchmark/iozone/iozone3_347.tar -P /tmp

# cd /tmp

# tar zxvf iozone3_347.tar

# cd iozone3_347

Using 'make' to get 'target' according to host arch such as $(uname -p)=x86_64, target=linux-AMD64

# make linux-AMD64

# ./iozone -a

3. check whether iozone thread is running on the guest

# ps -ef | grep 'iozone -a'


4. start migration from source to target
#virsh migrate demo --live qemu+ssh://<target_host_ip>/system

 
	
Expected Results:

expect step1-4 are successfully.
Notes:
Comments:

		177024 	[Host network interface management] destroy & start interface 	jyang 	None 	Auto 		Feature 	P2 	3810 	Edit
Setup:

1. Before destroy eth0, should stop NetworkManager

 # service NetworkManger stop
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. destroy eth0

   # virsh iface-destroy eth0

 

2. check the state of eth0

    # virsh iface-list --all

 

3. check if the network is good.

    # ping google.com

    # ping 10.66.70.* (select a machine in subnet 10.66.70.0/24)

 

4. start eth0

   # virsh iface-start eth0

 

5. check the state of eth0

    # virsh iface-list --all

 

6. if eth0 is configured using "dhcp" protocol, check if the network is good

    # ping google.com

    # ping 10.66.70.* (select a machine in subnet 10.66.70.0/24)

 

7. loop upper steps with passing MAC address but not the interface name to "iface-destoy", and "iface-start"

 

8. loop uuper steps on all other physical network interfaces, skip step 3 and step 6 on loopback device.
	
Expected Results:

step 2:

       eth0 will be inactive

 

step 3:

        network is not good

 

step  5:

        eth0 will be active

 

step  6:

        network will be good.
Notes:
Comments:

		177270 	[Migration] migrate a guest with heavy CPU load 	jialiu 	None 	Manual 		Stress 	P2 	3810 	Edit
Setup:

Install stress package on both SOURCE and TARGET host.

For RHEL5u5:

http://download.devel.redhat.com/brewroot/packages/stress/0.18.8/1.3.EL5/x86_64/stress-0.18.8-1.3.EL5.x86_64.rpm

For RHEL6:

http://download.devel.redhat.com/brewroot/packages/stress/0.18.8/1.3.EL6/x86_64/stress-0.18.8-1.3.EL6.x86_64.rpm
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. Prepare migration env.

# setenforce 0

# iptables -F

# mount <nfs_ip>:<share_storage_path> /mnt

2. Migrate the guest for 2 rounds

First Round:

On source host:

# time virsh migrate vr-migration-live qemu+ssh://<target_ip>/system

On target host:

# time virsh migrate vr-migration-live qemu+ssh://<source_ip>/system

Second Round:

On source host:

# time virsh migrate vr-migration-live qemu+ssh://<target_ip>/system

On target host:

# time virsh migrate vr-migration-live qemu+ssh://<source_ip>/system

 

3. On both SOURCE and TARGET host, give heavy CPU load by running the folloiwng command:

#  stress --cpu 8 --timeout 1000s

# top

4. Repeate step2.

5. Terminate the stress process in step 3.

6. Repeate step2.
	
Expected Results:

2. Migration is done successfully.

Take note the real time of migration to compare the difference between before and after give heavy CPU loading.

Output:

On SOURCE host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    0m12.194s
user    0m0.005s
sys    0m0.016s

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    0m12.183s
user    0m0.006s
sys    0m0.019s

On TARGET host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    0m12.542s
user    0m0.008s
sys    0m0.012s

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    0m12.432s
user    0m0.004s
sys    0m0.023s

3. Output on source and target host:

#  stress --cpu 8 --timeout 1000s
stress: info: [8161] dispatching hogs: 8 cpu, 0 io, 0 vm, 0 hdd

# top

...
Cpu(s): 99.9%us,  0.1%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st
...

NOTE:

The cpu load is about 100%

4. Output:

ON SOURCE host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    0m15.237s
user    0m0.007s
sys    0m0.011s

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    0m15.045s
user    0m0.006s
sys    0m0.019s

 

On TARGET host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    0m14.172s
user    0m0.003s
sys    0m0.019s

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    0m14.698s
user    0m0.006s
sys    0m0.017s

Compare the real time value with step 2, the real time is increased obviously.

5.

6. Output:

On SOURCE host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    0m12.537s
user    0m0.009s
sys    0m0.007s

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    0m12.357s
user    0m0.011s
sys    0m0.011s

On TARGET host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    0m12.130s
user    0m0.008s
sys    0m0.019s

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    0m12.854s
user    0m0.004s
sys    0m0.022s

NOTE:

After terminate stress process, the migration real time is almost decreased back to the vaule in step 2.
Notes:
Comments:

		177025 	[Host network interface management] dump interface's xml 	jyang 	None 	Auto 		Feature 	P2 	3820 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. dump the xml of eth0, and save it as eth0.xml

    # virsh iface-dumpxml eth0

 

2. /sbin/ifconfig

 

3. compare the result of step 1 and step 2. whether they are consistent, checkpoints are :

    1> MAC address

    2> IP address of both IPv4 and IPv6 if your system has ipv6 enabled.

    3> network prefix of both IPv4 and IPv6
	
Expected Results:

step 3:

       all checkpoints are passed.
Notes:
Comments:

		177271 	[Migration] migrate a guest with heavy MEM load 	jialiu 	None 	Manual 		Stress 	P2 	3820 	Edit
Setup:

Install stress package on both SOURCE and TARGET host.

For RHEL5u5:

http://download.devel.redhat.com/brewroot/packages/stress/0.18.8/1.3.EL5/x86_64/stress-0.18.8-1.3.EL5.x86_64.rpm

For RHEL6:

http://download.devel.redhat.com/brewroot/packages/stress/0.18.8/1.3.EL6/x86_64/stress-0.18.8-1.3.EL6.x86_64.rpm
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Prepare migration env.

# setenforce 0

# iptables -F

# mount <nfs_ip>:<share_storage_path> /mnt

2. Migrate the guest for 1 round

First Round:

On source host:

# time virsh migrate vr-migration-live qemu+ssh://<target_ip>/system

On target host:

# time virsh migrate vr-migration-live qemu+ssh://<source_ip>/system

 

3. On both SOURCE and TARGET host, give heavy MEM load by running the folloiwng command:

# top

# stress  --vm 27 --vm-bytes 256M --vm-keep --timeout 1000s

NOTE:

here, --vm 27 --vm-bytes 256M, that means it will cost 27x256M=6912M mem.

So make sure the available mem is bigger than 8G, and left available mem is less than 300M.

# top

4. Repeate step2.

5. Terminate the stress process in step 3.

6. Repeate step2.
	
Expected Results:

2. Migration is done successfully.

Take note the real time of migration to compare the difference between before and after give heavy CPU loading.

Output:

On SOURCE host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    0m12.194s
user    0m0.005s
sys    0m0.016s

 

On TARGET host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    0m12.542s
user    0m0.008s
sys    0m0.012s

 

3. Output on source and target host:

...
Mem:   7929016k total,   452600k used,  7476416k free,     2084k buffers
...

#  stress  --vm 27 --vm-bytes 256M --vm-keep --timeout 1000s
stress: info: [22036] dispatching hogs: 0 cpu, 0 io, 27 vm, 0 hdd

# top

...
Mem:   7929016k total,  7559404k used,   369612k free,     1892k buffers
...

NOTE:

The left available mem is about 370M.

4. Output:

ON SOURCE host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    1m1.842s
user    0m0.013s
sys    0m0.030s

 

On TARGET host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    1m34.731s
user    0m0.015s
sys    0m0.039s

 

Compare the real time value with step 2, the real time is increased obviously.

5.

6. Output:

On SOURCE host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.200/system
root@10.66.93.200's password:


real    0m12.537s
user    0m0.009s
sys    0m0.007s

 

On TARGET host:

# time virsh migrate vr-migration-live qemu+ssh://10.66.93.91/system
root@10.66.93.91's password:


real    0m12.130s
user    0m0.008s
sys    0m0.019s

 

NOTE:

After terminate stress process, the migration real time is almost decreased back to the vaule in step 2.
Notes:
Comments:

		177026 	[Host network interface management] dump interface's xml (--inactive) -878394 	jyang 	None 	Auto 		Feature 	P2 	3830 	Edit
Setup:

Package netcf is installed
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    878394 - From Run 53754

Actions:

1. dump eth0's xml via virsh with option "--inactive"

   # virsh iface-dumpxml eth0 --inactive

 

2. dump eth0's via ncftool

   # ncftool

   ncftool> dumpxml eth0

 

3. compare the result of step 1 and step2.

 

4. loop the upper steps on all others physical network interfaces

5 destroy the eth0 interface

# virsh iface-destroy eth0

5.1 check the information

# virsh iface-list --all
Name                 State      MAC Address
--------------------------------------------
eth0                 inactive   e8:39:32:58:f7:82
lo                     active     00:00:00:00:00:00

# virsh iface-dumpxml eth0
	
Expected Results:

step 3:

        the result of step1 and step2 should be same.

step 5:

should output correctly and no reportting error,such as

# virsh iface-dumpxml eth0
<interface type='ethernet' name='eth0'>
  <mac address='e8:39:32:58:f7:82'/>
</interface>
Notes:
Add the checkpoint 5 ,check the phsical interface's information while the phsical interface was in inactive state. --wangzhenfeng
Comments:

		177272 	[Migration] migrate a guest with hugepage memory 	jialiu 	None 	Manual (Autoproposed) 		--default-- 	P2 	3830 	Edit
Setup:

Following the following test case to enable libvirt support hugepage memory on both SOURCE host and TARGET host(Larger than guest)

39764 [memory management] huge pages

<https://tcms.engineering.redhat.com/case/39764/?from_plan=1950>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define and start a domain with hugepage memory on SOURCE host

2. Migrate the domain to taget host:

# virsh migrate --live <guestname> qemu+ssh://<taget_host_ip>/system

3. Follow the following test case to enable hugepage memory support on TARGET host

39764 [memory management] huge pages

<https://tcms.engineering.redhat.com/case/39764/?from_plan=1950>

4. Migrate the domain to taget host:

# virsh migrate --live <guestname> qemu+ssh://<taget_host_ip>/system
	
Expected Results:

2. Migrateion failed.

# virsh migrate --live vr-migration-live qemu+ssh://10.66.93.84/system
error: internal error hugetlbfs filesystem is not mounted

4. Migration is done successfully.
Notes:
Comments:

		177027 	[Host network interface management] edit interface's xml 	jyang 	None 	Auto 		Feature 	P2 	3840 	Edit
Setup:

Save the config file of eth0

    # cp /etc/sysconfig/network-scripts/ifcfg-eth0 /tmp/
	
Breakdown:

1. restore the config file of eth0

   # cp /tmp/ifcfg-eth0 /etc/sysconfig/network-scripts/

 

2. restart network

   # service network restart
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. edit eth0's xml  and save it. such as change MAC address

   # virsh iface-edit eth0

 

2. dump the xml of eth0 via ncftool (if it doesn't exist, install netcf)

   # ncftool

   ncftool> dumpxml eth0

 

3. get the configuration of eth0 via augtool (if it doesn't exist, install augeas)

   # augtool

   augtool> print /files/etc/sysconfig/network-scripts/ifcfg-eth0

 

4. # cat /etc/sysconfig/network-scripts/ifcfg-eth0

 

5. compare the result of step 1, step 2, step 3, step 4,  with each other. check if the informations are consistent. checkpoints:

    1> if something is missed

    2> if something's value if not consistent
	
Expected Results:

step 5:

      checkpoints are passed

 

 
Notes:
Comments:

		177273 	[Migration] migrate a guest with vcpu pinning on the same CPU model 	jialiu 	None 	Manual 		--default-- 	P2 	3840 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    migration

bug:

    No bug found

Actions:

1. Define and start a domain, specify the physical cpuset using "cpuset" attribute of vcpu element. xml as follows:

...
  <vcpu cpuset='2'>4</vcpu>
...

2. Check vcpu pin info

# virsh vcpuinfo vr-migration-live
VCPU:           0
CPU:            2
State:          running
CPU time:       1.5s
CPU Affinity:   --y-

VCPU:           1
CPU:            2
State:          running
CPU time:       1.7s
CPU Affinity:   --y-

VCPU:           2
CPU:            2
State:          running
CPU time:       1.3s
CPU Affinity:   --y-

VCPU:           3
CPU:            2
State:          running
CPU time:       1.2s
CPU Affinity:   --y-

3. Migrate the domain to taget host:

# virsh migrate --live <guestname> qemu+ssh://<target_host_ip>/system

4. Check the vcpu pin info on target host is the same as the output in step 2.

# virsh vcpuinfo vr-migration-live
VCPU:           0
CPU:            2
State:          running
CPU time:       1.5s
CPU Affinity:   --y-

VCPU:           1
CPU:            2
State:          running
CPU time:       1.7s
CPU Affinity:   --y-

VCPU:           2
CPU:            2
State:          running
CPU time:       1.3s
CPU Affinity:   --y-

VCPU:           3
CPU:            2
State:          running
CPU time:       1.2s
CPU Affinity:   --y-
	
Expected Results:

3. Migration is done successfully.

4. the vcpu pin info on target host is the same as the output in step 2.
Notes:
Comments:

		177031 	[Host network interface management] list interfaces 	jyang 	None 	Auto 		Feature 	P2 	3850 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. # virsh iface-list

2. # /sbin/ifconfig

3. compare the result of step 1 with result of step 2. checkpoint including:

   1> if the interface is exist.

   2> if the interface is active

   3> if  the MAC address of interface is consistent

4. # virsh iface-list --all

5. # virsh iface-list --inactive
	
Expected Results:

step 1:

      list the physical network interfaces successfully

step 3:

      All checkpoints are good.

step4:

    All active and inactive interfaces are listed out

step5:

  All inactive interfaces are listed out if have
Notes:
Comments:

		177274 	[Migration] Migrate a paused KVM guest - Bug 519204 	yoyzhang 	yoyzhang 	Auto 		Regression 	P2 	3850 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    migration
    virsh-rail

bug:

    No bug found

Actions:

1. Pause a kvm guest

2. Migrate the paused kvm guest via virt-manager or virsh command
	
Expected Results:

2. The guest keeps paused status after migration to the target machine
Notes:
Comments:

		177029 	[Host network interface management] Get interface name via MAC address 	jyang 	yoyzhang 	Auto 		Feature 	P2 	3860 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. get the MAC address of eth0

   # /sbin/ifconfig

 

2. using the MAC address to get the interface'name

   # virsh iface-name ${MAC}

 

3. loop step 1 and step 2 on all of physical network interfaces
	
Expected Results:

step 2:

      get the name of network interface successfully
Notes:
Comments:

		177030 	[Host network interface management] Get MAC address via interface name 	jyang 	None 	Auto 		Feature 	P2 	3870 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    host network interface management

bug:

    No bug found

Actions:

1. get the MAC address of eth0 via "iface-mac"

 # virsh iface-mac eth0

 

2. get the MAC address of eth0 via ifconfig

 # /sbin/ifconfig

 

3. check if the result of step 1 and step 2 are same.

 

4. loop upper steps on the left physical network interfaces.
	
Expected Results:

step 3:

      same.
Notes:
Comments:

		177276 	[Migration] Migrate to wrong uri - bug 730244, 759432 	weizhan 	weizhan 	Manual (Autoproposed) 		Regression 	P1 	3870 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. start a guest

2. do migration with

# virsh migrate --p2p guest qemu+ssh://1.1.1.1/system

3. do migration with

# virsh migrate --live guest qemu+ssh:/{dest ip}/system
	
Expected Results:

Step 2:
should report error:
error: operation failed: Failed to connect to remote libvirt URI
qemu+ssh://1.1.1.1/system

Step3:
should report error:
error: internal error unexpected QEMU URI path '/{dest ip}/system', try qemu:///system

Notes:
Comments:

		177275 	[Migration] Migrate guest with different connect way(tcp, tls) - Bug 491295 	yoyzhang 	yoyzhang 	Auto 		Regression 	P2 	3880 	Edit
Setup:

Prepare a guest, the img should be shared with test machines A and B.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    QE consumption
    virsh-rail

bug:

    No bug found

Actions:

On kvm system, migrate with TCP
1. Turn on --listen flag in /etc/sysconfig/libvirtd on destination host B
2.  Set configuration file in /etc/libvirt/libvirtd.conf on destination host B
.......................

listen_tls = 0
listen_tcp = 1
auth_tcp="none"
3. Restart libvirtd service with command:
# service libvirtd restart

4. issue the following commands on both hosts A and B

# setenforce 1

# setsebool virt_use_nfs on

# iptables -F


5. Migrate the guest from A to B
   # virsh migrate --live guest qemu+tcp://*.*.*.*/system
(*.*.*.* is target machine B's IP)

On kvm system, migrate with TLS

(Restore the configure from the TCP test first.

For tls environment setup, please refer to test case with summary "[Remote access] Connect to the hypervisor running on host using TLS without SASL via ipv4" )
<https://tcms.engineering.redhat.com/case/124654/?from_plan=5066>

 

1. Turn on --listen flag in /etc/sysconfig/libvirtd on destination host
2.  Set configuration file in /etc/libvirt/libvirtd.conf on destination
.......................
tls_port = "16514"
3. Restart libvirtd service with command:
    # service libvirtd restart

4. issue the following commands on both hosts A and B

# setenforce 1

# setsebool virt_use_nfs on

# iptables -F

5. Migrate the guest from A to B
   # virsh migrate --live guest qemu+tls://*.*.*.*/system
(*.*.*.* is target machine B's IP)


	
Expected Results:

On kvm system, migrate with TCP
Migarte guest from system A to system B successfully
On kvm system, migrate with TLS
Migarte guest from system A to system B successfully

Notes:
Comments:

		177277 	[Migration] Migrating the same guest again after terminating last migration with ctrl-c - bug 682953 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	3880 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration

bug:

    No bug found

Actions:

1. install a guest and start on source

2. do migration with

# virsh migrate --live domain_name qemu+ssh://{target hostname}/system --verbose

3. break the migration before it finished with ctrl+c

4. do migration again

# virsh migrate --live domain_name qemu+ssh://{target hostname}/system --verbose

 
	
Expected Results:

migration will succeed with no error
Notes:
Comments:

		177278 	[Migration] Migration after eject cdrom in guest - bug 725673 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	3890 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    822052 - From Run 44174

Actions:

1. Start a domain with cdrom
   # virsh start <domain>
   # virsh dumpxml <domain>
...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/iso.img'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
...
be sure that the image iso.img is not in the same dir on destination host

2. Log in to the guest and do
  # eject cdrom

3. Do live migration
  # virsh migrate --live <domain> qemu+ssh://host2/system

	
Expected Results:

Migration should succeed with no error
Notes:
Comments:

		177279 	[Migration] migration based on ssh connection with ipv6 address - Bug 624626 846013 	jialiu 	None 	Manual 		--default-- 	P2 	3910 	Edit
Setup:

set ipv6 address

Host A - Source Host:

# ifconfig eth0 inet6 add 3ffe::101/64

# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A6:FB:CC  
          inet addr:10.66.92.185  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: 3ffe::101/64 Scope:Global
          inet6 addr: fe80::225:64ff:fea6:fbcc/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:508086 errors:0 dropped:0 overruns:0 frame:0
          TX packets:457834 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:450309247 (429.4 MiB)  TX bytes:532076270 (507.4 MiB)
          Memory:febe0000-fec00000

Host B - Target Host:

# ifconfig eth0 inet6 add 3ffe::100/64
# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A6:FE:85  
          inet addr:10.66.93.84  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: 3ffe::100/64 Scope:Global
          inet6 addr: fe80::225:64ff:fea6:fe85/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1558 errors:0 dropped:0 overruns:0 frame:0
          TX packets:251 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:279618 (273.0 KiB)  TX bytes:35355 (34.5 KiB)
          Memory:febe0000-fec00000

 

At both target host and source host, run the following command:

1. setenforce 0

2. iptables -F; ip6table -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.5
    Regression

bug:

    No bug found

Actions:

1. Define and start a domain on SOURCE host

2. Migrate the domain to TARGET host via ssh connection

# virsh migrate --live <guestname> qemu+ssh://[3ffe::100]/system
	
Expected Results:

2. Migration is done successfully.

 

Bug 846013 is not fixed yet, need add steps after it verified
Notes:
Comments:

		177280 	[Migration] migration based on tcp connection with ipv6 address - Bug 624626 846013 	jialiu 	None 	Manual 		--default-- 	P2 	3920 	Edit
Setup:

set ipv6 address

Host A - Source Host:

# ifconfig eth0 inet6 add 3ffe::101/64

# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A6:FB:CC  
          inet addr:10.66.92.185  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: 3ffe::101/64 Scope:Global
          inet6 addr: fe80::225:64ff:fea6:fbcc/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:508086 errors:0 dropped:0 overruns:0 frame:0
          TX packets:457834 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:450309247 (429.4 MiB)  TX bytes:532076270 (507.4 MiB)
          Memory:febe0000-fec00000

Host B - Target Host:

# ifconfig eth0 inet6 add 3ffe::100/64
# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A6:FE:85  
          inet addr:10.66.93.84  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: 3ffe::100/64 Scope:Global
          inet6 addr: fe80::225:64ff:fea6:fe85/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1558 errors:0 dropped:0 overruns:0 frame:0
          TX packets:251 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:279618 (273.0 KiB)  TX bytes:35355 (34.5 KiB)
          Memory:febe0000-fec00000

 

At both target host and source host, run the following command:

1. setenforce 0

2. iptables -F; ip6tables -F

 

Configuration about tcp , you can refer

[Remote access] Connect to the hypervisor on host using a plain TCP connection without SASL via ipv4

 https://tcms.engineering.redhat.com/case/124649/?from_plan=5066
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.5
    Regression

bug:

    No bug found

Actions:

1. Define a domain on source host

2. Migrate the domain to target host

# virsh migrate --live <guestname> qemu+tcp://[3ffe::102]/system
	
Expected Results:

2. migration is done successfully.

 

 
Bug 846013 is not fixed yet, need add steps after it verified
Notes:
Comments:

		177085 	[libvirtd] check access control of readonly unix socket. 	jyang 	yoyzhang 	Auto 		--default-- 	P2 	3930 	Edit
Setup:

make sure libvirtd is running
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. connect to readonly unix socket

     # virsh -d 5 -c qemu+unix:///system?socket=/var/run/libvirt/libvirt-sock-ro

 

2. try some write actitions. e,g:

virsh # reboot 1
reboot: domain(DATA): 1
reboot: found option <domain>: 1
reboot: <domain> seems like domain ID
error: Failed to reboot domain 1
error: operation virDomainReboot forbidden for read only access

virsh # shutdown 1
shutdown: domain(DATA): 1
shutdown: found option <domain>: 1
shutdown: <domain> seems like domain ID
error: Failed to shutdown domain 1
error: operation virDomainShutdown forbidden for read only access

virsh # pool-list
Name                 State      Autostart
-----------------------------------------
default              active     yes       

virsh # autostart 1
autostart: domain(DATA): 1
autostart: found option <domain>: 1
autostart: <domain> seems like domain ID
error: Failed to mark domain 1 as autostarted
error: operation virDomainSetAutostart forbidden for read only access

virsh # net-autostart default
net-autostart: network(DATA): default
net-autostart: found option <network>: default
net-autostart: <network> trying as network NAME
error: failed to mark network default as autostarted
error: operation virNetworkSetAutostart forbidden for read only access

virsh #
	
Expected Results:

all the write actions will be forbidened,
Notes:
Comments:

		177281 	[Migration] migration based on TLS connection with ipv6 address - Bug 624626 846013 	jialiu 	None 	Manual 		--default-- 	P2 	3930 	Edit
Setup:

set ipv6 address

Host A - Source Host:

# ifconfig eth0 inet6 add 3ffe::101/64

# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A6:FB:CC  
          inet addr:10.66.92.185  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: 3ffe::101/64 Scope:Global
          inet6 addr: fe80::225:64ff:fea6:fbcc/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:508086 errors:0 dropped:0 overruns:0 frame:0
          TX packets:457834 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:450309247 (429.4 MiB)  TX bytes:532076270 (507.4 MiB)
          Memory:febe0000-fec00000

Host B - Target Host:

# ifconfig eth0 inet6 add 3ffe::100/64
# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 00:25:64:A6:FE:85  
          inet addr:10.66.93.84  Bcast:10.66.93.255  Mask:255.255.254.0
          inet6 addr: 3ffe::100/64 Scope:Global
          inet6 addr: fe80::225:64ff:fea6:fe85/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:1558 errors:0 dropped:0 overruns:0 frame:0
          TX packets:251 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:279618 (273.0 KiB)  TX bytes:35355 (34.5 KiB)
          Memory:febe0000-fec00000

 

At both target host and source host, run the following command:

1. setenforce 0

2. iptables -F; ip6table -F

 

For config ipv6 tls environment , you can use the script

https://tcms.engineering.redhat.com/plan/5066/#document

Note:

1.virt-pki-admin add_server "test server" intel-q9400-8-2.englab.nay.redhat.com    change the host name to ipv6 address

2.When #/virt-pki-admin.pl deploy_server intel-q9400-8-2.englab.nay.redhat.com , you can see something like below
The authenticity of host 'intel-q9400-8-2.englab.nay.redhat.com (3ffe::101)' can't be established.

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    Regression
    RHEL6.5

bug:

    No bug found

Actions:

1. Define a domain on source host

2. Migrate the domain to target host

# virsh migrate --live <guestname> qemu+tls://[3ffe::102]/system

3.You can use

#tcpdump -t -vv -n -i eth0 -s 512 host 3ffe::100  to check if  ipv6 work well

	
Expected Results:

 2. migration is done successfully.

 3.

[root@intel-q9400-4-8 ~]# tcpdump -t -vv -n -i eth0 -s 512 host 3ffe::100
tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 512 bytes
IP6 (hlim 255, next-header ICMPv6 (58) payload length: 32) 3ffe::100 > 3ffe::101: [icmp6 sum ok] ICMP6, neighbor advertisement, length 32, tgt is 3ffe::100, Flags [solicited, override]
      destination link-address option (2), length 8 (1): d8:d3:85:7c:b7:9a
        0x0000:  d8d3 857c b79a
IP6 (hlim 64, next-header TCP (6) payload length: 40) 3ffe::101.59589 > 3ffe::100.16514: Flags [S], cksum 0xa09b (correct), seq 1710278514, win 14400, options [mss 1440,sackOK,TS val 86675814 ecr 0,nop,wscale 7], length 0
IP6 (hlim 64, next-header TCP (6) payload length: 40) 3ffe::100.16514 > 3ffe::101.59589: Flags [S.], cksum 0x7510 (correct), seq 2028057779, ack 1710278515, win 14280, options [mss 1440,sackOK,TS val 165934201 ecr 86675814,nop,wscale 7], length 0

 

 

Bug 846013 is not fixed yet, need add steps after it verified
Notes:
Comments:

		177087 	[libvirtd] concurrent remote access 	jyang 	yoyzhang 	Both 		--default-- 	P2 	3940 	Edit
Setup:

1. save the following C code as "concurrent.c"

/*
 * connect to libvirtd concurrently. one thread is to shutdown a domain.
 * the other one is to start it.
 * Usage example: ./contls qemu+tcp://10.66.70.64/system vm1
 * Osier Yang <jyang@redhat.com> Tue Mar 9, 2010
 * how to compile:
 * # gcc -o concurrent -l pthread `pkg-config --cflags
 *   --libs libvirt` concurrent.c
 */

#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <errno.h>
#include <libvirt/libvirt.h>
#include <string.h>

typedef struct tls_conn {
    char *uri;
    char *domain;
} tls_conn, *tls_conn_ptr;

void * thr_fn1(void *conn) {
    if (conn == NULL) {
        printf("argument conn is null\n");
        pthread_exit((void *)1);
    }

    tls_conn_ptr tls_conn;
    virConnectPtr vir_conn;
    virDomainPtr dom;

    printf("thread 1\n");

    tls_conn = (tls_conn_ptr)conn;

    vir_conn = virConnectOpen(tls_conn->uri);

    if (vir_conn == NULL) {
        pthread_exit((void *)1);
    }

    dom = virDomainLookupByName(vir_conn, tls_conn->domain);

    if (dom == NULL) {
        pthread_exit((void *)1);
    }

    if ((virDomainDestroy(dom)) == -1) {
        pthread_exit((void *)1);
    } else {
        printf("thread 1: destroy %s successfully\n", tls_conn->domain);
    }

    return ((void *)0);
}

void * thr_fn2(void *conn) {
    if (conn == NULL) {
        printf("argument conn is null\n");
        pthread_exit((void *)1);
    }


    tls_conn_ptr tls_conn;
    virConnectPtr vir_conn;
    virDomainPtr dom;

    printf("thread 2\n");

    tls_conn = (tls_conn_ptr)conn;

    vir_conn = virConnectOpen(tls_conn->uri);

    if (vir_conn == NULL) {
        pthread_exit((void *)1);
    }


    dom = virDomainLookupByName(vir_conn, tls_conn->domain);

    if (dom == NULL) {
        pthread_exit((void *)1);
    }

    if ((virDomainCreate(dom)) == -1) {
        pthread_exit((void *)1);
    } else {
        printf("start %s successfully\n", tls_conn->domain);
    }

    return ((void *)0);
}


int main (int argc, char **argv) {
    if (argc != 3) {
        printf("Usage: %s <uri>\n", argv[0]);
        exit(EXIT_FAILURE);
    }

    int err;
    pthread_t tid1, tid2;
    void *tret;
    char *uri;
    char *domain;
    tls_conn_ptr conn;

    uri = argv[1];
    domain = argv[2];

    conn = (tls_conn_ptr)malloc(sizeof(tls_conn));

    conn->uri = uri;
    conn->domain = domain;

    err = pthread_create(&tid1, NULL, thr_fn1, (void *)conn);
    if (err != 0) {
        fprintf(stderr, "Failed on creating thread 1: %s\n",
            strerror(errno));
        exit(EXIT_FAILURE);
    }

    err = pthread_create(&tid2, NULL, thr_fn2, (void *)conn);
    if (err != 0) {
        fprintf(stderr, "Failed on creating thread 2: %s\n",
            strerror(errno));
        exit(EXIT_FAILURE);
    }

    err = pthread_join(tid1, &tret);
    if (err != 0) {
        fprintf(stderr, "Failed on joining thread 1: %s\n",
            strerror(errno));
        exit(EXIT_FAILURE);
    }

    printf("thread 1 exit code %d\n", (int)tret);

    err = pthread_join(tid2, &tret);
    if (err != 0) {
        fprintf(stderr, "Failed on joining thread 1: %s\n",
            strerror(errno));
        exit(EXIT_FAILURE);
    }

    printf("thread 2 exit code %d\n", (int)tret);

    free(conn);
    exit(EXIT_FAILURE);
}

2. compile it as:

  # gcc -o concurrent -l pthread `pkg-config --cflags --libs libvirt` concurrent.c

3. make it executable

  # chmod +x ./concurrent

4. make sure the domain which will be tested on is running.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    RHEL6.0
    remote access
    virsh-rail

bug:

    No bug found

Actions:

1.  concurrently connect to unix socket of libvirtd

      # ./concurrent qemu+unix:///system vm1

 

2. configure libvirtd so that it support tls transport method. (refer to case "[libvirtd] remote access via TLS")

     then concurrently connect to it.

      # ./concurrent qemu+tls://10.66.70.64/system vm1

 

3. configure libvirtd so that it support plain tcp transport method. (refer to case "[libvirt] remote access via plain tcp")

      then concurrently connect to it.

    # ./concurrent qemu+tcp://10.66.70.64/system vm1
	
Expected Results:

All of "step 1", "step 2", and "step 3":

no error happens when the transport configuration and the "setup" is correct. the following is an example of "step 3", the result of other two steps is similar.

[root@dhcp-66-70-128 libvirtd]# ./concurrent qemu+tcp://10.66.70.64/system vm1
thread 1
thread 2
thread 1: destroy vm1 successfully
thread 1 exit code 0
start vm1 successfully
thread 2 exit code 0
Notes:
Comments:

		177282 	[Migration] migration during vm boot 	gren 	yoyzhang 	Manual 		--default-- 	P2 	3940 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  prepare a guest virtual machine on source box, the xml description is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, During the proccess of booting of the vm, we begin to perform the migration

   # virsh migrate --live migratetest qemu+ssh://10.66.70.166/system
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.
Notes:
Comments:

		177088 	[libvirtd] daemon convention testing 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	3950 	Edit
Setup:

1. start libvirtd if it's not running

 # service libvirtd start
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    RHEL6.0

bug:

    No bug found

Actions:

1.  get parent process id,  process id, process group id, session id, controlling tty process group ID, tty, and stat of process libvirtd.

     # ps -o ppid,pid,pgid,sid,tpgid,tty,stat -C "libvirtd"
	
Expected Results:

     1. ppid = 1
     2. pgid should be equal to sid
     3. pid is not equal to pgid
     4. tpgid = -1
     5. tty = ?
     6. stat's value contains "l", may be "S", but not "s"
Notes:
Comments:

		177283 	[Migration] migration during vm installation 	gren 	yoyzhang 	Manual 		--default-- 	P2 	3950 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  define and start a guest virtual machine on source box, the xml description is as follows:
<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <kernel>/mnt/vmlinuz</kernel>                                 <= download these 2 file in build tree in shared dir 
    <initrd>/mnt/initrd.img</initrd>
   <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/mnt/migratetest.img'/>                        <= The image should also in shared dir
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system, it is /mnt in our case.

5, During the proccess of vm installation, we begin to perform the migration

6, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.

Note: Don't use virt-manager to install guest, else it will report error during migration, because the vmlinuz and initrd.img is not shared when using virt-manager
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should go on installing.
Notes:
Comments:

		177147 	[Log and debugging] log testing - 1 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	3960 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging
    RHEL6.0

bug:

    No bug found

Actions:

[case1]
1. set value of "log_level" in "/etc/libvirt/libvirtd.conf "to 1


2. set value of "log_outputs" in "/etc/libvirt/libvirtd.conf" to each of "1:syslog:libvirtd", "1:stderr", and  "1:file:libvirtd.log".


3. if libvirtd process is running, kill it

     # kill `pidof libvirtd`


4. start libvirtd in foreground:
   # libvirtd

 
	
Expected Results:

all level log messages are written to /var/log/messages when log_outputs is set as "1:syslog:libvirtd"

all level log messages are written to stderr when log_outputs is set as "1:stderr"

all level log messages are written to ./libvirtd.log when log_outputs is set as "1:file:libvirtd.log"
Notes:
Comments:

		177284 	[Migration] migration during vm reboot 	gren 	yoyzhang 	Manual 		--default-- 	P2 	3960 	Edit
Setup:

Because there is a bug 744967 vm will not boot up when migration during vm reboot

So please reboot  in guest instead of using virsh reboot guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0

bug:

    744967 - From Run 44549
    744967 - From Run 49843
    744967 - From Run 53304
    744967 - From Run 54138

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  prepare a guest virtual machine on source box, the xml description is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, During the proccess of rebooting, we begin to perform the migration.

6, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.
Notes:
Comments:

		177148 	[Log and debugging] log testing - 2 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	3970 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging
    RHEL6.0

bug:

    No bug found

Actions:


1. set value of "log_level" in "/etc/libvirt/libvirtd.conf" as 1

2. set value of "log_outputs" in "/etc/libvirt/libvirtd.conf" to each of "1:stderr", "2:stderr", "3:stderr", "4:stderr"

3. if libvirtd process is running, kill it

4. start it in foreground:
   # libvirtd


	
Expected Results:

all level log messages are written to stderr when log_outputs is set as "1:stderr"


"info", "warning", "error" log messages are written to stderr when log_outputs is set as "2:stderr"

"warning", "error" log messages are written to stderr when log_outputs is set as "3:stderr"

only "error" log messages are written to stderr when log_outputs is set as "4:stderr"
Notes:
Comments:

		177238 	[Migration] Block the storage domain connectivity on destination during migration - bug 705405 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	3970 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. prepare a individual nfs server which is not source or dest host

2. mount nfs on both source and dest host

3. start a guest which storage is located on shared dir

4. "setenforce 1" && "setsebool virt_use_nfs 1" && "iptables -F"on both sides

5. do migration on source 
#  virsh migrate --live vr-rhel6-i386-kvm qemu+ssh://10.66.83.175/system

6. at the same time, on dest, do 
# iptables -A OUTPUT -d {nfs_ip} -p tcp --dport 2049 -j DROP

7. on source host, see the guest status

	
Expected Results:

on source host, guest should not be shutoff if migration still not finished.
Notes:
Comments:

		177149 	[Log and debugging] log testing - 3 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	3980 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging
    RHEL6.0

bug:

    No bug found

Actions:

[case3]
1. set value of "log_level" in "/etc/libvirt/libvirtd.conf" as 4


2. set value of "log_outputs" in "/etc/libvirt/libvirtd.conf" as "1:stderr"


3. if libvirtd process is running, kill it

    # kill `pidof libvirtd`


4. start it in foreground:
   # libvirtd


 
	
Expected Results:

only "error" log message are written to stderr.
Notes:
Comments:

		177240 	[Migration] Change domain xml and name together during migration - Bug 744237, 741251 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	3980 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration

bug:

    No bug found

Actions:

1. Start a domain with shared storage on nfs

    # virsh start guest

2. Get the domain xml and chang the name on xml

   # virsh dumpxml guest > guest.xml

   Change the <name> on guest.xml

   # cat guest.xml

   <domain type='kvm' id='5'>
  <name>guest-new</name>
  <uuid>d19b22e6-419a-0310-3267-a19bf45c2bd9</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
......

3. Do migration with --xml and check target

# virsh migrate --live guest qemu+ssh://10.66.82.125/system --xml guest.xml

on target do

# virsh list --all

4. Do migration with --dname and --xml with different changed name

# virsh migrate --live guest qemu+ssh://10.66.82.125/system --xml guest.xml --dname guest-new2

5. Do migration with --dname and --xml with same changed name

# virsh migrate --live guest qemu+ssh://10.66.82.125/system --xml guest.xml --dname guest-new

6. Do migrate with --xml with original domain name, and --dname with new domain name

# virsh dumpxml guest > guest.xml

# virsh migrate --live guest qemu+ssh://10.66.82.125/system --xml guest.xml --dname guest-new


	
Expected Results:

Step 3:

error: invalid argument: target domain name doesn't match source name

There is no guest named guest-new on target host with shutoff status

Step 4:

error: invalid argument: target domain name doesn't match source name

Step 5:

error: invalid argument: target domain name doesn't match source name

Step 6:

migration succeed without error
Notes:
Comments:

		177105 	[libvirtd] network interface <target> after libvirtd restart - Bug 588046 	jialiu 	None 	Manual 		Regression 	P2 	3990 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    RHEL6.0

bug:

    No bug found

Actions:

1. Define a domain, you will get the following interface:

...
    <interface type='bridge'>
      <mac address='aa:bb:dd:dd:aa:bb'/>
      <source bridge='rhevm'/>
      <target dev='vnet0'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04'
function='0x0'/>    
    </interface>
...

2. When domain is active, restart libvirtd, then dump xml file for the domain

3. When domain is inactive, restart libvirtd, then dump xml file for the domain

	
Expected Results:

2. The auto-generated net (vnet0) target is not blanked out, it can be seen in dump xml ouput.

3. The auto-generated net (vnet0) target is blanked out, it can be not seen in dump xml ouput.
Notes:
Comments:

		177242 	[Migration] compare cpu number and memory size between before migration and after migration 	jyang 	yoyzhang 	Auto 		Feature 	P1 	3990 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort "
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. login into guest, and save  cpuinfo as "/tmp/cpuinfo-before"

    # cat /proc/cpuinfo > /tmp/cpuinfo-before

 

2. login into guest, and save  meminfo as "/tmp/meminfo-before"

    # cat /proc/meminfo > /tmp/meminfo-before

 

3. scp "/tmp/cpuinfo-before" and "/tmp/meminfo-before" to source host.

    # scp /tmp/cpuinfo-before /tmp/meminfo-before root@${source_host_ip}:/tmp

 

4.  migrate

    # virsh migrate --live migrate qemu+ssh://${target_host_ip}/system

 

5. wait util step 4 is finished

 

6. on target host, login into the migrated domain. save cpuinfo and meminfo as step 1, step2. just with diffrent names("/tmp/cpuinfo-after", "/tmp/meminfo-after")

 

7. in the guest, scp "/tmp/cpuinfo-after", "/tmp/meminfo-after" to source host.

    # scp /tmp/cpuinfo-after /tmp/meminfo-after root@${source_host_ip}:/tmp

 

8. compare on source host

   # cd /tmp

   # diff cpuinfo-before cpuinfo-after

    # diff meminfo-before meminfo-after
	
Expected Results:

step 8

       the two diff commands will almost empty. it means they are nearly the same.
Notes:
Comments:

		177106 	[libvirtd] option "--timeout" testing - 1 	jyang 	jyang 	Manual 		--default-- 	P1 	4000 	Edit
Setup:

stop libvirtd if it's running.

# service libvirtd stop
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    libvirtd

bug:

    No bug found

Actions:

Execute the command as non-root user:

1. $ libvirtd -t 5; sleep 5; pidof libvirtd

2. $ libvirtd --timeout 5; sleep 5; pidof libvirtd
	
Expected Results:

both "step 1" and "step2":
   after 5 secs, process exit, and return an empty result.

 
Notes:
Comments:

		177243 	[Migration] compare domain xml before migrated and after migrated 	jyang 	yoyzhang 	Manual (Autoproposed) 		Feature 	P2 	4000 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort "
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. dump domain xml, and save it as "/tmp/migrate-before.xml"

    # virsh dumpxml migrate > migrate-before.xml

 

2. migrate domain "migrate"

   # virsh migrate --live migrate qemu+ssh://${target_host_ip}/system

 

3. wait till step 1 is finished.

 

4. dump domain xml on target host, save it as "migrate-after.xml", and copy it to source host.

    # scp migrate-after.xml root@${source_host_ip}

 

5. compare the two domain xml with diff

    # diff migrate-before.xml migrate-after.xml
	
Expected Results:

step 5.

      no xml node is lost. the number of vcpu, and size of memory is not changed.

      some value can be changed. such as:

45,46c45,46
<     <console type='pty' tty='/dev/pts/24'>
<       <source path='/dev/pts/24'/>
---
>     <console type='pty' tty='/dev/pts/8'>
>       <source path='/dev/pts/8'/>

 

      and may be there is xml node added, for example, on source host, the selinux is diabled. but on target host selinux is enabled:

57a58,61
>   <seclabel type='dynamic' model='selinux'>
>     <label>system_u:system_r:svirt_t:s0:c791,c917</label>
>     <imagelabel>system_u:object_r:svirt_image_t:s0:c791,c917</imagelabel>
>   </seclabel>    
Notes:
Comments:

		177107 	[libvirtd] option "--timeout" testing - 2 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4010 	Edit
Setup:

1. stop libvirtd if it's running.

# service libvirtd stop

 

Refer bug: https://bugzilla.redhat.com/show_bug.cgi?id=572086#c27
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    libvirtd

bug:

    No bug found

Actions:

Execute the command as non-root user:

1. $ libvirtd -askdjkadj

2. $ libvirtd -tasdadad
	
Expected Results:

step1:

process exit immediately, and help messages are printed

step2:

$ libvirtd -tasdadad
2012-04-20 11:05:52.330+0000: 20364: info : libvirt version: 0.9.10, package: 13.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2012-04-18-22:57:51, hs20-bc2-4.build.redhat.com)
2012-04-20 11:05:52.330+0000: 20364: error : main:1349 : Invalid value for timeout
Notes:
Comments:

		177244 	[Migration] direct migration - bug 832373 	jyang 	yoyzhang 	Auto 		Feature 	P2 	4010 	Edit
Setup:

same as case "[Domain async job handling] migrate & domjobabort"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1 direct migratoin

  # virsh migrate --direct toy qemu+ssh://${target_host_ip}/system
	
Expected Results:

step 1:

     because "direct migration" means source libvirtd talks to target hypervisor directly, even don't need the target libvirtd is running, but currently it can talk to xend directly. qemu-kvm doesn't support this feature. so it's expected throws erros like:

should be

error: Requested operation is not valid: direct migration is not supported by the connection driver

but not

error: this function is not supported by the hypervisor: virDomainMigrateToURI
Notes:
Comments:

		177108 	[libvirtd] option "--timeout" testing - 3 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4020 	Edit
Setup:

1. stop libvirtd if it's running.

# service libvirtd stop
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    libvirtd

bug:

    No bug found

Actions:

Execute the command as non-root user:

1. start libvirtd with option "timeout" and give it a string but not number
   $ libvirtd --timeout sadadad
	
Expected Results:

process exit immediately, and help messages are printed

$ libvirtd --timeout sadadad

2012-04-20 11:05:52.330+0000: 20364: info : libvirt version: 0.9.10, package: 13.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2012-04-18-22:57:51, hs20-bc2-4.build.redhat.com)
2012-04-20 11:05:52.330+0000: 20364: error : main:1349 : Invalid value for timeout
Notes:
Comments:

		177247 	[Migration] Do readonly command during qemu migration - bug 704124 	weizhan 	None 	Manual 		Regression 	P1 	4020 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Setup migration environment
on both machine
# setsebool -P virt_use_nfs 1
# iptables -F


On source machine
# cat /etc/exports
/var/lib/libvirt/images *(rw,no_root_squash)

# service nfs start

# mount -o vers=3 10.66.5.110:/var/lib/libvirt/images /var/lib/libvirt/migrate

Prepare a qcow2 block device
# qemu-img create -f qcow2 /var/lib/libvirt/migrate/vm-disk.img  1G

# losetup /dev/loop0 /var/lib/libvirt/migrate/vm-disk.img

# qemu-img info /dev/loop0
image: /dev/loop0
file format: qcow2
virtual size: 1.0G (1073741824 bytes)
disk size: 0
cluster_size: 65536

2. Passthrough this qcow2 block device to a guest, then start
# virsh dumpxml rhel61
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='threads'/>
      <source file='/var/lib/libvirt/migrate/rhel61.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05'
function='0x0'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source dev='/dev/loop0'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05'
function='0x0'/>
    </disk>
...

On target machine
# mount -o vers=3 10.66.5.110:/var/lib/libvirt/images /var/lib/libvirt/migrate

# losetup /dev/loop0 /var/lib/libvirt/migrate/vm-disk.img

3. On the source machine
1)Before migration
# virsh domblkstat rhel61 vda
vda rd_req 24
vda rd_bytes 700416
vda wr_req 0
vda wr_bytes 0
vda errs 0

# virsh domblkstat rhel61 hda
hda rd_req 1208
hda rd_bytes 10046976
hda wr_req 1217
hda wr_bytes 6909952
hda errs 0

# virsh domblkinfo rhel61 /var/lib/libvirt/migrate/rhel61.img
Capacity:       4294967296
Allocation:     1386094592
Physical:       1386094592

#virsh domblkinfo rhel61  /dev/loop0
Capacity:       1073741824
Allocation:     104857600
Physical:       104857600


2) Migrating
# virsh migrate --live rhel61 qemu+ssh://10.66.4.216/system
root@10.66.4.216's password:

At the same time, open another terminals to check, found domblkstat and
domblkinfo can display value immediately. (do several times)
# virsh domblkstat rhel61 vda
vda rd_req 0
vda rd_bytes 0
vda wr_req 5
vda wr_bytes 28672
vda errs 0

# virsh domblkstat rhel61 hda
hda rd_req 1208
hda rd_bytes 10046976
hda wr_req 1235
hda wr_bytes 6991872
hda errs 0


# virsh domblkinfo rhel61 /var/lib/libvirt/migrate/rhel61.img
Capacity:       4294967296
Allocation:     1386094592
Physical:       1386094592

# virsh domblkinfo  rhel61  /dev/loop0
Capacity:       1073741824
Allocation:     0
Physical:       262144
	
Expected Results:

domblkinfo and domblkstat will show the result immediately with no hang
Notes:
Comments:

		177152 	[Log and debugging] per log for per-hypervisor 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4030 	Edit
Setup:

1. make sure there is at least one guest installed for each of qemu, lxc.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    log and debugging

bug:

    No bug found

Actions:

1. check whether the logs are seperated.

     # cat /var/log/libvirt/qemu/guestname.log

     # cat /var/log/libvirt/lxc/guestname.log
	
Expected Results:

/var/log/libvirt/qemu/guestname.log will only contains log messages of the guest on qemu.

/var/log/libvirt/lxc/guestname.log will only contains log messages of the guest on lxc
Notes:
Comments:

		177248 	[Migration] During migration dump domain xml - Bug 742277 	weizhan 	None 	Manual 		Regression 	P1 	4030 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Start a guest located on nfs
# virsh start guest

2. Ensure two hosts have no FQDN
change he hostname with unknow hostname with each other
on hostA: # hostname test1
on hostB: # hostname test2
 
3. Ensure /etc/hosts have no domain name resolve on two hosts
# cat /etc/hosts
127.0.0.1 localhost localhost.localdomain localhost4 loclhost4.localdomain4 ::1
localhost localhost.localdomain localhost6 localhost6.localdomain6 

4. On destination host, do
# while true; do virsh dumpxml guest; done 

5. On source host, do migration
# virsh migrate --live guest qemu+ssh://{dest ip}/system

6. Check the libvirtd status on destination host
# service libvirtd status

	
Expected Results:

Step 5:

Migration will fail

error: Unable to resolve address 'test2' service '49153': Name or service
not known 

Step 6:

libvirtd still running

 
Notes:
Comments:

		177121 	[libvirtd] send "SIGABRT" and "SIGKILL" to libvirtd 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4040 	Edit
Setup:

1 start libvirtd if it's not running

# service libvirtd start
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    libvirtd

bug:

    No bug found

Actions:

1. send signal "SIGABRT" to libvirtd

   # kill -SIGABRT `pidof libvirtd`

2. start libvirtd

ï»¿   # rm -f /var/run/libvirtd.pid 
   # libvirtd -d

3. send signal "SIGKILL" to libvirtd

   # kill -SIGKILL `pidof libvirtd`

4. start libvirtd

ï»¿   # rm -f /var/run/libvirtd.pid 
   # libvirtd -d
	
Expected Results:

all of "step 2", and "step 4"

     libvirtd can be successfully started, but not failed with errors.
Notes:
Comments:

		177249 	[Migration] During migration reboot guest with virsh command - Bug 744967 	weizhan 	None 	Manual 		Regression 	P1 	4040 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    rhel6.5

bug:

    No bug found

Actions:

1. Start a domain with shared storage on nfs

    # virsh start guest

2. Reboot guest with virsh command

   # virsh reboot guest

3. Do migration before guest shutdown

   #virsh migrate --live guest  qemu+ssh://{target ip}/system

4. After migration, check the guest status
	
Expected Results:

Step 4:

Migration will succeed, and guest will finish rebooting and running on target host.
Notes:
Comments:

		177122 	[libvirtd] send signal "SIGTERM", "SIGQUIT", "SIGINT" to libvirtd 	jyang 	jyang 	Manual 		--default-- 	P1 	4050 	Edit
Setup:

1. start libvirtd if it's not running

# service libvirtd start
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    libvirtd

bug:

    No bug found

Actions:

1. send signal "SIGTERM" to libvirtd
   # kill -SIGTERM `pidof libvirtd`

2. start libvirtd

   # service libvirtd start


3. send signal "SIGQUIT" to libvirtd

# kill -SIGQUIT `pidof libvirtd`

 

4. start libvirtd

  # service libvirtd start

 

5. send signal "SIGINT" to libvirtd 

# kill -SIGINT `pidof libvirtd`

 

6. start libvirtd

  # service libvirtd start
	
Expected Results:

all of "step 1", "step 3", and "step5":

    process libvirtd is terminated

# service libvirtd status

libvirtd dead but subsys locked

 

all of "step 2", "step 4", and "step 6":

    libvirtd can be started successfully.

# service libvirtd status
libvirtd (pid  1948) is running...
Notes:
Comments:

		177251 	[Migration] guest timer check after migration 	jialiu 	None 	Auto 		--default-- 	P1 	4050 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Define and start a domain on SOURCE host

2. In guest, run the following command:

# a=$(ssh root@<source_host_ip> date +%s); b=$(date +%s); expr $b - $a

3. migrate the domain to TARGET host

# virsh migrate --live <guestname> qemu+ssh://<taget_host_ip>/system

4. repeate step2

5. Compare the output of step2 and step4.
	
Expected Results:

2. Output in guest:

-11393

4. Output in guest:

-11393

5. The output is almost the same, the acceptable rang is 0 ~ 1 sec.
Notes:
Comments:

		177130 	[libvirtd] xmldesc with a typo should not make libvirtd segfault - Bug 523418 	yoyzhang 	yoyzhang 	Manual 		Regression 	P2 	4060 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    libvirtd

bug:

    No bug found

Actions:

1. # cat segfault.xml

  <domain type='kvm'>
  <name>segfault</name>
  <uuid>9ffe28b6-6134-4b1e-8804-1185f49c436f</uuid>
  <memory>256</memory>
  <currentMemory>256</currentMemory>
  <os>
    <type arch='i686' machine='pc'>hvm</type>
    <boot dev='hd'/>
  </os>
  <devices>
    <interface type='bridge'>
       <target dev='vnet8'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5910' autoport='no' listen='0'/>
  </devices>
</domain>

2. # virsh define segfault.xml
3. # service libvirtd status



	
Expected Results:

2. Output error info

error: Failed to define domain from segfault.xml
error: internal error No <source> 'bridge' attribute specified with <interface type='bridge'/>

3. Libvirt should be running

libvirtd (pid  3422) is running...
Notes:
Comments:

		177252 	[Migration] Kill destination during live migration 	nzhang 	None 	Manual 		--default-- 	P2 	4060 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

On source:
1. Start a VM.
   # virsh start <domain>

2. Live migrate the domain.
   # virsh migrate --live <domain> qemu+ssh://dest_host/system

On destination:
1. Aborts the running domain job durning migration on source.
   # virsh domjobabort --domain rhel6
	
Expected Results:

Check if the domain still can be normally bootup on source.
Notes:
Comments:

		177253 	[Migration] Live migrate with high memory load with no swap 	nzhang 	None 	Manual (Autoproposed) 		--default-- 	P2 	4070 	Edit
Setup:

Install a vm with no swap partition.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. In terminal 1, run memhog tool in guest, set the memsize parameter less than vm's physical memory.
   # memhog -r20 [memsize]

2. In terminal 2, live migrate the domain.
   # virsh migrate --live <domain> qemu+ssh://dest_host/system
	
Expected Results:

Check if the guest still can be migrated and can be normally boot up.
Notes:
Comments:

		177254 	[Migration] Live migration based on same cpu vendor - AMD 	dyuan 	dyuan 	Auto 		--default-- 	P2 	4080 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU type - AMD and support AMD-Vi

2, Firstly  prepare a guest virtual machine on source box, the xml description is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.


if you use live migration, the guest will be running then... migration start , the guest should be available the entire time

if you use offline migration, the guest will be paused on source machine then...migration start
Notes:
Comments:

		177255 	[Migration] Live migration based on same cpu vendor - intel 	gren 	None 	Auto 		--default-- 	P1 	4090 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types - intel and support VT-D

2, Firstly  prepare a guest virtual machine on source box, the xml description is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.


if you use live migration, the guest will be running then... migration start , the guest should be available the entire time

if you use offline migration, the guest will be paused on source machine then...migration start
Notes:
Comments:

		177256 	[Migration] Live migration between non-similar processors - different vendor 	yimwang 	None 	Manual 		--default-- 	P2 	4100 	Edit
Setup:

1, Select two machine with the different CPU vendor

2. For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1, Select two machines with the different vendor, then run migration:

# virsh migrate --live  guest qemu+ssh://${target_host_ip}/system

(If cannot migrate successfully, you can try to follow the step2-7 )

2, get each xml document describing capabilities of the 2 hosts by running:

#virsh capabilities

3, collect the <cpu> element from each of the xml documents and write them into a one file like the following:

<cpu>
      <arch>x86_64</arch>
            <model>Opteron_G3</model>
            <vendor>AMD</vendor>
            <topology sockets='1' cores='4' threads='1'/>
            <feature name='wdt'/>
            <feature name='skinit'/>
            <feature name='osvw'/>
            <feature name='3dnowprefetch'/>
            <feature name='cr8legacy'/>
            <feature name='extapic'/>
            <feature name='cmp_legacy'/>
            <feature name='3dnow'/>
            <feature name='3dnowext'/>
            <feature name='pdpe1gb'/>
            <feature name='fxsr_opt'/>
            <feature name='mmxext'/>
            <feature name='ht'/>
            <feature name='vme'/>
</cpu>

<cpu>
      <arch>x86_64</arch>
            <model>Penryn</model>
            <vendor>Intel</vendor>
            <topology sockets='1' cores='4' threads='1'/>
            <feature name='xtpr'/>
            <feature name='tm2'/>
            <feature name='est'/>
            <feature name='vmx'/>
            <feature name='ds_cpl'/>
            <feature name='monitor'/>
            <feature name='pbe'/>
            <feature name='tm'/>
            <feature name='ht'/>
            <feature name='ss'/>
            <feature name='acpi'/>
            <feature name='ds'/>
            <feature name='vme'/>
</cpu>

4. In this example, the file named intel-amd.xml,  then run command" virsh cpu-baseline intel-amd.xml"

# virsh cpu-baseline intel-amd.xml

5. remove the vendor line from intel-amd.xml, run cpu-baseline again

# virsh cpu-baseline intel-amd.xml

6. add the result of step 5 into domain xml, then start domain

...

<features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <cpu match='exact'>
    <model>core2duo</model>
    <feature policy='require' name='lahf_lm'/>
    <feature policy='require' name='cx16'/>
    <feature policy='require' name='ht'/>
    <feature policy='disable' name='ssse3'/>
  </cpu>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>

...

7. # virsh migrate --live  guest qemu+ssh://${target_host_ip}/system
	
Expected Results:

step 1:

migrate successfully, and the guest works well in target host.

step 4:

error: operation failed: CPU vendors do not match

step 5:

<cpu match='exact'>
  <model>core2duo</model>
  <feature policy='require' name='lahf_lm'/>
  <feature policy='require' name='cx16'/>
  <feature policy='require' name='ht'/>
  <feature policy='disable' name='ssse3'/>
</cpu>

step 7:


migrate successfully, and the guest works well in target host.
Notes:
Comments:

		177212 	[Managed save] Running domain can be manually saved via virsh 	jyang 	None 	Auto 		Feature 	P1 	4110 	Edit
Setup:

1. a healthy running guest, with vnc
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    managed save
    virsh-rail

bug:

    No bug found

Actions:

1. Run a domain:
   # virsh start <domain>

2. In a terminal, run:
   # virsh managedsave <domain>
   
3. check the save file
#ll /var/lib/libvirt/qemu/save

	
Expected Results:

2. The domain should be successfully saved, and no longer running.

domain shutoff

3. # ll /var/lib/libvirt/qemu/save
-rw------. 1 root root 198423578 Jan 13 18:20 domain.save

Notes:
Comments:

		177257 	[Migration] Live migration of a domain while playing online streaming media 	weizhan 	None 	Manual 		--default-- 	P2 	4110 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

  # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1. Install a guest with graphic and browser and shared storage located on nfs mounted dir

2. Log in to the guest and install flashplayer plugin

  download from : http://labs.adobe.com/downloads/flashplayer10_square.html

3. Start the browser and start a flash online

4. Do migration

  # virsh migrate --live guest qemu+ssh://{target ip}/system
	
Expected Results:

Migration should be finished successfully. On guest of target host, the flash still play.
Notes:
Comments:

		177221 	[Memory management] ballooning testing(API) 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4120 	Edit
Setup:

1. save the following C code as "setmem.c"


#include <stdio.h>
#include <stdlib.h>
#include <libvirt/libvirt.h>

int main(int argc, char **argv) {
        if (argc != 3) {
                printf("Usage: %s <domain name> <memory>\n", argv[0]);
                exit(EXIT_FAILURE);
        }

        virConnectPtr conn = NULL;
        virDomainPtr dom = NULL;
        char *uri = "qemu:///system";
        char *domain = argv[1];
        unsigned long memory = atoi(argv[2]);

        conn = virConnectOpen(uri);

        if (conn == NULL) {
                fprintf(stderr, "Failed on open connection.\n");
                exit(EXIT_FAILURE);
        }

        dom = virDomainLookupByName(conn, domain);

        if (dom == NULL) {
                fprintf(stderr, "Failed on lookup %s\n", domain);
                exit(EXIT_FAILURE);
        }

        if ((virDomainSetMemory(dom, memory)) == -1) {
                fprintf(stderr, "Failed on set memory %d to %s\n",
                        memory, domain);
                exit(EXIT_FAILURE);
        }

/*
        if (virDomainDestroy(dom) == -1) {
                fprintf(stderr, "Failed on destroying %s\n", domain);
                exit(EXIT_FAILURE);
        }

        if (virDomainCreate(dom) == -1) {
                fprintf(stderr, "Failed on starting %s\n", domain);
                exit(EXIT_FAILURE);
        }
*/

        exit(EXIT_SUCCESS);
}

2. compile it
  # gcc -o setmem `pkg-config --cflags --libs libvirt` setmem.c

3. make it excutable

  # chmod +x ./setmem

4. make sure the domain which will be tested on is running  and  virtio-balloon is loaded in the guest(supported by rhel5.5 or later).

.
	
Breakdown:

 

 

 
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    memory

bug:

    No bug found

Actions:

1. In the domain XML, set value of <memory> and <currentMemory>

   Test these cases on all left steps.

   A. memory and currentMemory all be 2G(host mem+swap > 2G)

   B. memory and currentMemory all be 800M

2. set current memory up to max memory via "./setmem", e.g.

   # ./setmem guestname 1048576

3. check the guest's memory info
   # cat /proc/meminfo | grep MemTotal

4. set current memory down to  a value  via "./setmem", e.g.

   # ./setmem guestname 528288

5. check the guest's memory info
   # cat /proc/meminfo | grep MemTotal

6. define a guest with memory and currentMemory all be 256M

    #virsh start guest

When guest is booting, use setmem change current memory to a low value, like 50M

   #./setmem guest 50M

Then guest will not boot in OS successfully, It will kernel panic.
	
Expected Results:


    expect :

   A. step 2. succeed, step 3 guest meminfo should change to value you set.

   B. step 2. failed. like error: invalid argument: cannot set memory higher than max memory

step 5:

     the memory of guest is equal to we set in step 4.
Notes:
Comments:

		177258 	[Migration] Live migration of a domain with a PCI device passed-through 	yimwang 	None 	Manual 		Feature 	P2 	4120 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort"

if pci device is nic, need to do it on dual nic machine
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. Start a domain with a PCI device passed-through
   # virsh start <domain>

2. Live migrate the domain;
   # virsh migrate --live <domain> qemu+ssh://host2/system


	
Expected Results:

2.The live migration should fail with
error: Requested operation is not valid: Domain with assigned non-USB host devices cannot be migrated.

The domain should continue to run on the source host.
Notes:
Comments:

		177222 	[Memory management] ballooning testing(virsh) 	jyang 	jyang 	Both 		--default-- 	P1 	4130 	Edit
Setup:

1. make sure the domain which will be tested on is active

2. virtio-balloon is loaded in the guest(supported by rhel5.5 or later).
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    memory
    virsh-rail

bug:

    No bug found

Actions:

1. In the domain XML, set value of <memory> and <currentMemory>

   Test these cases on all left steps.

   A. memory and currentMemory all be 2G(host mem+swap > 2G)

   B. memory and currentMemory all be 800M

2. set current memory up to max memory via "virsh setmem guest value", e.g.

   # virsh setmem guestname 1048576

3. check the guest's memory info
   # cat /proc/meminfo | grep MemTotal

4. set current memory down to  a value  via "virsh setmem guest value", e.g.

   # virsh setmem guestname 528288

5. check the guest's memory info
   # cat /proc/meminfo | grep MemTotal

6. define a guest with memory and currentMemory all be 256M

    #virsh start guest

When guest is booting, use setmem change current memory to a low value, like 50M

   #virsh setmem guest 50M

Then guest will not boot in OS successfully, It will kernel panic.
	
Expected Results:


    expect :

   A. step 2. succeed, step 3 guest meminfo should change to value you set.

   B. step 2. failed. like error: invalid argument: cannot set memory higher than max memory

step 5:

     the memory of guest is equal to we set in step 4.
Notes:
Comments:

		177259 	[Migration] Live migration of a domain with hotplugged disks 	yimwang 	None 	Manual (Autoproposed) 		--default-- 	P2 	4130 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. Start a domain;
   # virsh start <domain>

2. Create a disk on shared storage;
   # dd if=/dev/zero of=/path/to/shared/disk2.dsk bs=1 \
   count=1 seek=20G

3. Hotplug a disk to the domain;
   # virsh attach-device <domain> disk.xml
  # cat disk.xml
  <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/path/to/shared/disk2.dsk'/>
      <target dev='vdb' bus='virtio'/>
    </disk>

4. Live migrate the domain;
   # virsh migrate --live <domain> qemu+ssh://host2/system


	
Expected Results:

4.Pass criteria: The live migration should successfully finish and the hotplugged disk should be accessible in the domain.

Notes:
Comments:

		177226 	[Memory management] Change KVM guest memory info - bug 508266 	yoyzhang 	yoyzhang 	Manual 		Regression 	P2 	4140 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    memory

bug:

    No bug found

Actions:

1. Run # virt-manager

2. Open a shutoff guest, and go to Hardware tab

3. Change 'Change allocation' value (No exceed maximum allocation value), and click Apply

4. Change 'Maximum allocation' value to another number, and click Apply
	
Expected Results:

 

After step3, change allocation is applied to both 'Change allocation' and 'Current allocation', no side effect on 'Maximum allocation'

After step4, maximum allocation is applied, and no side effect on 'Change allocation' and 'Current allocation'
Notes:
Comments:

		177261 	[Migration] Live migration with multiple virtual network interfaces 	nzhang 	None 	Manual (Autoproposed) 		--default-- 	P2 	4140 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

1. Start a domain.
   # virsh start <domain>

2. Set up 3~7 virtual network interfaces

3. Live migrate the domain.
   # virsh migrate --live <domain> qemu+ssh://dest_host/system
	
Expected Results:

Check the those virtual network interfaces still exist after migration.
Notes:
Comments:

		177230 	[Memory management] huge pages 	jyang 	jyang 	Auto 		--default-- 	P1 	4150 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    memory
    virsh-rail

bug:

    No bug found

Actions:

1. set environment on host
      1> mount hugetlbfs, e.g:
    #mkdir /dev/hugepages

    Or # cat /etc/libvirt/qemu.conf

        hugetlbfs_mount = "/dev/hugepages"

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

Then guest memory backing files can be found in /dev/hugepages/libvirt/qemu/.


        #mount -t hugetlbfs hugetlbfs /dev/hugepages


      2> reserve memory for huge pages, e.g:
        #sysctl vm.nr_hugepages=100


      3> restart libvirtd service
    #service libvirtd restart

      4> cat meminfo before guest start
    # more /proc/meminfo |grep Huge
   

2. run a guest with the following xml added to domain xml.
     <memoryBacking><hugepages/></memoryBacking>

3. confirm the guest was booted correctly

4. check HugePages_Free in meminfo again, confirm the guest is using hugepages.
    # more /proc/meminfo |grep Huge
   

                                                   
	
Expected Results:

step1, 4>

AnonHugePages:     92160 kB
HugePages_Total:     100
HugePages_Free:      100
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB

 

step4, eg

AnonHugePages:    602112 kB
HugePages_Total:     100
HugePages_Free:       98
HugePages_Rsvd:        6
HugePages_Surp:        0
Hugepagesize:       2048 kB

Notes:
Comments:

		177262 	[Migration] Live migration with non-shared storage for kvm - 677220 894085 	weizhan 	weizhan 	Manual (Autoproposed) 		Function 	P1 	4150 	Edit
Setup:

1. Prepare 2 hosts

2. Close the iptable on both sides

   # iptables -F

 

Bug Bug 894085 - libvirt: vm pauses after live storage migration
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. Start a guest on source host which image is on local disk(without shared with target host)

2. Create a empty image on target host with the same size, directory and name as in source host

# qemu-img create /IMAGE_DIR/guest.img IMAGE_SIZE

3. Do migration

#virsh migrate --live --copy-storage-all guest qemu+ssh://{dest ip}/system

For Bug 894085 - libvirt: vm pauses after live storage migration:

1. prepare a cluster with 2 nfs storage.

2. the cluster include domain A and B 

3. start a guest in A 

4. In the Disks tab you can "move" the guest's image from A to B 

5. After disk live migration the guest is still running ,not pause
	
Expected Results:

Migration will succeed with no error.

On target host, the guest is running successfully.

"Bug 739071 - [RFE] Live migration with non-shared storage for kvm" it is different from this case because it wants to implement that the target image does not need to be created before migration
Notes:
Comments:

		177263 	[Migration] live migration with option "--persistent" 	jyang 	yoyzhang 	Auto 		Feature 	P2 	4160 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. migrate domain "migrate" with option "--live", "--persistent"

    # virsh migrate --live --persistent migrate qemu+ssh://${target_host_ip}/system

 

2. wait util step 1 finished, then check domain status both on source and target host

    # virsh list --all

 

3. on target host, destroy domain

    # virsh destroy migrate

 

4. on target host, check if domain is defined.

     # virsh list --all

 

5. on target host. start domain

     # virsh start migrate
	
Expected Results:

step 2:

       source host: domain is shutoff

       target host: domain is running

step 4:

       domain is defined.

step 5:

        can be started successfully.
Notes:
Comments:

		177264 	[Migration] live migration with option --undefinesource 	jialiu 	None 	Auto 		--default-- 	P2 	4170 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. live migration with option "--undefinesource"

    # virsh migrate --live --undefinesource migrate qemu+ssh://${target_host_ip}/system

 

2. check domain state on both source and target host.

    # virsh list --all
	
Expected Results:

step 2:

        on source host, domain "migrate" will be undefined.

        on target host, it's running
Notes:
Comments:

		177265 	[migration] migrate 100 guests at one time through TLS connection on hosts(48 cores CPU / 512GB mem) 	jiachen 	None 	Auto 		Stress 	P2 	4180 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    scalability

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177214 	[Managed save] Skip the URI during host shutdown when the URI cannot to be connectted to via initscript 	dyuan 	None 	Manual 		Regression 	P2 	4190 	Edit
Setup:

Check running guests on the URI by "libvirt-guests" when libvirt isn't installed,
      Should warn users that "libvirtd not installed; skipping this URI".
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

1. install a os without libvirtd or remove libvirt manually

# rpm -e libvirt

2. stop the servcie libvirt-guests, it shows:


#service libvirt-guests stop
Running guests on default URI: libvirtd not installed; skipping this URI.

3. start the service libvirt-guest

4. reboot host
# reboot


	
Expected Results:

2. no error message and should show skip the URI

#service libvirt-guests stop
Running guests on default URI: libvirtd not installed; skipping this URI.



4. 
Running guests on default URI:  libvirtd not installed; skipping this URI.
Stopping atd: [  OK  ]
Stopping abrt daemon: [  OK  ]

Notes:
Comments:

		177223 	[Memory Management] bounary test -- guest memory up to free host physical memory plus free swap size 	eli 	None 	Manual 		--default-- 	P1 	4200 	Edit
Setup:

Prepare a guest which can be run
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

1. get the free host physical memory and free swap size

# free

             total       used       free     shared    buffers     cached
Mem:       7995572    4112276    3883296          0     146392    3104144
-/+ buffers/cache:     861740    7133832
Swap:      8191992          0    8191992

 

2. edit the guest <memory> section

# virsh edit <guest-name>

<memory>free-host-physical-memory + free-swap-size</memory>

<currentMemory>free-host-physical-memory + free-swap-size</currentMemory>

# virsh start <guest-name>

 

3. login guest and check the guest memory allocation

guest> cat /proc/meminfo | grep -i memtotal
	
Expected Results:

2. guest start successfully

3. the memory of guest is approximate to "free-host-physical-memory + free-swap-size"
Notes:
Comments:

		177224 	[Memory Management] bounary test -- guest memory up to host physical memory 	eli 	None 	Manual 		--default-- 	P1 	4210 	Edit
Setup:

Prepare a guest which can run
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

1, get the host physical memory

# cat /proc/meminfo | grep -i memtotal

 

2. edit the guest <memory> section

# virsh edit <guest-name>

<memory>host-physical-memory</memory>

<currentMemory>host-physical-memory</currentMemory>

# virsh start <guest-name>

 

3. login guest and check the guest memory allocation

guest> cat /proc/meminfo | grep -i memtotal
	
Expected Results:

2. guest start successfully

3. the memory of guest is approximate to "host-physical-memory"
Notes:
Comments:

		177225 	[Memory Management] bounary test -- set guest memory down to 256M 	eli 	None 	Manual 		--default-- 	P1 	4220 	Edit
Setup:

Prepare a rhel6-64 guest which can run
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

 

1. edit the guest <memory> section

# virsh edit <guest-name>

<memory>262144</memory>

<currentMemory>262144</currentMemory>

# virsh start <guest-name>

 

2. login guest and check the guest memory allocation

guest> cat /proc/meminfo | grep -i memtotal
	
Expected Results:

1. guest started successfully

2. the memory of guest is approximate to 256M
Notes:
Comments:

		177229 	[Memory Management] functional test -- set guest memory 	eli 	None 	Manual 		--default-- 	P1 	4230 	Edit
Setup:

Prepare a guest which can run
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

1. edit the guest <memory> section

# virsh edit <guest-name>

<memory>1048576</memory>

<currentMemory>524288</currentMemory>

# virsh start <guest-name>

 

2. set guest memory using non-integer

# virsh setmem <guest-name> abc

 

3. set guest memory larger than max memory

# virsh setmem <guest-name> 1248576

 

4. reduce guest memory

# virsh setmem <guest-name> 655360

 

5. reboot guest then check the memory

# virsh shutdown <guest-name>

# virsh start <guest-name>

# virsh dumpxml <guest-name>

login guest

guest> cat /proc/meminfo | grep -i memtotal
	
Expected Results:

1. started successfully

2. error: Invalid value of 0 for memory size

3. error: Requested memory size 1248576 kb is larger than maximum of 1048576 kb

4. return nothing

5. the memory of guest is approximate to 655360
Notes:
Comments:

		177233 	[Memory Management] scalability test -- reduce guest memory and increase back 	eli 	None 	Manual 		--default-- 	P1 	4240 	Edit
Setup:

Prepare a guest which can be run
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. edit the guest <memory> section

# virsh edit <guest-name>

<memory>1048576</memory>

<currentMemory>1048576</currentMemory>

# virsh start <guest-name>

 

2. reduce guest memory

# virsh setmem <guest-name> 524288

 

3. login guest and check the guest memory allocation

guest> cat /proc/meminfo | grep -i memtotal

 

4. increase guest memory back

# virsh setmem <guest-name> 1048576

 

5. login guest and check the guest memory allocation

guest> cat /proc/meminfo | grep -i memtotal
	
Expected Results:
Notes:
Comments:

		177236 	[Migration] Bi-directional live migration between 2 hosts. 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P2 	4250 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Prepare 10 guests on each side (20 total) with shared image on nfs named mig0 ~ mig9 on source and mig10 ~ mig19 on target

3. On both side, setting tcp connection environment

a. Edit /etc/sysconfig/libvirtd
       LIBVIRTD_ARGS="--listen"

b. Edit /etc/libvirt/libvirtd.conf
       listen_tls = 0
       listen_tcp=1
       auth_tcp="none"

c. # service libvirtd restart

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration

bug:

    No bug found

Actions:

1.Start all the guest on both sides

   on source:

   for i in {0..9}; do virsh start mig$i; done

   on target:

   for i in {10..19}; do virsh start mig$i; done

2. Do migration on source with script

  # cat test.sh

  #!/bin/sh

  for i in {0..9};
  do
    ssh root@{target_ip} ./migrate-cmd.sh mig1$i &
    ./migrate-cmd.sh mig$i
  done

  # cat migrate-cmd.sh ( have this .sh file on both side with setting different ip of their target )

  #!/bin/sh

  virsh migrate --live $1 qemu+tcp://{target ip}/system &

Run: # sh test.sh
	
Expected Results:

Expect result:

1. The migration of all guests on both sides should be successful.

2. Log in all the guests to check that they all work fine include network

3. Check the disk type, cpu and mem on all the guest are the same as before migration
Notes:
Comments:

		177237 	[Migration] Bi-directional tunnelled migration between 2 hosts. 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P2 	4270 	Edit
Setup:

1. Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool -P virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F

2. Prepare 10 guests on each side (20 total) with shared image on nfs named mig0 ~ mig9 on source and mig10 ~ mig19 on target

3. Dispatch ssh public key of source host to target host.
    Creating your local public key pair
    # ssh-keygen -t rsa

    Copying the public key to remote host
    # ssh-copy-id -i ~/.ssh/id_rsa.pub root@{target ip}
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration

bug:

    No bug found

Actions:

1.Start all the guest on both sides

   on source:

   for i in {0..9}; do virsh start mig$i; done

   on target:

   for i in {10..19}; do virsh start mig$i; done

2. Do migration on both sides concurrently with script

  # cat test.sh

  #!/bin/sh

  for i in {0..9};
  do
    ssh root@{target_ip} ./migrate-cmd.sh mig1$i &
    ./migrate-cmd.sh mig$i
  done

  # cat migrate-cmd.sh ( have this sh on both side with setting different ip of their target )

  #!/bin/sh

  virsh migrate --live --p2p --tunnelled $1 qemu+ssh://{ip}/system &

Run: # sh test.sh
	
Expected Results:

1. The migration of all guests on both sides should be successful.

2. Log in all the guests to check that they all work fine include network

3. Check the disk type, cpu and mem on all the guest are the same as before migration
Notes:
Comments:

		177187 	[LXC] set memory 	jyang 	None 	Manual 		Feature 	P2 	4290 	Edit
Setup:

same as case "[LXC]container life cycle testing"

Connect  to the lxc hypervisor

#virsh -c lxc:///
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

1.  get the value of max memory and current  memory

     # virsh dominfo toy

 

2. set a new value to current memory, which is larger than max memory

     # virsh setmem toy ${larger_than_max_memory}

 

3. pass a invalid value to "virsh setmem", e.g

    # virsh setmem toy -1

    # virsh setmem toy abcde

 

4. umount cgroup filesystem

   # /etc/init.d/cgconfig stop
   # /etc/init.d/libvirtd restart
   # lscgroup
     cgroups can't be listed: Cgroup is not mounted

 

 

5. set a reasonble value to current memory, which is less than max memory

    # virsh setmem toy ${less_than_max_memory}

 

6. remount cgroup filesystem

# /etc/init.d/cgconfig start
   # /etc/init.d/libvirtd restart

7. re-start the guest to get the cgroup.

    #virsh destory toy

   #virsh start toy

    same as step 5 to set the current memory

# virsh setmem toy ${less_than_max_memory} 

  

8. check the value in "memory.usage_in_bytes" of cgroup memory subsystem

    # cat ${cgroup_mountpoint}/memory/libvirt/lxc/toy/memory.usage_in_bytes

 

9. get the value of max memory and current  memory, same as step 1.

 

10. start a lxc guest.
virsh # list --all 
 Id    Name                           State
----------------------------------------------------
 16719 lxc                            running

virsh # dumpxml lxc
...
  <memory unit='KiB'>1048576</memory>
  <currentMemory unit='KiB'>512000</currentMemory>
...


11, change mem of guest:
# for i in {1..100}; do virsh -c lxc:/// setmem lxc 100; done
error: operation failed: Failed to set memory for domain

error: operation failed: Failed to set memory for domain
sometimes setmem command get different results.

 12, when it pass, check the dominfo

virsh # dominfo lxc
Id:             18271
Name:           lxc
UUID:           c1b7d7aa-6b1b-ce42-9c0b-b65c4e21a08d
OS Type:        exe
State:          running
CPU(s):         1
CPU time:       0.0s
Max memory:     1048576 kB
Used memory:    320 kB
Persistent:     yes
Autostart:      disable
Managed save:   unknown
Security model: selinux
Security DOI:   0
Security label: system_u:system_r:virtd_t:s0-s0:c0.c1023 (enforcing)


	
Expected Results:

step 2:

       throw errors like:

error: Requested memory size 10000000 kb is larger than maximum of 500000 kb

 

step 3:

virsh # setmem toy adad
error: Invalid value of 0 for memory size
error: unknown error

virsh # setmem toy -1
error: Requested memory size 18446744073709551615 kb is larger than maximum of 1048576 kb

 

step 5:

       error like:

.......... Unable to get cgroup for .............

there is a bug https://bugzilla.redhat.com/show_bug.cgi?id=748354

 

step 7

        success

 

step 9

        the value of used memory we get in step 9 is equal to what we got in step 8.

 

Step 11 dont't fail.

step 12 as step results

BZ https://bugzilla.redhat.com/show_bug.cgi?id=829241 is not verify so let it NEED_UPDATE
Notes:
Comments:

		177188 	[LXC] shutdown/destroy container 	jyang 	None 	Manual 		Feature 	P2 	4300 	Edit
Setup:

same as case "[LXC] OS container suspend/resume"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    707801 - From Run 45525
    707801 - From Run 47626

Actions:

1. start the container

   # virsh -c lxc:/// start fedora-rawhide

 

2. destroy the container

   # virsh -c lxc:/// destroy fedora-rawhide

 

3. check cgroup filesystem

   # find ${cgroup_mountpiont} | grep fedora

 

4. destroy again

   # virsh -c lxc:/// destroy fedora-rawhide

 

5. start the container

 

6. shutdown the container

    # virsh -c lxc:/// shutdown fedora-rawhide

 

7. same as step 3

 

8. shutdown again

    # virsh -c lxc:/// shutdown fedora-rawhide
	
Expected Results:

step 2:

        successfully destroyed

 

step 3:

        no "fedora-rawhide" related item is in output

 

step 4:

         failed to destroy, with a errorï¼

        errorï¼Failed to destroy domain fedora-rawhide

        error: Requested operation is not valid: Domain is not running

 

step 6:

        success

 

step 7:

        no "fedora-rawhide" related item is in output

 

step 8:

        failed to shutdown, with a more freindly error msg.
Notes:
Comments:

		177365 	[Node devices] List all node devices on host 	yoyzhang 	None 	Auto 		--default-- 	P1 	4300 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    node devices
    QE consumption

bug:

    No bug found

Actions:

1. # virsh nodedev-list

2. # virsh nodedev-list --tree
	
Expected Results:

1. Output :

computer
net_00_24_21_0e_d2_10
pci_1002_4383
pci_1002_4384
pci_1002_4385
pci_1002_4390
pci_1002_4390_scsi_host
pci_1002_4390_scsi_host_0
pci_1002_4390_scsi_host_1

...............

2. Output in tree list

computer
 |
  +- net_a2_9f_b3_11_48_d9
  +- net_computer_loopback
  +- pci_1002_4383
  +- pci_1002_4384
  +- pci_1002_4385
  +- pci_1002_4390
  |   |
  |   +- pci_1002_4390_scsi_host
  |   |   |
  |   |   +- pci_1002_4390_scsi_host_scsi_device_lun0
  |   |   |   |
  |   |   |   +- storage_serial_Hitachi_HDT721032SLA380_STA2L7MT1U388B
  |   |   |     
  |   |   +- pci_1002_4390_scsi_host_scsi_host

.............................
Notes:
Comments:

		177189 	[LXC] undefine 	jyang 	None 	Manual 		Feature 	P2 	4310 	Edit
Setup:

same as case "[LXC] OS container suspend/resume"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

1.  make sure the container is in shutoff state

 

2. undefine

    # virsh -c lxc:/// undefine fedora-rawhide

 

3. check if the xml still exists

    # ls -l /etc/libvirt/lxc
	
Expected Results:

step 2:

       success

 

step 3:

       doesn't exist.
Notes:
Comments:

		177190 	[LXC] version output 	jyang 	yoyzhang 	Manual 		Feature 	P2 	4320 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

1. get the information about LXC driver's version. e.g

[root@dhcp-66-70-131 etc]# virsh -c lxc:///
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # version
Compiled against library: libvir 0.8.0
Using library: libvir 0.8.0
Using API: LXC 0.8.0
Running hypervisor: LXC 2.6.32

virsh # quit

 

2. check the kernel version, and compare with the value of "Running hypervisor" that we got in step 1. (only compare the the first three numbers)

   # rpm -q kernel
	
Expected Results:


the version should be same.
Notes:
Comments:

		177191 	[LXC]capabilities 	jyang 	None 	Manual 		Feature 	P2 	4330 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

For x86_64 system

 1. get capabilities

# virsh -c lxc:/// capabilities

2. confirm the output of capabilities is right. checkpoints:

    1> the CPU topology

     # cat /proc/cpuinfo

     check the cpu number

 

     # cat /sys/devices/system/cpu/present

     check the cpu ids. e.g

      [root@dhcp-66-70-131 libvirt]# cat /sys/devices/system/cpu/online
0-1

      it means that the output of capabilities should contain all the info about cpu 0, and cpu 1.

 

     # cat /sys/devices/system/cpu/cpu0/topology/core_siblings_list

     check wether the cpus are in the right cell. e.g.

[root@dhcp-66-70-131 libvirt]# cat /sys/devices/system/cpu/cpu0/topology/core_siblings_list
0-1

it means that cpu 0, and cpu 1 are in a same cell.

 

    2> the guest capabilities. e.g

      * the arch should be same as host's arch

      * the wordsize should be same as host's worldsize

      * the "os_type" should be "exe"

      * the domain type should be "lxc"   

For i386 system

1. get capabilities

# virsh -c lxc:/// capabilities
	
Expected Results:

For x86_64 system

 step 2:

       all the checkpoints are satisfied.

For i386 system

step1:

As the kernel doesn't supports NUMA, so output sample:

<capabilities>

  <host>
    <cpu>
      <arch>i686</arch>
    </cpu>
  </host>

  <guest>
    <os_type>exe</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/libexec/libvirt_lxc</emulator>
      <domain type='lxc'>
      </domain>
    </arch>
  </guest>

</capabilities>


Notes:
Comments:

		177170 	[LXC] Application container life cycle testing 	jyang 	None 	Manual 		--default-- 	P2 	4340 	Edit
Setup:

1. make sure cgroup filesystem is mounted, you can mount it anywhere you want, e.g

   # mkdir /cgroup

   # mount -t cgroup cgroup /cgroup

(It's mounted automatically on rhel6.2)

2.  create a simple application container which name is "toy"    

     # virsh -c lxc:/// define toy.xml

     the contents of toy.xml is listed following:

(if the arch is i386, pls change 'x86_64' to 'i686' in xml.)

<domain type='lxc'>
  <name>vm1</name>
  <uuid>386f5b25-43ee-9d62-4ce2-58c3809e47c1</uuid>
  <memory>500000</memory>
  <currentMemory>500000</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <interface type='network'>
      <mac address='52:54:00:f2:2c:ac'/>
      <source network='default'/>
      <target dev='veth0'/>
    </interface>
    <console type='pty'>
      <target port='0'/>
    </console>
  </devices>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

1. check the state

 #virsh -c lxc:/// list --all

 

2.  start the container

# virsh -c lxc:/// start vm1

 

3. check the state

 # virsh -c lxc:/// list --all

 

4. connect to the container via console

# virsh -c lxc:/// console toy

 

5. exit from the container. e.g

virsh # console toy
Connected to domain vm1
Escape character is ^]
sh-4.1# exit 1

6. check the state

 # virsh -c lxc:///lxc list --all
	
Expected Results:

step 1:

    it's shutoff

 

step 3:

    it's running

 

step 6:

    it's shutoff
Notes:
Comments:

		233086 	[LXC] Libvirt leaks libvirt_lxc processes on container shutdown - bug 879360 	gsun 	gsun 	Manual 		Regression 	P2 	4340 	Edit
Setup:

1. make sure cgroup filesystem is mounted, you can mount it anywhere you want, e.g

   # mkdir /cgroup

   # mount -t cgroup cgroup /cgroup

(It's mounted automatically on rhel6.2)

2.  create a simple application container which name is "toy"    

     # virsh -c lxc:/// define toy.xml

     the contents of toy.xml is listed following:

(if the arch is i386, pls change 'x86_64' to 'i686' in xml.)

<domain type='lxc'>
  <name>vm1</name>
  <uuid>386f5b25-43ee-9d62-4ce2-58c3809e47c1</uuid>
  <memory>500000</memory>
  <currentMemory>500000</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <interface type='network'>
      <mac address='52:54:00:f2:2c:ac'/>
      <source network='default'/>
      <target dev='veth0'/>
    </interface>
    <console type='pty'>
      <target port='0'/>
    </console>
  </devices>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC

bug:

    No bug found

Actions:

1. check the state

 #virsh -c lxc:/// list --all

 

2.  start the container

# virsh -c lxc:/// start vm1

 

3. check the state

 # virsh -c lxc:/// list --all

 

4. connect to the container via console

# virsh -c lxc:/// console toy

 

5. exit from the container. e.g

virsh # console toy
Connected to domain vm1
Escape character is ^]
sh-4.1# exit

virsh # quit

# ps aux|grep libvirt_lxc

6. check the state

 # virsh -c lxc:///lxc list --all
	
Expected Results:

step 1:

    it's shutoff

 

step 3:

    it's running

 

step 5:

# ps aux|grep libvirt_lxc
root     19015  0.0  0.0 103244   856 pts/3    S+   16:47   0:00 grep libvirt_lxc

no libvirt_lxc process

step 6:

    it's shutoff
Notes:
Comments:

		177195 	[Managed save] Cann't managed save a transient domain 	dyuan 	None 	Manual 		Regression 	P1 	4350 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save

bug:

    No bug found

Actions:

1. # virsh create dom.xml 
Domain dom-create created from dom.xml

# virsh list --all
 Id Name                 State
----------------------------------
  8 dom-create           running

2. # virsh managedsave dom-create


3. # virsh list --all
 Id Name                 State
----------------------------------
  8 dom-create           running

	
Expected Results:

step 2:

# virsh managedsave dom-create

error: Failed to save domain dom-create state
error: Requested operation is not valid: cannot do managed save for transient
domain

Notes:
Comments:

		177208 	[Managed save] invalid save file should not block the domain start - bug 730750 	dyuan 	None 	Manual 		Regression 	P1 	4360 	Edit
Setup:



Bug 730750 - Error restoring domain: cannot send monitor command '{"execute":"qmp_capabilities"}': Connection reset by peer

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    Regression

bug:

    No bug found

Actions:

1. start a domain

# virsh list --all

 Id Name                 State
----------------------------------
7  dom          running

2. save the domain, and before the save file is complete, make a copy of it.

# virsh managedsave dom


3. before the step 2 finished, make a copy of the save file

# cp /var/lib/libvirt/qemu/save/dom.save /home

4. after the save finished, cp the dom.save back to /var/lib/libvirt/qemu/save

# mv /var/lib/libvirt/qemu/save/dom.save /tmp

# cp /home/dom.save /var/lib/libvirt/qemu/save/

5. start the domain again

# virsh start dom
	
Expected Results:

step 5:

libvirt or virt-manager should determine the save file is corrupt either
continue to boot or prompt the user if they would like to remove, before
continuing to boot.

 
Notes:
Comments:

		177210 	[Managed save] kill libvirtd in the middle of managed save - bug 727254 	dyuan 	dyuan 	Both 		Regression 	P1 	4370 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh-rail
    managed save
    Regression
    rhel7

bug:

    No bug found

Actions:

1. # virsh managedsave dom & sleep 1; kill -SIGINT `pidof libvirtd`
[2] 19067

Normal status:
1.[root@pengzhimoutest ~]# virsh managedsave kvm1 & sleep 1; kill -SIGINT `pidof libvirtd`
[1] 8495
Domain kvm1 state saved by libvirt
[1]+  Done                    virsh managedsave kvm1
2.
# virsh list --all
error: Failed to reconnect to the hypervisor
error: no valid connection
error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory
3.# ps aux|grep libvirtd
root     19073  0.0  0.0 103304   876 pts/0    S+   06:48   0:00 grep libvirtd
4.[root@pengzhimoutest ~]# service libvirtd status
libvirtd dead but subsys locked
5.[root@pengzhimoutest ~]# ps aux|grep virsh
root      9038  0.0  0.0 103308   876 pts/1    S+   10:45   0:00 grep virsh
6.you can restore the saved vm after restart libvirtd.
	
Expected Results:

step 1:

libvirtd should cleanly exit, and be able to restart and notice that a managed
save had been in progress to properly clean up after that action.

 
Notes:
Comments:

		177388 	[NUMA] creating vm to place the vcpu in certain cell 	gren 	None 	Manual 		--default-- 	P1 	4370 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    NUMA
    RHEL6.0

bug:

    855218 - From Run 47352

Actions:

1, Install a new vm in a numa box. In the xml description of the vm, we could specify the physical cpuset using "cpuset"attribute of vcpu element. xml as follows:

...
  <vcpu cpuset='0'>3</vcpu>
...

2, then, output the vcpu information on the vm, we can see that all of the vcpu are pinned into the first physical cpu.

# virsh vcpuinfo migratetest
VCPU:           0
CPU:            0
State:          running
CPU time:       85.7s
CPU Affinity:   y-

VCPU:           1
CPU:            0
State:          running
CPU time:       111.0s
CPU Affinity:   y-

VCPU:           2
CPU:            0
State:          running
CPU time:       109.8s
CPU Affinity:   y-

3,In fact, the box have a numa cell with two physical cpu in it.

# virsh capabilities
<capabilities>

  <host>
    <cpu>
      <arch>x86_64</arch>
    </cpu>
    <topology>
      <cells num='1'>
        <cell id='0'>
          <cpus num='2'>
            <cpu id='0'/>
            <cpu id='1'/>
          </cpus>
        </cell>
      </cells>
    </topology>
  </host>
	
Expected Results:

1, The vcpu could be pinned into specific cell in numa box when first installing a vm

2, we can also use "virsh freecell" to look up available amount of memory in certain cell.

# virsh freecell --cellno 0
0: 2923560 kB
Notes:
Comments:

		177160 	[Longevity] keep a running guest for one week 	dyuan 	None 	Manual 		--default-- 	P2 	4380 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    longevity
    RHEL6.0

bug:

    No bug found

Actions:

TBD
	
Expected Results:

TBD
Notes:
Comments:

		177389 	[NUMA] guest numa topology - bug 832165 	gsun 	gsun 	Manual 		--default-- 	P1 	4380 	Edit
Setup:

NUMA machine with two or more nodes

Prepare a domain

# virsh list --all
 Id    Name                           State
----------------------------------------------------
-     esx4.1-rhel6.2-x86_64     shut off

# virsh dumpxml esx4.1-rhel6.2-x86_64

...

  <vcpu placement='static'>1</vcpu>

...
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NUMA

bug:

    832165 - From Run 44481
    832165 - From Run 47352
    832165 - From Run 48366
    832165 - From Run 54161

Actions:

1. modify vcpu to 16 and add numa parameters

# virsh edit esx4.1-rhel6.2-x86_64

...

  <vcpu placement='static'>16</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <cpu>
    <topology sockets='2' cores='4' threads='2'/>
    <numa>
      <cell cpus='0-7' memory='512000'/>    // two cells if host has 2 nodes, make cells equal host nodes number
      <cell cpus='8-15' memory='512000'/>
    </numa>
  </cpu>
...

Domain esx4.1-rhel6.2-x86_64 XML configuration edited.

2. start domain

# virsh start esx4.1-rhel6.2-x86_64
Domain esx4.1-rhel6.2-x86_64 started
	
Expected Results:

2.

# ps aux|grep qemu-kvm
qemu      4244 38.7  0.0 2125228 37860 ?       Sl   14:16   0:03 /usr/libexec/qemu-kvm -S -M rhel6.3.0 -enable-kvm -m 704 -smp 16,sockets=2,cores=4,threads=2 -numa node,nodeid=0,cpus=0-7,mem=500 -numa node,nodeid=1,cpus=8-15,mem=500 -name esx4.1-rhel6.2-x86_64 -uuid d4d1fd09-a792-a623-fc50-5b1511c9885a -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/esx4.1-rhel6.2-x86_64.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/mnt/esx4.1-rhel6.2-x86_64_esx4.1-rhel6.2-x86_64,if=none,id=drive-virtio-disk0,format=raw -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -drive if=none,media=cdrom,id=drive-ide0-1-0,readonly=on,format=raw -device ide-drive,bus=ide.1,unit=0,drive=drive-ide0-1-0,id=ide0-1-0 -netdev tap,fd=21,id=hostnet0,vhost=on,vhostfd=22 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=00:50:56:b9:7b:99,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -device usb-tablet,id=input0 -vnc 127.0.0.1:0 -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5
Notes:
Comments:

		177392 	[NUMA] show numa topology 	gren 	None 	Manual 		--default-- 	P1 	4380 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NUMA
    RHEL6.0

bug:

    No bug found

Actions:

 

# virsh capabilities
<capabilities>

  <host>
    <cpu>
      <arch>x86_64</arch>
    </cpu>
    <topology>
      <cells num='1'>
        <cell id='0'>
          <cpus num='2'>
            <cpu id='0'/>
            <cpu id='1'/>
          </cpus>
        </cell>
      </cells>
    </topology>
  </host>

  <guest>
    <os_type>hvm</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel5.4.0</machine>
      <machine>pc</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <pae/>
      <nonpae/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel5.4.0</machine>
      <machine>pc</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

</capabilities>
	
Expected Results:

1, List NUMA topology

2, List support capability for guests
Notes:
Comments:

		177962 	[NUMA] CPU topology parsing bug on special NUMA platform - bug 828729,bug874050 	gsun 	gsun 	Manual 		--default-- 	P1 	4380 	Edit
Setup:

Using AMD 6172 process machine, which is AMD Magny Cours platform:

amd-6172-512-1.englab.nay.redhat.com

https://virtlab.englab.nay.redhat.com/machine/381/details/

 

# cat /proc/cpuinfo |grep "model name"|tail -1
model name	: AMD Opteron(tm) Processor 6172

 

AMD 61xx series is also ok, make sure it have multiple (>=2) NUMA nodes.

AMD 61xx series (Magny Cours)
http://fpaste.org/gTDH/

===============================================================================

Prepare one host with AMD Bulldozer cpu

a host with AMD 6200 series cpu, which is AMD "Interlagos" platform, consist two MCM

(Multi-Chip Module) with 4 "Bulldozer" modules each, total 8 "Bulldozer" modules

Our team have the NUMA1(hp-dl585g7-01) machine . 


	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NUMA

bug:

    No bug found

Actions:

AMD Magny Cours platform
1.
# cat /proc/cpuinfo |grep processor |wc -l
48

2.
# cat /proc/cpuinfo |grep "cpu cores"|tail -1
cpu cores	: 12



3.
# virsh nodeinfo
CPU model:           x86_64
CPU(s):              48
CPU frequency:       2100 MHz
CPU socket(s):       1
Core(s) per socket:  6
Thread(s) per core:  1
NUMA cell(s):        8
Memory size:         529263712 KiB

==============================================================================
AMD Bulldozer CPU
 4.

# cat /proc/cpuinfo |grep "model name"|tail -1 model name : AMD Opteron(tm) Processor 6282 SE

 5.

# numactl --hardware
available: 8 nodes (0-7)
node 0 cpus: 0 4 8 12 16 20 24 28
node 0 size: 16349 MB
node 0 free: 15596 MB
node 1 cpus: 32 36 40 44 48 52 56 60
node 1 size: 16384 MB
node 1 free: 15931 MB
node 2 cpus: 1 5 9 13 17 21 25 29
node 2 size: 16384 MB
node 2 free: 15871 MB
node 3 cpus: 33 37 41 45 49 53 57 61
node 3 size: 16384 MB
node 3 free: 15845 MB
node 4 cpus: 2 6 10 14 18 22 26 30
node 4 size: 16384 MB
node 4 free: 15811 MB
node 5 cpus: 34 38 42 46 50 54 58 62
node 5 size: 16384 MB
node 5 free: 15917 MB
node 6 cpus: 35 39 43 47 51 55 59 63
node 6 size: 16384 MB
node 6 free: 15855 MB
node 7 cpus: 3 7 11 15 19 23 27 31
node 7 size: 16367 MB
node 7 free: 15869 MB
node distances:
node   0   1   2   3   4   5   6   7 
  0:  10  20  20  20  20  20  20  20 
  1:  20  10  20  20  20  20  20  20 
  2:  20  20  10  20  20  20  20  20 
  3:  20  20  20  10  20  20  20  20 
  4:  20  20  20  20  10  20  20  20 
  5:  20  20  20  20  20  10  20  20 
  6:  20  20  20  20  20  20  10  20 
  7:  20  20  20  20  20  20  20  10 

6.

# virsh nodeinfo

CPU model: x86_64

CPU(s): 64

CPU frequency: 2593 MHz

CPU socket(s): 1

Core(s) per socket: 64

Thread(s) per core: 1

NUMA cell(s): 1

Memory size: 132101788 KiB

 
	
Expected Results:

3.

The cpu cores from cpuinfo is 12, the nodeinfo output will be 6, the NUMA cells output will be 8

NOTICE: on the magny-cours machine, the processor is two numa nodes with 6 cores in one physical package ,so The cpuinfo output 
the physical package,then we got the number 12 ;however, the virsh nodeinfo output the numa nodes with 6 cores. so we got the number 6.
this node topology is far more important when coming to performance

 

6.

In case of unusual NUMA machines where we can't accurately detect the topology of the processor the data reported in the virNodeInfo structure is modified to correctly report the maximum number of processors in the host.

nodes: the number of NUMA cell, 1 for unusual NUMA topologies or uniform memory access; check capabilities XML for the actual NUMA topology

sockets: number of CPU sockets per node if nodes > 1, 1 in case of unusual NUMA topology

cores: number of cores per socket, total number of processors in case of unusual NUMA topology

threads: number of threads per core, 1 in case of unusual numa topology
Notes:
Comments:

		200102 	[NUMA] NUMA aware KSM support in libvirt - bug 840113 	gsun 	gsun 	Manual 		--default-- 	P1 	4380 	Edit
Setup:

NUMA box with nodes >= 2
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NUMA

bug:

    No bug found

Actions:

1. show info
# virsh node-memory-tune
Shared memory:
	shm_pages_to_scan 100
	shm_sleep_millisecs 20
	shm_pages_shared 0
	shm_pages_sharing 0
	shm_pages_unshared 0
	shm_pages_volatile 0
	shm_full_scans  0
	shm_merge_across_nodes 1

# cat /sys/kernel/mm/ksm/merge_across_nodes 
1

2. set merge across nodes as 0
# virsh node-memory-tune --shm-merge-across-nodes 0

# virsh node-memory-tune
Shared memory:
	shm_pages_to_scan 100
	shm_sleep_millisecs 20
	shm_pages_shared 0
	shm_pages_sharing 0
	shm_pages_unshared 0
	shm_pages_volatile 0
	shm_full_scans  0
	shm_merge_across_nodes 0

# cat /sys/kernel/mm/ksm/merge_across_nodes 
0

3. combine with page to scan and sleep
# virsh node-memory-tune --shm-merge-across-nodes 0 --shm-pages-to-scan 200 --shm-sleep-millisecs 10

# virsh node-memory-tune
Shared memory:
	shm_pages_to_scan 200
	shm_sleep_millisecs 10
	shm_pages_shared 0
	shm_pages_sharing 0
	shm_pages_unshared 0
	shm_pages_volatile 0
	shm_full_scans  0
	shm_merge_across_nodes 0

# cat /sys/kernel/mm/ksm/merge_across_nodes 
0
# cat /sys/kernel/mm/ksm/sleep_millisecs 
10
# cat /sys/kernel/mm/ksm/pages_to_scan 
200

4. reset as 1

# virsh node-memory-tune --shm-merge-across-nodes 1 --shm-pages-to-scan 100 --shm-sleep-millisecs 20


# virsh node-memory-tune
Shared memory:
	shm_pages_to_scan 100
	shm_sleep_millisecs 20
	shm_pages_shared 0
	shm_pages_sharing 0
	shm_pages_unshared 0
	shm_pages_volatile 0
	shm_full_scans  0
	shm_merge_across_nodes 1

# cat /sys/kernel/mm/ksm/pages_to_scan 
100
# cat /sys/kernel/mm/ksm/sleep_millisecs 
20
# cat /sys/kernel/mm/ksm/merge_across_nodes 
1

	
Expected Results:
Notes:
Comments:

		200111 	[NUMA] vcpupin fail with non-specific error msg when numad is running - bug 846620 	gsun 	gsun 	Manual 		--default-- 	P1 	4380 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    NUMA
    rhel6.5

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		218344 	[NUMA] guest numa topology with node memory alignment - bug 832165 	gsun 	gsun 	Manual 		--default-- 	P1 	4380 	Edit
Setup:

Prepare a domain

# virsh list --all
 Id    Name                           State
----------------------------------------------------
-     esx4.1-rhel6.2-x86_64     shut off

# virsh dumpxml esx4.1-rhel6.2-x86_64

...

  <vcpu placement='static'>1</vcpu>

...
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    rhel6.5
    NUMA

bug:

    No bug found

Actions:

1. modify vcpu to 16 and add numa parameters

# virsh edit esx4.1-rhel6.2-x86_64

...

  <vcpu placement='static'>16</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.3.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <cpu>
    <topology sockets='2' cores='4' threads='2'/>
    <numa>
      <cell cpus='0-7' memory='512000'/>
      <cell cpus='8-15' memory='512000'/>
    </numa>
  </cpu>
...

Domain esx4.1-rhel6.2-x86_64 XML configuration edited.

2. start domain

# virsh start esx4.1-rhel6.2-x86_64
Domain esx4.1-rhel6.2-x86_64 started


3. check in guest and host for node memory alignment

TBD
	
Expected Results:

2.

# ps aux|grep qemu-kvm
qemu      4244 38.7  0.0 2125228 37860 ?       Sl   14:16   0:03 /usr/libexec/qemu-kvm -S -M rhel6.3.0 -enable-kvm -m 704 -smp 16,sockets=2,cores=4,threads=2 -numa node,nodeid=0,cpus=0-7,mem=500 -numa node,nodeid=1,cpus=8-15,mem=500 -name esx4.1-rhel6.2-x86_64 -uuid d4d1fd09-a792-a623-fc50-5b1511c9885a -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/esx4.1-rhel6.2-x86_64.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/mnt/esx4.1-rhel6.2-x86_64_esx4.1-rhel6.2-x86_64,if=none,id=drive-virtio-disk0,format=raw -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -drive if=none,media=cdrom,id=drive-ide0-1-0,readonly=on,format=raw -device ide-drive,bus=ide.1,unit=0,drive=drive-ide0-1-0,id=ide0-1-0 -netdev tap,fd=21,id=hostnet0,vhost=on,vhostfd=22 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=00:50:56:b9:7b:99,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -device usb-tablet,id=input0 -vnc 127.0.0.1:0 -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

3. TBD
Notes:
Comments:

		177162 	[Longevity] keep the cold/hot pluged devices for more than 1 week 	ajia 	None 	Manual 		Feature 	P1 	4390 	Edit
Setup:

Host supports VT-d

For platform just support vt-d1(host kernel) and host kernel larger than 171 kernel, need the following steps:

    modprobe -r kvm_intel
    modprobe -r kvm
    modprobe kvm allow_unsafe_assigned_interrupts=1
    modprobe kvm_intel

At lease have 2 pci devices and 2 usb flash disks
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    PCI and USB device assignment
    longevity

bug:

    No bug found

Actions:

1. Prepare 2 guests

2. Guest 1:  cold-plug a pci device and a usb device and then start

Guest 2:  start first and then hot-plug a pci device and a usb device

3. Keep the guests running for more than 1 week, to make sure that all the devices will still exist

 

NOTE: The detail steps about hot/cold-plug pci/usb device please consult cases 95765  ,  95769 , 103575  , 111010
	
Expected Results:
Notes:
Comments:

		177164 	[Longevity] Run FTP application load in rhel guest 	ydu 	None 	Manual 		--default-- 	P2 	4400 	Edit
Setup:

1. Make sure the host installed the RHEL6-RC build, and keep all packages as default version.

2.Prepare a healthy rhel guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    longevity
    RHEL6.0

bug:

    No bug found

Actions:

1. Set up FTP server in guest

2. Run curl-loader tool to test FTP server

How to do it, please reference:

http://mirrorglass.englab.nay.redhat.com:8080/XWiki/bin/view/Main/document_ftp
	
Expected Results:

After step2, verify vm will run with high cpu and will not hang during FTP server testing.
Notes:
Comments:

		177165 	[Longevity] Run Mysql application load in rhel guest 	ydu 	None 	Manual 		--default-- 	P2 	4410 	Edit
Setup:

1. Make sure the host installed the RHEL6-RC build, and keep all packages as default version.

2.Prepare a healthy rhel guest guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    longevity
    RHEL6.0

bug:

    No bug found

Actions:

1. Set up My_sql in guest

2. Run siege tool to test My_sql server

How to do it, please reference:

http://mirrorglass.englab.nay.redhat.com:8080/XWiki/bin/view/Main/document_Mysql
	
Expected Results:

After step2, verify vm will run with high cpu and will not hang during My_sql server testing.
Notes:
Comments:

		177166 	[Longevity] Run Oracle application load in windows guest 	ydu 	None 	Manual 		--default-- 	P2 	4420 	Edit
Setup:

1. Make sure the host installed the RHEL6-RC build, and keep all packages as default version.

2.Prepare a healthy windows guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    longevity
    RHEL6.0

bug:

    No bug found

Actions:

1. Set up Oracle guest

2. Run Swingbench tool to test Oracle server

How to do it, please see:

http://mirrorglass.englab.nay.redhat.com:8080/XWiki/bin/view/Main/document_Oracle
	
Expected Results:

After step2, verify vm will run with high cpu and will not hang during Oracle server testing.
Notes:
Comments:

		177167 	[Longevity] Run SQL-server application load in windows guest 	ydu 	None 	Manual 		--default-- 	P2 	4430 	Edit
Setup:

1. Make sure the host installed the RHEL6-RC build, and keep all packages as default version.

2.Prepare a healthy windows guest.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    longevity
    RHEL6.0

bug:

    No bug found

Actions:

1. Set up SQL guest

2. Run SQL Query stress tool to test SQL server

How to do it, please see:

http://mirrorglass.englab.nay.redhat.com:8080/XWiki/bin/view/Main/document_SQL
	
Expected Results:

After step2, verify vm will run with high cpu and will not hang during SQL server testing.
Notes:
Comments:

		177173 	[LXC] autostart enable/disable 	jyang 	None 	Manual 		Feature 	P2 	4440 	Edit
Setup:

same as case "[LXC]container life cycle testing"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

1. set the container to be autostart

# virsh -c lxc:/// autostart toy

 

2. get the domain info of contianer

 # virsh -c lxc:/// dominfo toy

 

3. Reboot host

 

4. check the state of container

 # virsh -c lxc:/// list --all

 

5. set the autostart disabled

 # virsh -c lxc:/// autostart  toy --disable

 

6. same as step 2

 

7. same as step 3

 

8. same as step 4.
	
Expected Results:

step 2:

e.g:

virsh # dominfo vm1
Id:             -
Name:           vm1
UUID:           386f5b25-43ee-9d62-4ce2-58c3809e47c1
OS Type:        exe
State:          shut off
CPU(s):         1
Max memory:     500000 kB
Used memory:    500000 kB
Autostart:      enable

 

step 4:

     toy is running

 

step 6

e.g.

Id:             -
Name:           vm1
UUID:           386f5b25-43ee-9d62-4ce2-58c3809e47c1
OS Type:        exe
State:          shut off
CPU(s):         1
Max memory:     500000 kB
Used memory:    500000 kB
Autostart:      disable

step 8:

     it's shutoff
Notes:
Comments:

		177174 	[LXC] Check setting memory error message when cgroup is unmounted 	ajia 	None 	Manual 		Feature 	P2 	4450 	Edit
Setup:

same as case "[LXC]container life cycle testing"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    863931 - From Run 45525
    864336 - From Run 47626

Actions:

1.  get the value of max memory and current  memory and start the guest

     # virsh -c lxc:/// dominfo toy

     # virsh -c lxc:/// start toy

2. unmount cgroup

  # /etc/init.d/cgconfig stop
  # /etc/init.d/libvirtd restart
  # lscgroup

3. setmem of the lxc guest

# virsh -c lxc:/// setmem toy 512000

Note : the set memory should less than the guest's max's memory.

 

	
Expected Results:

step 2:

       It should throw errors like:   "cgroups can't be listed: Cgroup is not mounted"

step 3:

       It should throw errors like: "Requested operation is not valid: cgroups must be configured on the host"

 

there is a bug https://bugzilla.redhat.com/show_bug.cgi?id=748354
Notes:
Comments:

		177175 	[LXC] connect to OS container via console 	jyang 	None 	Manual 		Feature 	P2 	4460 	Edit
Setup:

same as case "[LXC] OS container suspend/resume"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    861564 - From Run 45525

Actions:

1. start the container

    # virsh -c lxc:/// start fedora-rawhide

 

2. connect to it via console

    # virsh -c lxc:/// console fedora-rawhide
	
Expected Results:

step 2

     successfully connected
Notes:
Comments:

		177169 	[LXC] Application container event register 	jyang 	yoyzhang 	Manual 		Feature 	P2 	4470 	Edit
Setup:

prepare an xml as follow:

# cat lxc.xml

<domain type='lxc'>
  <name>toy</name>
  <uuid>d1f4798b-bebf-d93c-1d97-fe1c1cb7c780</uuid>
  <memory>500000</memory>
  <currentMemory>500000</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <interface type='network'>
      <mac address='52:54:00:25:bf:e9'/>
      <source network='default'/>
    </interface>
    <console type='pty'>
      <target type='lxc' port='0'/>
    </console>
  </devices>
</domain>

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

0. run event-test.py first:

python /usr/share/doc/libvirt-python-0.9.10/events-python/event-test.py lxc:///

1. virsh -c lxc:///

Type:  'help' for help with commands
       'quit' to quit

virsh# define lxc.xml

2. virsh# start toy

3. virsh# destroy toy

4. virsh# start toy

virsh# suspend toy

5. virsh# resume toy

6. virsh# console toy

sh-4.1#

7. sh-4.1# exit

8. virsh# undefine toy
	
Expected Results:

# python /usr/share/doc/libvirt-python-0.9.10/events-python/event-test.py lxc:///
Using uri:lxc:///

myDomainEventCallback1 EVENT: Domain toy(-1) Defined Added
myDomainEventCallback2 EVENT: Domain toy(-1) Defined Added
myDomainEventCallback1 EVENT: Domain toy(9938) Started Booted
myDomainEventCallback2 EVENT: Domain toy(9938) Started Booted
myDomainEventCallback1 EVENT: Domain toy(-1) Stopped Shutdown
myDomainEventCallback2 EVENT: Domain toy(-1) Stopped Shutdown
myDomainEventCallback1 EVENT: Domain toy(9791) Started Booted
myDomainEventCallback2 EVENT: Domain toy(9791) Started Booted

myDomainEventCallback1 EVENT: Domain toy(9791) Suspended Paused
myDomainEventCallback2 EVENT: Domain toy(9791) Suspended Paused
myDomainEventCallback1 EVENT: Domain toy(9791) Resumed Unpaused
myDomainEventCallback2 EVENT: Domain toy(9791) Resumed Unpaused
myDomainEventCallback1 EVENT: Domain toy(-1) Stopped Destroyed
myDomainEventCallback2 EVENT: Domain toy(-1) Stopped Destroyed
myDomainEventCallback1 EVENT: Domain toy(-1) Undefined R
myDomainEventCallback2 EVENT: Domain toy(-1) Undefined R
Notes:
Comments:

		177177 	[LXC] get interface statistics 	jyang 	None 	Manual 		Feature 	P2 	4480 	Edit
Setup:

same as case "[LXC] OS container suspend/resume"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

1.  get the statics of network device of guest via "virsh". e.g

    # virsh -c  lxc:/// domifstat toy veth0

 

2. get the statistics via "netstat" in guest

    # netstat -ie

 

3. compare the statistics between what we got in step 1 and step 2. such as compare the value of "TX_BYTES"
	
Expected Results:

step 3:

        it should be nearly same.
Notes:
Comments:

		177179 	[LXC] kill process "/usr/libexec/libvirt_lxc" 	jyang 	yoyzhang 	Manual 		Feature 	P2 	4490 	Edit
Setup:

same as case "

	[LXC]container life cycle testin

"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

1. start toy

    # virsh -c lxc:/// start toy

 

2. get the PID of process "libvirt_lxc" which is running toy

    # ps -ef | grep libvirt_lxc | grep toy

 

3. kill the process. e.g

    # kill -9 23456

 

4. check toy's state

    # virsh -c lxc:/// list --all
	
Expected Results:

step 4:

         shutoff
Notes:
Comments:

		177180 	[LXC] make sure it will only show the process in the container. 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4500 	Edit
Setup:

same as case "[LXC]container life cycle testing"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    No bug found

Actions:

1. start the toy

 

2. connect to toy

    # virsh -c lxc:/// console toy

 

3. list processes

    # ps -ef

 

4. # cat ${cgroup_mountpoint}/cpu/libivrt/lxc/toy/tasks

 

5. check the PID got in step 4.
	
Expected Results:

step 3:

        for toy, because it's just a container for "/bin/sh". so it will only have one process.--- "/bin/sh"

 

step 5:

        one of the PID is of "libvirt_lxc", the other is process running in container ----- what we got in step 3.
Notes:
Comments:

		177547 	[Snapshot] Delete a snapshot 	ajia 	None 	Auto 		--default-- 	P1 	4500 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    snapshot
    virsh-rail

bug:

    No bug found

Actions:

# cat snapshot1.xml
<domainsnapshot>
  <name>snapshot1</name>
  <description>hello snapshot</description>
</domainsnapshot>

# virsh snapshot-create rhel5u4-qcow2 snapshot1.xml

# virsh snapshot-list rhel5u4-qcow2
 Name                 Creation Time             State
---------------------------------------------------
 snapshot1            2010-06-09 11:46:56 +0800 nostate

# virsh snapshot-current rhel5u4-qcow2
<domainsnapshot>
  <name>snapshot1</name>
  <description>hello snapshot</description>
  <state>shutoff</state>
  <creationTime>1276063927</creationTime>
  <domain>
    <uuid>daab6033-e29f-53a4-99e8-45c1a4df8881</uuid>
  </domain>
</domainsnapshot>

# virsh snapshot-delete rhel5u4-qcow2 snapshot1

# virsh snapshot-current rhel5u4-qcow2

# virsh snapshot-list rhel5u4-qcow2
 Name                 Creation Time             State
---------------------------------------------------

	
Expected Results:

can delete snapshot from domain
Notes:
Comments:

		177172 	[LXC] Application container with multi network interfaces 	jyang 	yoyzhang 	Manual 		Feature 	P2 	4510 	Edit
Setup:

1. create a domain with following xml:

<domain type='lxc'>
  <name>toy</name>
  <uuid>386f5b25-43ee-9d62-4ce2-58c3809e47c1</uuid>
  <memory>500000</memory>
  <currentMemory>500000</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/bin/sh</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <interface type='network'>
      <source network='default'/>
    </interface>
    <interface type='network'>
      <source network='default'/>
    </interface>
    <interface type='network'>
      <source network='default'/>
    </interface>
    <console type='pty'>
      <target port='0'/>
    </console>
  </devices>
</domain>

# virsh -c lxc:/// define toy.xml
	
Breakdown:

Bug 607496 - [LXC] Can not get ip address in LXC application guest [CLOSED WONTFIX]

No need to test it in RHEL6.
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL7

bug:

    607496 - From Run 45525
    607496 - From Run 47626

Actions:

1. start toy

 # virsh start toy

 

2. connect to toy

 # console toy

 

3. # ifconfig

 

4. # service network restart

Shutting down interface eth0:                              [  OK  ]
Shutting down loopback interface:                          [  OK  ]
Bringing up loopback interface:                            [  OK  ]
Bringing up interface eth0: 
Determining IP information for eth0... done.
                                                           [  OK  ]

5. # ifconfig eth0

6. test the interfaces. e.g

   # ping -I eth0 ${host_ip}
	
Expected Results:

3. list  3 network interfaces, include "lo"

 eth0      Link encap:Ethernet  HWaddr 52:54:00:EB:EE:66 
          inet6 addr: fe80::5054:ff:feeb:ee66/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:10 errors:0 dropped:0 overruns:0 frame:0
          TX packets:5 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:788 (788.0 b)  TX bytes:468 (468.0 b)

eth1      Link encap:Ethernet  HWaddr 52:54:00:B3:A3:D0 
          inet6 addr: fe80::5054:ff:feb3:a3d0/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:11 errors:0 dropped:0 overruns:0 frame:0
          TX packets:5 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:892 (892.0 b)  TX bytes:468 (468.0 b)

eth2      Link encap:Ethernet  HWaddr 52:54:00:57:DE:5D 
          inet6 addr: fe80::5054:ff:fe57:de5d/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:10 errors:0 dropped:0 overruns:0 frame:0
          TX packets:5 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:788 (788.0 b)  TX bytes:468 (468.0 b)

lo        Link encap:Local Loopback 
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:16436  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:0 (0.0 b)  TX bytes:0 (0.0 b)

5.

eth0      Link encap:Ethernet  HWaddr 52:54:00:EB:EE:66 
          inet addr:192.168.122.21  Bcast:192.168.122.255  Mask:255.255.255.0
          inet6 addr: fe80::5054:ff:feeb:ee66/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:32 errors:0 dropped:0 overruns:0 frame:0
          TX packets:14 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:2604 (2.5 KiB)  TX bytes:1526 (1.4 KiB)

4. Through each interface, can be successful

# ping -I eth0 10.66.4.198
PING 10.66.4.198 (10.66.4.198) 56(84) bytes of data.
64 bytes from 10.66.4.198: icmp_seq=1 ttl=63 time=1.55 ms
64 bytes from 10.66.4.198: icmp_seq=2 ttl=63 time=0.465 ms

Notes:
Comments:

		177559 	[Snapshot] List all of the snapshots for a domain 	ajia 	None 	Auto 		--default-- 	P1 	4510 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    snapshot
    virsh-rail

bug:

    837544 - From Run 42300

Actions:

# virsh snapshot-list rhel5u4-qcow2 
 Name                 Creation Time             State
---------------------------------------------------


# cat snapshot1.xml
<domainsnapshot>
  <name>snapshot1</name>
  <description>hello snapshot</description>
</domainsnapshot>

#virsh snapshot-create rhel5u4-qcow2 snapshot1.xml

[root@dhcp-66-70-62 xml]# virsh snapshot-list rhel5u4-qcow2
 Name                 Creation Time             State
---------------------------------------------------
 snapshot1            2010-06-09 11:46:56 +0800 nostate

# virsh snapshot-delete rhel5u4-qcow2 snapshot1

[root@dhcp-66-70-62 xml]# virsh snapshot-list rhel5u4-qcow2
 Name                 Creation Time             State
---------------------------------------------------

	
Expected Results:

can correctly display current domain snapshot
Notes:
Comments:

		177181 	[LXC] network device 	jyang 	None 	Manual 		Feature 	P2 	4520 	Edit
Setup:

same as "[LXC]container life cycle testing"
	
Breakdown:

Bug 607496 - [LXC] Can not get ip address in LXC application guest [CLOSED WONTFIX]

No need to test it in RHEL6.
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL7
    LXC

bug:

    607496 - From Run 45525
    607496 - From Run 47626

Actions:

1. connect to toy via console

   # virsh -c lxc:/// console toy

 

2. check the network devices of toy

   sh-4.1# ifconfig

 

3. get the network device name of toy

   # virsh -c lxc:/// dumpxml toy | grep interface -A 3

 

4. check whether the virtual network device of toy is attached to "virbr0"

   # brctl show virbr0

 

    if this step is failed. skip following ones

 

5. connect to toy via console and configure the network address using DHCP if there is no IP for it yet

   # dhclient

 

6. ping the host IP

    # ping ${host_ip}

 

7. repeat step 1 -- 6 , test with an os container.

(os container setup: same as case "[LXC] OS container suspend/resume")
	
Expected Results:

step 2:

        two interfaces, one is "eth0", the other is loopback device

There is a bug https://bugzilla.redhat.com/show_bug.cgi?id=607496

 

step 3:

        e.g.  veth0

 

step 4:

        the output will contain the virtual network device name we got in step 3

 

step 5:

        can get the IP successfully

 

step 6:

        success
Notes:
https://bugzilla.redhat.com/show_bug.cgi?id=607496

Disabled as bug 607496 - [LXC] Can not get ip address in LXC application guest was closed as wontfix.
Comments:

		177182 	[LXC] OS container suspend/resume --Bug 861564 	jyang 	None 	Manual 		Feature 	P2 	4530 	Edit
Setup:

1.  create a root filesytem with febootstrap

     # febootstrap --group-install="base" rawhide /tmp/rawhide

or #febootstrap --groupinstall="base" "Fedora 15" /tmp/fe15 http://download.englab.nay.redhat.com/pub/fedora/linux/releases/15/Fedora/i386/os/

 

2.  define a OS container with following xml:

(if the arch is i386, pls change 'x86_64' to 'i686' in xml.)

virsh # dumpxml fedora-rawhide
<domain type='lxc' id='30013'>
  <name>fedora-rawhide</name>
  <uuid>6222c8db-8764-9c54-8fed-2646b8c4ef78</uuid>
  <memory>32768</memory>
  <currentMemory>32768</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>exe</type>
    <init>/sbin/init</init>
  </os>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/libvirt_lxc</emulator>
    <filesystem type='mount'>
      <source dir='/tmp/rawhide'/>
      <target dir='/'/>
    </filesystem>
    <interface type='network'>
      <mac address='52:54:00:73:6b:43'/>
      <source network='default'/>
      <target dev='veth1'/>
    </interface>
    <console type='pty' tty='/dev/pts/7'>
      <source path='/dev/pts/7'/>
      <target port='0'/>
    </console>
  </devices>
</domain>

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    LXC
    RHEL6.0

bug:

    861564 - From Run 45525

Actions:

1. start container

   # virsh -c lxc:/// start fedora-rawhide

 

2. suspend container

   # virsh -c lxc:/// suspend fedora-rawhide

 

3. if step 2 failed, skip this step and following steps.

    # cat /cgroup/freezer/libvirt/lxc/fedora-rawhide/freezer.state

 

4. resume the container

   # virsh -c lxc:/// resume fedora-rawhide

 

5. if step 4 failed, skip this step

   # cat /cgroup/freezer/libvirt/lxc/fedora-rawhide/freezer.state
	
Expected Results:

step 2:

        success

 

step 3:

        FROZEN

 

step 4:

        success

 

step 5:

        THAWED
Notes:
Comments:

		177183 	[LXC] schedinfo --bug 860907 	jyang 	None 	Manual 		Feature 	P2 	4540 	Edit
Setup:

same as case "[LXC]container life cycle testing"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    LXC
    RHEL6.0

bug:

    860907 - From Run 45525

Actions:

1.  get scheduler parameters of "toy"

     # virsh -c lxc:/// schedinfo toy

Scheduler      : posix
cpu_shares     : 1024
vcpu_period    : 100000
vcpu_quota     : -1
 

2.  get the value of cpu shares from cgroup

     # cat /cgroup/cpu/libvirt/lxc/toy/cpu.shares

 

3. compare the value of cpu shares between we got in step 1 and step 2

 

4. set cpu_shares of toy

     # virsh -c lxc:/// schedinfo toy --set cpu_shares=2048

 

5. get schedule parameters of toy

      # virsh schedinfo toy

 

6.  same as step 2

 

7. same as step 3.
	
Expected Results:

stpe1:

Should Not appear:

Scheduler      : Unknown
  error: Requested operation is not valid: cgroup CPU controller is not mounted

 

 

step 3:

        equal

 

step5:

        the value of cpu_shares is equal to 2048

 

step 7.

       the output is 2048
Notes:
Comments:

		177184 	[LXC] schedinfo - give wrong parameters 	jyang 	yoyzhang 	Manual 		Feature 	P2 	4550 	Edit
Setup:

same as case "[LXC]container life cycle testing"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    LXC
    RHEL6.0

bug:

    860907 - From Run 45525

Actions:

 

1. pass arbitrary parameters to "--set" option, e.g

     # virsh -c lxc:/// schedinfo toy --set test=hello

 
	
Expected Results:

step 1

       report prompt error message.
Notes:
Comments:

		177185 	[LXC] set max memory 	jyang 	jyang 	Manual 		Feature 	P2 	4560 	Edit
Setup:

same as case "[LXC]container life cycle testing"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh cmd
    LXC

bug:

    863931 - From Run 45525

Actions:

1.  get the value of max memory and current  memory

     # virsh dominfo toy

 

2. set a new value to current memory, which is lower than current memory

     # virsh setmaxmem toy ${less_than_current_memory}

 

3. pass a invalid value to "virsh setmaxmem", e.g

   # virsh setmaxmem toy  0

    # virsh setmaxmem toy -1

    # virsh setmaxmem toy abcde

 

4. umount cgroup filesystem

# /etc/init.d/cgconfig stop
   # /etc/init.d/libvirtd restart
   # lscgroup
     cgroups can't be listed: Cgroup is not mounted

 

 5. set a reasonble value to current memory, which is less than max memory

    # virsh setmaxmem toy ${larger_than_current_memory}

 

6. remount cgroup filesystem

# /etc/init.d/cgconfig start
   # /etc/init.d/libvirtd restart

 

 7. re-start the guest to get the cgroup.

    #virsh destory toy

   #virsh start toy

    same as step 5 to set the current memory

# virsh setmaxmem toy ${larger_than_current_memory}

 

8. get the value of max memory and current  memory, same as step 1.
	
Expected Results:

step 2:

       throw errors like:

error:...............Cannot set max memory lower than current memory........................

step 3:

virsh # setmaxmem toy 0
error: Invalid value of 0 for memory size

virsh # setmaxmem toy -1
error: Invalid value of -1 for memory size

virsh # setmaxmem toy abcd
error: memory size has to be a number

 

step 5:

       error like:

.......... Unable to get cgroup for .............

 

step 7

        success

 

step 8

        the value of max memory we get in step 9 is equal to what we set in step 7.
Notes:
Comments:

		177186 	[LXC] set max memory for shutoff domain 	jyang 	jyang 	Manual 		Feature 	P1 	4570 	Edit
Setup:

same as case "[LXC]container life cycle testing"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    LXC

bug:

    No bug found

Actions:

1.  get the value of max memory and current  memory

     # virsh dominfo toy

 

2. set a new value to current memory, which is lower than current memory

     # virsh setmaxmem guest[name,id,uuid] ${less_than_current_memory}

 

3. pass a invalid value to "virsh setmaxmem", e.g

    # virsh setmaxmem toy -1

    # virsh setmaxmem toy abcde

 

4. umount cgroup filesystem

 

5. set a reasonble value to current memory, which is less than max memory

    # virsh setmaxmem toy ${larger_than_current_memory}

 

6. remount cgroup filesystem

 

7. same as step 5

 

8. get the value of max memory and current  memory, same as step 1.
	
Expected Results:

step 2:

       throw errors like:

error:...............Cannot set max memory lower than current memory........................

step 3:

virsh # setmem toy adad
error: Invalid value of 0 for memory size

virsh # setmem toy -1
error: Invalid value of -1 for memory size

 

step 5:

       error like:

.......... Unable to get cgroup for .............

 

step 7

        success

 

step 8

        the value of max memory we get in step 9 is equal to what we set in step 7.
Notes:
Comments:

		177137 	[lock manager] Incorrect path length check on sanlock lockspace limits directory name length to 47 characters BZ#735443 	whuang 	None 	Manual 		Regression 	P1 	4580 	Edit
Setup:

Base:

If sanlock service restart failed because of locked by something others , please use

#sanlock client shutdown -f 1

To stop sanlock and restart.Force kill sanlock will lead to the host auto reboot while the wdmd is running.It's by design.

Details refer:

Bug 888197 - Wdmd keep failed rem after force kill and restart sanlock service
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    lock manager
    Regression

bug:

    820173 - From Run 53794

Actions:

1.# getsebool -a | grep sanlock
sanlock_use_fusefs --> off
sanlock_use_nfs --> on
sanlock_use_samba --> off
virt_use_sanlock --> on

# tail -5 /etc/libvirt/qemu-sanlock.conf 
user = "sanlock"
group = "sanlock"
host_id = 1
auto_disk_leases = 1
disk_lease_dir = "/var/lib/libvirt/sanlock"

# tail -1 /etc/libvirt/qemu.conf 
lock_manager = "sanlock"

2.
Prepare a shutdown guest
# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     test                           shut off

3.start wdmd , sanlock , libvirtd service one by one
#service wdmd start
#service sanlock start
#service libvirtd start
#virsh start test


4.

#ll /var/lib/libvirt/sanlock
total 2048
-rw-------. 1 sanlock sanlock 1048576 Jan  5 14:03 f30a79d7d4b9c375cf1b1a5feb8e8e1f
-rw-r-----. 1 sanlock sanlock 1048576 Jan  5 14:04 __LIBVIRT__DISKS__

 

	
Expected Results:

Do not pop up error message  like:

"internal error Lockspace path
'/var/lib/libvirt/images/sanlock/__LIBVIRT__DISKS__' exceeded 48 characters"

 
Notes:
Comments:

		177139 	[lock manager] missing requirement of libvirt-lock-sanlock package BZ#738314 	whuang 	None 	Manual 		Regression 	P1 	4590 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    lock manager
    Regression

bug:

    No bug found

Actions:

# rpm -e augeas
error: Failed dependencies:
augeas is needed by (installed) libvirt-lock-sanlock-0.9.4-17.el6.x86_64

	
Expected Results:

 report error  , augeas is needed by libvirt-lock-sanlock
Notes:
Comments:

		177140 	[lock manager] RFE: QEMU driver does not prevent a disk being added to two guests at once in non-shared modeBZ#578121 	whuang 	None 	Manual 		Regression 	P1 	4600 	Edit
Setup:

Base:

If sanlock service restart failed because of locked by something others , please use

#sanlock client shutdown -f 1

To stop sanlock and restart.Force kill sanlock will lead to the host auto reboot while the wdmd is running.It's by design.

Details refer:

Bug 888197 - Wdmd keep failed rem after force kill and restart sanlock service




# getsebool -a | grep sanlock
sanlock_use_fusefs --> off
sanlock_use_nfs --> on
sanlock_use_samba --> off
virt_use_sanlock --> on

# tail -5 /etc/libvirt/qemu-sanlock.conf 
user = "sanlock"
group = "sanlock"
host_id = 1
auto_disk_leases = 1
disk_lease_dir = "/var/lib/libvirt/sanlock"

# tail -1 /etc/libvirt/qemu.conf 
lock_manager = "sanlock"


.start wdmd , sanlock , libvirtd service one by one
#service wdmd start
#service sanlock start
#service libvirtd start
#virsh start test


Need 2 hosts,do these steps on two host

 

 

Note: when in the second HOST host_id  =2 
Note:  disk_lease_dir  == nfs  mount target




$ ps -axuwf | grep sanlock
root      3953  2.0  5.8 325232 46748 ?        SLsl 12:13  sanlock daemon -U sanlock -G sanlock

$ service libvirtd start
Starting libvirtd daemon:                                  [  OK  ]
$ ps -axuwf | grep libvirtd
root      4093 10.5  0.8 387332  6816 ?        Sl   12:14   0:00 libvirtd
--daemon



Now create a disk image and two guests both using the same disk

$ qemu-img create /var/lib/libvirt/images/disk.img 100M
Formatting '/var/lib/libvirt/images/disk.img', fmt=raw size=104857600 

$ cat > /root/demo1.xml <<EOF
<domain type='qemu'>
  <name>demo1</name>
  <memory>219200</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <devices>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/disk.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes' listen='0.0.0.0'/>
  </devices>
</domain>
EOF

In the second HOST 

$ cat > demo2.xml <<EOF
<domain type='qemu'>
  <name>demo2</name>
  <memory>219200</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <devices>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/disk.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes' listen='0.0.0.0'/>
  </devices>
</domain>
EOF






	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    lock manager
    Regression

bug:

    850470 - From Run 48360
    820173 - From Run 53794

Actions:

If you make sure that '/var/lib/libvirt/sanlock' is an NFS mount, and configure two hosts each with separate 'host_id' in the qemu-sanlock.conf file, 

then this protection should work when 'demo1' is on the first host and 'demo2' is on the second host.

 Note that, need to restart sanlock daemon after mounting the nfs dir.

 

 

 

$ virsh create demo1.xml
Domain demo1 created from demo1.xml

$ virsh create demo2.xml
error: Failed to create domain from demo2.xml
error: Failed to acquire lock: File exists

	
Expected Results:

can not start at the same time

# virsh start $second_guest

error: Failed to start domain sanlock
error: internal error Failed to acquire lock: error -243


Notes:
Comments:

		177138 	[lock manager] Live snapshot and merge with sanlock enabled BZ#785736 	ajia 	None 	Manual (Autoproposed) 		Function 	P1 	4610 	Edit
Setup:

  need to enable sanlock use on selinux

 # setsebool virt_use_sanlock 1

 

 
	
Breakdown:

 

 
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    lock manager

bug:

    No bug found

Actions:

1. enable sanlock in qemu.conf

# grep lock_manager /etc/libvirt/qemu.conf
lock_manager = "sanlock"

2. enable disk leases in qemu-sanlock.conf

# tail -2 /etc/libvirt/qemu-sanlock.conf
auto_disk_leases = 0
require_lease_for_disks = 0

3. create a qcow2 disk image

# qemu-img create -f qcow2 /tmp/vm1disk1.qcow2 4G
Formatting '/tmp/vm1disk1.qcow2', fmt=qcow2 size=4294967296 encryption=off
cluster_size=65536

4. edit the following xml and let disk source file points to the above disk image path

# cat vm1.xml
<domain type='kvm'>
<name>vm1</name>
<memory>131072</memory>
<os>
<type arch='x86_64' machine='rhel6.2.0'>hvm</type>
<boot dev='hd'/>
<bootmenu enable='no'/>
</os>
<devices>
<disk type='block' device='disk'>
<driver name='qemu' type='qcow2'/>
<source dev='/tmp/vm1disk1.qcow2'/>
<target bus="ide" dev="hda"/>
</disk>
</devices>
</domain>

5. create a vm by xml file

# virsh create vm1.xml
Domain vm1 created from vm1.xml

# cat snapshot.xml
<domainsnapshot>
<disks>
<disk name='hda' snapshot='external'>
<source file='/tmp/vm1disk1snap1.qcow2'/>
</disk>
</disks>
</domainsnapshot>

6. create snapshot for the running vm with '--disk-only' option

# virsh snapshot-create vm1 --disk-only snapshot.xml

	
Expected Results:

All of steps are pass and libvirtd works well no die.

 
Notes:
Comments:

		177141 	[lock manager] Sanlock library license compatibility testing 	ajia 	None 	Manual 		Regression 	P1 	4610 	Edit
Setup:

Make sure sanlock and libvirt-lock-sanlock rpm packages are installed:

# rpm -q sanlock libvirt-lock-sanlock
sanlock-1.8-2.el6.x86_64
libvirt-lock-sanlock-0.9.13-3.el6.x86_6

 
	
Breakdown:

 

 
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    lock manager

bug:

    No bug found

Actions:

   1. Check sanlock RPM license


# rpm -q --qf '%{LICENSE}\n' sanlock

   2. Check whether the sanlock link library

# ldd /usr/lib64/libvirt/lock-driver/sanlock.so

 
	
Expected Results:

 Step1:

the output is like this: GPLv2+

Step2:

the output is like this:

        linux-vdso.so.1 =>  (0x00007fff4337b000)
        libsanlock_client.so.1 => /usr/lib64/libsanlock_client.so.1
(0x00007f6120ad5000)
        libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f61208b9000)
        libdl.so.2 => /lib64/libdl.so.2 (0x00007f61206b4000)
        libc.so.6 => /lib64/libc.so.6 (0x00007f6120313000)
        /lib64/ld-linux-x86-64.so.2 (0x0000003b73400000)

 
Notes:
Comments:

		189580 	[lock manager] Support customizable actions when sanlock leases are lost - bug 832156 	ajia 	None 	Manual 		Feature 	P1 	4610 	Edit
Setup:

 Base:

If sanlock service restart failed because of locked by something others , please use

#sanlock client shutdown -f 1

To stop sanlock and restart.Force kill sanlock will lead to the host auto reboot while the wdmd is running.It's by design.

Details refer:

Bug 888197 - Wdmd keep failed rem after force kill and restart sanlock service
	
Breakdown:

 

 
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    lock manager

bug:

    No bug found

Actions:

 1.

# getsebool -a | grep sanlock
sanlock_use_fusefs --> off
sanlock_use_nfs --> on
sanlock_use_samba --> off
virt_use_sanlock --> on

# tail -5 /etc/libvirt/qemu-sanlock.conf 
user = "sanlock"
group = "sanlock"
host_id = 1
auto_disk_leases = 1
disk_lease_dir = "/var/lib/libvirt/sanlock"

# tail -1 /etc/libvirt/qemu.conf 
lock_manager = "sanlock"

2.
Prepare a shutdown guest
# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     test                           shut off

3.start wdmd , sanlock , libvirtd service one by one
#service wdmd start
#service sanlock start
#service libvirtd start
#virsh start test


4.

#ll /var/lib/libvirt/sanlock
total 2048
-rw-------. 1 sanlock sanlock 1048576 Jan  5 14:03 f30a79d7d4b9c375cf1b1a5feb8e8e1f
-rw-r-----. 1 sanlock sanlock 1048576 Jan  5 14:04 __LIBVIRT__DISKS__

 
	
Expected Results:
Notes:
Comments:

		177142 	[lock manager] Sanlock socket is incorrectly labelled for SELinux BZ#735442, BZ#820173 	whuang 	None 	Manual 		Regression 	P1 	4620 	Edit
Setup:

Base:

If sanlock service restart failed because of locked by something others , please use

#sanlock client shutdown -f 1

To stop sanlock and restart.Force kill sanlock will lead to the host auto reboot while the wdmd is running.It's by design.

Details refer:

Bug 888197 - Wdmd keep failed rem after force kill and restart sanlock service



 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    lock manager
    Regression

bug:

    No bug found

Actions:

 1.

# tail -5 /etc/libvirt/qemu-sanlock.conf 
user = "sanlock"
group = "sanlock"
host_id = 1
auto_disk_leases = 1
disk_lease_dir = "/var/lib/libvirt/sanlock"

# tail -1 /etc/libvirt/qemu.conf 
lock_manager = "sanlock"


$ service libvirtd start
Starting libvirtd daemon:                                  [  OK  ]
$ ps -axuwf | grep libvirtd
root      4093 10.5  0.8 387332  6816 ?        Sl   12:14   0:00 libvirtd
--daemon


$ qemu-img create /var/lib/libvirt/images/disk.img 100M
Formatting '/var/lib/libvirt/images/disk.img', fmt=raw size=104857600 

$ cat > /root/demo1.xml <<EOF
<domain type='qemu'>
  <name>demo1</name>
  <memory>219200</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <devices>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/disk.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' autoport='yes' listen='0.0.0.0'/>
  </devices>
</domain>

 EOF


2. $ virsh create demo1.xml

error: Failed to start domain 
error: internal error Failed to open socket to sanlock daemon: Permission denied

ï»¿

3 $ grep AVC /var/log/audit/audit.log 

 

 4 setsebool virt_use_sanlock 1 

ï»¿

virsh create demo1.xml  

 

	
Expected Results:

2) type=AVC msg=audit(1318557438.961:31530): avc:  denied  { connectto } for  pid=21617 comm="libvirtd" path="/var/run/sanlock/sanlock.sock" scontext=system_u:system_r:svirt_t:s0:c770,c858 tcontext=unconfined_u:system_r:sanlock_t:s0 tclass=unix_stream_socket

 

 

3)  guest start up   and  audit log  NO AVC error 
Notes:
Comments:

		177143 	[lock manager] virt-sanlock-cleanup command doesn't work BZ# 738534 	whuang 	None 	Manual (Autoproposed) 		Regression 	P1 	4630 	Edit
Setup:

Base:

If sanlock service restart failed because of locked by something others , please use

#sanlock client shutdown -f 1

To stop sanlock and restart.Force kill sanlock will lead to the host auto reboot while the wdmd is running.It's by design.

Details refer:

Bug 888197 - Wdmd keep failed rem after force kill and restart sanlock service
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    lock manager
    Regression

bug:

    820173 - From Run 48360
    820173 - From Run 53794

Actions:

1.

# getsebool -a | grep sanlock
sanlock_use_fusefs --> off
sanlock_use_nfs --> on
sanlock_use_samba --> off
virt_use_sanlock --> on

# tail -5 /etc/libvirt/qemu-sanlock.conf 
user = "sanlock"
group = "sanlock"
host_id = 1
auto_disk_leases = 1
disk_lease_dir = "/var/lib/libvirt/sanlock"

# tail -1 /etc/libvirt/qemu.conf 
lock_manager = "sanlock"

2.
Prepare a shutdown guest
# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     test                           shut off

3.start wdmd , sanlock , libvirtd service one by one
#service wdmd start
#service sanlock start
#service libvirtd start
#virsh start test


4.

#ll /var/lib/libvirt/sanlock
total 2048
-rw-------. 1 sanlock sanlock 1048576 Jan  5 14:03 f30a79d7d4b9c375cf1b1a5feb8e8e1f
-rw-r-----. 1 sanlock sanlock 1048576 Jan  5 14:04 __LIBVIRT__DISKS__


5. run virt-sanlock-cleanup


In addition, sanlock daemon is running:
# service sanlock status
sanlock (pid 25417) is running...



	
Expected Results:

  step 5)  : no error
Notes:
Update configuration steps --lsu 20130106
Comments:

		177156 	[Longevity] Check virt-manager and rhel guests after run them for a long time 	ydu 	None 	Manual 		--default-- 	P2 	4640 	Edit
Setup:

1. Make sure finished case #57619 [Longevity] Run FTP application load in rhel guest.

2. Make sure finished case #57620 [Longevity] Run Mysql application load in rhel guest.

3. The two guests run on one machine.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    longevity
    RHEL6.0

bug:

    No bug found

Actions:

1. Run the virt-manager, and manage the two guests via virt-manager.

2. Keep FTP and Mysql applications running in each guest.

3. Keep virt-manager and two guests all running, and check them in two weeks.
	
Expected Results:

1. Virt-manager can work well.

2. Specific applications running.

3. After two weeks, all checks have no problem.
Notes:
Comments:

		177632 	[Stable guest ABI] Update lower version qemu-kvm with Windows domain 	jialiu 	None 	Manual 		--default-- 	P3 	4640 	Edit
Setup:

The work required is limited to QEMU and libvirt.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Ensure that a little lower version qemu-kvm is installed.

2. Install a windows guest using virt-install or virt-manager.

3. Check the XML configuration of the guest using virsh dumpxml

4.  Note the pci NIC address allocation (Control Panel ->Device Manager, select the NIC device and check it's properites), then shutdown the guest.

5. Upgrade qemu-kvm to the latest version.

6. Start the guest again.

7. Check the XML configuration of the guest using virsh dumpxml again.

8. Note the pci NIC address allocation (Control Panel ->Device Manager, select the NIC device and check it's properites) again, and compare the pci address with step 4.
	
Expected Results:

1.

2. Guest is installed successfully.

3.  Verify the machine property of the <os><type> element is rhel6.2.0.

4.

5.

6. The guest is still working fine,  no inactivation prompt is seen after qemu-kvm upgradation.

7. The machine property of the <os><type> element is still rhel6.2.0.

8. The pci address allocation is not changed.
Notes:
Comments:

		177157 	[Longevity] Check virt-manager and windows guests after run them for a long time 	ydu 	None 	Manual 		--default-- 	P2 	4650 	Edit
Setup:

1. Make sure finished case #57622 [Longevity] Run SQL-server application load in windows guest.

2. Make sure finished case #57624 [Longevity] Run Oracle application load in windows guest.

3. The two guests run on one machine.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    longevity
    RHEL6.0

bug:

    No bug found

Actions:

1. Run the virt-manager, and manage the two guests via virt-manager.

2. Keep SQL-server and Oracle applications running in each guest.

 

3. Keep virt-manager and two guests all running, and check them in two weeks.
	
Expected Results:

1. Virt-manager can work well.

2. Specific applications running.
3. After two weeks, all checks have no problem.
Notes:
Comments:

		177627 	[Stable guest ABI] Stable PCI addresses when adding device for windows guest 	jialiu 	None 	Manual 		--default-- 	P3 	4650 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Start a windows guest with one PCI NICs (such as: 8139 model)

2. Note the PCI slot numbers of the NIC in guest(Control Panel ->Device Manager, select the NIC device and check it's properites).

3. Shutdown the guest

4. Add the 2th NIC (such as: e1000 model) and start the guest
	
Expected Results:

1.

2.

3.

4. Check that the slot number of the 1th NIC hasn't changed, and guest is working fine.
Notes:
Comments:

		177113 	[libvirtd] reload configuration testing - 1 	jyang 	jyang 	Manual 		--default-- 	P4 	4660 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:


1. Install a guest with 1 vcpus and 512M memroy, suppose it's named "reload_test".

2. start guest "reload_test"
   # virsh start reload_test

3. check cpu numbers and total memory in guest "reload_test"
   # cat /proc/cpuinfo
   # expr $(cat /proc/meminfo | grep MemTotal | awk -F' ' '{print $2}') / 1024

4. change cpu numbers in /etc/libvirt/qemu/guest.xml into 4
   , and both memory into 1024M. e.g.
       <memory>1048576</memory>
       <vcpu>4</vcpu>


5. reload configurations.
   # kill -SIGHUP `pidof libvirtd`
   OR
   # service libvirtd reload

6. restart guest
   # virsh destroy reload_test
   # virsh start reload_test

7. check cpu number and total memory in guest "ConfReload"
   # cat /proc/cpuinfo
   # expr $(cat /proc/meminfo | grep MemTotal | awk -F' ' '{print $2}') / 1024
  
	
Expected Results:

step 4:

       CPU numbers: 1, but not 4 or others.
       Total Memory: 512M, but not 1024M or others.  

step 7:

        CPU numbers: 4, but not 1 or others.
        Total Memory: 1024M, but not 512M or others.
Notes:
Comments:

		177629 	[Stable guest ABI] Stable PCI addresses when removing device from linux guest 	jialiu 	None 	Manual 		--default-- 	P3 	4660 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Start a linux guest with two PCI NICs (such as: 1th NIC is of 8139 model and 2th NIC is fo e1000 model)

2. Note the PCI slot numbers of the two NIC in guest using lspci command.

3. Shutdown the guest

4. Remove the 1th NIC (such as: 8139 model) and start the guest

	
Expected Results:

1.

2.

3.

4. Check that the slot number of the 2th NIC hasn't changed.
Notes:
Comments:

		177114 	[libvirtd] reload configuration testing - 2 	jyang 	jyang 	Manual 		--default-- 	P4 	4670 	Edit
Setup:

make sure there is a guest of which "autostart" is disabled.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. # ln -s /etc/libvirt/qemu/${guestname}.xml/ /etc/libvirt/qemu/autostart/${guestname}.xml

2. send signal "SIGHUP" to libvirtd so that it reload configuration.

    # kill -SIGHUP `pidof libvirtd`

3. # virsh dominfo ${guestname} | grep Autostart

4. # rm -f /etc/libvirt/qemu/autostart/${guestname}.xml

5.  same as step 2.

    # kill -SIGHUP `pidof libvirtd`

6. # virsh dominfo ${guestname} | grep Autostart
 
	
Expected Results:

step 3:

      output: 

      Autostart: enable

 

step 6:

      output:

      Autostart: disable
Notes:
Comments:

		177630 	[Stable guest ABI] Stable PCI addresses when removing device from windows guest 	jialiu 	None 	Manual 		--default-- 	P3 	4670 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    stable guest ABI

bug:

    No bug found

Actions:

1. Start a widows guest with two PCI NICs (such as: 1th NIC is of 8139 model and 2th NIC is fo virtio model)

2. Note the PCI slot numbers of the two NIC in guest(Control Panel ->Device Manager, select the NIC device and check it's properites).

3. Shutdown the guest

4. Remove the 1th NIC (such as: virtio model) and start the guest
	
Expected Results:

1.

2.

3.

4. Check that the slot number of the 2th NIC hasn't changed
Notes:
Comments:

		177115 	[libvirtd] remote access via plain tcp. 	jyang 	jyang 	Both 		--default-- 	P1 	4680 	Edit
Setup:

Fisrt save "/etc/libvirt/libvirtd.conf" and "/etc/sysconfig/libvirtd" like "cp /etc/libvirt/libvirtd.conf  /etc/sysconfig/libvirtd ~"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirtd
    remote access
    RHEL6.0

bug:

    No bug found

Actions:

1. make sure the value of "auth_tcp" in "libvirtd.conf" is "none"


2. disable "listen_tls" and enable "listen_tcp" in "libvirtd.conf"

3. stop libvirtd
   # service libvirtd stop

4. start libvirtd with "--listen" option
   # libvirtd -d --listen

5. try to connect to libvirtd using "fred"
   # virsh -c qemu+tcp:///system

   and

   # virsh -c qemu+tcp://localhost/system 

6. set the value of "listen_addr" as the ip address of your machine. e.g.

    listen_addr = "10.66.70.128"

7. stop libvirtd

   # service libvirtd stop

8. start libvirtd with option "--listen"

   # libvirtd -d --listen

9. connect to libvirtd

   # virsh -c qemu+tcp://10.66.70.128/system

Note:

At the end, restore  "/etc/libvirt/libvirtd.conf" and "/etc/sysconfig/libvirtd" like "mv ~/libvirtd.conf /etc/libvirt/libvirtd.conf" and "mv ~/libvirtd /etc/sysconfig/libvirtd"

Then restart libvirtd by "service libvirtd restart"
	
Expected Results:

Each command of step 5 can successfully connect to libvirtd. e.g

[root@dhcp-66-70-64 /]# virsh -c qemu+tcp://localhost/system
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #

 

step 9:

   can connect to libvirtd successfully, e.g.[root@dhcp-66-70-128 TLS]# virsh -c qemu+tcp://10.66.70.128/system
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # quit
Notes:
Comments:

		177116 	[libvirtd] remote access via tcp+sasl(MD5) 	gren 	yoyzhang 	Both 		Feature 	P2 	4690 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirtd
    remote access
    RHEL6.0

bug:

    No bug found

Actions:


1. make sure the value of "mech_list" is "digest-md5" in "/etc/sasl2/libvirt.conf".

2. make sure the value of "auth_tcp" in "libvirtd.conf" is "sasl"

3. disable "listen_tls" and enable "listen_tcp" in "libvirtd.conf"

4. Add a user named "fred", and setting his password as "redhat"
   # saslpasswd2 -a libvirt fred
   Password: xxxxxx
   Again (for verification): xxxxxx

5. stop libvirtd
   # service libvirtd stop

6. start libvirtd with "--listen" option
   # libvirtd -d --listen

7. try to connect to libvirtd using "fred"
   # virsh -c qemu+tcp:///system

   and

   # virsh -c qemu+tcp://localhost/system   

   and

   # virsh -c qemu+tcp://{hostname}/system  

8. set the value of "listen_address" as the ip of your machine. e.g.

    listen_address = "10.66.70.128"

9. stop libvirtd

    # service libvirtd stop

10. start libvirtd with option "--listen"

 

11. connect to libvirtd like:

   # virsh -c qemu+tcp://10.66.70.128/system 


12. disable user "fred"
   # saslpasswd2 -a libvirt -d fred

13. try to connect to libvirtd using "fred"

   # virsh -c qemu+tcp://10.66.70.128/system   
	
Expected Results:

step 7:
        each command in this step can connect to libvirtd successfully, e.g:

[root@dhcp-66-70-64 /]# virsh -c qemu+tcp://localhost/system
Please enter your authentication name:fred
Please enter your password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #

 

step11:

        can connect to libvirtd successfully. e.g.

[root@dhcp-66-70-128 TLS]# virsh -c qemu+tcp://10.66.70.128/system

Please enter your authentication name:fred
Please enter your password:
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh #

step 13:

        each command in this step can't connect to libvirtd successfully. e.g:

[root@dhcp-66-70-64 /]# saslpasswd2 -d fred -a libvirt
[root@dhcp-66-70-64 /]# virsh -c qemu+tcp://localhost/system
Please enter your authentication name:fred
Please enter your password:
error: authentication failed
error: failed to connect to the hypervisor
Notes:
Comments:

		177117 	[libvirtd] remote access via TLS 	gren 	jyang 	Both 		Feature 	P1 	4700 	Edit
Setup:

require package "gnutls-utils" installed

Fisrt save "/etc/libvirt/libvirtd.conf" and "/etc/sysconfig/libvirtd" like "cp /etc/libvirt/libvirtd.conf  /etc/sysconfig/libvirtd ~"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirtd
    remote access
    RHEL6.0

bug:

    No bug found

Actions:

1. Create a private key for CA
   # certtool --generate-privkey > cakey.pem

2. self-sign it by creating a file with the signature details called ca.info containing:
   cn=Redhat QA,CN=10.66.70.128
   ca
   cert_signing_key

   # certtool --generate-self-signed --load-privkey cakey.pem \
     --template ca.info --outfile cacert.pem

   then delete ca.info
   # rm ca.info -f

3. copy certificate to right position
   # cp cacert.pem cakey.pem /etc/pki/CA

4. issuing server certificates
   1> Make a private key for the server:
   # certtool --generate-privkey > serverkey.pem

   2> and sign that key with the CA's private key by first creating a template file called server.info:
   organization = redhat-china
   cn = 10.66.70.128
   tls_www_server
   encryption_key
   signing_key

   3> sign
   # certtool --generate-certificate --load-privkey serverkey.pem \
     --load-ca-certificate cacert.pem --load-ca-privkey cakey.pem \
     --template server.info --outfile servercert.pem

5. copy server certificates to right position
   # mkdir -p /etc/pki/libvirt/private
   # cp serverkey.pem /etc/pki/libvirt/private
   # cp servercert.pem /etc/pki/libvirt

6. issuing client certificates
   1>. Make a private key
   # certtool --generate-privkey > clientkey.pem

   2> Create client.info and sign the certificate
   client.info:
    country = China
    state = Beijign
    locality = Beijing

    organization = Red Hat

    cn = client1
    tls_www_client
    encryption_key
    signing_key

   3> sign
   # certtool --generate-certificate --load-privkey clientkey.pem \
     --load-ca-certificate cacert.pem --load-ca-privkey cakey.pem \
     --template client.info --outfile clientcert.pem

7. copy client certificates to right position
   # cp clientkey.pem /etc/pki/libvirt/private
   # cp clientcert.pem /etc/pki/libvirt

8. enable "listen_tls" in "libvirtd.conf"

9. stop libvirtd
   # service libvirtd stop

10. start libvirtd with option "--listen"
   # libvirtd -d --listen

11. connect to libvirt with:
   # virsh -c qemu+tls://10.66.70.128/system

 

12. set the value of "listen_address" as the IP address of you machine. e.g.

      listen_address = "10.66.70.128"

 

13. stop libvirtd

     # service libvirtd stop

 

14. start libvirtd with option "--listen"

    # libvirtd -d --listen

 

15. connect to libvirtd like:

   # virsh -c qemu+tls://10.66.70.128/system

Note:

At the end, restore  "/etc/libvirt/libvirtd.conf" and "/etc/sysconfig/libvirtd" like "mv ~/libvirtd.conf /etc/libvirt/libvirtd.conf" and "mv ~/libvirtd /etc/sysconfig/libvirtd"

Then restart libvirtd by "service libvirtd restart"
	
Expected Results:

step 11:

    each command can connect to libvirtd successfully, e.g:

[root@dhcp-66-70-128 TLS]# virsh -c qemu+tls://10.66.70.128/system
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id Name                 State
----------------------------------
  - bar                  shut off
  - foo                  shut off
  - rhel5u3              shut off
  - virtio               shut off

virsh #

 

step15:

      can connect to libvirtd successfully. e.g.

[root@dhcp-66-70-128 TLS]# virsh -c qemu+tls://10.66.70.128/system
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # quit
Notes:
Comments:

		177125 	[libvirtd] Using a self recursive snapshot backing store *takes out* libvirtd--Bug 601067 	kxiong 	None 	Manual 		--default-- 	P2 	4710 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1.create qcow2 snapshot volume 
#qemu-img create -f qcow2 -o
backing_file=/home/images/sdbimagesnap2.qcow2,backing_fmt=host_device
/home/images/sdbimagesnap2.qcow2 20G

Formatting '/home/images/sdbimagesnap2.qcow2', fmt=qcow2 size=21474836480
backing_file='/home/images/sdbimagesnap2.qcow2' backing_fmt='host_device'
encryption=off cluster_size=0 

NOTE:(Bug 601067) qemu is changed,unsupprot to create an image with the same filename as the backing file.

	
Expected Results:

output:

Error: Trying to create an image with the same filename as the backing file

Notes:
Comments:

		177670 	[Storage] Using QCow2 disk encryption 	nzhang 	None 	Manual 		Feature 	P1 	4710 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    storage

bug:

    No bug found

Actions:

1. Define a secret with an XML document.
    # cat demo-secret.xml
    <secret ephemeral='no' private='no'>
      <uuid>0a81f5b2-8403-7b23-c8d6-21ccc2f80d6f</uuid>
      <usage type='volume'>
        <volume>/var/lib/libvirt/images/demo.qcow2</volume>
      </usage>
    </secret>

Note: " <volume>/var/lib/libvirt/images/demo.qcow2</volume>" can be changed to other dir for qemu can access, for example "/mnt/dir/demo.qcow2", and " <source file='/var/lib/libvirt/images/demo.qcow2'/>" in step5 must be changed too.

    # virsh secret-define demo-secret.xml
    Secret 0a81f5b2-8403-7b23-c8d6-21ccc2f80d6f created

2. Set the value of the secret.
    # MYSECRET=`echo "open seseme" | base64`
    # virsh secret-set-value 0a81f5b2-8403-7b23-c8d6-21ccc2f80d6f $MYSECRET
    Secret value set

3. Define and active a pool with the following XML.
    # cat demo-pool.xml
    <pool type='dir'>
      <name>VirtualMachines</name>
      <source>
      </source>
      <target>
        <path>/var/lib/libvirt/images</path>
         <permissions>
         <mode>0701</mode>
         <owner>-1</owner>
         <group>-1</group>
        </permissions>
      </target>
    </pool>
    # virsh pool-define demo-pool.xml
    Pool VirtualMachines defined from demo-pool.xml
    # virsh pool-start VirtualMachines
    Pool VirtualMachines started

Note: " <path>/var/lib/libvirt/images</path>" can be changed to other dir for qemu can access, because it is conflicted with default pool, for example you can change to"<path>/mnt/dir/</path>".

4. To create an encrypted volume within this pool with the follwing XML.

# cat demo-disk.xml
      <volume>
        <name>demo.qcow2</name>
        <capacity>5368709120</capacity>
        <target>
          <format type='qcow2'/>
          <encryption format='qcow'>
            <secret type='passphrase' uuid='0a81f5b2-8403-7b23-c8d6-21ccc2f80d6f'/>
          </encryption>
        </target>
      </volume>
    # virsh vol-create VirtualMachines demo-disk.xml
    Vol demo.qcow2 created from demo-disk.xml

5. With the following XML config which associates the qcow decryption âpassphraseâ with the secret we defined to define a new guest, and then start it.

# wget http://download.englab.nay.redhat.com/pub/rhel/released/RHEL-6/6.2/Server/x86_64/os/images/pxeboot/initrd.img

# wget http://download.englab.nay.redhat.com/pub/rhel/released/RHEL-6/6.2/Server/x86_64/os/images/pxeboot/vmlinuz

# cp initrd.img vmlinuz /var/lib/libvirt/boot/

# cat demo-guest.xml
    <domain type='kvm'>
      <name>demo</name>
      <memory>524288</memory>
      <vcpu>1</vcpu>
      <os>
        <type arch='x86_64' machine='pc'>hvm</type>
        <kernel>/var/lib/libvirt/boot/vmlinuz</kernel>
        <initrd>/var/lib/libvirt/boot/initrd.img</initrd>
        <boot dev='hd'/>
      </os>
      <devices>
        <emulator>/usr/libexec/qemu-kvm</emulator>
        <disk type='file' device='disk'>
          <driver name='qemu' type='qcow2'/>
          <source file='/var/lib/libvirt/images/demo.qcow2'/>
          <target dev='hda' bus='ide'/>
          <encryption format='qcow'>
            <secret type='passphrase' uuid='0a81f5b2-8403-7b23-c8d6-21ccc2f80d6f'/>
          </encryption>
        </disk>
        <interface type='network'>
          <mac address='52:54:00:53:a6:f0'/>
          <source network='default'/>
        </interface>
        <serial type='pty'>
          <target port='0'/>
        </serial>
        <console type='pty'>
          <target port='0'/>
        </console>
        <input type='tablet' bus='usb'/>
        <input type='mouse' bus='ps2'/>
        <graphics type='vnc' port='-1' autoport='yes'/>
      </devices>
    </domain>
    # virsh define demo-guest.xml
    Domain demo defined from demo-guest.xml
    # virsh start demo
    Domain demo started
    # virt-viewer demo

6. mount the img as the second disk to another guest.

or define a new guest using the img without encryption element in xml
	
Expected Results:

Everything written to the guest disk will now be encrypted using the secrets defined.


6. log in another guest, cannot read data from the second disk

or cannot start succefully, show the following error:

# virsh start demo-guest
error: Failed to start domain demo-guest
error: internal error unable to execute QEMU command 'cont': Device 'drive-ide0-0-0' is encrypted
Notes:
Comments:

		177129 	[libvirtd] When restarting libvirtd just after creating a domain the new process is not properly tracked - BZ#707894 	ydu 	None 	Manual 		Feature 	P2 	4720 	Edit
Setup:
	
Breakdown:

For now, this bug is hard to reproduce.
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177693 	[sVirt] CDROMs with <readonly/> for multiple guest access 	jialiu 	None 	Auto 		Feature 	P2 	4720 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Confirm the security model in use and SELinux is enabled.
         # getenforce

2. Create at least two vm's (virtual machines) to test with, they are named 
by "a" and "b"

3. Create a test iso image file that we will attach to the vm's:

       # mkdir /tmp/test_iso
       - now copy a mixture of text and binary files to /tmp/test_iso
       # mkisofs -R -o /tmp/shareable.iso /tmp/test_iso

       - copy the iso image to /var/lib/libvirt/images in my case the default
         storage pool and refresh it so the cdrom volume is available

       # cp /tmp/shareable.iso /var/lib/libvirt/images
       # virsh pool-list
       # virsh pool-refresh --pool default
       # virsh vol-list --pool default


4. Shut down vm "a" , add the following to domain config xml to attach the created iso
 <disk type='file' device='cdrom'>
 <source file='/var/lib/libvirt/images/shareable.iso'/>
 <target dev='hdc' bus='ide'/>
 <readonly/>
 </disk>


5. repeat step 4 to attach the readonly CDROM to vm "b"

6.  Start vm "a" and make sure it boots, login as root and create 
a mount point directory and mount the CDROM is readonly.
 # mkdir /mnt/test_iso
 # mount -o ro /dev/cdrom /mnt/test_iso

 - cd to /mnt/test_iso and confirm the files exist and are usable


7. Repeat step 6 for vm "b" to confirm the device is shareable

Shut down the test vm's and attach the created iso

 

	
Expected Results:

1.

# getenforce
Enforcing

2.

3.

# ll -Z /var/lib/libvirt/images/shareable.iso
-rw-r--r--. root root unconfined_u:object_r:virt_image_t:s0 share.iso

4. Connect ISO to vm "a" successfully.

5. Connect ISO to vm "b" successfully.

6. vm "a" is started successfully, and files in iso are usable, and the iso file is labeled with "system_u:object_r:virt_content_t:s0"

# ll -Z /var/lib/libvirt/images/shareable.iso
-rw-r--r--. qemu qemu system_u:object_r:virt_content_t:s0 share.iso

7. vm "b" is started successfully, and file in iso are usable.
Notes:
Comments:

		177133 	[libvirtd]libvirtd crashes after running virsh cpu-compare with unexpected input--Bug 630618 	kxiong 	None 	Manual 		--default-- 	P2 	4730 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. # cat cpu.xml
<cpu>
<topology sockets='1' cores='1' threads='1'/>
</cpu>
2. # virsh cpu-compare cpu.xml

3. #service libvirtd status

	
Expected Results:

2.It will show error message:

error: Failed to compare host CPU with cpu.xml

error: Requested operation is not valid: no CPU model specified

3.

ï»¿libvirtd (pid  32763) is running...
Notes:
Comments:

		177696 	[sVirt] guest create/destroy with virt-install and sVirt 	jialiu 	None 	Auto 		Feature 	P1 	4730 	Edit
Setup:

Make sure the security model in use and SELinux is enabled on your host.

# getenforce
Enforcing
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Create a virtual machine named vm1 using virt-install tool:

 # virt-install --accelerate --name vm1 -r 1024 \
--location  http://download.englab.nay.redhat.com/pub/rhel/released/RHEL-5-Server/U5/x86_64/os/  --debug  \
-f /var/lib/libvirt/images/vm1.img -s 5
or
Download an image file to  /var/lib/libvirt/images/ (such as: http://fileshare.englab.nay.redhat.com/pub/xenimages/jialiu/vm1.img)
# virt-install -n vm1 -r 1024 -f /var/lib/libvirt/images/vm1.img --import


2. Check the process labeling during the vm creation

     # ps -efZ | grep kvm

3. Check the image labeling during the vm creation 'svirt_image_t'

     For the file device assignment you should check labeling on

     # ls -lZ /var/lib/libvirt/images/$IMAGE

4. Confirm the installed vm has the correct security labeling and can be
      started and stopped.

     # virsh list --all
     # virsh dominfo vm1
     # virsh dumpxml vm1
     # virsh start vm1
     # virsh shutdown vm1
     # virsh list --all


5. Confirm the virtual machine can be deleted and the image container removed using the virt-manager UI.

     # virsh list --all
     # virsh destoy vm1
     # virsh undefine vm1
     # virsh list --all

     # ls -lZ /var/lib/libvirt/images/vm1.img
     # rm -f /var/lib/libvirt/images/vm1.img
     # ls -lZ /var/lib/libvirt/images/vm1.img

	
Expected Results:

1. All operations should complete successfully

2. Make sure the process is marked with svirt_t label, and owner is qemu

# ps -efZ|grep kvm
system_u:system_r:svirt_t:s0:c126,c508 qemu 25238  1 13 22:55 ?        00:00:05 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name vm1 -uuid f216ff41-1e2b-d434-b3ef-773cb947ecc0 -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/vm1.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -no-reboot -boot c -kernel /var/lib/libvirt/boot/virtinst-vmlinuz.mFoFIk -initrd /var/lib/libvirt/boot/virtinst-initrd.img.BJ_Y8z -append method=http://download.englab.nay.redhat.com/pub/rhel/released/RHEL-5-Server/U5/x86_64/os/ -drive file=/var/lib/libvirt/images/vm1.img,if=none,id=drive-ide0-0-0,boot=on,format=raw -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=21,id=hostnet0 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:23:20:87,bus=pci.0,addr=0x4 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3

3. Make sure the image file is marked with svirt_image_t label, and owner is qemu, the security context range is the same with qemu-kvm process, such as: c126,c508

# ll -Z /var/lib/libvirt/images/vm1.img
-rw-------. qemu qemu system_u:object_r:svirt_image_t:s0:c126,c508 /var/lib/libvirt/images/vm1.img


4.  libvirt should be automatically chown'ing files to qemu:qemu as vm is running, and that they go back to root:root upon guest shutdown.(This applies for all the same resources, disk, USB devices, PCI devices etc. as labeling for SELinux.)

When vm is running:
# ll -Z /var/lib/libvirt/images/vm1.img
-rw-------. qemu qemu system_u:object_r:svirt_image_t:s0:c126,c508 /var/lib/libvirt/images/vm1.img

When vm is stoped:

# ll -Z /var/lib/libvirt/images/vm1.img
-rw-------. root root system_u:object_r:virt_image_t:s0 /var/lib/libvirt/images/vm1.img

# virsh list --all
 Id Name                 State
----------------------------------
 30 vm1                  running

# virsh dominfo vm1
Id:             30
Name:           vm1
UUID:           f216ff41-1e2b-d434-b3ef-773cb947ecc0
OS Type:        hvm
State:          running
CPU(s):         1
CPU time:       86.8s
Max memory:     1048576 kB
Used memory:    1048576 kB
Autostart:      disable
Security model: selinux
Security DOI:   0
Security label: system_u:system_r:svirt_t:s0:c126,c508 (enforcing)

# virsh dumpxml vm1
...
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c126,c508</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c126,c508</imagelabel>
  </seclabel>
...


5. vm and image are deleted successfully, after vm is stopped, the owner of image file is changed back to "root:root", and the lable is "virt_image_t" without any security context range

# ll -Z /var/lib/libvirt/images/vm1.img
-rw-------. root root system_u:object_r:virt_image_t:s0 /var/lib/libvirt/images/vm1.img

Notes:
Comments:

		177091 	[libvirtd] Kill qemu process using 'kill' behaves as user shutdown command and not as 'lost connection with qemu process'--Bug 656845 	kxiong 	None 	Manual 		--default-- 	P2 	4740 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1.Define and start a domain.
# virsh start http_test
Domain http_test started

2.Edit "libvirtd.conf" file 
# vi /etc/libvirt/libvirtd.conf
log_level = 1
log_outputs="1:file:/var/lib/libvirt/images/libvirtd.log" 

3.# service libvirtd stop

4.# libvirtd

5.Open the second terminal.
# ps -ef|grep qemu
qemu      2038     1  1 04:31 ?        00:00:56 /usr/libexec/qemu-kvm -S -M
rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name http_test
-uuid 3d3a297b-8039-7143-033a-7ca7d9feb676 -nodefconfig -nodefaults -chardev
socket,id=monitor,path=/var/lib/libvirt/qemu/http_test.monitor,server,nowait
-mon chardev=monitor,mode=control -rtc base=utc -boot c -drive
file=/var/lib/libvirt/images/http_test.img,if=none,id=drive-ide0-0-0,format=raw,cache=none
-device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -drive
file=/var/lib/libvirt/images/test.sio,if=none,media=cdrom,id=drive-ide0-1-0,readonly=on,format=raw
-device ide-drive,bus=ide.1,unit=0,drive=drive-ide0-1-0,id=ide0-1-0 -netdev
tap,fd=26,id=hostnet0 -device
rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:7e:b1:46,bus=pci.0,addr=0x3
-chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc
127.0.0.1:0 -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x4 -device
virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5
root      6793  6774  0 05:36 pts/3    00:00:00 grep qemu

6.# kill -9 2038

	
Expected Results:

Get log from "/var/lib/libvirt/images/libvirtd.log" file
..............................
05:37:36.310: 6718: debug : qemuMonitorIO:580 : Triggering EOF callback error?
0
05:37:36.310: 6718: debug : qemuHandleMonitorEOF:1306 : Received EOF on
0x1bcaec0 'http_test'
05:37:36.310: 6718: debug : qemuHandleMonitorEOF:1313 : Monitor connection to
'http_test' closed without SHUTDOWN event; assuming the domain crashed
05:37:36.310: 6718: debug : qemudShutdownVMDaemon:4748 : Shutting down VM
'http_test' pid=2038 migrated=0
05:37:36.311: 6718: debug : qemuMonitorClose:694 : mon=0x1d43000
05:37:36.311: 6718: debug : virEventRemoveHandleImpl:163 : Remove handle w=5
05:37:36.311: 6718: debug : virEventRemoveHandleImpl:176 : mark delete 4 21
05:37:36.311: 6718: debug : virEventInterruptLocked:655 : Skip interrupt, 1
356493072
05:37:36.311: 6718: debug : qemuSecurityDACRestoreSecurityAllLabel:426 :
Restoring security label on http_test migrated=0
05:37:36.311: 6718: info : qemuSecurityDACRestoreSecurityFileLabel:80 :
Restoring DAC user and group on '/var/lib/libvirt/images/http_test.img'
05:37:36.311: 6718: info : qemuSecurityDACSetOwnership:40 : Setting DAC user
and group on '/var/lib/libvirt/images/http_test.img' to '0:0'
05:37:36.311: 6718: debug : SELinuxRestoreSecurityAllLabel:746 : Restoring
security label on http_test
05:37:36.311: 6718: info : SELinuxRestoreSecurityFileLabel:369 : Restoring
SELinux context on '/var/lib/libvirt/images/http_test.img'
05:37:36.385: 6718: info : SELinuxSetFilecon:323 : Setting SELinux context on
'/var/lib/libvirt/images/http_test.img' to 'system_u:object_r:virt_image_t:s0'
05:37:36.385: 6718: debug : virCgroupNew:555 : New group
/libvirt/qemu/http_test
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping 0:cpu
at /cgroup/cpu in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
1:cpuacct at /cgroup/cpuacct in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
2:cpuset at /cgroup/cpuset in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
3:memory at /cgroup/memory in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
4:devices at /cgroup/devices in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
5:freezer at /cgroup/freezer in
05:37:36.388: 6718: debug : virCgroupMakeGroup:497 : Make group
/libvirt/qemu/http_test
............................

Notes:
Comments:

		177697 	[sVirt] guest create/destroy with virt-manager and sVirt 	jialiu 	None 	Manual 		Feature 	P1 	4740 	Edit
Setup:

Make sure the security model in use and SELinux is enabled on your host.


# getenforce
Enforcing

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0

bug:

    No bug found

Actions:

1. Create a virtual machine named vm1 using virt-manager UI, or import an existent image.

2. Check the process labeling during the vm creation

     # ps -efZ | grep qemu-kvm

3. Check the image labeling during the vm creation 'svirt_image_t'

     For the file device assignment you should check labeling on

     # ls -lZ /var/lib/libvirt/images/$IMAGE

4. Confirm the installed vm has the correct security labeling and can be 
      started and stopped.

5. Confirm the virtual machine can be deleted and the image container
      removed using the virt-manager UI.

	
Expected Results:

1. All operations should complete successfully

2. Make sure the process is marked with svirt_t label, and owner is qemu

# ps -efZ |grep qemu-kvm
system_u:system_r:svirt_t:s0:c612,c701 qemu 3698   1 50 01:03 ?        00:00:55 /usr/libexec/qemu-kvm -S -M pc-0.12 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name xx -uuid fadb4f4d-0a98-b6e9-b891-21eee693f810 -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/xx.monitor,server,nowait -mon chardev=monitor,mode=control -boot c -drive file=/var/lib/libvirt/images/rhel5.4_i386.img,if=none,id=drive-ide0-0-0,boot=on -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -device rtl8139,vlan=0,id=net0,mac=52:54:00:43:e6:a2,bus=pci.0,addr=0x4 -net tap,fd=19,vlan=0,name=hostnet0 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3

3. Make sure the image file is marked with svirt_image_t label, and owner is qemu, the security context range is the same with qemu-kvm process, such as: c612,c701

# ls -lZ /var/lib/libvirt/images/rhel5.4_i386.img
-rwxr-xr-x. qemu qemu system_u:object_r:svirt_image_t:s0:c612,c701 /var/lib/libvirt/images/rhel5.4_i386.img

4.  libvirt should be automatically chown'ing files to qemu:qemu as vm is running, and that they go back to root:root upon guest shutdown.(This applies for all the same resources, disk, USB devices, PCI devices etc. as labeling for SELinux.)

When vm is running:

# ls -lZ /var/lib/libvirt/images/rhel5.4_i386.img
-rwxr-xr-x. qemu qemu system_u:object_r:svirt_image_t:s0:c612,c701 /var/lib/libvirt/images/rhel5.4_i386.img

When vm is stoped:

# ls -lZ /var/lib/libvirt/images/rhel5.4_i386.img
-rwxr-xr-x. root root system_u:object_r:virt_image_t:s0 /var/lib/libvirt/images/rhel5.4_i386.img

5. vm and image are deleted successfully, after vm is stopped.
Notes:
Comments:

		176751 	[log and debugging] libvirt Sanitized Host Logs for Guests - bug 822592 	gsun 	gsun 	Manual 		Feature 	P1 	4750 	Edit
Setup:

prepare a domain
# virsh list
 Id    Name                           State
----------------------------------------------------
 5     dom                            running

 

make sure the auditd service is in running status.

#service auditd status

If the status is not running, please start the service with the following command:

#service auditd start

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    log and debugging

bug:

    No bug found

Actions:

1.
# virsh destroy dom
Domain dom destroyed

2.
# ausearch -m virt_control


3. 
# virsh start dom

4.
# ausearch -m virt_resource |grep old-net -2

5.
# ausearch -m virt_resource|grep reason=open -2

 
	
Expected Results:

2.
# ausearch -m virt_control
----
time->Tue Jun  5 11:47:25 2012
type=VIRT_CONTROL msg=audit(1338868045.221:81037): user pid=3330 uid=0 auid=0 ses=32 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='virt=kvm op=stop reason=destroyed vm="dom" uuid=45f047f5-dc7a-e382-8391-9ccbf516be06 vm-pid=-1 exe="/usr/sbin/libvirtd" hostname=? addr=? terminal=? res=success'


4.
# ausearch -m virt_resource |grep old-net -2
----
time->Tue Jun  5 11:50:47 2012
type=VIRT_RESOURCE msg=audit(1338868247.890:81061): user pid=3330 uid=0 auid=0 ses=32 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='virt=kvm resrc=net reason=start vm="dom" uuid=45f047f5-dc7a-e382-8391-9ccbf516be06 old-net=? new-net=54:52:00:69:9B:AF exe="/usr/sbin/libvirtd" hostname=? addr=? terminal=? res=success'

5.
# ausearch -m virt_resource|grep reason=open -2
----
time->Tue Jun  5 11:50:47 2012
type=VIRT_RESOURCE msg=audit(1338868247.571:81059): user pid=3330 uid=0 auid=0 ses=32 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='virt=kvm resrc=net reason=open vm="dom" uuid=45f047f5-dc7a-e382-8391-9ccbf516be06 net=54:52:00:69:9B:AF path="/dev/vhost-net" rdev=0A:39 exe="/usr/sbin/libvirtd" hostname=? addr=? terminal=? res=success'

The single quotes on old-net, new-net, net in audit msg is removed, this is expected.

 
Notes:
Comments:

		177099 	[libvirtd] libvirtd fails to start due to mDNS requirement- bug 746111 	ydu 	None 	Manual 		Function 	P2 	4750 	Edit
Setup:
	
Breakdown:

But still assiged, will move to PROPOSED after bug verified.
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    libvirtd

bug:

    No bug found

Actions:

1.Download the libvirt rpm package which is going to be tested.

2.Check the dependency of this package, like:

#rpm -qpR libvirt-0.9.10-16.el6.x86_64.rpm |grep avahi-libs
avahi-libs

	
Expected Results:

2. Make sure avahi-libs is required of this libvirt.
Notes:
Comments:

		177699 	[sVirt] Guest with svirt disabled 	jialiu 	None 	Auto 		Feature 	P1 	4750 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Make sure your selinux is enabled on OS

2. Disable sVirt via /etc/libvirt/qemu.conf with setting security_driver="none"

3. Re-start libvirt service.

3. Start a vm.

4. Check selinux label of vm image and qemu-kvm process.

 

!! For executing the other svirt related case successfully, Enable sVirt after finished this case.
	
Expected Results:

1.

# getenforce
Enforcing

2. # service libvirtd restart

3. vm is started successfully.

4. Guests should be started without a svirt label.

# ps -efZ|grep qemu-kvm
unconfined_u:system_r:qemu_t:s0-s0:c0.c1023 qemu 13189 1  6 03:29 ?    00:00:43
/usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 2000 -smp
1,sockets=1,cores=1,threads=1 -name winxp -uuid
bd198956-a941-0bac-b2fa-3f998c864045 -nodefaults -chardev
socket,id=monitor,path=/var/lib/libvirt/qemu/winxp.monitor,server,nowait -mon
chardev=monitor,mode=control -rtc base=utc -boot c -drive
file=/var/lib/libvirt/images/winXP-32-virtio.raw,if=none,id=drive-ide0-0-0,boot=on,format=raw
-device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev
tap,fd=19,id=hostnet0 -device
rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:98:23:0e,bus=pci.0,addr=0x4
-chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc
127.0.0.1:0 -k en-us -vga cirrus -device
virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3

(NOTE: the label of qemu-kvm is qemu_t, but not  svirt_t)

# ll -Z /var/lib/libvirt/images/winXP-32-virtio.raw 
-rw-rw-r--. qemu qemu system_u:object_r:virt_image_t:s0
/var/lib/libvirt/images/winXP-32-virtio.raw

   (NOTE: the label is virt_image_t, but not svirt_image_t)
Notes:
Comments:

		177100 	[libvirtd] local access via unix+policykit 	gren 	yoyzhang 	Manual 		Feature 	P2 	4760 	Edit
Setup:

require package "PolicyKit" installed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    RHEL6.0

bug:

    No bug found

Actions:


1. ensure the libvirt is support PolicyKit. if not, skip this case.

2. set the value of "auth_unix_ro" and "auth_unix_rw" as "polkit"

3. create two user, one is "foo", the other is "bar"
   # useradd foo
   # user add bar

4. change the password of "foo" and "bar" to "redhat"
   # passwd foo
   # passwd bar

5. add the following text to "/etc/PolicyKit/PolicyKit.conf"
  <match action="org.libvirt.unix.manage">
    <match user="foo">
      <return result="yes"/>
    </match>
  </match>
  <match action="org.libvirt.unix.manage">
    <match user="bar">
      <return result="auth_admin"/>
    </match>
  </match>

6. restart libvirtd
   # service libvirtd restart
          
7. try to connect to "libvirtd" using both user "foo" and "bar"
   # su - foo
   $ virsh -c qemu+unix:///system
        
   # su - bar
   $ virsh -c qemu+unix:///system
     

	
Expected Results:

After step7:

user "foo" has both read and write permission.
 user "bar" has only read permission.
Notes:
Comments:

		177700 	[sVirt] guest with svirt on nfs 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P2 	4760 	Edit
Setup:

Background:

SELinux policy would not prevent sVirt from working in an NFS or samba environment.

You do not get the same isolation that you would get from svirt_t.  
If two virtual machines are running with images on nfs, 
SELinux would not prevent them from attacking each other.

But you can run this from an SELinux point of view by the following two method:

1. Setting the virt_use_nfs boolean.

  # setsebool -P virt_use_nfs 1

Doing this allows all virtual machine to read/write ANY nfs shares that do not specify a mount context.

2. Another option would be to set the mount point context on the NFS to "system_u:object_r:svirt_image_t:s0"

All virtual machines could then read/write only the share mounted with this context, no need to set the boolean.

  # mount -o context="system_u:object_r:svirt_image_t:s0" 
REMOTEMACHINE:/var/lib/libvirt/images /var/lib/libvirt/images

For more complex environments you could use static labeling, but you would
need the images in separate directories.

  # mount -o context="system_u:object_r:svirt_image_t:TopSecret" 
REMOTEMACHINE:/var/lib/libvirt/images/TopSecret /var/lib/libvirt/images/TopSecret

  # mount -o context="system_u:object_r:svirt_image_t:Secret" 
REMOTEMACHINE:/var/lib/libvirt/images/Secret /var/lib/libvirt/images/Secret

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0

bug:

    No bug found

Actions:

1. Mount a nfs directory including vm image to local mount point.

# mount 10.66.90.113:/vol/xenimage /mnt

2. Install a vm using the nfs vm image

3. Umount the nfs directory, re-mount again with context label:

# mount -o context="system_u:object_r:svirt_image_t:s0" 10.66.90.113:/vol/xenimage -o vers=3 /mnt

or setting the virt_use_nfs boolean.

# setsebool -P virt_use_nfs 1

4. Re-install the vm with the nfs image, and start it.
	
Expected Results:

1.

# ll -Z /
...
drwxr-xr-x. root root system_u:object_r:nfs_t:s0       mnt
...

2. VM failed to start with the following error:

qemu: could not open disk image /mnt/jialiu/rhel5u4_kvm.img: Permission denied

3. check the label of the mount point:

# ll -Z /
...
drwxr-xr-x. root root system_u:object_r:svirt_image_t:s0 mnt
...

4. The vm is working fine.
Notes:
Comments:

		177102 	[libvirtd] multi-threads testing - 1 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4770 	Edit
Setup:

1. Save the following C codes as "multi-threads.c"

 

/*
 * test max clients of libvirtd
 * Osier Yang <jyang@redhat.com>, Mar 4, 2010
 */

#include <stdio.h>
#include <stdlib.h>
#include <libvirt/libvirt.h>
#include <errno.h>
#include <sys/types.h>

int main (int argc, char **argv) {
        if (argc != 2) {
                printf("Usage: %s <max clients>\n", argv[0]);
                exit(EXIT_FAILURE);
        }

        virConnectPtr conn;
        char *uri = "qemu:///system";
        int max_clients = atoi(argv[1]);
        int i;
        pid_t pid;

        for (i = 0; i < max_clients; i++) {
                conn = virConnectOpen(uri);

                if (conn == NULL) {
                        fprintf(stderr, "i = %d\n", i + 1);
                        exit(EXIT_FAILURE);
                }

                //printf("conn%d: %p\n", i + 1, conn);
        }

        if ((pid = fork()) == -1) {
                fprintf(stderr, "Error: %s\n", strerror(errno));
        } else if(pid == 0){
                system("ps -Lf -C 'libvirtd'");
        } else {
                sleep(30);
        }

        exit(EXIT_SUCCESS);
}

2. Complie it as:
# gcc -o mthread `pkg-config --cflags --libs libvirt` mthread.c

3. make it excutable

# chmod +x ./mthread
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:


1. get max open files limit of system.
   # ulimit -n

   suppose the return value is MAX_FOPEN

2. get the amounts of files that libvirtd opened
   # lsof -p `pidof libvirtd` | wc -l

   suppose the return value is LIBVIRTD_FOPEN

3. set "max_clients" and "max_workers" as a value, which is close to but not more
than "MAX_FOPEN - LIBVIRTD_FOPEN",  suppose it is "VAL"

4. restart libvirtd
   # service libvirtd restart

5. excute mthread in current directory with "VAL" as a argument.
   # ./mthread VAL

6. open another terminal and execute following command when step 5 is not finished.
  # lsof -p `pidof libvirtd`
	
Expected Results:

step 5:

   1. process mthread is exit after not more than 2 mins.
   2. the output shows the information of "VAR + 1" threads.

 

step 6"

   1. there is no many strings like "can't indentify protocol" in the output
    
Notes:
Comments:

		177702 	[sVirt] Hot unplug normal PCI device with svirt 	jialiu 	None 	Auto 		--default-- 	P1 	4770 	Edit
Setup:

- Host supports VT-d

- Turn on iommu on kernel parameter, such as: intel_iommu=on

- Could lose connection with host after dettach network device from host

- Have finished test case '[Svirt] hotplug pci device to guest with svirt'
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0

bug:

    No bug found

Actions:

1. Make sure selinxu is "Enforcing"

2. # virsh detach-device <guest> nic.xml

3. On guest, run # lspci

4. Need check the context of pci device

# ll -Z /sys/bus/pci/devices/0000:00:19.0/resource

	
Expected Results:

1. 

# getenforce
Enforcing

2. The device could be hot-unplug from the guest

3. Could not see the unpluged device on guest

4. # ll -Z /sys/bus/pci/devices/0000:00:19.0/resource

-r--r--r--. root root system_u:object_r:sysfs_t:s0     /sys/bus/pci/devices/0000:00:19.0/resource

Notes:
Comments:

		177103 	[libvirtd] multi-threads testing - 2 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4780 	Edit
Setup:

1. Save the following C codes as "multi-threads.c"


/*
 * test max clients of libvirtd
 * Osier Yang <jyang@redhat.com>, Mar 4, 2010
 */

#include <stdio.h>
#include <stdlib.h>
#include <libvirt/libvirt.h>
#include <errno.h>
#include <sys/types.h>

int main (int argc, char **argv) {
        if (argc != 2) {
                printf("Usage: %s <max clients>\n", argv[0]);
                exit(EXIT_FAILURE);
        }

        virConnectPtr conn;
        char *uri = "qemu:///system";
        int max_clients = atoi(argv[1]);
        int i;
        pid_t pid;

        for (i = 0; i < max_clients; i++) {
                conn = virConnectOpen(uri);

                if (conn == NULL) {
                        fprintf(stderr, "i = %d\n", i + 1);
                        exit(EXIT_FAILURE);
                }

                //printf("conn%d: %p\n", i + 1, conn);
        }

        if ((pid = fork()) == -1) {
                fprintf(stderr, "Error: %s\n", strerror(errno));
        } else if(pid == 0){
                system("ps -Lf -C 'libvirtd'");
        } else {
                sleep(30);
        }

        exit(EXIT_SUCCESS);
}

2. Complie it as:
# gcc -o mthread `pkg-config --cflags --libs libvirt` multi-threads.c

3. make it excutable

# chmod +x ./mthread
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. set "max_clients" in "/etc/libvirt/libvirtd.conf" as 50.

2. set "max_workers" in "/etc/libvirt/libvirtd.conf" as 45.

3. restart libvirtd.
   # service libvirtd restart

4. excute mthread with 50 as a argument.
   # ./mthread 50
	
Expected Results:

the output shows information of 46 threads.
Notes:
Comments:

		177703 	[sVirt] hotplug pci device to guest with svirt 	jialiu 	None 	Auto 		--default-- 	P1 	4780 	Edit
Setup:

- Host supports VT-d

- Turn on iommu on kernel parameter, such as: intel_iommu=on

- Could lose connection with host after dettach network device from host

- guest's img should be in local dir, not remote dir.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0

bug:

    No bug found

Actions:

1. Check the NIC node device.
# virsh nodedev-list --tree|more

2. Make sure selinux is enabled.
# getenforce 
Enforcing

3. Start a VM.

4.  Prepare a xml file named nic.xml including the following context:

<hostdev mode='subsystem' type='pci' managed='yes'>
 <source>
  <address bus='0' slot='0x19' function='0'/>
 </source>
</hostdev>



5. Issue the command:

if the guest is rhel5, execute the following command in guest before attach-device:
# modprobe acpiphp

# virsh attach-device <guest-name> nic.xml



6. Check the context of pci device is the same as the context of qemu-kvm process
# ll -Z /sys/bus/pci/devices/0000:00:19.0/resource

	
Expected Results:

1. Check the NIC node device.

# virsh nodedev-list --tree|more
computer
 |
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_01_00_0
  |     
  +- pci_0000_00_03_0
  +- pci_0000_00_03_2
  +- pci_0000_00_03_3
  +- pci_0000_00_19_0
  |   |
  |   +- net_eth0_00_23_ae_8f_51_f4
  |     
-----snipped------------


2. 
3.  VM is started successfully
4. 
5.  No any error is seen. In guest, the hot-plug pci NIC is seen. PCI nic could get ip and guest could ping to host
6. The context of the pci NIC device have the same context with qemu-kvm process
# ps -efZ|grep kvm
system_u:system_r:svirt_t:s0:c126,c508 qemu 25238  1 13 22:55 ?        00:00:05 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name vm1 -uuid f216ff41-1e2b-d4 .........

# ll -Z /sys/bus/pci/devices/0000:00:19.0/resource 
-r--r--r--. qemu qemu system_u:system_r:svirt_image_t:s0:c126,c508  /sys/bus/pci/devices/0000:00:19.0/resource

Notes:
Comments:

		177104 	[libvirtd] multi-threads testing - 3 	jyang 	yoyzhang 	Manual 		--default-- 	P2 	4790 	Edit
Setup:

1. Save the following C codes as "multi-threads.c"


/*
 * test max clients of libvirtd
 * Osier Yang <jyang@redhat.com>, Mar 4, 2010
 */

#include <stdio.h>
#include <stdlib.h>
#include <libvirt/libvirt.h>
#include <errno.h>
#include <sys/types.h>

int main (int argc, char **argv) {
        if (argc != 2) {
                printf("Usage: %s <max clients>\n", argv[0]);
                exit(EXIT_FAILURE);
        }

        virConnectPtr conn;
        char *uri = "qemu:///system";
        int max_clients = atoi(argv[1]);
        int i;
        pid_t pid;

        for (i = 0; i < max_clients; i++) {
                conn = virConnectOpen(uri);

                if (conn == NULL) {
                        fprintf(stderr, "i = %d\n", i + 1);
                        exit(EXIT_FAILURE);
                }

                //printf("conn%d: %p\n", i + 1, conn);
        }

        if ((pid = fork()) == -1) {
                fprintf(stderr, "Error: %s\n", strerror(errno));
        } else if(pid == 0){
                system("ps -Lf -C 'libvirtd'");
        } else {
                sleep(30);
        }

        exit(EXIT_SUCCESS);
}

2. Complie it as:
# gcc -o mthread `pkg-config --cflags --libs libvirt` multi-threads.c

3. make it excutable

# chmod +x ./mthread
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1. set "max_clients" in "/etc/libvirt/libvirtd.conf" as 45.

2. set "max_workers" in "/etc/libvirt/libvirtd.conf" as 50.

3. restart libvirtd.
   # service libvirtd restart

4. excute mthread with each of 45 and 50 as  argument.
   # ./mthread 45
   # ./mthread 50
	
Expected Results:

step 4:  

   1. the output shows information of 46 threads when argument is 45.
   2. the output shows only 46 threads when argument is 50, and an error
   3. message like "libvir: Remote error : cannot recv data: Connection reset by
peer" is printed.
Notes:
Comments:

		177704 	[sVirt] Manually setting sVirt domain labels using virt-manager 	jialiu 	None 	Manual 		Feature 	P2 	4790 	Edit
Setup:

Background:
virt-manager 0.8.0 has UI for manually setting the domain label
(not storage labeling). For an existing VM, it's under
Details->Overview->Security. Mode can be toggled between dynamic and
static, with the option to specify a static label.


If virt-manager have bug as setting static label, pls direct edit domain xml.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0

bug:

    852609 - From Run 44483
    856897 - From Run 46173

Actions:

1. Prepare an VM which is not running.

2. Issue "virt-manager" to open virt-manager UI.

3. Select the existing VM, then "open" -> "details" -> "Overview" -> "Security"

4. Select "static" option, then specify a label,and apply it.

such as: "system_u:system_r:svirt_t:s0:c100,c200"

 

equal to direct adding static seclabel without relabel='no' to domain xml:

  <seclabel type='static' model='selinux' relabel='no'>
    <label>unconfined_u:system_r:svirt_t:s0:c100,c200</label>
  </seclabel>

 

5. Change context of guest image file, such as:

# chcon system_u:object_r:svirt_image_t:s0:c100,c200 /var/lib/libvirt/images/demo.img

6. Start the vm

7. Check svirt label of the qemu-kvm process.
	
Expected Results:

1.

2.

3. Svirt type can be toggled between dynamic and static.

4. The specified label is saved via virsh dumpoxml

# virsh dumpxml <your-guest-name>

.....
  <seclabel type='static' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c100,c200</label>
  </seclabel>
.....

5. change context successfully.

# ls -Z /var/lib/libvirt/images/demo.img 
 -rw-------. root root system_u:object_r:svirt_image_t:s0:c100,c200 /var/lib/libvirt/images/demo.img

6. vm is started successfullly.

7. label of the qemu-kvm process should be conform to your setting.
Notes:
Comments:

		177111 	[libvirtd] Reference leak in libvirtd remote*() functions--Bug 603442 	kxiong 	None 	Manual 		--default-- 	P2 	4800 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:

1.#mkdir /tmp/netfs
2.Create an XML file, save as netfspool.xml:
      <pool type="netfs">
        <name>netfs</name>
        <source>
          <host name="10.66.90.115"/>
          <dir path="/vol/libvirt1/auto"/>
        </source>
        <target>
          <path>/tmp/netfs</path>
        </target>
      </pool>
3.#virsh pool-destroy netfs
Pool netfs destroyed
4. # virsh pool-create netfspool.xml 
Pool netfs created from netfspool.xml

	
Expected Results:

Name:           netfs
UUID:           8b393483-5913-01aa-c717-dc2167b95877
State:          running
Persistent:     no
Autostart:      no
Capacity:       810.00 GB
Allocation:     745.93 GB
Available:      64.07 GB
For the second time,new UUID is used.

Notes:
Comments:

		177705 	[sVirt] migration with svirt - scenario 1 	jialiu 	None 	Auto 		--default-- 	P2 	4800 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare two host

Source: hostA,

TargetL hostB.

2. Make sure selinux is enforcing on hostA and hostB, and flush iptable rules for migration testing.

on hostA and hostB:

# getenforce
Enforcing

# iptables -F

3. On hostA and hostB, mount a nfs share directory with context option, where store the image file

# mount -o context="system_u:object_r:svirt_image_t:s0" 10.66.90.113:/vol/xenimage /mnt

 

4. Start a vm on source hostA

5. Migrate the running guest from source to target host.

# virsh migrate demo qemu+ssh://<hostB_IP>/system --live
	
Expected Results:

1.

2.

# getenforce
Enforcing
# iptables -F

3.  Make sure the image file is labelled with "system_u:object_r:svirt_image_t:s0"

# ll -Z /mnt/jialiu/demo.img
-rw-------. root root system_u:object_r:svirt_image_t:s0 /mnt/jialiu/demo.img

4. guest is started successfully

On hostA:

# ps -efZ|grep qemu
unconfined_u:system_r:svirt_t:s0:c360,c373 qemu 7428   1 50 02:27 ?        00:00:05 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name demo -uuid a0e61e7b-3b15-dcc0-e487-7c787bd960a1 -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/demo.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -drive file=/mnt/jialiu/demo.img,if=none,id=drive-ide0-0-0,boot=on -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=19,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:5e:49:1a,bus=pci.0,addr=0x4 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3
# ll -Z /mnt/jialiu/demo.img
-rw-------. qemu qemu system_u:object_r:svirt_image_t:s0 /mnt/jialiu/demo.img

5.Migration should be done successfully

On hostB:

# ps -efZ|grep qemu
unconfined_u:system_r:svirt_t:s0:c119,c204 qemu 7428   1 50 02:27 ?        00:00:05 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name demo -uuid a0e61e7b-3b15-dcc0-e487-7c787bd960a1 -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/demo.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -drive file=/mnt/jialiu/demo.img,if=none,id=drive-ide0-0-0,boot=on -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=19,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:5e:49:1a,bus=pci.0,addr=0x4 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3
# ll -Z /mnt/jialiu/demo.img
-rw-------. qemu qemu system_u:object_r:svirt_image_t:s0 /mnt/jialiu/demo.img
Notes:
Comments:

		177706 	[sVirt] migration with svirt - scenario 2 	jialiu 	None 	Auto 		--default-- 	P2 	4810 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    852609 - From Run 44483

Actions:

1. Prepare two host

Source: hostA,

TargetL hostB.

2. Make sure selinux is permissive on hostA and enforcing on hostB, and flush iptable rules for migration testing.

on hostA:

# getenforce
Permissive

On hostB:

# getenforce
Enforcing


On hostA and hostB:

# iptables -F

3. On hostA and hostB, mount a nfs share directory with context option, where store the image file

# mount -o context="system_u:object_r:svirt_image_t:s0" 10.66.90.113:/vol/xenimage /mnt

 

4. Start a vm on source hostA

5. Migrate the running guest from source to target host.

# virsh migrate demo qemu+ssh://<hostB_IP>/system --live
	
Expected Results:

1.

2.

3.  Make sure the image file is labelled with "system_u:object_r:svirt_image_t:s0"

# ll -Z /mnt/jialiu/demo.img
-rw-------. root root system_u:object_r:svirt_image_t:s0 /mnt/jialiu/demo.img

4. guest is started successfully

On hostA:

# ps -efZ|grep qemu
unconfined_u:system_r:svirt_t:s0:c360,c373 qemu 7428   1 50 02:27 ?        00:00:05 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name demo -uuid a0e61e7b-3b15-dcc0-e487-7c787bd960a1 -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/demo.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -drive file=/mnt/jialiu/demo.img,if=none,id=drive-ide0-0-0,boot=on -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=19,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:5e:49:1a,bus=pci.0,addr=0x4 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3
# ll -Z /mnt/jialiu/demo.img
-rw-------. qemu qemu system_u:object_r:svirt_image_t:s0 /mnt/jialiu/demo.img

5.Migration should be done successfully

On hostB:

# ps -efZ|grep qemu
unconfined_u:system_r:svirt_t:s0:c274,c507 qemu 7428   1 50 02:27 ?        00:00:05 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name demo -uuid a0e61e7b-3b15-dcc0-e487-7c787bd960a1 -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/demo.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -drive file=/mnt/jialiu/demo.img,if=none,id=drive-ide0-0-0,boot=on -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=19,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:5e:49:1a,bus=pci.0,addr=0x4 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc 127.0.0.1:0 -k en-us -vga cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3
# ll -Z /mnt/jialiu/demo.img
-rw-------. qemu qemu system_u:object_r:svirt_image_t:s0 /mnt/jialiu/demo.img
Notes:
Comments:

		177707 	[sVirt] Passthrough pci device with svirt 	jialiu 	None 	Auto 		--default-- 	P1 	4820 	Edit
Setup:

- Host supports VT-d

- Turn on iommu on kernel parameter, such as: intel_iommu=on

- Could lose connection with host after dettach network device from host
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0

bug:

    No bug found

Actions:

1. Check the NIC node device.
# virsh nodedev-list --tree|more

2. Make sure selinux is enabled.
# getenforce 
Enforcing

3. Prepare a VM which is not running

4.  Add the following lines to domain xml

<hostdev mode='subsystem' type='pci' managed='yes'>
 <source>
  <address bus='0' slot='0x19' function='0'/>
 </source>
</hostdev>

5. Start the VM.

6. Check the context of pci device is the same as the context of qemu-kvm process (both with svirt)
# ll -Z /sys/bus/pci/devices/0000:00:19.0/resource


	
Expected Results:

1. Check the NIC node device.

# virsh nodedev-list --tree|more
computer
 |
  +- pci_0000_00_00_0
  +- pci_0000_00_01_0
  |   |
  |   +- pci_0000_01_00_0
  |     
  +- pci_0000_00_03_0
  +- pci_0000_00_03_2
  +- pci_0000_00_03_3
  +- pci_0000_00_19_0
  |   |
  |   +- net_eth0_00_23_ae_8f_51_f4
  |     
-----snipped------------


2. 
3. 
4. 
5.  Vm is started successfully, and the passthrough NIC is seen in guest. NIC could get ip in guest and guest could ping to host
6. The context of the pci NIC device have the same context with qemu-kvm process
# ps -efZ|grep kvm
system_u:system_r:svirt_t:s0:c126,c508 qemu 25238  1 13 22:55 ?        00:00:05 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name vm1 -uuid f216ff41-1e2b-d4 .........

# ll -Z /sys/bus/pci/devices/0000:00:19.0/resource 
-r--r--r--. qemu qemu system_u:object_r:svirt_image_t:s0:c101,c612 /sys/bus/pci/devices/0000:00:19.0/resource

Notes:
Comments:

		177709 	[sVirt] save and restore with svirt - scenario 1 	jialiu 	None 	Auto 		Feature 	P1 	4830 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Make sure the selinux is enforcing

# getenforce

2. Start a VM

# virsh start demo

3. In guest, issue some command, such as: ls, or ifconfig

4. Save the running vm, and check the context of the save file.

# virsh save demo /tmp/demo.save

5. Restore the vm

# virsh restore /tmp/demo.save
	
Expected Results:

1.

# getenforce
Enforcing

2. VM is started successfully

3. Rember the current ouput of the command

4, save should be finished successfully,

# virsh save demo /tmp/demo.save
Domain demo saved to /tmp/demo.save

# ll -Z /tmp/demo.save
-rw-------. root root unconfined_u:object_r:virt_image_t:s0 /tmp/demo.save

5. vm is restored successfully, and the output of the step3 still is seen.
Notes:
Comments:

		177710 	[sVirt] save and restore with svirt - scenario 2 	jialiu 	None 	Auto 		--default-- 	P1 	4840 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    852609 - From Run 44483

Actions:

1. Set the selinux to permissive

# setenforce 0

2. Start a VM

# virsh start demo

3. In guest, issue some command, such as: ls, or ifconfig

4. Save the running vm, and check the context of the save file.

# virsh save demo /tmp/demo.save

5. Restore the vm

# virsh restore /tmp/demo.save
	
Expected Results:

1. set selinux to permissive successfully

# getenforce
Permissive

2. VM is started successfully

3. Rember the current ouput of the command

4, save should be finished successfully,

# virsh save demo /tmp/demo.save
Domain demo saved to /tmp/demo.save

# ll -Z /tmp/demo.save
-rw-------. root root unconfined_u:object_r:virt_image_t:s0 /tmp/demo.save

5. vm is restored successfully, and the output of the step3 still is seen.
Notes:
Comments:

		177711 	[sVirt] save and restore with svirt - scenario 3 	jialiu 	yoyzhang 	Auto 		--default-- 	P2 	4850 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    sVirt
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Make sure the selinux is enforcing

# getenforce

2. Start a VM

# virsh start demo

3. In guest, issue some command, such as: ls, or ifconfig

4. Save the running vm, and check the context of the save file.

# virsh save demo /tmp/demo.save

5. Set selinux to permissive

# setenforce 0
6. Restore the vm

# virsh restore /tmp/demo.save
	
Expected Results:

# getenforce
Enforcing

2. VM is started successfully

3. Rember the current ouput of the command

4, save should be finished successfully,

# virsh save demo /tmp/demo.save
Domain demo saved to /tmp/demo.save

# ll -Z /tmp/demo.save
-rw-------. root root unconfined_u:object_r:virt_image_t:s0 /tmp/demo.save

5.

# getenforce
Permissive

6. vm is restored successfully, and the output of the step3 still is seen.
Notes:
Comments:

		177070 	[libvirt-qpid] Check whether remote libvirt-qpid deamon can be connected 	ajia 	None 	Manual 		Feature 	P2 	4860 	Edit
Setup:

# yum -y install libvirt-qpid qpid-cpp-server python-qpid qpid-tools
# chkconfig libvirt-qpid on
# chkconfig qpidd on
# cat /etc/qpidd.conf
auth=no
# service libvirt-qpid start
# service qpidd start

and target machine need also to setup following the above steps

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    remote access
    libvirt-qpid
    RHEL6.0

bug:

    No bug found

Actions:

First of all, make sure start qpidd and libvirt-qpid service on target machine

1.# ps aux|grep qpid|grep -v grep

qpidd     1794  0.0  0.0 447728  2764 ?        Ssl  04:50   0:00 /usr/sbin/qpidd --data-dir /var/lib/qpidd --daemon

root      2343  0.5  0.0 391188  5360 ?        Ssl  04:59   0:00 libvirt-qpid --daemon

2.open a terminal on local machine and using libvirt-qpid to connect target machine

#libvirt-qpid 10.66.70.x:5672             (5672 is a default port)

3.open a new terminal on remote machine and run qpid-tool, need to check class number of 'com.redhat.libvirt'

and connection, and verify if can remotely manage node, domain, pool and volume.

#qpid-tool

qpid: list
Summary of Objects by Type:
    Package                 Class         Active  Deleted
    =======================================================
    org.apache.qpid.broker  session       3       0
    org.apache.qpid.broker  exchange      8       0
    org.apache.qpid.broker  vhost         1       0
    org.apache.qpid.broker  broker        1       0
    org.apache.qpid.broker  system        1       0
    org.apache.qpid.broker  binding       18      0
    com.redhat.libvirt      domain        2       0
    org.apache.qpid.broker  connection    3       0
    org.apache.qpid.broker  queue         5       0
    org.apache.qpid.broker  subscription  5       0
    com.redhat.libvirt      pool          2       0
    com.redhat.libvirt      volume        2       0
    com.redhat.libvirt      node          2       0
qpid: list connection
Object Summary:
    ID   Created   Destroyed  Index
    =====================================================
    128  09:07:27  -          0-0-1-0-3:127.0.0.1:42812
    129  09:07:28  -          0-0-1-0-3:127.0.0.1:42813
    130  09:10:33  -          0-0-1-0-3:127.0.0.1:58400

qpid: show 129
Object of type: org.apache.qpid.broker:connection(561dc09b-8de5-a301-3b21-45c2cdd8a90d)
    Attribute          129
    =======================================
    vhostRef           0-0-1-0-3
    address            u'127.0.0.1:42813'
    incoming           True
    SystemConnection   False
    federationLink     False
    authIdentity       u'anonymous'
    remoteProcessName  u'libvirt-qpid'
    remotePid          2552
    remoteParentPid    1
    shadow             False
    closing            False
    framesFromClient   1110
    framesToClient     0
    bytesFromClient    289894
    bytesToClient      0
	
Expected Results:

can remotely manage node, domain, pool and volume resource.
Notes:
Comments:

		177712 	[sVirt] save and restore with svirt - scenario 4 	jialiu 	yoyzhang 	Auto 		--default-- 	P2 	4860 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    sVirt
    virsh-rail

bug:

    No bug found

Actions:

1. Set selinux to permissive

# setenforce 0

2. Start a VM

# virsh start demo

3. In guest, issue some command, such as: ls, or ifconfig

4. Save the running vm, and check the context of the save file.

# virsh save demo /tmp/demo.save

5. Set selinux to Enforcing

# setenforce 1

6. Restore the vm

# virsh restore /tmp/demo.save
	
Expected Results:

1.

# getenforce
Permissive

2. VM is started successfully

3. Rember the current ouput of the command

4, save should be finished successfully,

# virsh save demo /tmp/demo.save
Domain demo saved to /tmp/demo.save

# ll -Z /tmp/demo.save
-rw-------. root root unconfined_u:object_r:virt_image_t:s0 /tmp/demo.save

5.
# getenforce
Enforcing
6. vm is restored successfully, and the output of the step3 still is seen.
Notes:
Comments:

		177721 	[sVirt] start guest with image labelled by incorrect security context 	jialiu 	yoyzhang 	Auto 		--default-- 	P2 	4870 	Edit
Setup:

Make sure the selinux is "Enforcing"

# getenforce

Enforcing

 

If virt-manager have bug as setting static label, pls direct edit domain xml.

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    sVirt

bug:

    No bug found

Actions:

1. Prepare a shutoff VM

2. Check context of the vm image.

# ls -Z /var/lib/libvirt/images/
-rw-------. root root system_u:object_r:virt_image_t:s0 vm2.img

3. Change the context on the image

# chcon system_u:object_r:svirt_image_t:s0:c302,c449 vm2.img


4. In virt-manager, select the shutoff VM, then click view->Details->Overview->Security->Static, then input a different label value, such as:  system_u:system_r:svirt_t:s0:c302,c448, at last, click 'Apply'

 

equal to adding static seclabel without relabel to domain xml:

  <seclabel type='static' model='selinux'>
    <label>unconfined_u:system_r:svirt_t:s0:c302,c448</label>
  </seclabel>

5. # virsh start vm2
	
Expected Results:

1.

2.

3. Change the context successfully.

4.

5. Verify SELinux is blocking the illegal access

pops up error info:

error: Failed to start domain vm2
error: internal error Process exited while reading console log output: char device redirected to /dev/pts/2
qemu: could not open disk image /var/lib/libvirt/images/vm2.img: Permission denied

Notes:
Comments:

		177728 	[Update device flags] change the media in an existing CDROM device on the fly 	jialiu 	None 	Auto 		Feature 	P2 	4880 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    QE consumption
    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1. Prepare 2 different ISO files.

Create temp1.iso and temp2.iso

# mkisofs -o /var/lib/libvirt/images/temp1.iso /tmp

Do some changes in /tmp directory, such as, add/change/remove some new file to make a different iso file

# mkisofs -o /var/lib/libvirt/images/temp2.iso /tmp

2. Define a guest with a cdrom devcie witch is connected with temp1.iso. Make sure the following is seen in your domain xml file.

......

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/temp1.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>
.......

3. Prepare a xml as following:

# cat cdrom.xml
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/temp2.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>

4. When doaming is inactive, try to update the media in cdrom device.

# virsh list --all
 Id Name                 State
----------------------------------
  - rhel6                shut off


# virsh update-device rhel6 cdrom.xml

5. Start the domain.

# virsh start rhel6

For Linux:

6. In guest, mount the cdrom device, and check the media's context.

# mount /dev/cdrom /media

# ls /media

7. Try to update the media in cdrom device

# virsh update-device rhel6 cdrom.xml

8. Umount the media in cdrom.

# umount /media

9. Update the media in cdrom device

# virsh update-device rhel6 cdrom.xml

10. In guest, mount the new media and check the context of the media.

# mount /dev/cdrom /media

# ls /media

For Windows:

6.Open the cdrom in guest.

7. Try to update the media in cdrom device

# virsh update-device win2008 cdrom.xml
	
Expected Results:

4. When doaming is inactive, update device is not allowed.

# virsh update-device rhel6 cdrom.xml
error: Failed to update device from cdrom.xml
error: Requested operation is not valid: cannot attach device on inactive domain

For RHEL guest:

6. The media has the same context as temp1.iso.

7. When the media is using, the following prompt is expected:

# virsh update-device rhel6 cdrom.xml
error: Failed to update device from cdrom.xml
error: internal error unable to execute QEMU command 'change': Device 'drive-ide0-1-0' is locked

9. Command is run successfully.

# virsh update-device rhel6 cdrom.xml
Device updated successfully

10. Makes sure the context of this media is the same as temp2.iso, the media is changed successfully on the fly.

 

For Windows guest:

6. The media has the same context as temp1.iso.

7. Command is run successfully.

# virsh update-device win2008 cdrom.xml
Device updated successfully
Makes sure the context of this media is the same as temp2.iso, the media is changed successfully on the fly.
Notes:
Comments:

		177076 	[Libvirtd status checking] 256 autostarted network reloading 	jiachen 	None 	Auto 		Stress 	P2 	4890 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177077 	[Libvirtd status checking] 256 non-autostarted network reloading (active + inactive) 	jiachen 	None 	Auto 		Stress 	P2 	4900 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177737 	[Update device flags] Update spice password for live domain 	jialiu 	None 	Manual 		Regression 	P2 	4900 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers
    RHEL6.0

bug:

    No bug found

Actions:

1. Define a domain with spice graphical device as following:

<domain type='kvm'>
  <name>demo</name>
  <uuid>6bd72aef-3661-8400-ab2c-c15c935cdd85</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/demo.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:71:a8:bb'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' autoport='no' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>

2. Start the domain via "vrish start" command

3. Install spice-client spice-server

    #yum install spice-client spice-server

    Note: if spicec file was not found under /usr/libexec, download the src rpm from https://brewweb.devel.redhat.com/packageinfo?packageID=18921, rpm -ivh install the src rpm.

4. Use spice-client to check guest windows.

# /usr/libexec/spicec -h 127.0.0.1 -p 5900

5. Prepare a xml file for graphics update.

    Before this, you need dump and check the guest xml, since libvirtd will add a default listen address it.

   # cat spice.xml 

<graphics type='spice' port='5900' autoport='no' listen='0.0.0.0' keymap='en-us' passwd='hello'>
<listen type='address' address='0.0.0.0'/>
</graphics>

6. Change spice password on the fly for the guest.

# virsh update-device <guestname> spice.xml

7. Connect guest windows again.

# /usr/libexec/spicec -h 127.0.0.1 -p 5900 -w hello

 
	
Expected Results:

4. Guest windows is displayed successfully.

6. The command is run successfully. The original guest window is closed automatically.

# virsh update-device test spice.xml
Device updated successfully

7. With the new password, the guest window is connected successfully.
Notes:
Comments:

		177078 	[Libvirtd status checking] 256 non-autostarted storage pools reloading (active + inactive) 	jiachen 	None 	Auto 		Stress 	P2 	4910 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177738 	[Update device flags] Update vnc password for live domain - Bug 524623, 596100 	jialiu 	None 	Manual 		Regression 	P2 	4910 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers
    RHEL6.0

bug:

    No bug found

Actions:

1. Define a domain with a vnc password in xml config file.

...

    <graphics type='vnc' port='-1' autoport='yes' listen='127.0.0.1' keymap='en-us' passwd='aaabbb'/>

...

2. Start the domain.

3. Use virt-viewer and virt-manager to log in guest windows

4. Input your vnc password in domain config xml file

5. Change vnc password on the fly.

# cat vnc_passwd.xml
    <graphics type='vnc' port='0' autoport='yes' listen='127.0.0.1' keymap='en-us' passwd='111111'/>


# virsh update-device winxp vnc_passwd.xml
Device updated successfully

6. Close the original guest windows, connetc guest windows again.

# virt-viewer <guestname>

7. Input the orignal vnc password "aaabbb"

8. Input the new vnc password "111111"

9. Destroy the guest in the previous test:

   # virsh destroy ${domain_name}

10. Start the guest:
   # virsh start ${domain_name}

11. Confirm if vnc connection still use the new password:
   # virt-viewer ${domain_name}\

12. Input the new vnc password "111111"

13. Input the orignal vnc password "aaabbb"
	
Expected Results:

2. Guest is started successfully

3. Prompt that you need input your password.

4. Here, input "aaabbb"

5. update-device command successfully.

6. Prompt that you need input your password.

7. The auth failed.

8. The auth is successful, the guest windows is connected succesfully.

12. The auth failed. (If success, chek bug 596100)

13. The auth is successful, the guest windows is connected succesfully.
Notes:
Comments:

		177079 	[Libvirtd status checking] Reloading 512 running guests 	jiachen 	None 	Auto 		Stress 	P2 	4920 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177058 	[ipv4 Bi-migration] migrate 250 guests at one time through TLS connection on hosts(48 cores CPU / 512GB mem) 	jiachen 	None 	Manual 		Regression 	P1 	4930 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    scalability

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177764 	[Virtio] Create domain with virtio block device driver (only for Windows) 	jyang 	jyang 	Auto 		--default-- 	P2 	4930 	Edit
Setup:

1. Need to install latest virtio-win rpm package on host first.

#rpm  -ivh virtio-win-1.1.16-1.el6.noarch.rpm

 

For rhel6.1, please use this virtio-win version to test: virtio-win-prewhql-0.1.zip
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    installation

bug:

    No bug found

Actions:

1.Create  disk image.

# qemu-img create /var/lib/libvirt/images/win_virtio.img 15G

2. Defined  virtio guest.

For Windows:

#cat win_virtio.xml

<domain type='kvm'>
  <name>win_virtio</name>+
  <uuid>6785f9df-12e9-3f5a-32b3-39de72020e12</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>2</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/sr0'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/win_virtio.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <disk type='file' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source file='/usr/share/virtio-win/virtio-win-1.1.16.vfd'/>
      <target dev='fda' bus='fdc'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
    </controller>
    <controller type='fdc' index='0'/>
    <interface type='network'>
      <mac address='52:54:00:e7:7e:04'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <sound model='ac97'>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
    </video>
    <memballoon model='virtio'>
    </memballoon>
  </devices>
</domain>

2. Start guest.

# virsh start win_virtio

3.Install the windows guest

 NOTE: (windows 2008 & windows 7)In the page of  "where do you want to install Windows ",Please click the "Load driver" label' and select disk driver from Floppy device.

4.After Install finished,edit domain XML.

  #virsh edit  win_virtio

 Delete that xml " <boot dev='cdrom'/>" in domain  XML

 5.Start domain guest.

#virsh start win_virtio

6.Install network drvier form Floppy device.
	
Expected Results:

3. The guest could be installed successfully

no error output

5.1 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

5.2 firefox or IE application should be launched

5.3 The output value should be nearly equal to the following value given in xml confile file

  <memory>1048576</memory>

5.4  vcpu number should be equal to the following value given in xml config file

  <vcpu>2</vcpu>

6. Should ping to host successfully

COMMENTS:

    If it doesn't work, then check the following file in the host for errors:
    # cat /var/log/libvirt/qemu/[guestname].log
Notes:
Comments:

		177059 	[ipv4 Bi-migration] migrate 50 guests at one time through TLS connection on hosts(48 cores CPU / 512GB mem) 	jiachen 	None 	Auto 		Stress 	P2 	4940 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    scalability

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177774 	[Virtio] Replace a domain's existing SCSI/IDE block device driver to virtio 	jyang 	yoyzhang 	Auto 		--default-- 	P1 	4940 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Install a guest with SCSI or IDE disk.

2. On Fedora 9 or later,  if the disk is which holds the guest's
   root filesystem, you first need to ensure that the guest will be
   able to mount the virtio disk during bootup.

   # mkinitrd --with virtio_pci --with virtio_blk -f /boot/initrd-$(uname -r).img $(uname -r)


3. shutdown the guest and modify the disk model like the following:
   # virsh edit guestname

    <disk type='...' device='disk'>
      ...
      <target dev='vda' bus='virtio'/>
    </disk>

4. start the guest
   # virsh start guestname
	
Expected Results:

1.  you will use virtio block driver(virtio_blk). check it by

    # /sbin/lsmod | grep virtio
    [shows virtio_pci, virtio_blk others loaded]

OR

     # dmesg | grep virtio

OR

     # find /lib/modules/$(uname -r)/ | grep virtio

 

COMMENTS:

    If it doesn't work, then check the following file in the host for errors:
    # cat /var/log/libvirt/qemu/[guestname].log
Notes:
Comments:

		177060 	[ipv4-migration] migrate 500 guests at one time through TLS connection on hosts(48 cores CPU / 512GB mem) 	jiachen 	None 	Manual 		Regression 	P1 	4950 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    scalability

bug:

    No bug found

Actions:
	
Expected Results:
Notes:
Comments:

		177773 	[Virtio] Replace a domain's existing e1000/RTL8139 network driver to virtio 	jyang 	jyang 	Auto 		--default-- 	P1 	4950 	Edit
Setup:

make sure there is at least one guest exists for testing.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual disks

bug:

    No bug found

Actions:


1. First, shut down the guest and then edit its configuration file:
   # virsh edit guestname

2. In the <interface> section, add a virtio model, like this:
   <interface type='network'>
      ...
      <model type='virtio' />
   </interface>

3. start the guest
   # virsh start guestname


	
Expected Results:

the guest will use virtio network driver(virtio_net), check it by

    # lsmod | grep virtio

 OR

    # dmesg | grep virtio

 OR

     # find /lib/modules/$(uname -r) | grep virtio


COMMENTS:

    If it doesn't work, then check the following file in the host for errors:
    # cat /var/log/libvirt/qemu/[guestname].log
Notes:
Comments:

		177061 	[Libvirt domain event handler] Deliberately leading to domain I/O errors 	yimwang 	None 	Manual 		--default-- 	P2 	5000 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirt domain event handler

bug:

    No bug found

Actions:

1.In the first terminal run

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py  qemu:///system

2.# virsh start demo

3.# qemu-img create foo.img 10M

4. Create a disk desription file as following XML:
# cat disk.xml
<disk type="file" device="disk">
  <source file="/var/lib/libvirt/migrate/foo.img"/>
  <target dev="vdb" bus="virtio"/>
</disk>

5.# virsh attach-device demo disk.xml

6.Logging in the guest and  partitioning & mounting &writing full data to the disk.

7.Starting a new domain, loop step 1-6.

	
Expected Results:

  4. Device attached successfully

7.# python event-test.py qemu:///system
Using uri:qemu:///system
myDomainEventIOErrorCallback: Domain example(19)
/var/lib/libvirt/images/test.img ide0-0-1 2

Notes:
Comments:

		177062 	[Libvirt domain event handler] event-test.py help info 	jialiu 	yoyzhang 	Manual 		Feature 	P2 	5010 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt domain event handler
    RHEL6.0

bug:

    No bug found

Actions:

1. Open terminal, issue the following command:

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py --help

or

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py -h

2. run event-test.py with incorrect argument as following:

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py -a
	
Expected Results:

1. Help info is printed out as following:

   usage: event-test.py [uri]
   uri will default to qemu:///system

2. A prompt about incorrect argument is printed out as following:

option -a not recognized
usage: event-test.py [uri]
   uri will default to qemu:///system
Notes:
Comments:

		177064 	[Libvirt domain event handler] Other stopping/rebooting operation for domain 	yimwang 	None 	Manual 		--default-- 	P1 	5020 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirt domain event handler

bug:

    No bug found

Actions:

1. Start the libvirtd service, and then open two terminals.

2.  In the first terminal run:

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py  qemu:///system

3.1. Logging in the guest and running shutdown -h now or init 0

3.2  Logging in the guest and running shutdown -r now or reboot

3.3 kill -9 the QEMU process

for example:
# ps -ef | grep qemu-kvm
qemu     29983     1 17 08:36 ?        00:00:42 /usr/libexec/qemu-kvm -S -M
rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name example
-uuid 61b80955-e644-55b4-763a-f4274a43883c -nodefconfig -nodefaults -chardev
socket,id=monitor,path=/var/lib/libvirt/qemu/example.monitor,server,nowait -mon
chardev=monitor,mode=control -rtc base=utc -boot c -drive
file=/var/lib/libvirt/images/example,if=none,id=drive-ide0-0-0,boot=on,format=qcow2,cache=none
-device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -drive
if=none,media=cdrom,id=drive-ide0-1-0,readonly=on,format=raw -device
ide-drive,bus=ide.1,unit=0,drive=drive-ide0-1-0,id=ide0-1-0 -netdev
tap,fd=30,id=hostnet0 -device
rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:c3:e5:de,bus=pci.0,addr=0x3
-chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc
127.0.0.1:1 -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x4 -device
virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5
root     30346 24760  0 08:40 pts/9    00:00:00 grep qemu-kvm

[root@dhcp-66-92-184 ~]# kill -9 29983

	
Expected Results:

2. Running event-test.py to listen domain event 

3.1) Listen stopping operation
myDomainEventCallback1 EVENT: Domain rhel6u3_qcow2(12) Shutdown F
myDomainEventCallback2 EVENT: Domain rhel6u3_qcow2(12) Shutdown F
myDomainEventCallback1 EVENT: Domain rhel6u3_qcow2(12) Stopped Shutdown
myDomainEventCallback2 EVENT: Domain rhel6u3_qcow2(12) Stopped Shutdown

3.2) Listen rebooting operation
myDomainEventRTCChangeCallback: Domain kvm1(2) -2
myDomainEventRebootCallback: Domain kvm1(2)

3.3) Listen killing qemu process operation
myDomainEventCallback1 EVENT: Domain kvm1(4) Stopped Failed
myDomainEventCallback2 EVENT: Domain kvm1(4) Stopped Failed


Notes:
Comments:

		177065 	[Libvirt domain event handler] Run 'hwclock --systohc'(on the domain) to change the domain clock 	yimwang 	None 	Manual 		Feature 	P2 	5030 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt domain event handler

bug:

    No bug found

Actions:

1.  In the first terminal run:

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py  qemu:///system

2.  Logging in the guest and running  'hwclock --systohc'
	
Expected Results:

2. Listen the domain clock changing
# python event-test.py qemu:///system
Using uri:qemu:///system
myDomainEventRTCChangeCallback: Domain kvm1(5) -1

Notes:
Comments:

		177066 	[Libvirt domain event handler] Running watchdog action such as pasue action 	yimwang 	None 	Manual 		--default-- 	P1 	5040 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt domain event handler

bug:

    No bug found

Actions:

1.In the first terminal run:

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py  qemu:///system


2.Make sure watchdog has been installed on the guest
# yum install watchdog 

3.Editor domain xml config to support watchdog, for example:
# virsh edit example
......
<watchdog model='i6300esb' action='pause'/>
...... 

4.Staring domain and modifying watchdog.conf, for example:
......
interval        = 130
watchdog-device = /dev/watchdog
......

5.In guest, start watchdog process.
# watchdog -f

	
Expected Results:

5 Listen watchdog event
# python event-test.py qemu:///system
Using uri:qemu:///system
myDomainEventWatchdogCallback: Domain rhel62(8) 1
myDomainEventCallback1 EVENT: Domain rhel62(8) Suspended Watchdog
myDomainEventCallback2 EVENT: Domain rhel62(8) Suspended Watchdog

Notes:
Comments:

		177067 	[Libvirt domain event handler] Track all events for basic operations on a domain 	jialiu 	None 	Auto 		Feature 	P1 	5050 	Edit
Setup:

Prepare a domain config file, and vm image.

If needed, do some neccessary change according to your requirement.

<domain type='kvm'>
  <name>bb</name>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu'/>
      <source file='/mnt/jialiu/rhel5u4_kvm.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'/>
    <interface type='network'>
      <mac address='52:54:00:53:a6:f0'/>
      <source network='default'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
    </video>
  </devices>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    libvirt domain event handler
    RHEL6.0
    QE consumption

bug:

    845958 - From Run 44732

Actions:

1. Start the libvirtd service, and then open two terminals.

2.  In the first terminal run:

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py  qemu:///system

3. In the second ternminal run:

# virsh create example.xml

# virsh destroy example

# virsh define example.xml
Domain example defined from example.xml 

# virsh start example
Domain example started 

# virsh setmem --live example

# virsh suspend example
Domain example suspended 

# virsh resume example
Domain example resumed 

]# virsh save example /tmp/example.sav
Domain example saved to /tmp/example.sav 

# virsh restore /tmp/example.sav
Domain restored from /tmp/example.sav 

NOTE: config the migration environment for the test

this one is migrating from source machine to target machine.
# virsh migrate example qemu+ssh://10.66.92.155/system
root@10.66.92.155's password:  

this one is migrating back to the source host.
# virsh migrate example qemu+ssh://10.66.92.184/system
root@10.66.92.184's password:  

# virsh shutdown example
Domain example is being shutdown 

#virsh undefine example

Domain example has been undefined

 

4. There's new events for domain (pmsuspend and pmwakeup), even handler alse need to track them.

4.1 First need add 

....

 <os>
    <type arch='x86_64' machine='rhel6.4.0'>hvm</type>
    <loader>/usr/share/seabios/bios.bin</loader>
    <boot dev='hd'/>
  </os>

 <pm>
     <suspend-to-mem enabled='yes'/>
     <suspend-to-disk enabled='yes'/>
   </pm>

 

 ....

<channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/r6x86_64.agent'/>
      <target type='virtio' name='org.qemu.guest_agent.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>

..... 

 

in the domain xml then restart domain.

 

 

 

4.2 In the domain setup  latest qemu-guest-agent in the qemu-kvm pkg  then run 

https://brewweb.devel.redhat.com/packageinfo?packageID=18283

# qemu-ga -d 

#pm-suspend 
4.3 On the host, check the domain status

# virsh list
 Id    Name                           State
----------------------------------------------------
 3     rhel6                            pmsuspended

4.4 move mouse in the domain will wakeup domain or run virsh pmwakeup on host

#virsh dompmwakeup rhel6

Domain rhel6 successfully woken up

 
	
Expected Results:

# python event-test.py qemu:///system
Using uri:qemu:///system

create:
myDomainEventCallback1 EVENT: Domain example(42) Started Booted
myDomainEventCallback2 EVENT: Domain example(42) Started Booted

destroy:
myDomainEventCallback1 EVENT: Domain example(-1) Stopped Destroyed
myDomainEventCallback2 EVENT: Domain example(-1) Stopped Destroyed

define:
myDomainEventCallback1 EVENT: Domain example(-1) Defined Added
myDomainEventCallback2 EVENT: Domain example(-1) Defined Added

start:
myDomainEventCallback1 EVENT: Domain example(9) Started Booted
myDomainEventCallback2 EVENT: Domain example(9) Started Booted

setmem 
myDomainEventBalloonChangeCallback: Domain rhel6(11) 1047552
myDomainEventBalloonChangeCallback: Domain rhel6(11) 187392

suspend:
myDomainEventCallback1 EVENT: Domain example(9) Suspended Paused
myDomainEventCallback2 EVENT: Domain example(9) Suspended Paused

resume:
myDomainEventCallback1 EVENT: Domain example(9) Resumed Unpaused
myDomainEventCallback2 EVENT: Domain example(9) Resumed Unpaused

save:
myDomainEventCallback1 EVENT: Domain example(-1) Stopped Saved
myDomainEventCallback2 EVENT: Domain example(-1) Stopped Saved

restore:
myDomainEventCallback1 EVENT: Domain example(10) Started Restored
myDomainEventCallback2 EVENT: Domain example(10) Started Restored

migrate:
myDomainEventCallback1 EVENT: Domain example(10) Suspended Migrated
myDomainEventCallback2 EVENT: Domain example(10) Suspended Migrated
myDomainEventCallback1 EVENT: Domain example(-1) Stopped Migrated
myDomainEventCallback2 EVENT: Domain example(-1) Stopped Migrated

migrate back to the source machine:
myDomainEventCallback1 EVENT: Domain example(11) Started Migrated
myDomainEventCallback2 EVENT: Domain example(11) Started Migrated
myDomainEventCallback1 EVENT: Domain example(11) Resumed Migrated
myDomainEventCallback2 EVENT: Domain example(11) Resumed Migrated

shutdown:
myDomainEventCallback1 EVENT: Domain example(11) Stopped Shutdown
myDomainEventCallback2 EVENT: Domain example(11) Stopped Shutdown

undefine:
myDomainEventCallback1 EVENT: Domain example(-1) Undefined R
myDomainEventCallback2 EVENT: Domain example(-1) Undefined R

4.

 4.2 

myDomainEventPMSuspendCallback: Domain rhel6(10) system pmsuspend
myDomainEventCallback1 EVENT: Domain rhel6(10) PMSuspended Memory
myDomainEventCallback2 EVENT: Domain rhel6(10) PMSuspended Memory

4.4

myDomainEventPMWakeupCallback: Domain b(10) system pmwakeup
myDomainEventCallback1 EVENT: Domain b(10) Started Wakeup
myDomainEventCallback2 EVENT: Domain b(10) Started Wakeup

 

 

 

 

 

Notes:
Add event handler about balloon event
-setmem Bug 884650 - Add support for qemu-kvm's BALLOON_CHANGE event to avoid using monitor in virDomainGetXMLDesc
Comments:

		177068 	[Libvirt domain event handler] Track all events for multiple domains 	jialiu 	yoyzhang 	Manual 		Feature 	P2 	5060 	Edit
Setup:

Prepare to define 2 different domain. one is named "a", another is "b".
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt domain event handler
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the libvirtd service, and then open two terminals.

2.  In the first terminal run:

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py  qemu:///system

3. In the second ternminal run, define "a" domain.

# virsh define a.xml

4. start your guest.

# virsh start a.

5. suspend your guest.

# virsh suspend a

6. resume your guest.

# virsh resume a

7. define "b" domain.

# virsh define b.xml

8. start "b" domain

# virsh start b

9. save "b" domain.

# virsh save b /root/xx

10. restore "b" domain.

# virsh restore /root/xx

11. shutdown "a" and "b" domain.

# virsh shutdown a

and

# virsh destroy b

12. undefine "a" and "b" domain.

# virsh undefine a

# virsh undefine b
	
Expected Results:

1.

2. Python script works fine, no any error is seen.

3. doamin "a" is defined successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain a(-1) Defined Added
myDomainEventCallback2 EVENT: Domain a(-1) Defined Added

4. doamin "a" is started successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain a(16) Started Booted
myDomainEventCallback2 EVENT: Domain a(16) Started Booted

(NOTE: 13 is "a"'s domain ID)

5. doamin "a" is suspended successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain a(14) Suspended Paused
myDomainEventCallback2 EVENT: Domain a(14) Suspended Paused

6. doamin "a" is resumed successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain a(16) Resumed Unpaused
myDomainEventCallback2 EVENT: Domain a(16) Resumed Unpaused

7. doamin "b" is defined successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain b(-1) Defined Added
myDomainEventCallback2 EVENT: Domain b(-1) Defined Added

8. doamin "b" is started successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain b(16) Started Booted
myDomainEventCallback2 EVENT: Domain b(16) Started Booted

(NOTE: 14 is "b"'s domain ID)

9. doamin "b" is saved successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain b(-1) Stopped Saved
myDomainEventCallback2 EVENT: Domain b(-1) Stopped Saved


10. doamin "b" is restored successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain b(19) Started Restored
myDomainEventCallback2 EVENT: Domain b(19) Started Restored

11. doamin "a" and "b" are stoped successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain a(19) Stopped Shutdown
myDomainEventCallback2 EVENT: Domain a(19) Stopped Shutdown

myDomainEventCallback1 EVENT: Domain b(20) Stopped Destroyed
myDomainEventCallback2 EVENT: Domain b(20) Stopped Destroyed

12. doamin is undefine successfully, and go back to the first terminal, the following meessag is seen:

myDomainEventCallback1 EVENT: Domain a(-1) Undefined R
myDomainEventCallback2 EVENT: Domain a(-1) Undefined R
myDomainEventCallback1 EVENT: Domain b(-1) Undefined R
myDomainEventCallback2 EVENT: Domain b(-1) Undefined R

Notes:
Comments:

		177069 	[Libvirt domain event handler]VNC & SPICE connecting & disconnecting 	yimwang 	None 	Manual 		--default-- 	P1 	5070 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirt domain event handler

bug:

    854135 - From Run 44732

Actions:

1.  In the first terminal run:

# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py  qemu:///system

2. Editor domain xml config to support vnc or spice, and then start domain using virt-manager & virt-viewer for vnc and using /usr/libexec/spicec for spice.

# virsh edit example
For vnc:
......
<graphics type='vnc' port='5900' autoport='no' listen='127.0.0.1'
keymap='en-us'/>
......

# virt-viewer example

For spice:
......
<graphics type='spice' port='5900' autoport='no' keymap='en-us'/>
......
# /usr/libexec/spicec -h 127.0.0.1 -p 5900

	
Expected Results:

2.
# python event-test.py qemu:///system
Using uri:qemu:///system
myDomainEventGraphicsCallback: Domain example(16) 0 none
myDomainEventGraphicsCallback: Domain example(16) 1 none

Notes:
Comments:

		177038 	[Installation - cdrom] Install guest from boot.iso 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P1 	5110 	Edit
Setup:

- Host with boot cdrom insert
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    installation
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. # qemu-img create /var/lib/libvirt/images/bootiso_test.img 8G

# virsh define bootiso_test.xml

<domain type='kvm'>
  <name>kvm1</name>
  <uuid>bc2f4f7e-02c9-d9b0-9fb8-fbc35a7777f3</uuid>
  <memory>2097152</memory>
  <currentMemory>2097152</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/bootiso_test.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <disk type='block' device='cdrom'>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:03:87:c8'/>
      <source network='default'/>
      <filterref filter='no-arp-spoofing'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' autoport='no' keymap='en-us'/>
    <sound model='ac97'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='vga' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
</domain>




2. # virsh start bootiso_test

3.  User virt-viewer(do not run backgroud) to view guest graphic, and continue wtih installtion step by step.

# virt-viewer bootiso_test

Check the ouput in terminal during installation

Choose 'URL' in step of 'Installation Method'     

input url: http://download.englab.nay.redhat.com/pub/rhel/nightly/RHEL6.0-20100608.n.2/6.0/Server/x86_64/os

Make sure install graphic console for guest    

4. After the guest boot up after installation, do the following check in guest

(if the guest does not boot up successfully, pls check the log /var/log/messages)

4.1 # ping ${host_ip}

4.2 #mkdir testfolder

    # touch testfolder/testfile

   # vim /testfolder/testfile

  # cat /testfolder/testfile

  # cp testfolder testfolder.bak

# dd if=/dev/zero of=./write_test bs-1024 count=10240

# ll -h write_test

4.3 # firefox

4.4 # cat /proc/meminfo | grep MemTotal

4.5 # cat /pro/cpuinfo

4.6 # lspci

4.7 check the log for a running guest

# cat /var/log/libvirt/qemu/bootiso_test.log
	
Expected Results:

3. The guest could be installed successfully

no error output

4.1 Should ping to host successfully

4.2 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

4.3 firefox application should be launched

4.4 The output value should be nearly equal to the following value given in xml confile file

  <memory>1048576</memory>

4.5  vcpu number should be equal to the following value given in xml config file

  <vcpu>1</vcpu>

4.6 net device model should be virtio

00:04.0 Ethernet controller : Qumranet, Inc. VIrtio network device

sound device model should be AC97

00:05.0 Multimedia audio controller: Intel Coreporation 82801AA AC'97 Audio Controller

4.7 no error output
Notes:
Comments:

		177754 	[Virtio-serial] Create a socket channel with unix type 	gren 	None 	Manual 		--default-- 	P2 	5110 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtio-serial

bug:

    No bug found

Actions:

1. Add the following XML to a guest domain with the latest rhel6:

   <controller type='virtio-serial' index='0'/>
    <channel type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/foo'/>
      <target type='virtio' name='org.linux-kvm.port.1'/>
    </channel>

2. Start the guest domain.

3, On the guest OS, 

# lsmod|grep virtio
virtio_console         15495  0 
virtio_balloon          4281  0 
virtio_pci              6733  0 
virtio_ring             7169  3 virtio_console,virtio_balloon,virtio_pci
virtio                  4824  3 virtio_console,virtio_balloon,virtio_pci

  #lspci

  00:06.0 Communication controller: Qumranet, Inc. Virtio console

 #ll /sys/bus/pci/drivers/virtio-pci

   ...
   /0000:00:04.0 -> /sys/devices/pci0000:00/0000:00:04.0/
  .....

# cat /sys/class/virtio-ports/vport0p0/name
org.linux-kvm.port.1

3, On the guest,  send data to host via /dev/vport0p0

echo a >/dev/vport0p0
 
receive data from host 

# nc -U /var/lib/libvirt/qemu/foo

4. On the host,    send data to guest

# echo a | nc -U /var/lib/libvirt/qemu/foo

 on the guest 
# cat /dev/vport0p0
a
a

	
Expected Results:

Make sure you can get the exact result in steps , without error msg .
Notes:
Comments:

		177039 	[Installation - cdrom] Install guest from cdrom - spice - HDA 	yoyzhang 	None 	Manual 		Feature 	P1 	5120 	Edit
Setup:

- Host with os installation cdrom insert
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    installation
    RHEL6.0

bug:

    No bug found

Actions:

1. # qemu-img create /var/lib/libvirt/images/cdrom_test.img 8G

# virsh define cdrom_test.xml

<domain type='kvm'>
  <name>cdrom_test</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/cdrom_test.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <disk type='block' device='cdrom'>
      <driver name='qemu'/>
      <source dev='/dev/sr0'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>
    <interface type='network'>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/2'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='-1' autoport='yes' keymap='en-us' listen='0.0.0.0'/>

    <sound model='ich6'>
    </sound>

    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
    </video>
  </devices>
</domain>

2. # virsh start cdrom_test

3.  User spicec (do not run backgroud) to view guest graphic, and continue wtih installtion step by step.

#  /usr/libexec/spicec -h <hostip or 127.0.0.1> -p 5900

Check the ouput in terminal during installation

     Make sure install graphic console for guest

4. After the guest boot up after installation, do the following check in guest

(if the guest does not boot up successfully, pls check the log /var/log/messages)

4.1 # ping ${host_ip}

4.2 #mkdir testfolder

    # touch testfolder/testfile

   # vim /testfolder/testfile

  # cat /testfolder/testfile

  # cp testfolder testfolder.bak

# dd if=/dev/zero of=./write_test bs=1024 count=10240

# ll -h write_test

4.3 # firefox

4.4 # cat /proc/meminfo | grep MemTotal

4.5 # cat /proc/cpuinfo

4.6 # lspci

4.7 check the log for a running guest

# cat /var/log/libvirt/qemu/cdrom_test.log
	
Expected Results:

3. The guest could be installed successfully

no error output

4.1 Should ping to host successfully

4.2 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

4.3 firefox application should be launched

4.4 The output value should be nearly equal to the following value given in xml confile file

  <memory>1048576</memory>

4.5  vcpu number should be equal to the following value given in xml config file

  <vcpu>1</vcpu>

4.6 net device model should be virtio

00:04.0 Ethernet controller : Qumranet, Inc. VIrtio network device

sound device model should be HDA

00:04.0 Audio device: Intel Corporation 82801FB/FBM/FR/FW/FRW (ICH6 Family)
High Definition Audio Controller (rev 01)

and the sound can be heard from guest.

For 4.8 and earlier guest, could click Applications->System Settings->Soundcard Detection-> Play test sound

4.7 no error output
Notes:
Comments:

		177040 	[Installation - net] Install guest from http 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P1 	5130 	Edit
Setup:

- Host with network connected
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    installation
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. download the initrd.img and vmlinuz from os tree, and put into '/var/lib/libvirt/boot'

** NB: Pls use the latest os tree instead of the following url.

# wget http://download.englab.nay.redhat.com/pub/rhel/rel-eng/RHEL6.0-20100608.2/6.0/Server/x86_64/os/images/pxeboot/initrd.img

# wget http://download.englab.nay.redhat.com/pub/rhel/rel-eng/RHEL6.0-20100608.2/6.0/Server/x86_64/os/images/pxeboot/vmlinuz

# cp initrd.img vmlinuz /var/lib/libvirt/boot/

# qemu-img create /var/lib/libvirt/images/http_test.img 8G

2. # virsh define http_test.xml

<domain type='kvm'>
  <name>http_test</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <kernel>/var/lib/libvirt/boot/vmlinuz</kernel>
    <initrd>/var/lib/libvirt/boot/initrd.img</initrd>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/http_test.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <interface type='network'>
      <source network='default'/>
      <model type='rtl8139'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/2'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5902' autoport='yes' keymap='en-us'/>
    <sound model='es1370'>
      <alias name='sound0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c443,c777</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c443,c777</imagelabel>
  </seclabel>
</domain>

3. # virsh start http_test

4.  User virt-viewer(do not run backgroud) to view guest graphic, and continue wtih installtion step by step.

# virt-viewer http_test

Check the ouput in terminal during installation

Choose 'URL' in step of 'Installation Method'

input url: http://download.englab.nay.redhat.com/pub/rhel/nightly/RHEL6.0-20100608.n.2/6.0/Server/x86_64/os

Make sure install graphic console for guest

5. After the guest boot up after installation, do the following check in guest

(if the guest does not boot up successfully, pls check the log /var/log/messages)

5.1 # ping ${host_ip}

5.2 #mkdir testfolder

    # touch testfolder/testfile

   # vim /testfolder/testfile

  # cat /testfolder/testfile

  # cp testfolder testfolder.bak

# dd if=/dev/zero of=./write_test bs-1024 count=10240

# ll -h write_test

5.3 # firefox

5.4 # cat /proc/meminfo | grep MemTotal

5.5 # cat /pro/cpuinfo

5.6 # lspci

5.7 check the log for a running guest

# cat /var/log/libvirt/qemu/http_test.log
	
Expected Results:

4. The guest could be installed successfully

no error output

5.1 Should ping to host successfully

5.2 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

5.3 firefox application should be launched

5.4 The output value should be nearly equal to the following value given in xml confile file

  <memory>1048576</memory>

5.5  vcpu number should be equal to the following value given in xml config file

  <vcpu>1</vcpu>

5.6 net device model should be rtl8139

00:04.0 Ethernet controller : Reaoltek Semiconductor Co., Ltd, RTL-8139/8139C/8139C+

sound device model should be es1370

00:05.0 Multimedia audio controller: Ensoniq ES1370 [AudioPCI]

5.7 no error output
Notes:
Comments:

		177041 	[Installation - net] Install guest from nfs - vnc - ac97 	yoyzhang 	yoyzhang 	Manual 		acceptance 	P1 	5140 	Edit
Setup:

- Host should be configed with br0 ( according to bug 615144, this test case fails on nat host)
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    installation

bug:

    No bug found

Actions:

1. download the initrd.img and vmlinuz from os tree, and put into '/var/lib/libvirt/boot'

** NB: Pls use the latest os tree instead of the following url.

# wget http://download.englab.nay.redhat.com/pub/rhel/rel-eng/RHEL6.0-20100608.2/6.0/Server/x86_64/os/images/pxeboot/initrd.img

# wget http://download.englab.nay.redhat.com/pub/rhel/rel-eng/RHEL6.0-20100608.2/6.0/Server/x86_64/os/images/pxeboot/vmlinuz

# cp initrd.img vmlinuz /var/lib/libvirt/boot/

# qemu-img create /var/lib/libvirt/images/nfs_test.img 8G

2. # virsh define nfs_test.xml

<domain type='kvm'>
  <name>nfs_test</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <kernel>/var/lib/libvirt/boot/vmlinuz</kernel>
    <initrd>/var/lib/libvirt/boot/initrd.img</initrd>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/nfs_test.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <interface type='bridge'>
      <source bridge='br0'/>
      <model type='e1000'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <console type='pty' tty='/dev/pts/2'>
      <source path='/dev/pts/2'/>
      <target port='0'/>
      <alias name='serial0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5902' autoport='yes' keymap='en-us'/>
    <sound model='ac97'>
      <alias name='sound0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c443,c777</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c443,c777</imagelabel>
  </seclabel>
</domain>

3. # virsh start nfs_test

4.  User virt-viewer(do not run backgroud) to view guest graphic, and continue wtih installtion step by step.

# virt-viewer nfs_test

Check the ouput in terminal during installation

Choose 'NFS directory' in step of 'Installation Method'

input nfs server name : nfs.englab.nay.redhat.com

input directory : /pub/rhel/nightly/RHEL6.0-20100608.n.2/6/Server/x86_64/os

Make sure install graphic console for guest

5. After the guest boot up after installation, do the following check in guest

(if the guest does not boot up successfully, pls check the log /var/log/messages)

5.1 # ping ${host_ip}

5.2 #mkdir testfolder

    # touch testfolder/testfile

   # vim /testfolder/testfile

  # cat /testfolder/testfile

  # cp testfolder testfolder.bak

# dd if=/dev/zero of=./write_test bs-1024 count=10240

# ll -h write_test

5.3 # firefox

5.4 # cat /proc/meminfo | grep MemTotal

5.5 # cat /pro/cpuinfo

5.6 # lspci

5.7 check the log for a running guest

# cat /var/log/libvirt/qemu/nfs_test.log
	
Expected Results:

4. The guest could be installed successfully

no error output

5.1 Should ping to host successfully

5.2 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

5.3 firefox application should be launched

5.4 The output value should be nearly equal to the following value given in xml confile file

  <memory>1048576</memory>

5.5  vcpu number should be equal to the following value given in xml config file

  <vcpu>1</vcpu>

5.6 net device model should be e1000

00:04.0 Ethernet controller : Intel Corporation 82540EM Gigabit Ethernet Controller

sound device model should be ac97

00:05.0 Multimedia audio controller: Intel Corporation 82801AA AC'97 Audio Controller (rev 01)

5.7 no error output
Notes:
Comments:

		177042 	[Installation - pxe] Install guest from pxe 	yoyzhang 	None 	Auto 		Feature 	P1 	5150 	Edit
Setup:

1. Create a virtual network as guided in https://tcms.engineering.redhat.com/case/50299/?from_plan=1950
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    installation
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Install a guest from pxe

You can install via virt-manager or other methods, just make sure the network is the bridge which you create.

2. 

After the guest boot up after installation, do the following check in guest

(if the guest does not boot up successfully, pls check the log /var/log/messages)

2.1 # ping ${host_ip}

2.2 #mkdir testfolder

    # touch testfolder/testfile

   # vim /testfolder/testfile

  # cat /testfolder/testfile

  # cp testfolder testfolder.bak

# dd if=/dev/zero of=./write_test bs-1024 count=10240

# ll -h write_test

2.3 # firefox

2.4 # cat /proc/meminfo | grep MemTotal

2.5 # cat /pro/cpuinfo

2.6 # lspci

5.7 check the log for a running guest

# cat /var/log/libvirt/qemu/http_test.log
	
Expected Results:

2.1 Should ping to host successfully

2.2 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

2.3 firefox application should be launched

2.4 The output value should be nearly equal to the following value given in xml confile file

  <memory>1048576</memory>

2.5  vcpu number should be equal to the following value given in xml config file

  <vcpu>1</vcpu>

2.6 net device model should be rtl8139

00:04.0 Ethernet controller : Reaoltek Semiconductor Co., Ltd, RTL-8139/8139C/8139C+

(note: after bug 834812 fix, the default net device modle should be e1000)

sound device model should be es1370

00:05.0 Multimedia audio controller: Ensoniq ES1370 [AudioPCI]

2.7 no error output
Notes:
Comments:

		177054 	[interface hotplug] [6.2 FEAT] libvirt: Enable 'passthru' mode for direct interfaces - bug 693839 	gsun 	gsun 	Manual 		--default-- 	P1 	5160 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp


Support for a new mode 'passthru' to the domain XML

  <interface type='direct'>
      <mac address='54:52:20:74:60:01'/>
      <source dev='eth10' mode='passthru'/>
      <model type='virtio'/>

 Using a SR-IOV card to do this.

1. # lspci |grep 82576
03:00.0 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
03:00.1 Ethernet controller: Intel Corporation 82576 Gigabit Network Connection
(rev 01)
03:10.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.1 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.2 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)
03:10.3 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

# virsh nodedev-list --tree
...
+- pci_0000_00_01_0
  |   |
  |   +- pci_0000_03_00_0
  |   |   |
  |   |   +- net_eth0_00_1b_21_39_8b_18
  |   |     
  |   +- pci_0000_03_00_1
  |   |   |
  |   |   +- net_eth1_00_1b_21_39_8b_19
  |   |     
  |   +- pci_0000_03_10_0
  |   |   |
  |   |   +- net_eth3_66_63_43_58_c1_89
  |   |     
  |   +- pci_0000_03_10_1
  |   |   |
  |   |   +- net_eth4_2e_fe_96_10_51_91
  |   |     
  |   +- pci_0000_03_10_2
  |   |   |
  |   |   +- net_eth18_26_f3_15_91_63_b8
  |   |     
  |   +- pci_0000_03_10_3
  |       |
  |       +- net_eth10_32_f4_90_ed_62_c7

...

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual networks

bug:

    No bug found

Actions:

1. Select a interface (etc. eth18) to passthrough for the guest

# ifconfig eth18
eth18     Link encap:Ethernet  HWaddr 26:F3:15:91:63:B8  
          inet addr:10.66.4.172  Bcast:10.66.7.255  Mask:255.255.252.0
          inet6 addr: fe80::24f3:15ff:fe91:63b8/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:402 errors:0 dropped:0 overruns:0 frame:0
          TX packets:18 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:57895 (56.5 KiB)  TX bytes:5074 (4.9 KiB)

# ping www.google.com -I eth18
PING www.l.google.com (74.125.93.105) from 10.66.4.172 eth18: 56(84) bytes of
data.
64 bytes from qw-in-f105.1e100.net (74.125.93.105): icmp_seq=1 ttl=45 time=263
ms


# virsh list --all
 Id Name                 State
----------------------------------
  - rhel-x86_64          shut off

2. Prepare the following vf xml 
    <interface type='direct'>
      <mac address='26:F3:15:91:63:B8'/>
      <source dev='eth18' mode='passthrough'/>
      <model type='virtio'/>
   </interface>

3. Edit the guest xml and add the vf xml above to passthrough, then save guest
xml.
# virsh edit rhel-x86_64
Domain rhel-x86_64 XML configuration edited.

# virsh dumpxml rhel-x86_64
....
    <interface type='network'>
      <mac address='52:54:00:a8:6f:0b'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
function='0x0'/>
    </interface>
    <interface type='direct'>
      <mac address='26:f3:15:91:63:b8'/>
      <source dev='eth18' mode='passthrough'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </interface>

...

4. Start the guest and check 

# virsh start rhel-x86_64
error: Failed to start domain rhel-x86_64
error: error creating macvtap type of interface: Invalid argument

	
Expected Results:

4.

error: Failed to start domain rhel-x86_64
error: error creating macvtap type of interface: Invalid argument

This is not expected, the guest should be started and the interface works fine in the guest.

 
Notes:
Comments:

		177056 	[interface hotplug] [RHEL6-Beta] 'virsh attach-interface' succeeds even if a nonexistent script file is specified to the option --script - bug 638633 	gsun 	gsun 	Manual 		Regression 	P1 	5170 	Edit
Setup:

Prepare a guest

# virsh dumpxml test

...

    <interface type='network'>
      <mac address='52:54:00:69:43:ea'/>
      <source network='default'/>
      <target dev='vnet3'/>
      <alias name='net1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </interface>

...
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Interface hotplug
    Regression
    virtual networks

bug:

    No bug found

Actions:

1. attach a interface with --script option, --script follow an non-existing file

# virsh attach-interface rhel6u2 --type ethernet virbr0 --script xyz
error: No support for ethernet in command 'attach-interface'

2. redo with bridge interface

# virsh attach-interface rhel6u2 --type bridge virbr0 --script xyz
error: Failed to attach interface
error: unsupported configuration: scripts are not supported on interfaces of
type bridge

 

	
Expected Results:

Step 1 & step 2 all will fail.
Notes:
Comments:

		177019 	[Host network interface management] Define an ethernet with local ipv6 address 	yoyzhang 	None 	Manual 		Feature 	P2 	5180 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

TBD as there is no DHCPIPv6 service machine in Raycom office

# cat ipv6-local.xml

   <interface type='ethernet' name='eth0'>

      <start mode='onboot'/>

      <protocol family='ipv6'>

      </protocol>

    </interface>ï»¿
	
Expected Results:

TBD
Notes:
Comments:

		177022 	[Host network interface management] Define an ethernet wtih both autoconf and dhcp ipv6 	yoyzhang 	None 	Manual 		Feature 	P2 	5190 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

TBD as no DHCPIPv6 service machine

   <interface type='ethernet' name='eth0'>

     <start mode='onboot'/>

     <protocol family='ipv6'>

       <autoconf/>

       <dhcp/>

     </protocol>

   </interface>

	
Expected Results:

TBD
Notes:
Comments:

		177032 	[Host network interface management] Roll back host interfaces to last known good configuration 	yoyzhang 	None 	Manual 		Feature 	P2 	5200 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management

bug:

    No bug found

Actions:

TBD as bug569567 is moved to 6.2
1. Snapshot current network configuration before change
2. Modify the network
3. Reboot the host, the host should roll back to the last known good configuration, and if it is confirmed ok, commit it.
	
Expected Results:
Notes:
Comments:

		177033 	[host network interface management] transaction-oriented API for handling host interfaces BZ 737149 	vbian 	None 	Manual 		Function 	P2 	5210 	Edit
Setup:


1. Before destroy eth0, should stop NetworkManager

  #service NetworkManager stop
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management
    rhel6.5

bug:

    No bug found

Actions:

#setenforce 0
#virsh iface-list
Name                 State      MAC Address
--------------------------------------------
eth0                 active     1c:6f:65:06:ba:42
lo                   active     00:00:00:00:00:00
#virsh iface-begin
Network config change transaction started
#virsh iface-destroy eth0
Interface eth0 destroyed
#virsh iface-undefine eth0
Interface eth0 undefine
# virsh iface-list
Name                 State      MAC Address
--------------------------------------------
lo                   active     00:00:00:00:00:00d
# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 1C:6F:65:06:BA:42  
          UP BROADCAST RUNNING PROMISC SLAVE MULTICAST  MTU:1500  Metric:1
          RX packets:483557 errors:0 dropped:0 overruns:0 frame:0
          TX packets:137066 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:447740691 (426.9 MiB)  TX bytes:63361669 (60.4 MiB)
          Interrupt:16 Memory:fc300000-fc320000 
the eth0 removed
# virsh iface-rollback
Network config change transaction rolled back
#virsh iface-list
Name                 State      MAC Address
--------------------------------------------
eth0                 active     1c:6f:65:06:ba:42
lo                   active     00:00:00:00:00:00
#ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 1C:6F:65:06:BA:42  
          inet addr:10.66.6.166  Bcast:10.66.7.255  Mask:255.255.252.0
          UP BROADCAST RUNNING PROMISC SLAVE MULTICAST  MTU:1500  Metric:1
          RX packets:484311 errors:0 dropped:0 overruns:0 frame:0
          TX packets:137173 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:447817510 (427.0 MiB)  TX bytes:63376608 (60.4 MiB)
          Interrupt:16 Memory:fc300000-fc320000 

the interface rollback to snapshot config.
verify begin-commit-rollback transaction:
run command line:
#setenforce 0
#virsh iface-list
Name                 State      MAC Address
--------------------------------------------
eth0                 active     1c:6f:65:06:ba:42
lo                   active     00:00:00:00:00:00
#virsh iface-begin
Network config change transaction started
#virsh iface-destroy eth0
Interface eth0 destroyed
#virsh iface-undefine eth0
Interface eth0 undefine
# virsh iface-list
Name                 State      MAC Address
--------------------------------------------
lo                   active     00:00:00:00:00:00d
# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 1C:6F:65:06:BA:42  
          UP BROADCAST RUNNING PROMISC SLAVE MULTICAST  MTU:1500  Metric:1
          RX packets:483557 errors:0 dropped:0 overruns:0 frame:0
          TX packets:137066 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:447740691 (426.9 MiB)  TX bytes:63361669 (60.4 MiB)
          Interrupt:16 Memory:fc300000-fc320000 
the eth0 removed
#virsh iface-commit
Network config change transaction committed
# virsh iface-rollback
error: Failed to rollback network config change transaction
error: Requested operation is not valid: failed to rollback transaction:
Operation invalid in this state - Running '/etc/rc.d/init.d/netcf-transaction
change-rollback' operation is invalid in this state
# virsh iface-list
Name                 State      MAC Address
--------------------------------------------
lo                   active     00:00:00:0
# ifconfig eth0
eth0      Link encap:Ethernet  HWaddr 1C:6F:65:06:BA:42  
          UP BROADCAST RUNNING PROMISC SLAVE MULTICAST  MTU:1500  Metric:1
          RX packets:485369 errors:0 dropped:0 overruns:0 frame:0
          TX packets:137466 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:447908150 (427.1 MiB)  TX bytes:63402346 (60.4 MiB)
          Interrupt:16 Memory:fc300000-fc320000 
0:00:00

interface change commit,and can't rollback

	
Expected Results:

Bug 737149 - RFE: [netcf] restore live interface state during config transaction rollbackï»¿ isn't fixed. So the case NEEDUPDATE
Notes:
Comments:

		176993 	[Hooks] Check audit log records kvm related operation 	xhu 	xhu 	Manual 		Function 	P2 	5250 	Edit
Setup:

1 Set selinux to enforing status


# setenforce 1
# getenforce
Enforcing

2 enable audit

# service auditd start
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    hooks

bug:

    No bug found

Actions:

Do the following operations one by one

* Start/Stop of VM


* All disk and network devices assigned at VM start


* All disk and network hotplug and unplug operations
   attach disk: https://tcms.engineering.redhat.com/case/56983/?from_plan=1950

* All disk CDROM/floppy media change operations

  Use virt-manager to add CDROM/floppy device to domain, then restart the domain. Disconnect the file in the CDROM/floppy, then grep the file from the log.

  # grep /var/lib/libvirt/images/foo.img /var/log/audit/audit.log| grep update

* Change in memory, number of VCPUs

  Use virt-manager to modify domain memory and vcpu number, check the log.

  # grep update /var/log/audit/audit.log |grep mem

  # grep update /var/log/audit/audit.log |grep vcpu

* Audit record for the generation of the MCS label tying it to the VM name.

* Cgroup ACLs
1.   Make sure cgconfig is enabled
   # service cgconfig start

make sure there is a defined guest

2. # mkdir -p /cgroup/devices/libvirt/qemu/$guest_name

If guest is running, destroy it first.

# virsh destroy $guest_name

3. # virsh start $guest_name

4. # grep cgroup /var/log/audit/audit.log

* Macvtap interface
   https://tcms.engineering.redhat.com/case/89925/

   # grep macvtap /var/log/audit/audit.log
	
Expected Results:

Audit log file (/var/log/audit/audit.log) should record all the operations

Destroy:

type=VIRT_CONTROL msg=audit(1311735909.577:2226): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='op=stop reason=destroyed vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'

Start:

type=VIRT_MACHINE_ID msg=audit(1311735914.739:2227): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a vm-ctx=system_u:system_r:svirt_t:s0:c431,c871 img-ctx=system_u:object_r:svirt_image_t:s0:c431,c871: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=ANOM_PROMISCUOUS msg=audit(1311735914.840:2228): dev=vnet0 prom=256 old_prom=0 auid=500 uid=0 gid=0 ses=1
type=SYSCALL msg=audit(1311735914.840:2228): arch=c000003e syscall=16 success=yes exit=0 a0=18 a1=89a2 a2=7fb86f3b6860 a3=7 items=0 ppid=1 pid=5355 auid=500 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=1 comm="libvirtd" exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 key=(null)
type=VIRT_RESOURCE msg=audit(1311735914.841:2229): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=net reason=open vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a net='52:54:00:8D:57:CA' path="/dev/net/tun" rdev=0A:C8: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=ANOM_PROMISCUOUS msg=audit(1311735914.842:2230): dev=vnet1 prom=256 old_prom=0 auid=500 uid=0 gid=0 ses=1
type=SYSCALL msg=audit(1311735914.842:2230): arch=c000003e syscall=16 success=yes exit=0 a0=18 a1=89a2 a2=7fb86f3b6860 a3=7 items=0 ppid=1 pid=5355 auid=500 uid=0 gid=0 euid=0 suid=0 fsuid=0 egid=0 sgid=0 fsgid=0 tty=(none) ses=1 comm="libvirtd" exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 key=(null)
type=VIRT_RESOURCE msg=audit(1311735914.843:2231): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=net reason=open vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a net='52:54:00:43:DD:E2' path="/dev/net/tun" rdev=0A:C8: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=AVC msg=audit(1311735914.914:2232): avc:  denied  { getattr } for  pid=18018 comm="qemu-kvm" name="/" dev=sda3 ino=2 scontext=system_u:system_r:svirt_t:s0:c431,c871 tcontext=system_u:object_r:fs_t:s0 tclass=filesystem
type=SYSCALL msg=audit(1311735914.914:2232): arch=c000003e syscall=138 success=no exit=-13 a0=9 a1=7fff13d3a150 a2=3 a3=48 items=0 ppid=1 pid=18018 auid=500 uid=107 gid=107 euid=107 suid=107 fsuid=107 egid=107 sgid=107 fsgid=107 tty=(none) ses=1 comm="qemu-kvm" exe="/usr/libexec/qemu-kvm" subj=system_u:system_r:svirt_t:s0:c431,c871 key=(null)
type=VIRT_RESOURCE msg=audit(1311735915.107:2233): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=disk reason=start vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-disk="?" new-disk="/var/lib/libvirt/images/aaa.img": exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=VIRT_RESOURCE msg=audit(1311735915.107:2234): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=disk reason=start vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-disk="?" new-disk="/dev/sr0": exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=VIRT_RESOURCE msg=audit(1311735915.107:2235): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=net reason=start vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-net='?' new-net='52:54:00:8D:57:CA': exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=VIRT_RESOURCE msg=audit(1311735915.107:2236): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=net reason=start vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-net='?' new-net='52:54:00:43:DD:E2': exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=VIRT_RESOURCE msg=audit(1311735915.107:2237): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=mem reason=start vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-mem=0 new-mem=1048576: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=VIRT_RESOURCE msg=audit(1311735915.107:2238): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=vcpu reason=start vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-vcpu=0 new-vcpu=1: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'
type=VIRT_CONTROL msg=audit(1311735915.107:2239): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='op=start reason=booted vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'

Disk hotplug:

type=VIRT_RESOURCE msg=audit(1311759577.906:2896): user pid=13176 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=disk reason=attach vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-disk="?" new-disk="/var/lib/libvirt/images/floopy.img": exe="/usr/sbin/libvirtd" hostname=? addr=? terminal=? res=success'

Disk unhotplug:

type=VIRT_RESOURCE msg=audit(1312904826.800:352): user pid=4615 uid=0 auid=0 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='virt=kvm resrc=disk reason=detach vm="rhel6" uuid=d39ce525-fb33-3a6a-8f03-ded842370fad old-disk="/var/lib/libvirt/images/test.img" new-disk="?": exe="/usr/sbin/libvirtd" hostname=? addr=? terminal=? res=success'

Network hotplug:

type=VIRT_RESOURCE msg=audit(1311735741.371:2204): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=net reason=attach vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-net='?' new-net='52:54:00:43:DD:E2': exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'

Network unhotplug:

type=VIRT_RESOURCE msg=audit(1312904985.187:358): user pid=4615 uid=0 auid=0 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='virt=kvm resrc=net reason=detach vm="rhel6" uuid=d39ce525-fb33-3a6a-8f03-ded842370fad old-net='52:54:00:1B:6F:E9' new-net='?': exe="/usr/sbin/libvirtd" hostname=? addr=? terminal=? res=success'

CDROM:

type=VIRT_RESOURCE msg=audit(1311735774.617:2205): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=disk reason=update vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-disk="/dev/sr0" new-disk="?": exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'

Floppy:

type=VIRT_RESOURCE msg=audit(1311758142.254:2782): user pid=13176 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=disk reason=update vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-disk="/var/lib/libvirt/images/bb.iso" new-disk="?": exe="/usr/sbin/libvirtd" hostname=? addr=? terminal=? res=success'

 

Memory:

type=VIRT_RESOURCE msg=audit(1311735879.704:2223): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=mem reason=update vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-mem=2097152 new-mem=1048576: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'

VCPU:

type=VIRT_RESOURCE msg=audit(1311735946.799:2240): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=vcpu reason=update vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a old-vcpu=1 new-vcpu=2: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'

MCS label:

subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023

cgroup:

type=VIRT_RESOURCE msg=audit(1311302127.073:76707): user pid=9502 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=cgroup reason=allow vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a cgroup="/cgroup/devices/libvirt/qemu/aaa/" class="path" path=/dev/random rdev=01:08 acl=rw: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'

macvtap:

type=VIRT_RESOURCE msg=audit(1311738533.691:2300): user pid=5352 uid=0 auid=500 ses=1 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=net reason=open vm="aaa" uuid=ad0e5e2f-1d03-d918-5c33-b39bd8c7a20a net='54:52:00:54:9E:F4' path="macvtap0" rdev=?: exe=2F7573722F7362696E2F6C69627669727464202864656C6574656429 hostname=? addr=? terminal=? res=success'

Notes:
Comments:

		176994 	[Hooks] daemon restart - return non-zero 	jialiu 	None 	Auto 		--default-- 	P2 	5260 	Edit
Setup:

If log file - /tmp/daemon.log is existing before doing this case, please delete the log file before running the following steps.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. create /etc/libvirt/hooks/daemon.

   # cat /etc/libvirt/hooks/daemon
   #! /bin/bash
   echo "$0" "$@" >> /tmp/daemon.log
   exit 1

2. # chmod +x /etc/libvirt/hooks/daemon

3. restart libvirtd

   # service libvirtd restart

4. check /tmp/daemon.log.

# cat /tmp/daemon.log
	
Expected Results:

/tmp/daemon.log is created, and the contents will be:
/etc/libvirt/hooks/daemon - shutdown - shutdown
/etc/libvirt/hooks/daemon - start - start
Notes:
Comments:

		176995 	[Hooks] daemon restart - return zero 	jialiu 	None 	Auto 		--default-- 	P1 	5270 	Edit
Setup:

If log file - /tmp/daemon.log is existing before doing this case, please delete the log file before running the following steps.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:


1. create /etc/libvirt/hooks/daemon.

   # cat /etc/libvirt/hooks/daemon
   #! /bin/bash
   echo "$0" "$@" >> /tmp/daemon.log
   exit 0

2. # chmod +x /etc/libvirt/hooks/daemon

3. restart libvirtd

   # service libvirtd restart


4. Check log file:

# cat /tmp/daemon.log
	
Expected Results:

/tmp/daemon.log is created, and the contents will be:
/etc/libvirt/hooks/daemon - shutdown - shutdown
/etc/libvirt/hooks/daemon - start - start
Notes:
Comments:

		200827 	[Hooks] kill daemon with SIGHUP 	gsun 	gsun 	Manual (Autoproposed) 		--default-- 	P1 	5270 	Edit
Setup:

If log file - /tmp/daemon.log is existing before doing this case, please delete the log file before running the following steps.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks

bug:

    No bug found

Actions:


1. create /etc/libvirt/hooks/daemon.

   # cat /etc/libvirt/hooks/daemon
   #! /bin/bash
   echo "$0" "$@" >> /tmp/daemon.log
   exit 0

2. # chmod +x /etc/libvirt/hooks/daemon

3. kill libvirtd with SIGHUP

# ps aux|grep libvirtd
root     27897  1.6  0.1 509020  7260 ?        Sl   14:40   0:00 libvirtd --daemon

# kill -SIGHUP 27897

4. Check log file:

# cat /tmp/daemon.log

5. modify script with return non-zero

   # cat /etc/libvirt/hooks/daemon
   #! /bin/bash
   echo "$0" "$@" >> /tmp/daemon.log
   exit 1

# service libvirtd restart

Redo steps 3
	
Expected Results:

4.

/tmp/daemon.log is created, and the contents will be:
/etc/libvirt/hooks/daemon - reload begin SIGHUP

5.

/etc/libvirt/hooks/daemon - shutdown - shutdown
/etc/libvirt/hooks/daemon - start - start
/etc/libvirt/hooks/daemon - reload begin SIGHUP
Notes:
Comments:

		176996 	[Hooks] daemon start - return non-zero 	jialiu 	None 	Auto 		--default-- 	P2 	5280 	Edit
Setup:

If log file - /tmp/daemon.log is existing before doing this case, please delete the log file before running the following steps.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:



1. stop libvirtd if it's runnning
   # service libvirtd stop

2. create /etc/libvirt/hooks/daemon.

   # cat /etc/libvirt/hooks/daemon
   echo "$0" "$@" >> /tmp/daemon.log
   exit 1

3. # chmod +x /etc/libvirt/hooks/daemon

4. start libvirtd

   # service libvirtd start

5. Check log file:

#  cat /tmp/daemon.log
	
Expected Results:


  /tmp/daemon.log is created, and the contents will be:
 /etc/libvirt/hooks/daemon - start - start
Notes:
Comments:

		176997 	[Hooks] daemon start - return zero 	xhu 	None 	Manual (Autoproposed) 		Feature 	P1 	5290 	Edit
Setup:

If log file - /tmp/daemon.log is existing before doing this case, please delete the log file before running the following steps.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    hooks

bug:

    No bug found

Actions:

1. stop libvirtd if it's runnning
   # service libvirtd stop

2. create /etc/libvirt/hooks/daemon.

   # cat /etc/libvirt/hooks/daemon

   #!/bin/bash
   echo "$0" "$@" >> /tmp/daemon.log
   exit 0

3. # chmod +x /etc/libvirt/hooks/daemon

4. start libvirtd

   # service libvirtd start

5. Check log file:

#  cat /tmp/daemon.log
	
Expected Results:

  /tmp/daemon.log is created, and the contents will be:
 /etc/libvirt/hooks/daemon - start - start
Notes:
Comments:

		176998 	[Hooks] daemon stop - return non-zero 	jialiu 	None 	Auto 		--default-- 	P2 	5300 	Edit
Setup:

If log file - /tmp/daemon.log is existing before doing this case, please delete the log file before running the following steps.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:



1. ensure libvirtd is running

2. create /etc/libvirt/hooks/daemon.

   # cat /etc/libvirt/hooks/daemon
   echo "$0" "$@" >> /tmp/daemon.log
   exit 1

3. # chmod +x /etc/libvirt/hooks/daemon

4. stop libvirtd

   # service libvirtd stop

5. Check log file:

# cat /tmp/daemon.log
	
Expected Results:

/tmp/daemon.log is created, and the contents will be:
/etc/libvirt/hooks/daemon - shutdown - shutdown
Notes:
Comments:

		176999 	[Hooks] daemon stop - return zero 	xhu 	None 	Manual (Autoproposed) 		Feature 	P1 	5310 	Edit
Setup:

If log file - /tmp/daemon.log is existing before doing this case, please delete the log file before running the following steps.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    hooks

bug:

    No bug found

Actions:


1. ensure libvirtd is running

2. create /etc/libvirt/hooks/daemon.

   # cat /etc/libvirt/hooks/daemon
   #!/bin/bash
   echo "$0" "$@" >> /tmp/daemon.log
   exit 0

3. # chmod +x /etc/libvirt/hooks/daemon

4. stop libvirtd

   # service libvirtd stop

5. Check log file:

# cat /tmp/daemon.log
	
Expected Results:

/tmp/daemon.log is created, and the contents will be:
/etc/libvirt/hooks/daemon - shutdown - shutdown
Notes:
Comments:

		177002 	[Hooks] qemu - return non-zero 	jialiu 	None 	Auto 		--default-- 	P1 	5320 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:



1. make sure you have a running domain, suppose it's name is "toy"

2. create /etc/libvirt/hooks/qemu

   #mkdir /etc/libvirt/hooks
    # cat /etc/libvirt/hooks/qemu
   #! /bin/bash
   if [[ $1 = "toy" ]]; then
       echo "$0" "$@" >> /tmp/qemu.log
       exit 1
   fi

  # chmod +x /etc/libvirt/hooks/qemu

3. restart libvirtd


4. then start it
   # virsh start toy

5. Check log file - /tmp/qemu.log

# cat /tmp/qemu.log
	
Expected Results:

step 3:

Since 0.9.13, the script is called when restart, if script fail, the domain fail.

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]


# cat /tmp/qemu.log

/etc/libvirt/hooks/qemu toy reconnect begin -
/etc/libvirt/hooks/qemu toy stopped end -
/etc/libvirt/hooks/qemu toy release end -

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     toy                               shut off

step 4:

"start" is aborted with error like following:

# virsh start toy
error: Failed to start domain kvm-rhel6u3-i386
error: Hook script execution failed: Hook script /etc/libvirt/hooks/qemu qemu failed with error code 256

step 5:

for start, /tmp/qemu.log" is created with contents:
/etc/libvirt/hooks/qemu toy prepare begin -
/etc/libvirt/hooks/qemu toy stopped end -
/etc/libvirt/hooks/qemu toy release end -

 
Notes:
Comments:

		177003 	[Hooks] qemu - return zero 	jialiu 	None 	Auto 		--default-- 	P1 	5330 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:


1. make sure you have a domain, suppose it's name is "toy"

2. create /etc/libvirt/hooks/qemu

   #mkdir /etc/libvirt/hooks
    # cat /etc/libvirt/hooks/qemu
   #! /bin/bash
   if [[ $1 = "toy" ]]; then
       echo "$0" "$@" >> /tmp/qemu.log
       exit 0
   fi
# chmod +x /etc/libvirt/hooks/qemu
3. restart libvirtd

4. if it's not in "shutoff" state, "shutdown" it.
   # virsh shutdown toy

   if it's in shutoff state, start it
   # virsh start toy
	
Expected Results:

3.

# cat /tmp/qemu.log

/etc/libvirt/hooks/qemu toy reconnect begin -

4.


for "shutdown",

after "toy" is fully shutdown, "/tmp/qemu.log" will be created with contents as following:
[root@dhcp-66-65-163 hooks]# cat /tmp/qemu.log
/etc/libvirt/hooks/qemu toy stopped end -
/etc/libvirt/hooks/qemu toy release end -

for "start":
[root@dhcp-66-65-163 hooks]# cat /tmp/qemu.log

/etc/libvirt/hooks/qemu toy prepare begin -
/etc/libvirt/hooks/qemu toy start begin -
/etc/libvirt/hooks/qemu toy started begin -

Notes:
Comments:

		185916 	[Hooks] lxc - return zero 	gsun 	gsun 	Manual (Autoproposed) 		--default-- 	P1 	5330 	Edit
Setup:

prepare a lxc domain

# virsh -c lxc:///
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id    Name                           State
----------------------------------------------------
 -     toy                            shut off

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks
    LXC

bug:

    842979 - From Run 44731

Actions:


1. make sure you have a domain, suppose it's name is "toy"

2. create /etc/libvirt/hooks/lxc
   # cat /etc/libvirt/hooks/lxc
   #! /bin/bash
   echo "$0" "$@" >> /tmp/lxc.log
   exit 0

3. restart libvirtd

4. if it's not in "shutoff" state, "destroy" it.
   # virsh -c lxc:/// destroy toy

   if it's in shutoff state, start it
   # virsh -c lxc:/// start toy
	
Expected Results:

4.


for "destroy",

# cat /tmp/lxc.log
/etc/libvirt/hooks/lxc toy stopped end -
/etc/libvirt/hooks/lxc toy release end -

for "start":

# cat /tmp/lxc.log
/etc/libvirt/hooks/lxc toy prepare begin -
/etc/libvirt/hooks/lxc toy start begin -
/etc/libvirt/hooks/lxc toy started begin -

 

If the lxc container start failed, then the lxc.log should be :

# cat /tmp/lxc.log

/etc/libvirt/hooks/lxc toy prepare begin -
/etc/libvirt/hooks/lxc toy start begin -
/etc/libvirt/hooks/lxc toy stopped end -
/etc/libvirt/hooks/lxc toy release end -

Notes:
Comments:

		185917 	[Hooks] lxc - return non-zero 	gsun 	gsun 	Manual (Autoproposed) 		--default-- 	P1 	5330 	Edit
Setup:

prepare a lxc domain

# virsh -c lxc:///
Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # list --all
 Id    Name                           State
----------------------------------------------------
 -     toy                            shut off

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks
    LXC

bug:

    842979 - From Run 44731

Actions:


1. make sure you have a domain, suppose it's name is "toy"

2. create /etc/libvirt/hooks/lxc
   # cat /etc/libvirt/hooks/lxc
   #! /bin/bash
   echo "$0" "$@" >> /tmp/lxc.log
   exit 1

3. restart libvirtd

4.start the guest

# virsh -c lxc:/// start toy
error: Failed to start domain toy
error: Hook script execution failed: Hook script /etc/libvirt/hooks/lxc lxc failed with error code 256

	
Expected Results:

4.

it will report the error ,this is the expected result

when do start domain it will fail and in lxc.log:

# cat /tmp/lxc.log
/etc/libvirt/hooks/lxc toy prepare begin -


 
Notes:
Comments:

		200240 	[Hooks] Libvirt is missing important hooks - bug 825820 	gsun 	gsun 	Manual (Autoproposed) 		--default-- 	P1 	5330 	Edit
Setup:

prepare a domain

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 -     rhel6u2                        shut off

Prepare a script
# cat /etc/libvirt/hooks/qemu
#! /bin/bash
echo "$0" "$@" >> /tmp/qemu.log
exit 0

Add x mod
# chmod +x /etc/libvirt/hooks/qemu
Then restart the libvirtd service 
# service libvirtd restart



	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks

bug:

    871273 - From Run 49502

Actions:

1. start the domain
# virsh start rhel6u2
Domain rhel6u2 started

2. restart libvirtd
# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]


3. destroy domain
# virsh destroy rhel6u2
Domain rhel6u2 destroyed

4. attach domain
# qemu-img create /var/lib/libvirt/images/foo.img 1G
Formatting '/var/lib/libvirt/images/foo.img', fmt=raw size=1073741824 

start a domain not under libvirt management:
# /usr/libexec/qemu-kvm -cdrom /var/lib/libvirt/images/foo.img -monitor unix:/tmp/demo,server,nowait -name foo -uuid cece4f9f-dff0-575d-0e8e-01fe380f12ea  &
[1] 1688

# virsh qemu-attach 1688
Domain foo attached to pid 1688

# virsh list --all
 Id    Name                           State
----------------------------------------------------
 2     foo                            running
 -     rhel6u2                        shut off

Check in log:

# cat /tmp/qemu.log 
...
/etc/libvirt/hooks/qemu foo attach begin -

 
	
Expected Results:

1.
check in log:
# cat /tmp/qemu.log
/etc/libvirt/hooks/qemu rhel6u2 prepare begin -
/etc/libvirt/hooks/qemu rhel6u2 start begin -
/etc/libvirt/hooks/qemu rhel6u2 started begin -

 2.

check in log:
...
/etc/libvirt/hooks/qemu rhel6u2 reconnect begin -

3.

 

check in log:
...
/etc/libvirt/hooks/qemu rhel6u2 stopped end -
/etc/libvirt/hooks/qemu rhel6u2 release end -

 

4.

# cat /tmp/qemu.log 
...
/etc/libvirt/hooks/qemu foo attach begin -

 

 

 
Notes:
Comments:

		220605 	[Hooks] migration with hook 	gsun 	gsun 	Manual (Autoproposed) 		--default-- 	P1 	5330 	Edit
Setup:

prepare two hosts for migration, make sure they can find each other

do following step on both machine:

1.

# setsebool virt_use_nfs on

2.

mount nfs storage on both side with -o vers=3
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    hooks

bug:

    No bug found

Actions:


1. make sure you have a domain on host1, suppose it's name is "toy"

2. create /etc/libvirt/hooks/qemu

Do this on both hosts:

   #mkdir /etc/libvirt/hooks
    # cat /etc/libvirt/hooks/qemu
   #! /bin/bash
   if [[ $1 = "toy" ]]; then
       echo "$0" "$@" >> /tmp/qemu.log
       exit 0
   fi
# chmod +x /etc/libvirt/hooks/qemu
3. restart libvirtd on both hosts

4. start the domain on host1
   # virsh start toy

5. migrate the domain from host1 to host2

# virsh migrate --live toy qemu+ssh://$host2_address/system

6. modify the hook with return non-zero

On host1:

   # vim /etc/libvirt/hooks/qemu
   #! /bin/bash
   if [[ $1 = "toy" ]]; then
       echo "$0" "$@" >> /tmp/qemu.log
       exit 1
   fi

7. migrate guest back from host2 to host1

on host2:

# virsh migrate --live toy qemu+ssh://$host1_address/system

8. modify hook

on host1 modify it back with return 0:

   # vim /etc/libvirt/hooks/qemu
   #! /bin/bash
   if [[ $1 = "toy" ]]; then
       echo "$0" "$@" >> /tmp/qemu.log
       exit 0
   fi

on host2 modify with non-zero:

   # vim /etc/libvirt/hooks/qemu
   #! /bin/bash
   if [[ $1 = "toy" ]]; then
       echo "$0" "$@" >> /tmp/qemu.log
       exit 1
   fi

9.  redo migration from host2

on host2:

# virsh migrate --live toy qemu+ssh://$host1_address/system

 
	
Expected Results:

3.

# cat /tmp/qemu.log

/etc/libvirt/hooks/qemu toy reconnect begin -

4.
for "start":
# cat /tmp/qemu.log

/etc/libvirt/hooks/qemu toy prepare begin -
/etc/libvirt/hooks/qemu toy start begin -
/etc/libvirt/hooks/qemu toy started begin -

5. migrate success

on host2:

# virsh list
 Id    Name                           State
----------------------------------------------------
 1     toy                        running

check log on both hosts

on host1:

/etc/libvirt/hooks/qemu toy stopped end -
/etc/libvirt/hooks/qemu toy release end -

on host2:

# cat /tmp/qemu.log
/etc/libvirt/hooks/qemu toy migrate begin -
/etc/libvirt/hooks/qemu toy prepare begin -
/etc/libvirt/hooks/qemu toy start begin -
/etc/libvirt/hooks/qemu toy started begin -

7.

on host2:

error: Hook script execution failed: Hook script /etc/libvirt/hooks/qemu qemu failed with error code 256

on host1:

# cat /tmp/qemu.log
/etc/libvirt/hooks/qemu toy migrate begin -

9.

on host2:

migrate success

# cat /tmp/qemu.log
/etc/libvirt/hooks/qemu toy stopped end -
/etc/libvirt/hooks/qemu toy release end -

on host1:

# virsh list
 Id    Name                           State
----------------------------------------------------
 4     toy                        running

# cat /tmp/qemu.log
/etc/libvirt/hooks/qemu toy migrate begin -
/etc/libvirt/hooks/qemu toy prepare begin -
/etc/libvirt/hooks/qemu toy start begin -
/etc/libvirt/hooks/qemu toy started begin -

Notes:
Comments:

		177007 	[host network interface management] correct work flow for undefine interface BZ 721232 	vbian 	None 	Manual 		Regression 	P2 	5340 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    host network interface management
    upstream

bug:

    No bug found

Actions:

1.make sure interface eth0 active
 # virsh iface-list --all
Name                 State      MAC Address
--------------------------------------------
eth0                 active     1c:6f:65:06:ba:42
lo                   active     00:00:00:00:00:00

2. # virsh iface-undefine eth0


3. 

 # virsh iface-list --all
Name                 State      MAC Address
--------------------------------------------
eth0                 active     1c:6f:65:06:ba:42
lo                   active     00:00:00:00:00:00

4.  # virsh iface-destroy eth0

5. # virsh undefine eth0 

#ifconfig eth0

# ls /etc/sysconfig/network-scripts/ifcfg-eth0

 

 

	
Expected Results:

2. at step 2, you will fail to undefine the interface

5. at step 5, you can undefine eth0 after destroy it, and the config file can be removed , and eth0 doesn't have IP any more
Notes:
Comments:

		177286 	[Migration] migration with 1-16 VCPUs 	gren 	yoyzhang 	Auto 		--default-- 	P2 	5340 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  prepare a list of guest virtual machines with different vcpu number on source box, the xml description is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.
Notes:
Comments:

		176941 	[Guest kernel debugging] Send Alt+SysRq+C to trigger guest kdump 	yoyzhang 	None 	Manual 		Feature 	P1 	5360 	Edit
Setup:

# yum install kexec-tools
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

The RFE bug is defered to be fixed on 6.2

1. Config system kernel to preserve small memory
   
   title Fedora (2.6.35.6-45.fc14.x86_64)
   root (hd0,5)
   kernel /boot/vmlinuz-2.6.35.6-45.fc14.x86_64 root=UUID=ef344a74-a490-4cdc-8124-7306fa039b9d ro rd_NO_LUKS rd_NO_LVM rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYTABLE=us rhgb quiet crashkernel=128M@64M
   initrd /boot/initramfs-2.6.35.6-45.fc14.x86_64.img

2. Reboot system

3. Start kdump service
 
   # chkconfig kdump on
 
   # service kdump start

   # service kdump status
     Kdump is operational

4. Config the target type
 
   # vim /etc/kdump.conf
   .......
   path /var/crash # specify path to save core dump file
   default shell # Action to default when system crashes
   .......

5. Send SysRq+Alt+C to guest
 
   # virsh sendkey $GUEST 'sysrq+alt+c'
	
Expected Results:

After step2,  check wheather reserve memory succeeds

#dmesg | less
.......
[    0.000000] Reserving 128MB of memory at 64MB for crashkernel (System RAM: 9088MB)
.......

After step3, Check capture kernel image is generated

# ll /boot/ -h
total 33M
......
-rw-r--r--. 1 root root  14M Oct  9 14:31 initramfs-2.6.32-71.el6.x86_64.img
-rw-r--r--. 1 root root 4.5M Oct  9 14:51 initrd-2.6.32-71.el6.x86_64kdump.img
-rw-r--r--. 1 root root 7.4M Oct 15 00:08 initrd.img
-rw-r--r--. 1 root root 1.8M Oct 15 00:08 vmlinuz
-rwxr-xr-x. 1 root root 3.7M Sep  1 13:48 vmlinuz-2.6.32-71.el6.x86_64
.......

After step5,

The system reboot to the 2nd kernel (capture kernel), displaying the following process in console

.......
copying 100% complete
Finished capture core copying
.......

Then system reboot automatically to the 1st kernel (of course, a good kernel). Check core dump file is saved in specified path
 
# ll /var/crash/127.0.0.1-2010-12-10-16\:15\:59/
total 87448
rw-------. 1 root root 89451575 Dec 10 16:16 vmcore
Notes:
Comments:

		176942 	[Guest kernel debugging] Send NMI to trigger guest kdump 	yoyzhang 	None 	Manual 		Feature 	P1 	5370 	Edit
Setup:

# yum install kexec-tools
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

The RFE bug is defered to be fixed on 6.2

1. Config system kernel to preserve small memory
   
   title Fedora (2.6.35.6-45.fc14.x86_64)
   root (hd0,5)
   kernel /boot/vmlinuz-2.6.35.6-45.fc14.x86_64 root=UUID=ef344a74-a490-4cdc-8124-7306fa039b9d ro rd_NO_LUKS rd_NO_LVM rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYTABLE=us rhgb quiet crashkernel=128M@64M
   initrd /boot/initramfs-2.6.35.6-45.fc14.x86_64.img

2. Reboot system

3. Start kdump service
 
   # chkconfig kdump on
 
   # service kdump start

   # service kdump status
     Kdump is operational

4. Config the target type
 
   # vim /etc/kdump.conf
   .......
   path /var/crash # specify path to save core dump file
   default shell # Action to default when system crashes
   .......

5. Send NMI to guest
 
   # virsh sendkey $GUEST 'NMI 0'
	
Expected Results:

After step2,  check wheather reserve memory succeeds

#dmesg | less
.......
[    0.000000] Reserving 128MB of memory at 64MB for crashkernel (System RAM: 9088MB)
.......

After step3, Check capture kernel image is generated

# ll /boot/ -h
total 33M
......
-rw-r--r--. 1 root root  14M Oct  9 14:31 initramfs-2.6.32-71.el6.x86_64.img
-rw-r--r--. 1 root root 4.5M Oct  9 14:51 initrd-2.6.32-71.el6.x86_64kdump.img
-rw-r--r--. 1 root root 7.4M Oct 15 00:08 initrd.img
-rw-r--r--. 1 root root 1.8M Oct 15 00:08 vmlinuz
-rwxr-xr-x. 1 root root 3.7M Sep  1 13:48 vmlinuz-2.6.32-71.el6.x86_64
.......

After step5,

The system reboot to the 2nd kernel (capture kernel), displaying the following process in console

.......
copying 100% complete
Finished capture core copying
.......

Then system reboot automatically to the 1st kernel (of course, a good kernel). Check core dump file is saved in specified path
 
# ll /var/crash/127.0.0.1-2010-12-10-16\:15\:59/
total 87448
rw-------. 1 root root 89451575 Dec 10 16:16 vmcore
Notes:
Comments:

		176944 	[Guest kernel debugging]Check the dumped images can be analyzed with crash tool sucessfully 	yupzhang 	None 	Manual 		Function 	P1 	5380 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    virt-v2v

Tag:

    No tag found

bug:

    No bug found

Actions:

1.Start a guest
 # virsh start rhel5u5

2. Dump the core of a domain 
#virsh dump rhel5u5 /tmp/dump

3. # ll -h /tmp

4. Check guest kernel version
#uname -r
2.6.18-183.el5

5. Install the same version kernel-debug package on host
# rpm -ivh kernel-debuginfo-2.6.18-183.el5.x86_64.rpm kernel-debuginfo-common-2.6.18-183.el5.x86_64.rpm
Preparing...                ########################################### [100%]
   1:kernel-debuginfo-common########################################### [ 50%]
   2:kernel-debuginfo       ########################################### [100%]

6.# crash /usr/lib/debug/lib/modules/2.6.18-183.el5/vmlinux /tmp/dump 

	
Expected Results:

1. # virsh start rhel5u5
Domain rhel5u5 started

2.#virsh dump rhel5u5 /tmp/dump

Domain rhel5u5 dumped to /tmp/dump

3.# ll -h /tmp
total 445M
-rw-r--r-- 1 root root 445M Dec 30 15:24 dump

6. Check the dumped images can be analyzed with crash tool sucessfully

# crash /usr/lib/debug/lib/modules/2.6.18-183.el5/vmlinux /tmp/dump 
crash 4.1.2-1.el5
Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009  Red Hat, Inc.
Copyright (C) 2004, 2005, 2006  IBM Corporation
Copyright (C) 1999-2006  Hewlett-Packard Co
Copyright (C) 2005, 2006  Fujitsu Limited
Copyright (C) 2006, 2007  VA Linux Systems Japan K.K.
Copyright (C) 2005  NEC Corporation
Copyright (C) 1999, 2002, 2007  Silicon Graphics, Inc.
Copyright (C) 1999, 2000, 2001, 2002  Mission Critical Linux, Inc.
This program is free software, covered by the GNU General Public License,
and you are welcome to change it and/or distribute copies of it under
certain conditions.  Enter "help copying" to see the conditions.
This program has absolutely no warranty.  Enter "help warranty" for details.

GNU gdb 6.1                                     
Copyright 2004 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB.  Type "show warranty" for details.
This GDB was configured as "x86_64-unknown-linux-gnu"...

      KERNEL: /usr/lib/debug/lib/modules/2.6.18-183.el5/vmlinux
    DUMPFILE: /tmp/dump
        CPUS: 1
        DATE: Wed Dec 30 15:24:09 2009
      UPTIME: 00:22:14
LOAD AVERAGE: 0.02, 0.05, 0.23
       TASKS: 91
    NODENAME: localhost.localdomain
     RELEASE: 2.6.18-183.el5
     VERSION: #1 SMP Mon Dec 21 18:37:42 EST 2009
     MACHINE: x86_64  (2992 Mhz)
      MEMORY: 1 GB
       PANIC: ""
         PID: 0
     COMMAND: "swapper"
        TASK: ffffffff80308b60  [THREAD_INFO: ffffffff803fa000]
         CPU: 0
       STATE: TASK_RUNNING (ACTIVE)
     WARNING: panic task not found

crash> bt
PID: 0      TASK: ffffffff80308b60  CPU: 0   COMMAND: "swapper"
 #0 [ffffffff803fbeb8] schedule at ffffffff80063f96
 #1 [ffffffff803fbec0] thread_return at ffffffff80063ff8
 #2 [ffffffff803fbf68] default_idle at ffffffff8006c3a5
 #3 [ffffffff803fbf90] cpu_idle at ffffffff800497b7
crash> 

 
Notes:
Comments:

		176946 	[Guest kernel debugging]domstate --reason for stop domain - bug 749096 	yupzhang 	None 	Manual 		Regression 	P1 	5390 	Edit
Setup:

VERIFIED Bug 749096 - Libvirt doesn't detect crashed domains correctly after restarted libvirtd
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    rhel7
    guest kernel debugging
    Regression

bug:

    No bug found

Actions:

1.#virsh start aaa

2.#service libvirtd stop

3.use nc Stop the guest.

Or kill the guest with #kill -9 $VM_PID

# nc -U /var/lib/libvirt/qemu/aaa.monitor
{"QMP": {"version": {"qemu": {"micro": 1, "minor": 12, "major": 0}, "package": "(qemu-kvm-0.12.1.2)"}, "capabilities": []}}
{"execute":"qmp_capabilities"}   ----> input
{"return": {}}                          -----> output
{"execute":"q"}                       -----> input
{"return": {}}                           ------output
{"timestamp": {"seconds": 1350534832, "microseconds": 351148}, "event": "SHUTDOWN"}       -----> output


4. # /etc/init.d/libvirtd start
Starting libvirtd daemon:                                  [  OK  ]

5.# virsh domstate aaa --reason
shut off (crashed)

	
Expected Results:
Notes:
Comments:

		177310 	[Migration] suspend migration testing 	jyang 	yoyzhang 	Auto 		Feature 	P2 	5390 	Edit
Setup:

1. A RHEL6 host with kvm, with a running domain.

2. Another kvm host, which also is a RHEL6.

3. On both source host, and target host, libvirtd is running.

4. On both source and target machine:
    #iptables -F
    # setenforce 1
    # setsebool -P virt_use_nfs 1

  make sure that the selinux is enforcing

5. Setup nfs service on Source target machine
 
    5.1 add following line into "/etc/exports"

     /var/lib/libvirt/images 10.66.70.144(rw,no_root_squash,async) 127.0.0.1(rw,no_root_squash,async)

    replace "10.66.70.144" to your destinate host IP

    5.2 service nfs start

6. Mount the nfs filesystem on source host to  both destinate host and source host.
    6.1 on destination machine
     Create /var/lib/libvirt/migrate on destinate host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     # mount -t nfs ${source_host_ip}:/var/lib/libvirt/images/  /var/lib/libvirt/migrate/

    6.2 On source machine
     Create /var/lib/libvirt/migrate on source host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     mount localhost:/var/lib/libvirt/images /var/lib/libvirt/migrate


7. After step 6, make sure your migration domain's disk image locate in "/var/lib/libvirt/migrate/" (such as :/var/lib/libvirt/migrate/migrate.img)on source machine(when you migrate the guest during installation,the iso also should be locate in "/var/lib/libvirt/migrate").
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. start domain "guest"

   # virsh start guest

 

2. wait for the domain fully started.

 

3. migrate

   # virsh migrate --suspend guest qemu+ssh://${dest_host_ip}/system  

 

4. when step 3 finished. check state of domain "migrate" on both source host and destinate host

    # virsh domstate guest

 

5. resume domain on tartget host

     # virsh resume guest

6. Destroy domain on target host

# virsh shutdown guest

7. start domain on source host

     # virsh start guest
	
Expected Results:

step 4.

       on source host. domain "guest" is shutoff

       on target host. it's paused.

 

step 5.

       can be resumed, and running normally.

 

step 7.

      can be started, and running normally
Notes:
Comments:

		176952 	[guest kernel debugging]send SysRq key strokes to guests 	yupzhang 	None 	Manual 		Function 	P1 	5400 	Edit
Setup:

 Examples:
        virsh # send-key <domain> 37 18 21
        virsh # send-key <domain> KEY_RIGHTCTRL KEY_C
        virsh # send-key <domain> --codeset xt 37 18 21
        virsh # send-key <domain> --holdtime 1000 0x15 18 0xf
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    guest kernel debugging
    Regression

bug:

    No bug found

Actions:

1.#virsh start rhel6,and open a terminal

2.# virsh send-key test 37

     virsh send-key test 18

     virsh send-key test 21

3.# virsh send-key test KEY_RIGHTSHIFT KEY_C

4.# virsh send-key test --codeset xt 16 17 18

5.# virsh send-key test --holdtime 1000 0x15

7.Test codeset

https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/keymaps%20for%20virsh%20send-key

  1). Start a Windows guest 

  2). Add the following new registory key at HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\i8042prt\Parameters  on Windows guest.
         key-name : "CrashOnCtrlScroll"
         DWORD value: 1

On my win7 guest , it looks like

                              Name                  Type              Data

Parameters   |    (Default)               REG_SZ      (value not set)

                        |    LayerDriver JPN  REG_DWORD   0x00000001(1)

                        |        snip.........

                        |      CrashOnCtrlScroll   REG_DWORD  0x0000001(1)     <-----this is added

    It should be a DWORD value

                            
    3). Restart the Windows guest.

   4).After guest start fully , test linux codeset

    #virsh send-key win2008 --codeset linux 0x61 0x46 0x46

   5). Test atset3 codeset

   #virsh send-key win2008 --codeset atset3 0x58 0x5f 0x5f

   6). Test xt_kbd codeset

   #virsh send-key win2008 --codeset xt_kbd 0x11d 0x46 0x46

   7).Test win32 codeset

   #virsh send-key win2008 --codeset win32 0xa3 0x91 0x91

   8). Test usb codeset

   #virsh send-key win2008  --codeset usb 0xe4 0x47 0x47

   9). Test rfb codeset

   #virsh send-key win2008 --codeset rfb 0x9d 0x46 0x46
	
Expected Results:

2.# virsh send-key test 37;virsh send-key test 18;virsh send-key test 21

In guest,the 'key' print.

3.# virsh send-key test KEY_RIGHTSHIFT KEY_C
In guest,the 'C' print.

4.# virsh send-key test --codeset xt 16 17 18
In guest,the 'qwe' print

5.# virsh send-key test --holdtime 1000 0x15
In guest[with Graphical console]:
[root@localhost ~]#yyyyyyyyyyyyyyy

In guest[without Graphical console]:

 [root@localhost ~]#y

7. 
4)-9). The guest should crash.

  

Notes:
Comments:

		177311 	[Migration] suspend migration with option --undefinesource 	jyang 	yoyzhang 	Auto 		Feature 	P2 	5400 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort "
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    migration
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. suspend migration with option "--undefinesource"

    # virsh migrate --suspend --undefinesource migrate qemu+ssh://${target_host_ip}/system

 

2. check domain state on both source and target host.

    # virsh list --all
	
Expected Results:

step 2:

        on source host, domain "migrate" will be undefined.

        on target host, it's paused.
Notes:
Comments:

		176955 	[virtual networks] macvtap (type='direct') 	xhu 	None 	Manual 		Feature 	P3 	5410 	Edit
Setup:

Firstly, you must run "124156 [Guest network driver] NIC virtio driver"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    virtual networks

bug:

    No bug found

Actions:

1. Starting with the guest domain definition in the previous steps,
   remove the interface definition and replace it with:
   <interface type='direct'>
     <mac address='54:52:00:54:9e:f4'/>
     <source dev='eth0' mode='bridge'/>
     <target dev='macvtap1'/>
     <model type='virtio'/>
   </interface>

2. Start the guest and verify normal connectivity. Two notes:

   a) In this mode of operation, the guest will be bridged to the
      physical network, so you will need a dhcp server running on the
      physical network.

   b) Also in this mode, the guest will NOT be able to communicate with
 the host. The dhcp server (and any other resource used for testing)
      must be on a separate machine.

3. destroy the domain, change mode from 'bridge' to 'private', start the guest

    check in  guest, the domain can ping outside but fail ping host

4. destroy the domain, change mode from 'bridge' to 'vepa', start the guest

    check in  guest, the domain can ping outside but fail ping host


5. destroy the domain, change mode from 'bridge' to 'passthrough', start the guest

    Note:
    Host will lost connection

    check in  guest, the domain can ping outside 

    
6. Perform this same test with the vhost-net module loaded and not loaded.(You can refer to "124157 [Guest network driver] NIC virtio driver with and without vhost-net ")

	
Expected Results:

2.

Guest can ping outside but fail ping host

 

3.

Guest can ping outside but fail ping host

 

4.

Guest can ping outside but fail ping host

 

 5.

Guest can ping outside but fail ping host

 

 
Notes:
Comments:

		177315 	[Migration] Upload/download a big file by FTP from RHEL/Fedora guest during migration BZ 823453 	gren 	yoyzhang 	Manual 		--default-- 	P2 	5410 	Edit
Setup:

Do case 

124944 [virtual networks] Bridged network, eth + macvtap + bridge

first to prepare virtual network on both side
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0
    regression

bug:

    851981 - From Run 44174

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  prepare a guest virtual machine on source box, the xml description is as follows:

<domain>
 <name>migratetest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/migratetest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='bridge-net'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='8192' heads='1'/>
    </video>
  </devices>
</domain>

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.

6, During the proccess of migration, upload or download a big file by guest vm .
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.
3, Ensure the migration has no effect on uploading or downloading
Notes:
Comments:

		176917 	[Graphical framebuffers] SPICE password ticketing and reset 	vbian 	None 	Manual 		Feature 	P2 	5420 	Edit
Setup:

we should use virt-viewer to connect guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1. get system UTC date
   # date -u
   for example you get Tue Jan  4 06:25:48 UTC 2011
2. create the guest with the following graphic device
...
    <graphics type='spice' port='-1' tlsPort='-1' autoport='yes' passwd='aaabbb' passwdValidTo='2011-01-04T06:28:00' listen='10.66.93.134'/>
...
    Note: please define the passwdValidTo to a time later than the date you got from step 1. Also 10.66.93.134 is host ip.

3. start the guest , and access the spice interface
   # virsh start #guest

   #virt-viewer #guest

   #remote-viewer spice://10.66.93.134:5900

 input password 'aaabbb'  

4. close the spice dialog, and reopen the spicec after the password expeired

   #virt-viewer #guest

   #remote-viewer spice://10.66.93.134:5900

  input password 'aaabbb'

 5. shutdown the guest , and reset the guest password in the xml file
   # virsh destroy $guest
   # virsh edit
   edit the spicepassword to a new one, such as 'cccddd'

NOTE: we need update the valid time too
6. start the guest , and access the spice interface
   # virsh start $guest


   #virt-viewer #guest  

   #remote-viewer spice://10.66.93.134:5900

input password 'cccddd'
	
Expected Results:

3. you could get the spice interface with the correct password inputted aaabbb
4. you CAN'T get the spice interface with the old expeired password aaabbb
6. you could get the the spice interface again with the new set password cccddd
Notes:
we also need change the valid time in step 5,it won't work only if we only change the passwd
Comments:

		177316 	[Migration] Upload/download a big file by FTP from Windows guest during migration 	gren 	yoyzhang 	Manual 		--default-- 	P2 	5420 	Edit
Setup:

Do case 

124970 [Virtual Networks] Shared physical network

first to prepare bridge network for guest

 

It is same with case 124485
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    RHEL6.0

bug:

    No bug found

Actions:

On KVM hypervisor

1, Select two machine with the same CPU types

2, Firstly  prepare a Windows guest virtual machine on source box.

 

3, If we choose ssh protocal as the transport, then, neccessary to open ssh port on iptables.

4, For migration, we need to share the images file of the vm between the source and target machine, we can use an alternative approach to implement it instead of using a real sharing storage device that is to export the directory of vm images file as a NFS share folder , the target machine mount it at the same point on the local system.

5, Using command "virsh migrate --live migratetest qemu+ssh://10.66.70.166/system" start the migration.

6, During the proccess of migration, upload or download a big file by guest vm .
	
Expected Results:

1, All operations should complete successfully

2, After migration, the vm should start and stop successfully.
3, Ensure the migration has no effect on uploading or downloading .
Notes:
Comments:

		176919 	[Graphical framebuffer] Spice wan support - bug 667628 	vbian 	vbian 	Manual 		Regression 	P1 	5430 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

TBD
	
Expected Results:
Notes:
Comments:

		176920 	[Graphical framebuffer] Support connected parameter in set_password -- Bug 707212, 852675 	vbian 	None 	Manual 		Function 	P1 	5440 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Regression
    graphical framebuffers

bug:

    852675 - From Run 44548

Actions:

step:
part 1:
1. Create a guest with following graphical framebuffer section
# virsh dumpxml --security-info spice
<graphics type='spice' port='5900' tlsPort='5901' autoport='yes'
listen='0.0.0.0' passwd='aaabbb' connected='disconnect'/>

2. Create a file with updating password, and connected parameter 
# cat spice-new.xml
    <graphics type='spice' port='5900' tlsPort='5901' autoport='yes'
listen='0.0.0.0' passwd='cccddd' connected='disconnect'/>

3. update spice setup
# virsh update-device spice spice-new.xml
Device updated successfully



part 2:
step 1:# virsh dumpxml --security-info spice
<graphics type='spice' port='5900' tlsPort='5901' autoport='yes'
listen='0.0.0.0' passwd='aaabbb' connected='disconnect'/>

step 2:# cat spice-new.xml
    <graphics type='spice' port='5900' tlsPort='5901' autoport='yes'
listen='0.0.0.0' passwd='cccddd' connected='keep'/>

step 3:# virsh update-device spice spice-new.xml
Device updated successfully



part 3:
step 1:# virsh dumpxml --security-info spice
<graphics type='spice' port='5900' tlsPort='5901' autoport='yes'
listen='0.0.0.0' passwd='aaabbb' connected='disconnect'/>

step 2:# cat spice-new.xml
    <graphics type='spice' port='5900' tlsPort='5901' autoport='yes'
listen='0.0.0.0' passwd='cccddd' connected='fail'/>

step 3:# virsh update-device spice spice-new.xml
error: Failed to update device from spice-new.xml
error: internal error unable to execute QEMU command
'__com.redhat_set_password': Could not set password



part 4:
step 1:# virsh dumpxml --security-info vnc
<graphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0' passwd='aaabbb'
connected='keep'/>

step 2:# cat vnc-new.xml
    <graphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0'
passwd='cccddd' connected='keep'/>

step 3:# virsh update-device spice spice-new.xml
Device updated successfully


part 5:
step 1:# virsh dumpxml --security-info spice
<graphics type='spice' port='5900' tlsPort='5901' autoport='yes'
listen='0.0.0.0' passwd='aaabbb' connected='disconnect'/>

step 2:# cat spice-new.xml
    <graphics type='spice' port='5900' tlsPort='5901' autoport='yes'
listen='0.0.0.0' passwd='aaabbb' connected='disconnect'/>

step 3:# virsh update-device spice spice-new.xml
Device updated successfully


part 6:
step 1:
#virsh dumpxml test --security-info 
<graphics type='spice' autoport='yes' listen='0.0.0.0' passwd= 'aaabbb' connected= 'disconnect'> 
   <listen type='address' address='0.0.0.0'/> </graphics>
step 2:

#cat spice-new.xml 
<graphics type='spice' port='5900' tlsPort='5901' autoport='yes' listen='0.0.0.0' passwd= 'cccddd' connected= 'fail'/>

step 3:# virsh update-device spice spice-new.xml

step 4: .Check the guest's xml 
#virsh dumpxml test --security-info 

step 5: Close the virt-viewer window and reopen it 
#virt-viewer test

 

 






	
Expected Results:

verify:

part1:

after step 3, spice console got disconnected

part2:

after step 3, spice console session got kept

part3:

after step 3, threw error when failed to update passwd , and spice session kept

part4:

after step 3,vnc connection keeps connected .

part5:

after step 3, with the same password , after updating device , spice console gets

disconnected

without connected option set , guest could be started successfully . 

part 6:

after step 3, will get error

error: Failed to update device from spice-new.xml
error: internal error unable to execute QEMU command '__com.redhat_set_password': Could not set password

 after step 4,

<graphics type='spice' autoport='yes' listen='0.0.0.0' passwd= 'aaabbb' connected= 'disconnect'> 
     <listen type='address' address='0.0.0.0'/> 
</graphics>

 after step 5,

 password "aaabbb" works not "cccddd".

 

 
Notes:
Comments:

		177649 	[Storage] Disk based storage pool -- Bug 570286 	nzhang 	yoyzhang 	Auto 		Feature 	P2 	5440 	Edit
Setup:

1, find a machine with two hard disk,

2, install an OS on only one harddisk, leaving the other without filesystem

 

such as:

intel-i7-12-1.englab.nay.redhat.com

 

bug 570286 was moved to virtualization tools, but not RHEL6. So no need to test the pool-delete(step 10) in RHEL6 unless it's fixed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0

bug:

    570286 - From Run 42300
    570286 - From Run 43702
    570286 - From Run 44254
    570286 - From Run 44736
    570286 - From Run 47630
    570286 - From Run 48367
    570286 - From Run 50663
    570286 - From Run 54156

Actions:

1. Prepare a disk pool xml:
# cat pool-disk-test.xml

ï»¿<pool type='disk'>
  <name>sdb</name>
  <source>
    <device path='/dev/sdb'>
    </device>
    <format type='dos'/>
  </source>
  <target>
    <path>/dev</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

2. define the disk pool
   # virsh pool-define pool-disk-test.xml
   Pool sdb defined from pool-disk-test.xml

3. Build the disk pool, that will format the /dev/sdb

Before build it, you'd better run

#dd if=/dev/zero of=/dev/sdb bs=1M count=10

to clear your disk.

# virsh pool-build sdb
Pool sdb built


4. Start the pool
   # virsh pool-start sdb
   Pool sdb started

   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   sdb            active     no

5. Parepare a vol xml file

# cat vol-partition-test.xml
<volume>
  <name>sdb5</name>
  <key>/dev/sdb5</key>
  <source>
    <device path='/dev/sdb'>
    </device>
  </source>
  <capacity unit='M'>100</capacity>
  <target>
    <path>/dev/sdb5</path>
  </target>
</volume>

6. Create a vol in the disk pool.
   # virsh vol-create sdb vol-partition-test.xml
   # virsh vol-list --pool sdb
   Name                 Path                                    
   -----------------------------------------
   sdb5                 /dev/sdb5                             

   #mkfs.ext4 /dev/sdb5

7. Mount the vol and test
   # mount /dev/sdb5 /mnt
   # cd /mnt
   # for i in {1..100}; do touch "hello${i}"; done
   # umount /mnt

8. Delete the vol in the disk pool
   # virsh vol-delete /dev/sdb5

9. Destroy the pool, check if the pool state was inactive.
   # virsh pool-destroy sdb
   Pool test-disk destroyed
   # virsh pool-list --all
   Name                 State      Autostart
   -----------------------------------------
   sdb            inactive   no

Skip step 10 if bug 570286 is not fixed.

10. Delete the inactive pool, check if the pool can be delete
    # virsh pool-delete sdb

Pool sdb deleted

11. Undefine the inactive pool, check if the pool can be undefined.
   # virsh pool-undefine sdb
   Pool sdb has been undefined

	
Expected Results:

5. Use fdisk -l or parted tools to check the new created partition is seen in ouput.

7. Check if the file can be touched successfully

8. Use fdisk -l or parted tools to check the created partition is deleted successfully.

 

Confirm if all the steps is correct.
Notes:
Comments:

		176922 	[Graphical framebuffers] Symbolic network name, to be evaluated to IP on the destination host 	vbian 	vbian 	Manual 		Regression 	P1 	5450 	Edit
Setup:

prepare two machines  one target,one source
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers

bug:

    851981 - From Run 45750
    851981 - From Run 47615

Actions:

1. virtual network
on target
# virsh net-dumpxml br1
<network>
<name>br1</name>
<uuid>25f0190f-4589-9430-2bb1-33e73ff61f43</uuid>
<bridge name='br1' stp='on' delay='0' />
<mac address='52:54:00:86:02:E8'/>
<ip address='10.0.1.1' netmask='255.255.255.0'>
<dhcp>
<range start='10.0.1.2' end='10.0.1.254' />
</dhcp>
</ip>
</network>
on source
# virsh net-dumpxml br1
<network>
<name>br1</name>
<uuid>e03baaeb-97a6-5ba4-0e14-59caf52fa021</uuid>
<bridge name='br1' stp='on' delay='0' />
<mac address='52:54:00:96:87:7C'/>
<ip address='10.0.0.1' netmask='255.255.255.0'>
<dhcp>
<range start='10.0.0.2' end='10.0.0.254' />
</dhcp>
</ip>
</network>
# virsh dumpxml guest
...
<source network='br1'/>
...
<graphics type='vnc' port='-1' autoport='yes' keymap='en-us'>
<listen type='network' network='br1'/>
</graphics>
...
# virsh start guest
# virsh dumpxml guest
...
<graphics type='vnc' port='5900' autoport='yes' keymap='en-us'>
<listen type='network' address='10.0.0.1' network='br1'/>
</graphics>
...
can connect with #vncviewer 10.0.0.1:5900
# virsh migrate --live guest qemu+ssh://10.66.83.215/system
no error
on target
# virsh dumpxml guest
...
<graphics type='vnc' port='5901' autoport='yes' keymap='en-us'>
<listen type='network' address='10.0.1.1' network='br1'/>
</graphics>
...
can connect with

#remote-viewer vnc://10.0.1.1:5901

2. macvtap (type=direct)
on target && source
# virsh net-dumpxml direct-net
<network>
<name>direct-net</name>
<uuid>a109c4c7-8095-1945-37e3-eae0b6bc2af8</uuid>
<forward dev='eth0' mode='bridge'>
<interface dev='eth0'/>
</forward>
</network>
on source
# virsh dumpxml guest
...
<source network='direct-net'/>
...
<graphics type='vnc' port='-1' autoport='yes' keymap='en-us'>
<listen type='network' network='direct-net'/>
</graphics>
...
start guest and #virsh dumpxml guest
...
<graphics type='vnc' port='5900' autoport='yes' keymap='en-us'>
<listen type='network' address='10.66.83.217' network='direct-net'/>
</graphics>
...
can connect with remote-viewer vnc://10.66.83.217:5900
# virsh migrate --live guest qemu+ssh://10.66.83.215/system
no error
on target
# virsh dumpxml guest
...
<graphics type='vnc' port='5901' autoport='yes' keymap='en-us'>
<listen type='network' address='10.66.83.215' network='direct-net'/>
</graphics>
...
can connect with remote-viewer vnc://10.66.83.215:5901

3. host bridge
first configure host bridge , for using ssh to remote access machine  , you can use

#brctl addbr breth0 ; brctl addif breth0 eth0 ; ifconfig breth0 up ; dhclient breth0

to create brdige , just wait a moment , you can reconnect the remote computer again.

 

here I use is breth0
on target && source
# virsh net-dumpxml red-network
<network>
<name>red-network</name>
<uuid>86424210-3a62-f2ce-f07f-ec29e412340d</uuid>
<forward mode='bridge'/>
<bridge name='breth0' />
</network>
on source
# virsh dumpxml guest
...
<source network='red-network'/>
...
<graphics type='vnc' port='-1' autoport='yes' keymap='en-us'>
<listen type='network' network='red-netowork'/>
</graphics>
...
start the guest and # virsh dumpxml guest
...
<graphics type='vnc' port='5900' autoport='yes' keymap='en-us'>
<listen type='network' address='10.66.83.217' network='red-network'/>
</graphics>
...
can connect with vncviewer 10.66.83.217:5900
# virsh migrate --live guest qemu+ssh://10.66.83.215/system
no error
on target
# virsh dumpxml guest
...
<graphics type='vnc' port='5901' autoport='yes' keymap='en-us'>
<listen type='network' address='10.66.83.215' network='red-network'/>
</graphics>
...
can connect with remote-viewer vnc://10.66.83.215:5901
	
Expected Results:
Notes:
Comments:

		176924 	[Graphical framebuffers]libvirt generate right xml for smartcard -- bug 677308 	zpeng 	None 	Manual 		Function 	P1 	5460 	Edit
Setup:

prepare a healthy guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

paste the following section into guest.xml for smartcard support:

1: pasted

    <smartcard mode='host'/> 

2: pasted 

    <smartcard mode='host-certificates'>
      <certificate>cert1</certificate>
      <certificate>cert2</certificate>
      <certificate>cert3</certificate>
      <database>/etc/pki/nssdb/</database>
    </smartcard>  

3: pasted
    <smartcard mode='passthrough' type='tcp'>
      <source mode='bind' host='127.0.0.1' service='2001'/>
      <protocol type='raw'/>
      <address type='ccid' controller='0' slot='0'/>
    </smartcard> 

4: pasted 
    <smartcard mode='passthrough' type='spicevmc'/> 

 

	
Expected Results:

virsh dumpxml $guest_name

1: after step 1 , verify:

got
    <smartcard mode='host'>
      <address type='ccid' controller='0' slot='0'/>
    </smartcard>
2:

got 
    <smartcard mode='host-certificates'>
    </smartcard>

 3: got 
    <smartcard mode='passthrough' type='tcp'> 
      <alias name='smartcard0'/>
 </smartcard> 

   4: got

       <smartcard mode='passthrough' type='spicevmc'>

        <alias name='smartcard0'/>

        <address type='ccid' controller='0' slot='0'/>

      </smartcard>

 

   verify no ">" missed after parsing .
Notes:
Comments:

		176927 	[Guest kernel debugging] Compress guest core dump to bzip2 	yoyzhang 	None 	Auto 		Feature 	P2 	5470 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    virsh-rail

bug:

    No bug found

Actions:

1. Get a running guest named 'rhel6'

# virsh start rhel6

2. Get guest default formatted (Raw) core dump

# virsh dump rhel6  rhel6.raw

3. Check guest status

# virsh list --all

4. Check the size and format of the dump file

# qemu-img info rhel6.raw

5. Run case "[Guest kernel debugging]Check the dumped images can be analyzed with crash tool sucessfully " with the dump file

6. Add the following line in /etc/libvirt/qemu.conf

  # dump_image_format = "raw"
 dump_image_format = "bzip2"

7. # service libvirtd restart

8. # virsh dump rhel6 rhel6.bzip2

9.  Check guest status

# virsh list --all

10. Check the format of dump file should be bzip2 and the size is much less than rhel6.raw

# qemu-img info rhel6.bzip2

# file rhel6.bzip2

11.Becompress file rhel6.bzip2(#bzip -d rhel6.zip2) and run case "[Guest kernel debugging]Check the dumped images can be analyzed with crash tool sucessfully " with the unzip file
	
Expected Results:

Step2

should take mins to finish then output

Domain rhel6 dumped to rhel6.raw


Step3

guest should be still in running status
 

Step4

output

image: rhel6.raw
file format: raw
virtual size: 544M (570856448 bytes)
disk size: 545M

step8

should take mins to finish then output

Domain rhel6 dumped to rhel6.bzip2

step9

guest should be in running status

step10

# qemu-img info rhel6.bzip2

image: rhel6.bzip2
file format: raw
virtual size: 91M (95524864 bytes)
disk size: 91M

# file rhel6.bzip2

rhel6.bzip2: bzip2 compressed data, block size = 900k
Notes:
Comments:

		177677 	[Storage]define and build the disk type of pool 	gren 	yoyzhang 	Auto 		--default-- 	P2 	5470 	Edit
Setup:

1, find a machine with two hard disk,

2, install an OS on only one harddisk, leaving the other without filesystem
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0

bug:

    No bug found

Actions:

1, First define a storage pool of disk type , the xml description is as follows:

    the pool format types include:  dos dvh gpt mac bsd  pc98  sun

<pool type='disk'>
  <name>sdb</name>
  <source>
    <device path='/dev/sdb'>
    </device>
    <format type='dos'/>
  </source>
  <target>
    <path>/dev</path>
    <permissions>
      <mode>0700</mode>
      <owner>0</owner>
      <group>0</group>
    </permissions>
  </target>
</pool>

2, Second, run command "virsh pool-build sdb" to build the pool

[root@intel-i7-12-1 tmp]# virsh pool-build sdb
Pool sdb built

[root@intel-i7-12-1 tmp]#
	
Expected Results:

ensure all the operation on the pool format types  are successful.
Notes:
Comments:

		176928 	[Guest kernel debugging] Compress guest core dump to gzip 	yoyzhang 	None 	Auto 		Feature 	P1 	5480 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    virsh-rail

bug:

    No bug found

Actions:

1. Get a running guest named 'rhel6'

# virsh start rhel6

2. Get guest default formatted (Raw) core dump

# virsh dump rhel6 rhel6.raw

3. Check guest status

# virsh list --all

4. Check the size and format of the dump file

# qemu-img info rhel6.raw

5. Add the following line in /etc/libvirt/qemu.conf

 # dump_image_format = "raw"
 dump_image_format = "gzip"

6. # service libvirtd restart

7. # virsh dump rhel6 rhel6.gzip

8.  Check guest status

# virsh list --all

9. Check the format of dump file should be gzip and the size is much less than rhel6.raw

# qemu-img info rhel6.gzip

# file rhel6.gzip

10 Becompress file rhel6.gzip(#mv rhel6.gzip rhel6.gz #gunzip -d rhel6.gz) and run case "[Guest kernel debugging]Check the dumped images can be analyzed with crash tool sucessfully " with the unzip file
	
Expected Results:

Step2

should take mins to finish then output

Domain rhel6 dumped to rhel6.raw

Step3

guest should be still in running status

Step4

output

image: rhel6.raw
file format: raw
virtual size: 544M (570856448 bytes)
disk size: 545M

step7

should take mins to finish then output

Domain rhel6 dumped to rhel6.gzip

step8

guest should be in running status

step9

# qemu-img info rhel6.gzip

image: rhel6.gzip
file format: raw
virtual size: 131M (136982016 bytes)
disk size: 131M

# file rhel6.gzip

rhel6.gzip: gzip compressed data, from Unix, last modified: Thu Nov 25 04:26:31 2010
Notes:
Comments:

		177644 	[Storage] Delete a storage pool - Bug 496579 	nzhang 	yoyzhang 	Auto 		Feature 	P2 	5480 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0

bug:

    No bug found

Actions:

1. Define a storage pool with following XML.

<pool type="dir">
  <name>test-dir</name>
  <target>
    <path>/var/lib/libvirt/images/test-dir</path>
  </target>
</pool>



# virsh pool-define test-dir.xml
Pool test-dir defined from test-dir.xml
# ll /var/lib/libvirt/images/test-dir
ls: cannot access /var/lib/libvirt/images/test-dir: No such file or directory

2. Check if the pool was defined in the domain list.
# virsh pool-list --all
Name                 State      Autostart 
-----------------------------------------
default              active     yes       
pool-mig             inactive   no        
pool-migration       inactive   no        
test-dir             inactive   no        


3. Build the pool
# virsh pool-build test-dir
Pool test-dir built
# ll /var/lib/libvirt/images/test-dir
total 0




4. Delete the storage pool.
# virsh pool-delete test-dir
Pool test-dir deleted
# ll /var/lib/libvirt/images/test-dir
ls: cannot access /var/lib/libvirt/images/test-dir: No such file or directory

5. # virsh pool-list --all
Name                 State      Autostart 
-----------------------------------------
default              active     no        
test-dir             inactive   no      




	
Expected Results:

The storage pool can be deleted successfully, and no errors.

Acctually virsh delete means virsh unbuild.
Notes:
Comments:

		176929 	[Guest kernel debugging] Compress guest core dump to invalid format - bug669586 	yoyzhang 	None 	Manual (Autoproposed) 		Negative test 	P2 	5490 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    Regression

bug:

    No bug found

Actions:

1. Get a running guest named 'rhel6'

# virsh start rhel6

2. Add the following line in /etc/libvirt/qemu.conf

# dump_image_format = "raw"
 dump_image_format = "abc

3. # service libvirtd restart

4. # virsh dump rhel6 rhel6.abc

5.#qemu-img info rhel6.abc

6.Check the log in /var/log/libvirt/libvirtd.log

7.Check the comment in /etc/libvirt/qemu.conf
	
Expected Results:

4.# virsh dump rhel6 rhel6.abc
Domain test dumped to rhel6.abc
5.# qemu-img info rhel6.abc
image: rhel6.abc
file format: raw
virtual size: 282M (295681024 bytes)
disk size: 282M

 6.There should have warning message:
06:44:21.882: 8723: warning : getCompressionType:2737 : Invalid dump image
format specified in configuration file, using raw

7.

# dump_image_format is used when you use 'virsh dump' at emergency
# crashdump, and if the specified dump_image_format is not valid, or
# the requested compression program can't be found, this falls
# back to "raw" compression.

Notes:
Comments:

		176930 	[Guest kernel debugging] Compress guest core dump to invalid format - bug669586 	yoyzhang 	yoyzhang 	Manual 		Negative test 	P2 	5500 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    Regression

bug:

    No bug found

Actions:

1. Get a running guest named 'f14'

# virsh start f14

2. Add the following line in /etc/libvirt/qemu.conf

# dump_image_format = "raw"
 dump_image_format = "abc

3. # service libvirtd restart

4. # virsh dump f14 f14.abc
	
Expected Results:

Should pops up an error as below

error: Failed to core dump domain f14 to f14.abc
error: operation failed: Invalid dump image format specified in configuration file
Notes:
Comments:

		176931 	[Guest kernel debugging] Compress guest core dump to lzop 	yoyzhang 	None 	Auto 		Feature 	P2 	5510 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    virsh-rail

bug:

    No bug found

Actions:

1. Get a running guest named 'rhel6'

# virsh start rhel6

2. Get guest default formatted (Raw) core dump

# virsh dump rhel6  rhel6.raw

3. Check guest status

# virsh list --all

4. Check the size and format of the dump file

# qemu-img info rhel6.raw

5. Add the following line in /etc/libvirt/qemu.conf

  # dump_image_format = "raw"
 dump_image_format = "lzop"

6. # service libvirtd restart

7. # virsh dump rhel6 rhel6.lzop

8.  Check guest status

# virsh list --all

9. Check the format of dump file should be lzop and the size is much less than rhel6.raw

# qemu-img info rhel6.lzop

# file rhel6.lzop

10.Becompress file rhel6.lzop(#lzop -d rhel6.lzop) and run case "[Guest kernel debugging]Check the dumped images can be analyzed with crash tool sucessfully " with the unzip file
	
Expected Results:

Step2

should take mins to finish then output

Domain rhel6 dumped to rhel6.raw

Step3

guest should be still in running status

Step4

output

image: rhel6.raw
file format: raw
virtual size: 544M (570856448 bytes)
disk size: 545M

step7

should take mins to finish then output

Domain rhel6 dumped to rhel6.lzop

step8

guest should be in running status

step9

# qemu-img info rhel6.lzop

image: rhel6.lzop
file format: raw
virtual size: 131M (136982016 bytes)
disk size: 131M

# file rhel6.lzop

rhel6.lzop: lzop compressed data - version 1.020, LZO1X-1, os: Unix
Notes:
Comments:

		176932 	[Guest kernel debugging] Compress guest core dump to xz 	yoyzhang 	None 	Auto 		Feature 	P2 	5520 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    virsh-rail

bug:

    No bug found

Actions:

1. Get a running guest named 'rhel6'

# virsh start rhel6

2. Get guest default formatted (Raw) core dump

# virsh dump rhel6  rhel6.raw

3. Check guest status

# virsh list --all

4. Check the size and format of the dump file

# qemu-img info rhel6.raw

5. Add the following line in /etc/libvirt/qemu.conf

  # dump_image_format = "raw"
 dump_image_format = "xz"

6. # service libvirtd restart

7. # virsh dump rhel6 rhel6.xz

8.  Check guest status

# virsh list --all

9. Check the format of dump file should be xz and the size is much less than rhel6.raw

# qemu-img info rhel6.xz

# file rhel6.xz

10.Becompress file rhel6.xz(#xz -d rhel6.xz) and run case "[Guest kernel debugging]Check the dumped images can be analyzed with crash tool sucessfully " with the unzip file
	
Expected Results:

Step2

should take mins to finish then output

Domain rhel6 dumped to rhel6.raw

Step3

guest should be still in running status
 
Step4

output

image: rhel6.raw
file format: raw
virtual size: 544M (570856448 bytes)
disk size: 545M

step7

should take mins to finish then output

Domain rhel6 dumped to rhel6.xz

step8

guest should be in running status

step9

# qemu-img info rhel6.xz

image: rhel6.xz
file format: raw
virtual size: 111M (116794880 bytes)
disk size: 112M

# file rhel6.xz

f14.xz: xz compressed data
Notes:
Comments:

		176933 	[Guest kernel debugging] Compress guest save file to lzop/gzip/bzip2/xz 	yoyzhang 	None 	Manual (Autoproposed) 		Feature 	P1 	5530 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1. Get a running guest named 'rhel6'

# virsh start rhel6

2. Get guest default formatted (Raw) core dump

# virsh save rhel6  rhel6.raw

3. Check guest status

# virsh list --all

Need to restore guest
#virsh restore rhel6.raw

4. Check the size and format of the save file

# qemu-img info rhel6.save

5. Add the following line in /etc/libvirt/qemu.conf

 # save_image_format = "raw"
 save_image_format = "gzip"

6. # service libvirtd restart

7. # virsh save rhel6 rhel6.gzip

8.  Check guest status

# virsh list --all


9. Check the format of save file should be gzip and the size is much less than rhel6.raw

# qemu-img info rhel6.gzip

# file rhel6.gzip

10. Check restore successfully

#virsh restore rhel6.gzip

Loop step5-step10 with format lzop, bzip2, xz, then should save to coressponding format and the file should be less size than raw format
	
Expected Results:

Step2

should take mins to finish then output

Domain rhel6 saved to rhel6.raw

Step3

guest should be still in shutoff status

Step4

output

image: rhel6.save
file format: raw
virtual size: 545M (571062784 bytes)
disk size: 545M

step7

should take mins to finish then output

Domain rhel6 saved to rhel6.gzip

step8

guest should be in shutoff status

step9

# qemu-img info rhel6.gzip

image: rhel6.gzip
file format: raw
virtual size: 166M (174380544 bytes)
disk size: 166M

# file rhel6.gzip

rhel6.gzip: data

step10

# virsh restore rhel6.gzip
Domain restored from rhel6.gzip

Check the guest works fine
Notes:
Comments:

		176934 	[Guest kernel debugging] Compress guest save to invalid format 	yoyzhang 	None 	Manual (Autoproposed) 		Negative test 	P2 	5540 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1. Get a running guest named 'f14'

# virsh start f14

2. Add the following line in /etc/libvirt/qemu.conf

# save_image_format = "raw"
 save_image_format = "abc

3. # service libvirtd restart

4. # virsh save f14 f14.abc
	
Expected Results:

Should pops up an error as below

error: Failed to save domain f14 to f14.abc
error: operation failed: Invalid save image format specified in configuration file
Notes:
Comments:

		176935 	[Guest kernel debugging] Core dump a non-live guest should reprot error 	yoyzhang 	None 	Manual (Autoproposed) 		Negative test 	P2 	5550 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1. Get a shut off status guest

# virsh list --all

Id Name                 State
----------------------------------
- rhel6                shut off

2. # virsh dump rhel6 rhel6.dump
	
Expected Results:

Step 2 should report an error like below

error: Failed to core dump domain rhel6 to rhel6.dump
error: Requested operation is not valid: domain is not running
Notes:
Comments:

		176936 	[Guest kernel debugging] Libvirt could deliver human monitor commands to guest 	yoyzhang 	None 	Manual (Autoproposed) 		Feature 	P1 	5560 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1. Prepare a running guest

# virsh list --all
 Id Name                 State
----------------------------------
  1 rhel6                running

2. # virsh qemu-monitor-command rhel6 --hmp 'info kvm'

3. # virsh qemu-monitor-command rhel6 \

'{"execute":"human-monitor-command","arguments":{"command-line":"info kvm"}}'

4. # virsh qemu-monitor-command rhel6 --hmp  "info cpus"

	
Expected Results:

2. Output

kvm support: enabled

3. Output

{"return":"kvm support: enabled\r\n","id":"libvirt-149"}


4. output
* CPU #0: pc=0xffffffff8006c389 (halted) thread_id=11047 
  CPU #1: pc=0xffffffff8006c389 (halted) thread_id=11048 

Notes:
Comments:

		176937 	[Guest kernel debugging] Run virsh save with option --xml to containing updated XML for the target 	yupzhang 	None 	Manual (Autoproposed) 		Function 	P1 	5570 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    virt-v2v

Tag:

    guest kernel debugging

bug:

    834196 - From Run 44191
    834196 - From Run 46171
    834196 - From Run 49513
    834196 - From Run 51770
    834196 - From Run 53796

Actions:


1.Start a guest
# virsh start rhel6

2.# virsh dumpxml rhel6 > rhel6.xml

3.Copy the image of guest to /tmp/
# cp /var/lib/libvirt/images/rhel6.img /tmp/

4.Edit rhel6.xml to change /var/lib/libvirt/images/rhel6.img to /tmp/rhel6.img

5.Save the guest with --xml 
# virsh save rhel6 rhel6.save --xml rhel6.xml 

6. Check that xml embedded in binary rhel6.save did change
#vim rhel6.save  

7. Restore guest with --xml 
# virsh restore rhel6.save --xml rhel6.xml 

8.Check the disk /tmp/rhel6.img in use
#virsh dumpxml rhel6 

 

	
Expected Results:

5.# virsh save rhel6 rhel6.save --xml rhel6.xml
Domain rhel6 saved to rhel6.save

6.Check the XML embedded in rhel6.save.

#vim rhel6.save

LibvirtQemudSave^B^@^@^@Â¤^O^@^@^A^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@<domain type='kvm'>
  <name>rhel6</name>
  <uuid>9a09c9d8-2d2b-93ff-ad45-cb1bfaccda8e</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.2.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/tmp/rhel6.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='bridge'>
      <mac address='52:54:00:27:c5:44'/>
      <source bridge='rhevm'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>

.....

7.# virsh restore rhel6.save --xml rhel6.xml
Domain restored from rhel6.save

8. 

#virsh dumpxml rhel6

....

<disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/tmp/rhel6.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
...
Notes:
Comments:

		176938 	[Guest kernel debugging] Save a shut off guest - bug 677547 	yoyzhang 	None 	Manual 		Regression 	P2 	5580 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1.# virsh list --all
 Id Name                 State
----------------------------------
  - rhel6                shut off

2. # virsh save rhel6 rhel6.save

3. # touch rhel6.save

4. # virsh save rhel6 rhel6.save

	
Expected Results:

2. # virsh save rhel6 rhel6.save

error: Failed to save domain rhel6 to rhel6.save
error: Requested operation is not valid: domain is not running

4. # virsh save rhel6 rhel6.save
error: Failed to save domain rhel6 to rhel6.save
error: Requested operation is not valid: domain is not running

Notes:
Comments:

		177643 	[Storage] Create a qcow2 file using libvirt storage APIs - Bug 504119 	yoyzhang 	yoyzhang 	Auto 		Regression 	P3 	5580 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

 [Steps]
1. check the current vol list

# virsh vol-list default


2. Create vol in default pool

# virsh vol-create-as default mydddd 2G --allocation 10M --format qcow2


3. Check the create vol file.

# virsh vol-list default

# virsh vol-dumpxml <your-vol-path>

# qemu-img info <your-vol-path>

 
	
Expected Results:

1. Output :

# virsh vol-list default
Name                 Path                                    
-----------------------------------------
winXP-32-virtio.raw  /var/lib/libvirt/images/winXP-32-virtio.raw

2. Output:

# virsh vol-create-as default mydddd 2G --allocation 10M --format qcow2

3. The vol is created, size, format is the same as expected

# virsh vol-list default
Name                 Path                                    
-----------------------------------------
mydddd               /var/lib/libvirt/images/mydddd          
winXP-32-virtio.raw  /var/lib/libvirt/images/winXP-32-virtio.raw

# virsh vol-dumpxml /var/lib/libvirt/images/mydddd
<volume>
  <name>mydddd</name>
  <key>/var/lib/libvirt/images/mydddd</key>
  <source>
  </source>
  <capacity>2147483648</capacity>
  <allocation>143360</allocation>
  <target>
    <path>/var/lib/libvirt/images/mydddd</path>
    <format type='qcow2'/>
    <permissions>
      <mode>0600</mode>
      <owner>0</owner>
      <group>0</group>
      <label>unconfined_u:object_r:virt_image_t:s0</label>
    </permissions>
  </target>
</volume>

# qemu-img info /var/lib/libvirt/images/mydddd
image: /var/lib/libvirt/images/mydddd
file format: qcow2
virtual size: 2.0G (2147483648 bytes)
disk size: 140K
cluster_size: 65536

 
Notes:
Comments:

		176939 	[guest kernel debugging] Save a shut off guest - bug 677547 	yoyzhang 	yoyzhang 	Manual (Autoproposed) 		Regression 	P2 	5590 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging
    Regression

bug:

    No bug found

Actions:

1.# virsh list --all
 Id Name                 State
----------------------------------
  - rhel6                shut off

2. # virsh save rhel6 rhel6.save

3. # touch rhel6.save

4. # virsh save rhel6 rhel6.save

	
Expected Results:

2. # virsh save rhel6 rhel6.save

error: Failed to save domain rhel6 to rhel6.save
error: Requested operation is not valid: domain is not running

4. # virsh save rhel6 rhel6.save
error: Failed to save domain rhel6 to rhel6.save
error: Requested operation is not valid: domain is not running

Notes:
Comments:

		176893 	[Graphical framebuffers] Host firewall port listening for different VNC ports 	vbian 	None 	Manual 		Feature 	P2 	5600 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1. Add following lines in /etc/sysconfig/iptables
-A INPUT -p tcp  -m multiport --dports 5900:6000 -j REJECT

2. edit the guest xml file like this
...
    <graphics type='vnc' port='-1' autoport='yes'/>
...

3. start the guest , and try to access the vnc interface
   # virsh start guest

use virt-viewer to connect the guest , use virt-viewer, remote-viewer
   # virt-viewer #guest

   #remote-viewer vnc://$ip:$port
4. modify the /etc/sysconfig/iptables file like this
   change the following line
   -A INPUT -p tcp -m multiport --dports 5900:6000 -j REJECT
   To
   -A INPUT -p tcp -m multiport --dports 5900:6000 -j ACCEPT
5. restart iptables , and try to access the vnc interface again
   # service iptables restart
   #virt-viewer #guest

   #remote-viewer vnc://$ip:$port

	
Expected Results:

3. you CAN'T access vnc interface
5. you could access vnc interface
Notes:
Comments:

		176895 	[Graphical framebuffers] multiple spice migration 	vbian 	None 	Manual 		Function 	P1 	5610 	Edit
Setup:

[Prerequisite]

1. check current UTC time 

# date -u

Thu Aug  4 02:12:51 UTC 2011


2. define the guest with the following xml file
<domain type='kvm' id='4'>
  <name>test</name>
  <uuid>1490cddf-cd72-c1a0-ac25-9a8ad0d81d56</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/cdrom_test.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:b5:d6:4c'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/6'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <serial type='vc'>
      <target port='1'/>
      <alias name='serial1'/>
    </serial>
    <console type='pty' tty='/dev/pts/6'>
      <source path='/dev/pts/6'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <channel type='pty'>
      <source path='/dev/pts/7'/>
      <target type='virtio' name='test.virtio.pty'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='0'/>
    </channel>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' tlsPort='5901' autoport='yes' passwd='aaabbb' passwdValidTo='2011-08-04T02:14:01'>
      <channel name='main' mode='secure'/>
      <channel name='record' mode='insecure'/>
    </graphics>
    <sound model='ac97'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c360,c691</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c360,c691</imagelabel>
  </seclabel>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    856068 - From Run 44548
    855870 - From Run 45750
    855870 - From Run 47615
    855870 - From Run 49803
    855870 - From Run 54142

Actions:

1. backup the sysconfig file
   # cp /etc/libvirt/qemu.conf /etc/sysconfig/qemu.conf.bk

2. modify the followings in qemu.conf
   -# spice_listen = "0.0.0.0"
   + spice_listen = "0.0.0.0"

   -# spice_tls = 1
   + spice_tls = 1

   -# spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"
   + spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"

3. perform the following script, to generate the cert files for ssl , and then copy *.pem file info /etc/pkil/libvirt-spice directory,and also scp /etc/pki/libvirt-spice/* to the target machine /etc/pki/libvirt-spice
#!/bin/bash

SERVER_KEY=server-key.pem

# creating a key for our ca
if [ ! -e ca-key.pem ]; then
    openssl genrsa -des3 -out ca-key.pem 1024
fi
# creating a ca
if [ ! -e ca-cert.pem ]; then
    openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem  -subj "/C=IL/L=Raanana/O=Red Hat/CN=my CA"
fi
# create server key
if [ ! -e $SERVER_KEY ]; then
    openssl genrsa -out $SERVER_KEY 1024
fi
# create a certificate signing request (csr)
if [ ! -e server-key.csr ]; then
    openssl req -new -key $SERVER_KEY -out server-key.csr -subj "/C=IL/L=Raanana/O=Red Hat/CN=my server"
fi
# signing our server certificate with this ca
if [ ! -e server-cert.pem ]; then
    openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem
fi

# now create a key that doesn't require a passphrase
openssl rsa -in $SERVER_KEY -out $SERVER_KEY.insecure
mv $SERVER_KEY $SERVER_KEY.secure
mv $SERVER_KEY.insecure $SERVER_KEY

# show the results (no other effect)
openssl rsa -noout -text -in $SERVER_KEY
openssl rsa -noout -text -in ca-key.pem
openssl req -noout -text -in server-key.csr
openssl x509 -noout -text -in server-cert.pem
openssl x509 -noout -text -in ca-cert.pem

# copy *.pem file to /etc/pki/libvirt-spice
if [[ -d "/etc/pki/libvirt-spice" ]]
then
    cp ./*.pem /etc/pki/libvirt-spice
else
    mkdir /etc/pki/libvirt-spice
        cp ./*.pem /etc/pki/libvirt-spice
fi

# echo --host-subject
echo "your --host-subject is" \" `openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "` \"

4. restart libvirtd to rescan the configuration
   # service libvirtd restart

5. define and start the guest
   # virsh define guest.xml
   # virsh start guest

6. access the spice interface with ssl connection on source machine
   # remote-viewer  spice://10.66.93.134/?tls-port=5901 --spice-host-subject "C=IL,L=Raanana,O=Red Hat,CN=my server" --spice-ca-file /etc/pki/libvirt-spice/ca-cert.pem

input password

7. migrate guest on source machine between source and target machine with the spice session connected

[on source machine]   # virsh migrate --live guest qemu+ssh://$destination_IP/system

[on target machine]   # virsh migrate --live guest qemu+ssh://$source_IP/system

Do above loop for 20 times 

 

   Note:
   10.66.93.134 --- your host IP
   5900 --- the port you defined in xml graphic section
   5901 --- the tlsPort you defined in xml graphic section
   redhat --- the passwd you set in the xml graphic section
   "C=IL,L=Raanana,O=Red Hat,CN=my server" --- is got from command #openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "
	
Expected Results:

During step 7 , spice session always keeps connected . And there is no error for migration .

keep following command performing on both source and target machines

# tail -f /var/log/libvirtd.log

 

make sure during migration , there is no error record for libvirtd.log
Notes:
Comments:

		176896 	[Graphical framebuffers] RDP 	yimwang 	None 	Manual 		--default-- 	P2 	5620 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

kvm hypervisor:

1, Try to use RDP to install a new guest

<domain type='kvm'>
  <name>videotest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/rdptest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <disk type='file' device='cdrom'>
      <source file='/tmp/boot.iso'/>
      <readonly/>
      <target dev='hdc' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
     <graphics type='rdp' port='0' autoport='yes'/>
  </devices>
</domain>

2.Define the domain.

# virsh define test.xml
Domain videotest defined from test.xml

3.Start the domain.

# virsh start videotest

 
	
Expected Results:

1. RDP is not supported for QEMU, but setting type='rdp' should give
   a useful error report.
   (Note: Currently it does not provide an error report, we should
   track this and fix it in a later RHEL update.)


Notes:
Comments:

		176897 	[Graphical framebuffers] SDL 	gren 	gren 	Manual 		Feature 	P4 	5630 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    framebuffers

bug:

    No bug found

Actions:

kvm hypervisor:

1, Try to use SDL to install a new guest

<domain type='kvm'>
  <name>videotest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/sdltest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <disk type='file' device='cdrom'>
      <source file='/tmp/boot.iso'/>
      <readonly/>
      <target dev='hdc' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='sdl' display=':0.0'/>
  </devices>
</domain>

2, After install, start the vm using the following xml file:

<domain type='kvm'>
  <name>videotest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel5.4.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' cache='none'/>
      <source file='/var/lib/libvirt/images/sdltest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='network'>
      <mac address='54:52:00:54:9e:f4'/>
      <source network='default'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='sdl' display=':0.0'/>
  </devices>
</domain>

On Xen hypervisor:

1, boot off CDROM to install a new vm using SDL graphical framebuffers:

<domain type='xen'>
  <name>xensdltest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type>hvm</type>
    <loader>/usr/lib/xen/boot/hvmloader</loader>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/lib64/xen/bin/qemu-dm</emulator>
    <disk type='file' device='disk'>
      <driver name='file'/>
      <source file='/var/lib/xen/images/xensdltest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <disk type='file' device='cdrom'>
      <driver name='file'/>
      <source file='/root/Desktop/boot.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
    </disk>
    <interface type='bridge'>
      <mac address='00:16:36:26:94:3a'/>
      <source bridge='virbr0'/>
      <script path='vif-bridge'/>
      <target dev='vif1.0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/1'/>
      <target port='0'/>
    </serial>
    <console type='pty' tty='/dev/pts/1'>
      <source path='/dev/pts/1'/>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='sdl' display=':0.0'/>
  </devices>
</domain>

2, Boot up the stalled vm with SDL

<domain type='xen'>
  <name>xensdltest</name>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type>hvm</type>
    <loader>/usr/lib/xen/boot/hvmloader</loader>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/lib64/xen/bin/qemu-dm</emulator>
    <disk type='file' device='disk'>
      <driver name='file'/>
      <source file='/var/lib/xen/images/xensdltest.img'/>
      <target dev='hda' bus='ide'/>
    </disk>
    <interface type='bridge'>
      <mac address='00:16:36:26:94:3a'/>
      <source bridge='virbr0'/>
      <script path='vif-bridge'/>
      <target dev='vif1.0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/1'/>
      <target port='0'/>
    </serial>
    <console type='pty' tty='/dev/pts/1'>
      <source path='/dev/pts/1'/>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='sdl' display=':0.0'/>
  </devices>
</domain>
	
Expected Results:

1, ensure the SDL could display windows normally during install.

2, ensure some operation on the vm could be performed via SDL .

3, After vm starts and reboot, the SDL works well.
Notes:
Comments:

		176898 	[Graphical framebuffers] SDL 	yimwang 	None 	Manual 		Feature 	P1 	5640 	Edit
Setup:

For a guest use "sdl" graphic card, it need to access to xserver on host.. "xhost + localhost" tell xserver that you are permitted to access server.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers
    RHEL6.0

bug:

    No bug found

Actions:

1. # echo $DISPLAY

:6.0

# echo $XAUTHORITY
/root/.Xauthority


2. edit the guest xml
 <graphics type='sdl' display=":6.0" xauth="/root/.Xauthority"/>

3. 
# xhost + localhost
localhost being added to access control list
 
4. start guest
# virsh start guest

5. There is an additional SDL 'fullscreen' attribute, so if SDL works,
 try with;
 <graphics type='sdl' fullscreen='yes'/>

	
Expected Results:

( Note: There have been recommendations to set permissions on the xauth file so it can be accessed by the qemu emulator running as user 'qemu', but it didn't seem to work for on F13.)

1. the sdl window pops up.

2.Additional  attribute 'fullscreen' must be work fine.
Notes:
Comments:

		176899 	[Graphical framebuffers] set the spice listening address 	vbian 	None 	Manual 		Feature 	P2 	5650 	Edit
Setup:

make sure all of the options in /etc/libvirt/qemu.conf are commented out

make sure virt-viewer latest pkg installed
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1. define a guest with the following device
...
  <graphics type='spice' port='5910' autoport='no'/>
    <video>
      <model type='qxl' vram='8192' heads='1'/>
    </video>
...
2. start the guest , and access the spice interface
   # virsh start guest

   #virt-viewer $guestname

   #remote-viewer spice://10.66.93.134:5910

   #remote-viewer spice://127.0.0.1:5910
 


3. change the guest xml like this
     <graphics type='spice' port='5910' autoport='no' listen='0.0.0.0'/>
4. restart the guest , and access the spice interface
   #remote-viewer spice://127.0.0.1:5910

   #remote-viewer spice://10.66.93.134:5910 

	
Expected Results:

2. you will get
   # #remote-viewer spice://10.66.93.134:5910
   Warning: failed to connect: Connection refused (111)
 
   # /usr/libexec/spicec -h 127.0.0.1 -p 5910
   spice interface prompts
4. both of 10.66.93.134 and 127.0.0.1 could access the spice interface
Notes:
Comments:

		176900 	[Graphical framebuffers] spice 	gren 	yoyzhang 	Manual 		Feature 	P2 	5660 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers
    RHEL6.0

bug:

    No bug found

Actions:

1. Define a domain with spice graphical device as following:

<domain type='kvm'>
  <name>demo</name>
  <uuid>6bd72aef-3661-8400-ab2c-c15c935cdd85</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/demo.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:71:a8:bb'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' autoport='no' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>

2. Start the domain via "vrish start" command

3. Install virt-viewer

4. Use virt-viewer to check guest windows.

#remote-viewer spice://127.0.0.1:5900

 
	
Expected Results:

4. Guest windows is seen, and guest is working fine

shift + F12 can release mouse course from guest windows.

Check domain log (/var/log/libvirt/qemu/<guestname>.log):

LC_ALL=C PATH=/sbin:/usr/sbin:/bin:/usr/bin QEMU_AUDIO_DRV=spice /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name winxp -uuid 056048d6-9c40-05c4-84d4-aefe7177bffe -nodefaults -chardev socket,id=monitor,path=/var/lib/libvirt/qemu/winxp.monitor,server,nowait -mon chardev=monitor,mode=control -rtc base=utc -boot c -drive file=/var/lib/libvirt/images/winXP-32-virtio-el6.qcow2,if=none,id=drive-ide0-0-0,boot=on,format=qcow2 -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -netdev tap,fd=24,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:df:0f:42,bus=pci.0,addr=0x4 -chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -spice port=5900,addr=127.0.0.1,disable-ticketing -k en-us -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x5 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3
Notes:
Comments:

		176901 	[Graphical framebuffers] spice migration before passwdValidTo 	vbian 	None 	Manual 		Function 	P1 	5670 	Edit
Setup:

[Prerequisite]

1. check current UTC time 

# date -u

Thu Aug  4 02:12:51 UTC 2011


2. define the guest with the following xml file
<domain type='kvm' id='4'>
  <name>test</name>
  <uuid>1490cddf-cd72-c1a0-ac25-9a8ad0d81d56</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/cdrom_test.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:b5:d6:4c'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/6'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <serial type='vc'>
      <target port='1'/>
      <alias name='serial1'/>
    </serial>
    <console type='pty' tty='/dev/pts/6'>
      <source path='/dev/pts/6'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <channel type='pty'>
      <source path='/dev/pts/7'/>
      <target type='virtio' name='test.virtio.pty'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='0'/>
    </channel>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' tlsPort='5901' autoport='yes' passwd='aaabbb' passwdValidTo='2011-08-04T02:14:01'>
      <channel name='main' mode='secure'/>
      <channel name='record' mode='insecure'/>
    </graphics>
    <sound model='ac97'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c360,c691</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c360,c691</imagelabel>
  </seclabel>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    856068 - From Run 44548
    856068 - From Run 45750
    855870 - From Run 47615
    855870 - From Run 54142

Actions:

1. backup the sysconfig file
   # cp /etc/libvirt/qemu.conf /etc/sysconfig/qemu.conf.bk

2. modify the followings in qemu.conf
   -# spice_listen = "0.0.0.0"
   + spice_listen = "0.0.0.0"

   -# spice_tls = 1
   + spice_tls = 1

   -# spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"
   + spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"

3. perform the following script, to generate the cert files for ssl , and then copy *.pem file info /etc/pkil/libvirt-spice directory,and also scp /etc/pki/libvirt-spice/* to the target machine /etc/pki/libvirt-spice
#!/bin/bash

SERVER_KEY=server-key.pem

# creating a key for our ca
if [ ! -e ca-key.pem ]; then
    openssl genrsa -des3 -out ca-key.pem 1024
fi
# creating a ca
if [ ! -e ca-cert.pem ]; then
    openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem  -subj "/C=IL/L=Raanana/O=Red Hat/CN=my CA"
fi
# create server key
if [ ! -e $SERVER_KEY ]; then
    openssl genrsa -out $SERVER_KEY 1024
fi
# create a certificate signing request (csr)
if [ ! -e server-key.csr ]; then
    openssl req -new -key $SERVER_KEY -out server-key.csr -subj "/C=IL/L=Raanana/O=Red Hat/CN=my server"
fi
# signing our server certificate with this ca
if [ ! -e server-cert.pem ]; then
    openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem
fi

# now create a key that doesn't require a passphrase
openssl rsa -in $SERVER_KEY -out $SERVER_KEY.insecure
mv $SERVER_KEY $SERVER_KEY.secure
mv $SERVER_KEY.insecure $SERVER_KEY

# show the results (no other effect)
openssl rsa -noout -text -in $SERVER_KEY
openssl rsa -noout -text -in ca-key.pem
openssl req -noout -text -in server-key.csr
openssl x509 -noout -text -in server-cert.pem
openssl x509 -noout -text -in ca-cert.pem

# copy *.pem file to /etc/pki/libvirt-spice
if [[ -d "/etc/pki/libvirt-spice" ]]
then
    cp ./*.pem /etc/pki/libvirt-spice
else
    mkdir /etc/pki/libvirt-spice
        cp ./*.pem /etc/pki/libvirt-spice
fi

# echo --host-subject
echo "your --host-subject is" \" `openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "` \"

4. restart libvirtd to rescan the configuration
   # service libvirtd restart

5. define and start the guest
   # virsh define guest.xml
   # virsh start guest

6. access the spice interface with ssl connection on source machine
   # remote-viewer spice://10.66.93.134/?port=5900\&tls-port=5901 --spice-host-subject "C=IL,L=Raanana,O=Red Hat,CN=my server" --spice-ca-file /etc/pki/libvirt-spice/ca-cert.pem

If remote-viewe got error pls use specec do this case again double safety

# spicec -h 10.66.93.134 -p 5900 -s 5901 --host-subject "C=IL,L=Raanana,O=Red Hat,CN=my server" --ca-file /etc/pki/libvirt-spice/ca-cert.pem --secure-channels main --enable-channels all -w redhat

 

input password

7. migrate guest on source machine before the passwd get expired

   # virsh migrate --live guest qemu+ssh://$destination_IP/system

 

   Note:
   10.66.93.134 --- your host IP
   5900 --- the port you defined in xml graphic section
   5901 --- the tlsPort you defined in xml graphic section
   redhat --- the passwd you set in the xml graphic section
   "C=IL,L=Raanana,O=Red Hat,CN=my server" --- is got from command #openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "
	
Expected Results:

1. make sure after migration , the spice session on source machine didn't disconnect
Notes:
Comments:

		176902 	[Graphical framebuffers] support for unix sockets - libvirt/vnc 	vbian 	None 	Manual 		Function 	P1 	5680 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

Steps :
1. Make sure guest can start successfully with virt-manager. Also the guest
console can display correctly

2. # vi /etc/libvirt/qemu.conf
enable vnc_auto_unix_socket = 1

# service libvirtd restart
Stopping libvirtd daemon:                                  [  OK  ]
Starting libvirtd daemon:                                  [  OK  ]

3. Start the guest
# virsh start a
Domain a started

# virsh dumpxml a
...
   <input type='mouse' bus='ps2'/>
   <graphics type='vnc' socket='/var/lib/libvirt/qemu/a.vnc'/>
...

# ps -ef |grep kvm
qemu     16945     1 24 01:20 ?        00:00:18 /usr/libexec/qemu-kvm -S -M
rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name a -uuid
8d130497-79fb-6ac9-f2fe-c66e5474eb35 -nodefconfig -nodefaults -chardev
socket,id=charmonitor,path=/var/lib/libvirt/qemu/a.monitor,server,nowait -mon
chardev=charmonitor,id=monitor,mode=control -rtc base=utc -boot c -drive
file=/var/lib/libvirt/images/qcow2,if=none,id=drive-virtio-disk0,format=qcow2,cache=none
-device
virtio-blk-pci,bus=pci.0,addr=0x4,drive=drive-virtio-disk0,id=virtio-disk0
-netdev tap,fd=22,id=hostnet0 -device
virtio-net-pci,netdev=hostnet0,id=net0,mac=52:54:00:5f:3a:d1,bus=pci.0,addr=0x5
-chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0
-usb -device usb-tablet,id=input0 -vnc unix:/var/lib/libvirt/qemu/a.vnc -vga
cirrus -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x3

4.Start virt-manager with user root to check the display
# virt-manager

Double click guest , the virt-manager displays guest graphical interface .

	
Expected Results:
Notes:
Comments:

		176903 	[Graphical framebuffers] vnc -878779 	gren 	yoyzhang 	Manual 		Feature 	P1 	5690 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers
    RHEL6.0

bug:

    No bug found

Actions:

  1. Define guest with a graphics device like:
.........................................
      <graphics type='vnc' port='-1' autoport='yes' passwd="foobar"/>
.........................................

   2. Connecting with vncviewer or virt-viewer . 
      # virsh start  $vmname 
     2.1 # virsh dumpxml --security-info $vmname
     2.2 #virsh domdisplay $vmname --include-password

3.Change password. 
   # cat change-passwd.xml
   <graphics type='vnc' port='-1' autoport='yes'  listen='127.0.0.1' passwd="new"/>

   # virsh update-device $vmname change-passwd.xml

4. Shutdown the guest and edit the guest xml to set a different VNC listen address. For VNC to be
   remotely accessible:
   <graphics type='vnc' listen="0.0.0.0" port="5910" autoport='no'/>

Or 
Changing the default VNC listen address in /etc/libvirt/qemu.conf should

   have a similar effect for VMs without an explicit listen address
   set. then restart libvirtd.

# service libvirtd restart

5. Opening the port value in the host firewall should allow VNC to be
 accessed from another machine. such as:

# iptables -F
# vncviewer [hostip]:[port] 

 

	
Expected Results:

2.Connecting with vncviewer or virt-viewer should show a password prompt,\

2.1 Verify XML change was accepted with " <graphics type='vnc' port='-1' autoport='yes' passwd="foobar"/>".

2.2 can display the vnc passwd successfully,like:

vnc://:foobar@localhost:0

3. Reopen the guest console, the new password will be worked instead of old passwd

5. The guest could be opened on remote host successfully
Notes:
add a command in actions 2.2 :virsh domdisplay $vmname --include-password
--wangzhenfeng
Comments:

		176904 	[Graphical framebuffers] VNC password ticketing and reset 	vbian 	None 	Manual 		Regression 	P1 	5700 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1. get system UTC date
   # date -u
   for example you get Tue Jan  4 06:25:48 UTC 2011
2. create the guest with the following graphic device
...
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us' passwd='aaabbb' passwdValidTo='2011-01-04T06:28:00'/>
...
    Note: please define the passwdValidTo to a time later than the date you got from step 1

3. start the guest , and access the vnc interface
   # virsh start #guest
   # vncviewer $hostIP:$vncport

4. close the vnc dialog, and reopen the vncviewer after the password expeired
   # vncviewer $hostIP:$vncport

5. shutdown the guest , and reset the guest password in the xml file
   # virsh destroy $guest
   # virsh edit
   edit the vncpassword to a new one
6. start the guest , and access the vnc interface
   # virsh start $guest
   # vncviewer $hostIP:$vncport

	
Expected Results:

3. you could get the vnc interface with the correct password inputted
4. you CAN'T get the vnc interface with the old expeired password
6. you could get the the vnc interface again with the new set password
Notes:
Comments:

		176905 	[Graphical framebuffers] VNC password ticketing and reset 	vbian 	vbian 	Manual 		Regression 	P1 	5710 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers
    Regression

bug:

    No bug found

Actions:

1. get system UTC date
   # date -u
   for example you get Tue Jan  4 06:25:48 UTC 2011

2. create the guest with the following graphic device
...
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us' passwd='aaabbb' passwdValidTo='2011-01-04T06:28:00'/>
...
    Note: please define the passwdValidTo to a time later than the date you got from step 1

3. start the guest , and access the vnc interface
   # virsh start #guest
   # vncviewer $hostIP:$vncport (if listening address is 127.0.0.1, $hostIP is 127.0.0.1)

or # virt-viewer $guestname
    # remote-viewer vnc://$hostIP:$vncport

4. close the vnc dialog, and reopen the vncviewer after the password expeired
   # vncviewer $hostIP:$vncport

or # virt-viewer $guestname
    # remote-viewer vnc://$hostIP:$vncport

5. shutdown the guest , and reset the guest password in the xml file
   # virsh destroy $guest
   # virsh edit
   edit the vncpassword to a new one

6. start the guest , and access the vnc interface
   # virsh start $guest
   # vncviewer $hostIP:$vncport

or # virt-viewer $guestname
    # remote-viewer vnc://$hostIP:$vncport
	
Expected Results:

3. you could get the vnc interface with the correct password inputted
4. you CAN'T get the vnc interface with the old expeired password
6. you could get the the vnc interface again with the new set password
Notes:
Comments:

		176906 	[Graphical framebuffers] vnc tls connection 	vbian 	None 	Manual 		Function 	P1 	5720 	Edit
Setup:

1.

- 2 host installed with kvm kernel

- libvirtd service is running on both system

-require package "gnutls-utils" installed

2.

Make sure 2 hosts UTC time was same.

#date -U

if not please set it.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

On server (dhcp-66.70.159.nay.redhat.com)

1. Set up a Certificate Authority (CA)

1.1 # certtool --generate-privkey > cakey.pem

1.2 self-sign cakey.pem by creating a file with the signature details called ca.info containing:

cn = dhcp-66.70.159.nay.redhat.com
ca
cert_signing_key

1.3 # certtool --generate-self-signed --load-privkey cakey.pem --template ca.info --outfile cacert.pem

2. Create server certificates

2.1 certtool --generate-privkey > serverkey.pem

2.2 sign that key with the CA's private key by first creating a template file called server.info
    
organization = Red Hat
cn = dhcp-66.70.159.nay.redhat.com
tls_www_server
encryption_key
signing_key

2.3 # certtool --generate-certificate --load-privkey serverkey.pem --load-ca-certificate cacert.pem \
--load-ca-privkey cakey.pem --template server.info --outfile servercert.pem

3. Copy CA key and server key to correct directory

3.1 # cp cakey.pem cacert.pem /etc/pki/CA

3.2 # mkdir -p /etc/pki/libvirt/private

3.3 # cp serverkey.pem /etc/pki/libvirt/private

3.4 # cp servercert.pem /etc/pki/libvirt

4. Copy CA key to client(10.66.70.162) into correct directory

4.1 # scp cakey.pem cacert.pem root@10.66.70.162:/etc/pki/CA

5. Turn on libvird monitor listening in /etc/sysconfig/libvirtd
  -- uncomment LIBVIRTD_ARGS="--listen"

6. Edit /etc/libvirt/libvirtd.conf
  -- enbale listen_tls = 1

auth_tls = "none"

7. configure qemu.conf file

    enable  vnc_listen = "0.0.0.0" 

vnc_tls = 1
 

 vnc_tls_x509_verify = 1

8.  Create cert file in libvirt-vnc 

# mkdir -m 750 /etc/pki/libvirt-vnc

# ln -s /etc/pki/CA/cacert.pem /etc/pki/libvirt-vnc/ca-cert.pem

# ln -s /etc/pki/libvirt/servercert.pem /etc/pki/libvirt-vnc/server-cert.pem

# ln -s /etc/pki/libvirt/private/serverkey.pem /etc/pki/libvirt-vnc/server-key.pem

 

 

 # chgrp qemu /etc/pki/libvirt \
                   /etc/pki/libvirt-vnc \
                   /etc/pki/libvirt/private \
                   /etc/pki/libvirt/servercert.pem \
                   /etc/pki/libvirt/private/serverkey.pem

 # chmod 750 /etc/pki/libvirt \
                  /etc/pki/libvirt-vnc \
                  /etc/pki/libvirt/private

 # chmod 440 /etc/pki/libvirt/servercert.pem \
                  /etc/pki/libvirt/private/serverkey.pem

 

 

9. # service libvirtd restart

10. # service iptables stop

11. # virsh start guest

12. # ps -aef |grep guest   to see the vnc port of guest

 

On client (dhcp-66.70.162.nay.redhat.com)

13.  Create client certificates

13.1 # certtool --generate-privkey > clientkey.pem

13.2 Act as CA and sign the certificate.  Create client.info containing:

country = GB
state = London
locality = London
organization = Red Hat
cn = dhcp-66.70.162.nay.redhat.com
tls_www_client
encryption_key
signing_key

13.3 # certtool --generate-certificate  --load-privkey clientkey.pem --load-ca-certificate /etc/pki/CA/cacert.pem \
--load-ca-privkey /etc/pki/CA/cakey.pem --template client.info --outfile clientcert.pem

14. Copy client key to correct directory

14.1 # mkdir -p /etc/pki/libvirt/private

14.2 # cp clientkey.pem /etc/pki/libvirt/private

14.3 # cp clientcert.pem /etc/pki/libvirt/

15. virt-viewer -c qemu+tls://<hostname>/system <domain-name>
	
Expected Results:

Make sure you could see the guest on the client host
Notes:
Comments:

		176908 	[Graphical framebuffer] Configuring Spice compression options 	vbian 	vbian 	Manual 		Regression 	P1 	5730 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

TBD
	
Expected Results:
Notes:
Comments:

		176910 	[Graphical framebuffer] Console screenshot 	vbian 	vbian 	Manual 		Regression 	P1 	5740 	Edit
Setup:

prepare a guest with vnc or spice graphics
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

step:

1: start the guest

2: in console run command:

#virsh screenshot $guestname

3: prepare a spice win guest in the multi-monitors env, config the guest with 2 qxl video and install the qxl-win driver in the win guest.

4: start the win guest, virt-viewer the win guest, two windows poped up.

5: run command:

#virsh screenshot $guestname --screen 0

#virsh screenshot $guestname --screen 1
	
Expected Results:

verify:

after step 2:

Screenshot saved to rhel6.3-2012-05-01-22:50:23.ppm, with type of image/x-portable-pixmap

The screenshot is saved in the foloer which you run the command in step2. Open ppm file to ensure the screenshot worked correctly.

after step 5:

two ppm file saved, the screenshot is saved in the foloer which you run the command in step5.

Open the two ppm file ,ensure screenshot worked well.
Notes:
Comments:

		176912 	[Graphical framebuffers] Spice disable-copy-paste 	vbian 	vbian 	Manual 		Regression 	P1 	5750 	Edit
Setup:

1. got a healthy guest , configure the guest with following XML section

<domain type='kvm' id='21'>
  <name>Xdemo</name>
  <uuid>d586c19e-570e-e188-610c-ece7bbdba41c</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.1.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='threads'/>
      <source file='/var/lib/libvirt/images/Xdemo.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:43:d5:b3'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <model type='virtio'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <console type='unix'>
      <source mode='bind' path='/var/lib/libvirt/qemu/serial.sock'/>
      <target type='virtio' port='0'/>
      <alias name='console0'/>
    </console>
    <channel type='spicevmc'>
      <target type='virtio' name='com.redhat.spice.0'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='1'/>
    </channel>
    <input type='tablet' bus='usb'>
      <alias name='input0'/>
    </input>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' tlsPort='5901' autoport='yes'>
      <streaming mode='filter'/>
      <clipboard copypaste='no'/>
    </graphics>
    <sound model='ich6'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' vram='65536' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c345,c392</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c345,c392</imagelabel>
  </seclabel>
</domain>

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1:use Xdemo.xml to define guest
2:set spice listening port with TLS port set 
https://tcms.engineering.redhat.com/case/91173/
#virsh start Xdemo

#ps -ef | grep kvm

 

	
Expected Results:

make sure you could see this from the ps show

spice
port=5900,tls-port=5901,addr=127.0.0.1,disable-ticketing,x509-dir=/etc/pki/libvirt-spice,streaming-video=filter,disable-copy-paste

 
Notes:
Comments:

		176913 	[Graphical framebuffer] SPICE English keymap support 	vbian 	None 	Manual 		Regression 	P1 	5760 	Edit
Setup:

make sure you have a RHEL guest with X interface installed
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1. Define a domain with spice graphical device as following:

<domain type='kvm'>
  <name>demo</name>
  <uuid>6bd72aef-3661-8400-ab2c-c15c935cdd85</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/demo.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:71:a8:bb'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' autoport='no' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>

2. Start the domain via "vrish start" command

3. Install spice-client

    #yum install spice-client

4. Use spice-client to check guest .

# /usr/libexec/spicec -h 127.0.0.1 -p 5900

5. in the guest perform the following command

    # system-config-keyboard

    select the keyboard "en_US"  , press OK

6. open gedit , type  whatever in it
	
Expected Results:

1. make sure there is nothing missed for what you typed in gedit .
Notes:
Comments:

		177044 	[Interface hotplug] - Attach interface from args 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P1 	5760 	Edit
Setup:

Make sure your domain is running

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual networks

bug:

    No bug found

Actions:


[Steps]
1. Issue below command in gnome-terminal:
   # virsh attach-interface demo network default vnet1
2. Issue command:
   # virsh dumpxml demo
3. Check demo Hardware tab
	
Expected Results:

1. Output:
Interface attached successfully
 
2. Verify that  interface  works fine.

# ping www.google.com

 

3. Verify that this interface is added
Notes:
Comments:

		176914 	[Graphical framebuffers] SPICE English keymap support 	vbian 	vbian 	Manual 		Regression 	P1 	5770 	Edit
Setup:

make sure you have a RHEL guest with X interface installed
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers
    Regression

bug:

    No bug found

Actions:

1. Define a domain with spice graphical device as following:

<domain type='kvm'>
  <name>demo</name>
  <uuid>6bd72aef-3661-8400-ab2c-c15c935cdd85</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/demo.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:71:a8:bb'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' autoport='no' keymap='en-us'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>

2. Start the domain via "vrish start" command

3. Install virt-viewer

4. Use virt-viewer to check guest .

#remote-viewer spice://127.0.0.1:5900

5. in the guest perform the following command

    # system-config-keyboard

    select the keyboard "en_US"  , press OK

6. open gedit , type  whatever in it
	
Expected Results:

1. make sure there is nothing missed for what you typed in gedit .
Notes:
Comments:

		177045 	[Interface hotplug] - Detach interface from args 	yoyzhang 	yoyzhang 	Auto 		--default-- 	P1 	5770 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-device:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual networks

bug:

    No bug found

Actions:

[Prerequisites]
Have finished test case with summary '56684 Attach interface from args'
[Steps]
1. Issue below command in gnome-terminal:
   # virsh detach-interface demo network --mac 00:16:36:1f:16:7a
2. Issue below command:
   # virsh dumpxml demo
3. Check demo Hardware tab
	
Expected Results:

1. Output:
 Interface detached successfully
2. Verify that info about 00:16:36:1f:16:7ais removed from demo detail info
3. Verify the interface is removed
Notes:
Comments:

		176915 	[Graphical framebuffer] SPICE German keymap support 	vbian 	None 	Manual 		Regression 	P3 	5780 	Edit
Setup:

1. make sure you have physical German keyboard at hand

2. make sure you have one guest with Graphic interface installed
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1. Define a domain with spice graphical device as following:

<domain type='kvm'>
  <name>demo</name>
  <uuid>6bd72aef-3661-8400-ab2c-c15c935cdd85</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/demo.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:71:a8:bb'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' autoport='no' keymap='de'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>

2. Start the domain via "vrish start" command

3. Install spice-client

    #yum install spice-client

4. Use spice-client to check guest windows.

# /usr/libexec/spicec -h 127.0.0.1 -p 5900

5. in the guest perform the following command

    # system-config-keyboard

    select the keyboard "German"  , press OK

6. open gedit , type  whatever in it
	
Expected Results:

make sure there is nothing missed for what you typed in gedit .ï»¿
Notes:
Comments:

		177135 	[libvirtd]libvirtd crashes if client quits unexpectedly - bug 727071 	ydu 	ydu 	Manual 		Feature 	P2 	5780 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1. 

 # for ((i=0; i < 50; i++)); do virsh managedsave vm1 & done; killall virsh

2. 

# virsh list --all

 

	
Expected Results:

1.  libvirtd is still running

2. can get domains list.
Notes:
Comments:

		176916 	[Graphical framebuffers] SPICE German keymap support 	vbian 	vbian 	Manual 		Regression 	P3 	5790 	Edit
Setup:

1. make sure you have physical German keyboard at hand

2. make sure you have one guest with Graphic interface installed
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers
    Regression

bug:

    No bug found

Actions:

1. Define a domain with spice graphical device as following:

<domain type='kvm'>
  <name>demo</name>
  <uuid>6bd72aef-3661-8400-ab2c-c15c935cdd85</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/demo.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:71:a8:bb'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' autoport='no' keymap='de'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
  </devices>
</domain>

2. Start the domain via "vrish start" command

3. make sure virt-viewer installed.

4. Use virt-viewer to check guest windows.

#remote-viewer spice://127.0.0.1:5900

5. in the guest perform the following command

    # system-config-keyboard

    select the keyboard "German"  , press OK

6. open gedit , type  whatever in it
	
Expected Results:

make sure there is nothing missed for what you typed in gedit .ï»¿
Notes:
Comments:

		177081 	[libvirtd] libvirtd.conf error causes libvirtd to exit silently - bug 728654 	ydu 	ydu 	Manual 		Feature 	P2 	5790 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1.  Edit ï»¿/etc/libvirt/libvirtd.conf, and an unterminated stringfor log_outputs

log_outputs="1:file:/tmp/libvirt_debug.log

2. #killall libvird

3. #libvirtd

4.

   # cat /tmp/libvirt_debug.log

cat: /tmp/libvirt_debug.log: No such file or directory

 

	
Expected Results:

3. #libvirtd

[root@pengzhimoutest libvirt]# libvirtd
14:12:34.657: 6624: info : libvirt version: 0.9.4, package: 16.el6 (Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2011-10-07-11:24:26, x86-006.build.bos.redhat.com)
14:12:34.657: 6624: error : main:1365 : Can't load config file '/etc/libvirt/libvirtd.conf'

4. # cat /tmp/libvirt_debug.log

cat: /tmp/libvirt_debug.log: No such file or directory

 

 
Notes:
Comments:

		176907 	[Graphical framebuffers] After spice passwdValidTo get expired, migration will disconnect the spice connection session 	vbian 	None 	Manual 		Function 	P1 	5800 	Edit
Setup:

[Prerequisite]

1. check current UTC time 

# date -u

Thu Aug  4 02:12:51 UTC 2011


2. define the guest with the following xml file
<domain type='kvm' id='4'>
  <name>test</name>
  <uuid>1490cddf-cd72-c1a0-ac25-9a8ad0d81d56</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/cdrom_test.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:b5:d6:4c'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/6'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <serial type='vc'>
      <target port='1'/>
      <alias name='serial1'/>
    </serial>
    <console type='pty' tty='/dev/pts/6'>
      <source path='/dev/pts/6'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <channel type='pty'>
      <source path='/dev/pts/7'/>
      <target type='virtio' name='test.virtio.pty'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='0'/>
    </channel>
    <input type='mouse' bus='ps2'/>
    <graphics type='spice' port='5900' tlsPort='5901' autoport='yes' passwd='aaabbb' passwdValidTo='2011-08-04T02:14:01'>
      <channel name='main' mode='secure'/>
      <channel name='record' mode='insecure'/>
    </graphics>
    <sound model='ac97'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='qxl' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c360,c691</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c360,c691</imagelabel>
  </seclabel>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    856068 - From Run 44548
    856068 - From Run 45750
    855870 - From Run 47615
    855870 - From Run 49803
    855870 - From Run 54142

Actions:

1. backup the sysconfig file
   # cp /etc/libvirt/qemu.conf /etc/sysconfig/qemu.conf.bk

2. modify the followings in qemu.conf
   -# spice_listen = "0.0.0.0"
   + spice_listen = "0.0.0.0"

   -# spice_tls = 1
   + spice_tls = 1

   -# spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"
   + spice_tls_x509_cert_dir = "/etc/pki/libvirt-spice"

3. perform the following script, to generate the cert files for ssl , and then copy *.pem file info /etc/pkil/libvirt-spice directory,and also scp /etc/pki/libvirt-spice/* to the target machine /etc/pki/libvirt-spice
#!/bin/bash

SERVER_KEY=server-key.pem

# creating a key for our ca
if [ ! -e ca-key.pem ]; then
    openssl genrsa -des3 -out ca-key.pem 1024
fi
# creating a ca
if [ ! -e ca-cert.pem ]; then
    openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem  -subj "/C=IL/L=Raanana/O=Red Hat/CN=my CA"
fi
# create server key
if [ ! -e $SERVER_KEY ]; then
    openssl genrsa -out $SERVER_KEY 1024
fi
# create a certificate signing request (csr)
if [ ! -e server-key.csr ]; then
    openssl req -new -key $SERVER_KEY -out server-key.csr -subj "/C=IL/L=Raanana/O=Red Hat/CN=my server"
fi
# signing our server certificate with this ca
if [ ! -e server-cert.pem ]; then
    openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem
fi

# now create a key that doesn't require a passphrase
openssl rsa -in $SERVER_KEY -out $SERVER_KEY.insecure
mv $SERVER_KEY $SERVER_KEY.secure
mv $SERVER_KEY.insecure $SERVER_KEY

# show the results (no other effect)
openssl rsa -noout -text -in $SERVER_KEY
openssl rsa -noout -text -in ca-key.pem
openssl req -noout -text -in server-key.csr
openssl x509 -noout -text -in server-cert.pem
openssl x509 -noout -text -in ca-cert.pem

# copy *.pem file to /etc/pki/libvirt-spice
if [[ -d "/etc/pki/libvirt-spice" ]]
then
    cp ./*.pem /etc/pki/libvirt-spice
else
    mkdir /etc/pki/libvirt-spice
        cp ./*.pem /etc/pki/libvirt-spice
fi

# echo --host-subject
echo "your --host-subject is" \" `openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "` \"

4. restart libvirtd to rescan the configuration
   # service libvirtd restart

5. define and start the guest
   # virsh define guest.xml
   # virsh start guest

6. access the spice interface with ssl connection on source machine , and keep the spice session connected
   # remote-viewer spice://10.66.93.134/?tls-port=5901 --spice-host-subject "C=IL,L=Raanana,O=Red Hat,CN=my server" --spice-ca-file /etc/pki/libvirt-spice/ca-cert.pem

7. wait until spice passwdValidTo get expired , don't close the spice session on source machine migrate guest on source machine 

   # date -u

      Thu Aug  4 02:17:20 UTC 2011  

   # virsh migrate --live guest qemu+ssh://$destination_IP/system

 

   Note:
   10.66.93.134 --- your host IP
   5900 --- the port you defined in xml graphic section
   5901 --- the tlsPort you defined in xml graphic section
   redhat --- the passwd you set in the xml graphic section
   "C=IL,L=Raanana,O=Red Hat,CN=my server" --- is got from command #openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10- -d " "
	
Expected Results:

when migration start, the spice session closed cause of disconnected.

Bug 818041 - After spice passwdValidTo get expired, migration will disconnect the spice connection session 
Notes:
Comments:

		176874 	[domain async job handling] cancel a stuck migration job -- Bug 725373 	vbian 	None 	Manual 		Function 	P1 	5810 	Edit
Setup:
	
Breakdown:

TBD because the bug

the migration command still hangs.

 
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    async job handling
    Regression

bug:

    No bug found

Actions:

# virsh -r list
 Id Name                 State
----------------------------------
  8 NFS-FOR-TEMP         running

# virsh migrate NFS-FOR-TEMP --live --p2p
qemu+tls://10.35.116.1/system


on other shell:
]# iptables -A OUTPUT -d 10.35.116.1 -j REJECT
# virsh domjobabort NFS-FOR-TEMP 

	
Expected Results:

1. the migration should return.

error: operation failed: domain save job: canceled by client

2.Check the libvirtd log and error output,it should be appropriateï¼refer to bug 799478,which is not fixed now.

Notes:
Comments:

		176875 	[domain async job handling] correct prompt info -- Bug 727047 	vbian 	None 	Manual 		Function 	P1 	5820 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    async job handling
    Regression

bug:

    No bug found

Actions:

# virsh list --all
 Id Name                 State
----------------------------------
  - dom                  shut off

# virsh domjobabort dom
error: Requested operation is not valid: domain is not running

# virsh start dom
Domain dom started

# virsh domjobabort dom
error: Requested operation is not valid: no job is active on the domain

# virsh save dom /tmp/dom.save & sleep 2; virsh domjobabort dom
[1] 12034

# error: Failed to save domain dom to /tmp/dom.save
error: operation failed: domain save job: canceled by client

   # virsh start dom

    # virsh dump --crash toy /tmp/dom.crash &sleep 2;  virsh domjobabort dom

   [1]13037

  

# error: Failed to dump domain dom to /tmp/dom.crash
error: operation failed: domain save job: canceled by client

 

 

 

	
Expected Results:

Make sure all of the message you get from your command are the same as the result shown in action part
Notes:
Comments:

		176876 	[Domain async job handling] crash coredump & domjobabort 	jyang 	yoyzhang 	Auto 		Feature 	P2 	5830 	Edit
Setup:

1. a running domain, suppose its name is "toy"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    async job handling
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. live coredump a domain

   # virsh dump --crash toy toy.crash

2. before step 1 fnished, open another terminal, abort the coredump job by domjobabort with domain name as the argument:

   # virsh domjobabort toy

3. check the domain status

   # virsh list --all

4. shutdown the domain

    if the domain is not running, skip

    # virsh shutdown toy

 

5. start domain, and repeat step 1

6. before step 1 fnished, open another terminal, abort the coredump job by domjobabort with domain ID as the argument:

   # virsh domjobabort 34

7. repeat step 3

8. repeat step 4

 

9. start domain, and repeat step 1

10. before step 1 fnished, open another terminal, abort the coredump job by domjobabort with domain UUID as the argument:

   # virsh domjobabort 9d45b95a-ffaa-54e9-4f42-74a5106eff7c

11. repeat step 3

12. repeat step 4
	
Expected Results:

step 1, 5, 9:

      after the live coredump finished, the output will be:

       Domain toy dumped to toy.crash

 

step 2, 6, 10:

       if the command has been executed successfully,

the output will be an empty line.

       In the terminal which execute step 1, or step 4, or step 7, the output will be like:

       virsh # dump --crash toy toy.crash
error: Failed to core dump domain toy to toy.crash
error: operation failed: domain core dump job: canceled by client

 

step 3, 7, 11:

       toy is running

 

step 4, 8, 12:

        toy can be shutoff successfully
Notes:
Comments:

		176877 	[Domain async job handling] crash coredump & domjobinfo 	jyang 	None 	Auto 		Feature 	P1 	5840 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    async job handling
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. open a terminal, save a domain as:

   # virsh start toy

    # virsh dump --crash toy /tmp/toy.crash

 

2. before the step 1 finished, opening another terminal, get the job info as much as could(repeat this step) by domjobinfo with domain name as argument:

    # virsh domjobinfo toy

 

3. after the step 1 finished. check domain status by:

     # virsh list --all

 

4. open a terminal, save a domain as:

   # virsh start toy

    # virsh dump --crash toy /tmp/toy.crash

 

5. before the step 4 finished, opening another terminal, get the job info as much as could(repeat this step) by domjobinfo with domain ID as the argument:

    # virsh domjobinfo 35

 

6. after the step 4 finished. check domain status by:

     # virsh list --all

 

7. open a terminal, save a domain as:

   # virsh start toy

    # virsh dump --crash toy /tmp/toy.crash

 

8. before the step 4 finished, opening another terminal, get the job info as much as could(repeat this step) by domjobinfo with domain UUID as the argument:

    # virsh domjobinfo  9d45b95a-ffaa-54e9-4f42-74a5106eff7c

 

9. after the step 4 finished. check domain status by:

     # virsh list --all
	
Expected Results:

step 1:

      should take mins to finish

      after finished. the output will be:

       virsh # dump --crash toy /tmp/toy.crash
       Domain toy dumped to /tmp/toy.crash

 

step 2:

        before step 1 finished. the output will be like:

virsh # domjobinfo toy
Job type:         Unbounded   
Time elapsed:     975          ms
Data processed:   149.414 MB
Data remaining:   890.961 MB
Data total:       1.016 GB
Memory processed: 149.414 MB
Memory remaining: 890.961 MB
Memory total:     1.016 GB

         After step 1 finished, the output will be like:

virsh # domjobinfo toy
error: Requested operation is not valid: domain is not running

 

step 3:

        the domain is shutoff

 

step 4:

        similar with step 1

 

step 5:

         similar with step 2

 

step 6:

          similar with step 3

 

step 7:

           similar with step 1, step 4

 

step 8:

            similar with step 2, step 5

 

step 9:

             similar with step 3,  step 6
Notes:
Comments:

		176878 	[Domain async job handling] live coredump & domjobabort 	jyang 	yoyzhang 	Auto 		Feature 	P2 	5850 	Edit
Setup:

1. a running domain, suppose its name is "toy"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    async job handling
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. live coredump a domain

   # virsh dump --live toy toy.live

2. before step 1 fnished, open another terminal, abort the coredump job by domjobabort with domain name as the argument:

   # virsh domjobabort toy

3. check the domain status

   # virsh list --all

4. shutdown the domain

    if the domain is not running, skip

    # virsh shutdown toy

 

5. start domain, and repeat step 1

6. before step 1 fnished, open another terminal, abort the coredump job by domjobabort with domain ID as the argument:

   # virsh domjobabort 34

7. repeat step 3

8. repeat step 4

 

9. start domain, and repeat step 1

10. before step 1 fnished, open another terminal, abort the coredump job by domjobabort with domain UUID as the argument:

   # virsh domjobabort 9d45b95a-ffaa-54e9-4f42-74a5106eff7c

11. repeat step 3

12. repeat step 4
	
Expected Results:

step 1, 5, 9:

      after the live coredump finished, the output will be:

       Domain toy dumped to toy.live

 

step 2, 6, 10:

       if the command has been executed successfully,

the output will be an empty line.

       In the terminal which execute step 1, or step 4, or step 7, the output will be like:

       virsh # dump --live toy toy.live
error: Failed to core dump domain toy to toy.live
error: operation failed: domain core dump job: canceled by client

 

step 3, 7, 11:

       toy is still running

 

step 4, 8, 12:

        toy can be shutdown successfully
Notes:
Comments:

		176950 	[guest kernel debugging]Run virsh save with option --bypass-cache to avoid file system cache when saving 	yupzhang 	yupzhang 	Manual 		Function 	P1 	5850 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1.Start a guest.

2.Before virsh save guest,flush and free caches

#sync

# echo 3 > /proc/sys/vm/drop_caches

# head -n4 /proc/meminfo

3.Run virsh save without --bypass-cache option

#virsh save rhel6 rhel6.save

#head -n4 /proc/meminfo

4.Start the guest and clean cache again

# echo 3 > /proc/sys/vm/drop_caches

# head -n4 /proc/meminfo

5. Run virsh save with --bypass-cache option

# virsh dump save rhel6.save --bypass-cache

# head -n4 /proc/meminfo

	
Expected Results:

2.# head -n4 /proc/meminfo
MemTotal:        8060940 kB
MemFree:         6883020 kB
Buffers:            2116 kB
Cached:            71572 k

3.

# virsh save rhel6 rhel6.save
Domain rhel6 saved to rhel6.save

# head -n4 /proc/meminfo
MemTotal:        8060940 kB
MemFree:         7061636 kB
Buffers:            5456 kB
Cached:           312344 kB

4. # head -n4 /proc/meminfo
MemTotal:        8060940 kB
MemFree:         7367768 kB
Buffers:            1440 kB
Cached:            70860 kB

5.# virsh save rhel6 rhel6.save --bypass-cache
Domain rhel6 saved to rhel6.save

# head -n4 /proc/meminfo
MemTotal:        8060940 kB
MemFree:         7321652 kB
Buffers:            4164 kB
Cached:            77912 kB

Notes:
Comments:

		176879 	[Domain async job handling] live coredump & domjobinfo 	jyang 	yoyzhang 	Auto 		Feature 	P2 	5860 	Edit
Setup:

1. a running domain, suppose its name is "toy"
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    async job handling
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. open a terminal, save a domain as:

    # virsh dump --live toy toy.live

 

2. before the step 1 finished, opening another terminal, get the job info as much as could(repeat this step) by domjobinfo with domain name as argument:

    # virsh domjobinfo toy

 

3. after the step 1 finished. check domain status by:

     # virsh list --all

 

4. open a terminal, save a domain as:

    # virsh dump --live toy toy.live

 

5. before the step 4 finished, opening another terminal, get the job info as much as could(repeat this step) by domjobinfo with domain ID as the argument:

    # virsh domjobinfo 35

 

6. after the step 4 finished. check domain status by:

     # virsh list --all

 

7. open a terminal, save a domain as:

    # virsh dump --live toy toy.live

 

8. before the step 4 finished, opening another terminal, get the job info as much as could(repeat this step) by domjobinfo with domain UUID as the argument:

    # virsh domjobinfo  9d45b95a-ffaa-54e9-4f42-74a5106eff7c

 

9. after the step 4 finished. check domain status by:

     # virsh list --all

4. open a terminal, save a domain as:

    # virsh dump --live toy toy.live

 

5. before the step 4 finished, opening another terminal, get the job info as much as could(repeat this step) by domjobinfo with domain ID as the argument:

    # virsh domjobinfo 35

 

6. after the step 4 finished. check domain status by:

     # virsh list --all
	
Expected Results:

step 1:

      should take mins to finish

      after finished. the output will be:

       virsh # dump --live toy toy.live
       Domain toy dumped to toy.live

 

step 2:

        before step 1 fnished, the output will be like:

virsh # domjobinfo toy
Job type:         Unbounded   
Time elapsed:     975          ms
Data processed:   149.414 MB
Data remaining:   890.961 MB
Data total:       1.016 GB
Memory processed: 149.414 MB
Memory remaining: 890.961 MB
Memory total:     1.016 GB

       After step 1 finished, step output will be like:

virsh # domjobinfo toy
Job type:         None

 

step 3:

        the domain is still running

 

step 4:

        similar with step 1

 

step 5:

         similar with step 2

 

step 6:

          similar with step 3

 

step 7:

           similar with step 1, step 4

 

step 8:

            similar with step 2, step 5

 

step 9:

             similar with step 3,  step 6
Notes:
Comments:

		176880 	[Domain async job handling] migrate & domjobabort 	jyang 	yoyzhang 	Auto 		Feature 	P2 	5870 	Edit
Setup:

1. A RHEL6 host with kvm, with a running domain.

2. Another kvm host, which also is a RHEL6.

3. On both source host, and target host, libvirtd is running.

4. On both source and target machine:
    #iptables -F
    #setenforce 1

    #setsebool virt_use_nfs  on

5. Setup nfs service on Source target machine
 
    5.1 add following line into "/etc/exports"

     /var/lib/libvirt/images 10.66.70.144(rw,no_root_squash,async) 127.0.0.1(rw,no_root_squash,async)

    replace "10.66.70.144" to your destinate host IP

    5.2 service nfs start

6. Mount the nfs filesystem on source host to  both destinate host and source host.
    6.1 on destination machine
     Create /var/lib/libvirt/migrate on destinate host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     # mount -t nfs ${source_host_ip}:/var/lib/libvirt/images/  /var/lib/libvirt/migrate/

    6.2 On source machine
     Create /var/lib/libvirt/migrate on source host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     mount localhost:/var/lib/libvirt/images /var/lib/libvirt/migrate


7. After step 6, make sure your migration domain's disk image locate in "/var/lib/libvirt/migrate/" (such as :/var/lib/libvirt/migrate/migrate.img)on source machine(when you migrate the guest during installation,the iso also should be locate in "/var/lib/libvirt/migrate").

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    async job handling
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. start domain "migrate" on srouce host

   # virsh start migrate

 

2. wait for the domain fully started.

 

3. migrate

   # virsh migrate --live migrate qemu+ssh://${dest_host_ip}/system  

 

4. before step 3 finished, cancel migrate job

   # virsh domjobabort migrate

 

5.  check state of domain "migrate" on both source host and destinate host

    # virsh domstate migrate

 

6. shutdown "migrate" on source host.

     # virsh shutdown migrate

 

7. start "migrate" on source host

      # virsh start migrate
	
Expected Results:

step 4:

     the migrate job is successfully canceled.

step 5:

      on source host, migrate is still running, on destinate host, there is no domain named "migrate'

       check in guest. the guest is running well without any error, such as Buffer I/O error

step 6:

       the domain could be shutdown successfully

step 7:

        the domain could be started successfully.
Notes:
Comments:

		176881 	[Domain async job handling] migrate & domjobinfo 	jyang 	yoyzhang 	Auto 		Feature 	P2 	5880 	Edit
Setup:

1. A RHEL6 host with kvm, with a running domain.

2. Another kvm host, which also is a RHEL6.

3. On both source host, and target host, libvirtd is running.

4. On both source and target machine:
    #iptables -F
    #setenforce 1    

   #setsebool virt_use_nfs  on


5. Setup nfs service on Source target machine
 
    5.1 add following line into "/etc/exports"

     /var/lib/libvirt/images 10.66.70.144(rw,no_root_squash,async) 127.0.0.1(rw,no_root_squash,async)

    replace "10.66.70.144" to your destinate host IP

    5.2 service nfs start

6. Mount the nfs filesystem on source host to  both destinate host and source host.
    6.1 on destination machine
     Create /var/lib/libvirt/migrate on destinate host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     # mount -t nfs ${source_host_ip}:/var/lib/libvirt/images/  /var/lib/libvirt/migrate/

    6.2 On source machine
     Create /var/lib/libvirt/migrate on source host if it doesn't exist.
     # mkdir /var/lib/libvirt/migrate

     mount localhost:/var/lib/libvirt/images /var/lib/libvirt/migrate


7. After step 6, make sure your migration domain's disk image locate in "/var/lib/libvirt/migrate/" (such as :/var/lib/libvirt/migrate/migrate.img)on source machine(when you migrate the guest during installation,the iso also should be locate in "/var/lib/libvirt/migrate").
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    async job handling
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. start domain "migrate"

   # virsh start migrate

 

2. wait for the domain fully started.

 

3. migrate

   # virsh migrate --live migrate qemu+ssh://${dest_host_ip}/system  

 

4. before step 3 finished, get migrate job info as many times as could.

   # virsh domjobinfo migrate

 

5. when step 3 finished. check state of domain "migrate" on both source host and destinate host

    # virsh domstate migrate
	
Expected Results:

step 4:

      before step 3 is finished, the output will be like:

virsh # domjobinfo migrate
Job type:         Unbounded   
Time elapsed:     975          ms
Data processed:   149.414 MB
Data remaining:   890.961 MB
Data total:       1.016 GB
Memory processed: 149.414 MB
Memory remaining: 890.961 MB
Memory total:     1.016 GB

       After step 3 finished, on destinate output will be like:

virsh # domjobinfo migrate
Job type:         None

 on source host, output will be like:

virsh # domjobinfo migrate

error: Requested operation is not valid: domain is not running

 

step 5:

      on source host, migrate is shutoff, on destinate host, migrate is running.
Notes:
Comments:

		176882 	[Domain async job handling] Pause a domain while being migrated - bug 726650 	yimwang 	None 	Auto 		Feature 	P2 	5890 	Edit
Setup:

same as case "[domain async job handling] migrate & domjobabort "
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    async job handling
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. start domain "migrate"

   # virsh start migrate

2. wait for the domain fully started.

3. migrate

   # virsh migrate --live migrate qemu+ssh://${dest_host_ip}/system  

4.Before step 3 finished,open the other terminal and suspend guest os on source host .

    # virsh suspend migrate

5. after step 3 finished. Check the domain running on the destination and responding to user interaction.

  # ifconfig

 # ping www.google.com

 
	
Expected Results:

3. Check the domain is running and responding to user interaction.

4.Check the domain is paused and not responding to user interaction any more.

5. Check the domain running on the destination and responding to user interaction.

Notes:
Comments:

		177578 	[snapshot]snapshot with --disk-only --bug 747115 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	5900 	Edit
Setup:

Prepare running qcow2 guest
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression
    QE consumption

bug:

    837544 - From Run 43379
    852668 - From Run 44628

Actions:

1. Prerequisite: install qemu-kvm-rhev packages.

# virsh snapshot-create f16 --disk-only 
Domain snapshot 1332999912 created

after snapshot guest xml 

 <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/f16qcow2.1332999912'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>

 2. Prerequisite: install qemu-kvm-rhel packages.

# virsh snapshot-create f16 --disk-only 
error: unsupported configuration: live disk snapshot not supported with this QEMU binary

3) shutdown the domain then do disk-only snapshot 

# virsh list  --all
 Id    Name                           State
----------------------------------------------------
-     b1                          shut off

 

# virsh snapshot-create-as b1 --disk-only

 Domain snapshot 1332999913 created






	
Expected Results:

Same as steps

 

 

step2  qemu-kvm   for rhel 

error message should be clear as step
Notes:
Comments:

		176883 	[Domain async job handling] save & domjobabort 	jyang 	None 	Auto 		Feature 	P1 	5910 	Edit
Setup:

1. a running domain, suppose its name is "toy"

 

2. make sure the domain is fully started. because if it's just in the process of booting, the actions such as "save", "dump" will cost little time, we will not have time cancel it via "domjobabort".
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    async job handling
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. save a domain

 # virsh save toy toy.save

 

2. cancel the job before step 1 finished.

  # virsh domjobabort toy

 

3. shutdown domain

  # virsh shutdown toy
	
Expected Results:

step 1:

      if it's finished. the output will be:

      ---------------------------------------

      Domain toy saved to toy.save

 

      if it's canceled before finished. the output will be:

      ---------------------------------------

      error: Failed to save domain toy to toy.save

      error: operation failed: domain save job: canceled by client

 

step 2:

       output empty line if successed.

       or error message throwed if the save job is finished yet:

        --------------------------------

        error: Requested operation is not valid: domain is not running

 

step 3:

        the domain can be shutdown successfully.
Notes:
Comments:

		177580 	[snapshot]snapshot XML description --bug 741510 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	5910 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1 # virsh snapshot-list rhel6
 Name                 Creation Time             State
------------------------------------------------------------
 s1                   2010-02-05 20:02:36 -0500 running

2 # virsh snapshot-dumpxml rhel6 s1
<domainsnapshot>
.....
   <domain type='kvm'>
.......
   </domain>
</domainsnapshot>

	
Expected Results:

Step 2 not like:

<domainsnapshot>
.....
<domain type='kvm'>
......
</domain>
</domainsnapshot>

 
Notes:
Comments:

		176884 	[Domain async job handling] save & domjobinfo 	yimwang 	None 	Auto 		Feature 	P2 	5920 	Edit
Setup:

1. a running domain, suppose its name is "toy"

 

2. make sure the domain is fully started.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    async job handling
    RHEL6.0

bug:

    No bug found

Actions:

1. save a domain

# virsh save toy /tmp/toy.save

2. Open anther terminal and run command before step 1 finished.

# virsh domjobinfo toy

after step 1 finished, domain state is shutoff. so start domain for step 3

# virsh start toy

or

# virsh restore toy.save

3. save a domain

# virsh  managedsave toy

4. Open anther terminal and run command before step 3 finished.

# virsh domjobinfo toy

 

 

 

	
Expected Results:

2.# virsh domjobinfo toy
Job type:         Unbounded   
Time elapsed:     29747        ms
Data processed:   1.678 GB
Data remaining:   346.242 MB
Data total:       2.016 GB
Memory processed: 1.678 GB
Memory remaining: 346.242 MB
Memory total:     2.016 GB

...

Final

# virsh domjobinfo toy
error: Requested operation is not valid: domain is not running

4.# virsh domjobinfo toy
Job type:         Unbounded   
Time elapsed:     1295         ms
Data processed:   5.438 MB
Data remaining:   2.011 GB
Data total:       2.016 GB
Memory processed: 5.438 MB
Memory remaining: 2.011 GB
Memory total:     2.016 GB

Notes:
Comments:

		176886 	[Floppy disk support] Test floppy media insert/eject.bug-625319 	yimwang 	yimwang 	Manual 		Feature 	P2 	5930 	Edit
Setup:

1.a healthy guest, which is shutoff

2.if the guest os is rhel5, pls do modprobe first when login into the guest.

#modprobe floppy

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    floppy disk support
    Regression

bug:

    No bug found

Actions:

1.  dump the xml of guest

   # virsh dumpxml toy > toy.xml

2.  insert following xml into the "<device>" node of toy.xml

    <disk type='block' device='floppy'>
      <driver name='qemu' type='raw'/>
      <source dev='/var/lib/libvirt/images/floppy1.img'/>
      <target dev='fda' bus='fdc'/>
      <readonly/>
    </disk>


3. undefine guest
 # virsh undefine toy
# qemu-img create floppy1.img 10M
# qemu-img create floppy2.img 10M

4. define guest with "toy.xml"
  # virsh define toy.xml

5.Create a floppy eject XML.
 # cat eject.xml

   <disk type='block' device='floppy'>
     <target dev='fda' bus='fdc'/>
   </disk>



6.Create a floppy insert XML.


  # cat insert.xml
   <disk type='block' device='floppy'>
     <source dev='/var/lib/libvirt/images/floppy2.img'/>
     <target dev='fda' bus='fdc'/>
   </disk>




7. Eject floppy.

 # virsh update-device toy eject.xml

8.Inset floppy.

# virsh update-device toy insert.xml  

# mount /dev/fd0 /mnt
# touch /mnt/test.txt

	
Expected Results:

7. Check the floppy1.img is disconnected from guest

8. Check floppy2.img is attached to guest successfully and could be mounted in guest, read and write in guest
Notes:
Comments:

		177582 	[snapshot]snapshot-parent should error when current snapshot is a root --bug 742410 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	5930 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:


when current snapshot is a root, you can't get any error information by virsh
snapshot-parent, however, the command will return 1.

Make sure you have no snapshots of your guest. So the "hello" snapshot is the root snapshot.

1. cat /tmp/snap.xml 
<domainsnapshot>
  <name>hello</name>
  <state>shutoff</state>
</domainsnapshot>

2. virsh snapshot-create vr-rhel5u4-x86_64-kvm snap.xml

3. virsh snapshot-list vr-rhel5u4-x86_64-kvm --parent

4. virsh snapshot-parent vr-rhel5u4-x86_64-kvm hello

5. echo $?

	
Expected Results:


The Step4 shoult error like:
error: snapshot 'hello' does not have a parent

The step 5 should be 1

Notes:
Comments:

		176887 	[Floppy disk support] Assign a block type floppy to guest 	ydu 	None 	Manual (Autoproposed) 		Feature 	P2 	5940 	Edit
Setup:

1. A healthy guest, which is shutoff
2. If the guest os is rhel6, pls do modprobe first when login into the guest.  

    #modprobe floppy
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. edit the xml of the guest, and add the following xml in node "<device>" to use 'block' disk type  

#virsh edit toy 

    <disk type='block' device='floppy'>

        <driver name='qemu' type='raw'/>

      <source dev='/var/lib/libvirt/images/fd.img'/>

       <target dev='fda' bus='fdc'/>

    </disk>
2. create a floppy disk fd.img  

#dd if=/dev/zero of=/var/lib/libvirt/images/fd.img bs=1 count=10 seek=1474K


3. format the floppy disk in ext3 format

 #mkfs.ext3 /var/lib/libvirt/images/fd.img
4. start the guest  

#virsh start toy

5. login into guest, mount the floopy disk, create a file(NOTE: not exceeding the capacity of the floppy disk)

#mount /dev/fd0 /mnt

#cd /mnt 

# touch test.txt  

then write some content in the file and save it

#cat test.txt
	
Expected Results:
Notes:
Comments:

		176892 	[Graphical framebuffers] Host firewall port listening for different Spice ports 	vbian 	None 	Manual 		Feature 	P2 	5950 	Edit
Setup:

make sure you have done the case  Spice SSL connection

https://tcms.engineering.redhat.com/case/95445/?from_plan=4050

make sure virt-viewer installed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    graphical framebuffers

bug:

    No bug found

Actions:

1. Add following lines in /etc/sysconfig/iptables
-A INPUT -p tcp -m multiport --dports 5900:6000 -j REJECT

2. edit the guest xml file like this
...
  <graphics type='spice' port='-1' tlsPort='-1' autoport='yes' passwd='redhat'>
    <channel name='main' mode='secure'/>
    <channel name='record' mode='insecure'/>
  </graphics>
...

3. start the guest , and try to access the vnc interface
   # virsh start guest
   # remote-viewer spice://$ip/?tls-port=5901 --spice-host-subject "C=IL,L=Raanana,O=Red Hat,CN=my server" --spice-ca-file /etc/pki/libvirt-spice/ca-cert.pem
4. modify the /etc/sysconfig/iptables file like this
   change the following line
   -A INPUT -p tcp -m multiport --dports 5900:6000 -j REJECT
   To
   -A INPUT -p tcp -m multiport --dports 5900:6000 -j ACCEPT
5. restart iptables , and try to access the vnc interface again
   # service iptables restart
   # remote-viewer spice://$ip/?tls-port=5901 --spice-host-subject "C=IL,L=Raanana,O=Red Hat,CN=my server" --spice-ca-file /etc/pki/libvirt-spice/ca-cert.pem
	
Expected Results:

3. you CAN'T access spice interface
5. you could access spice interface

Notes:
Comments:

		176848 	[Disk hotplug] Detach virtual disks to guest over the top limitation via detach-disk 	ajia 	None 	Auto 		Stress 	P1 	5960 	Edit
Setup:

do case 95358 [Disk hotplug] Attach virtual disks to guest over the top limitation via attach-disk first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Detach 26 disks from guest

# for i in {a..z}; do virsh detach-disk <guest> vd$i; done

2. Login guest to check

# fdisk -l | grep /dev/vd

 
	
Expected Results:

step1. All the guest can be detached successfully. No disk-a .. disk-z can be find in domain xml

step2. no virtio disk can be find
Notes:
Comments:

		176850 	[Disk hotplug] eject CDROM device media IN guest -- bug 575160 	vbian 	None 	Manual 		Regression 	P1 	5970 	Edit
Setup:

1. define a guest with the following XML section in guest xml
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/sr0'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
2. Insert a CD into host optical drive

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    disk hotplug

bug:

    No bug found

Actions:

1. Start the guest
   # virsh start guest
2. Look up the original XML for the guest
   # virsh dumpxml guest |grep cdrom -A 7
3. Eject CDROM from guest
   [in guest] run "# eject /dev/sr0"
4. Check the eject result in GUEST
   [in guest] run "# mount -o loop /dev/sr0 /mnt;ls /mnt"
5. Look up the XML for the guest after ejecting the CDROM
   # virsh dumpxml guest |grep cdrom -A 7

	
Expected Results:

2. you could see the guest xml cdrom section like this:
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/sr0'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
4. in guest you can't mount the sr0 , and you can see the output like this :
   /dev/sr0: No medium found
5. you could see the guest XML cdrom section changed to be like this :
   <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>

Notes:
Comments:

		177784 	[Virtual disks] Eject CDROM/Floppy device on guest - bug 575160 	weizhan 	weizhan 	Manual 		Regression 	P1 	5980 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virtual disks

bug:

    No bug found

Actions:

1. prepare a running domain with cdrom iso file inserted
# virsh dumpxml rhel6u2
...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/aaa.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
...
2. login domain and eject cdrom
In guest run:
# eject /dev/sr0

3. recheck domain xml
# virsh dumpxml rhel6u2
...
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/aaa.iso'/>
      <target dev='hdc' bus='ide' tray='open'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>
...

Now tray='open'.

4. do save/restore and migrate
4.1 save/restore
# virsh save rhel6u2 /tmp/aaa

Domain rhel6u2 saved to /tmp/aaa
# virsh restore /tmp/aaa
Domain restored from /tmp/aaa

check in domain, cdrom is empty.

4.2 migrate
# virsh migrate rhel6u2 qemu+ssh://$remote_host/system

Migrate success, log in domain on remote host and check, cdrom is empty.

5. destroy domain and restart
# virsh destroy rhel6u2
Domain rhel6u2 destroyed

# virsh dumpxml rhel6u2
    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/aaa.iso'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' target='0' unit='0'/>
    </disk>

# virsh start rhel6u2
Domain rhel6u2 started

check in domain, cdrom iso is back. 
So, if user want the iso file back, need to destroy the domain and start it
again.

	
Expected Results:

Follow all the step result
Notes:
Comments:

		176851 	[disk hotplug] Hot-plug a large capacity disk 	weizhan 	None 	Manual 		--default-- 	P1 	5990 	Edit
Setup:

Find a host with large disk(more than 1T)

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Start a domain: # virsh start demo

2. # qemu-img create -f raw  foo.img 1T

3. # virsh attach-disk demo /var/lib/libvirt/images/foo.img vdb
Disk attached successfully     

4. Check if the new disk file has been added into domain
    # virsh dumpxml demo
       ....
       <disk type='file' device='disk'>
          <driver name='qemu' type='raw'/>
          <source file='/var/lib/libvirt/images/foo.img'/>
          <target dev='vdb' bus='virtio'/>
       </disk>
       ....

5. Run command 'fdisk -l' in guest

6. # virsh detach-disk demo vdb

7. Run command 'fdisk -l' in guest
	
Expected Results:

step 3:

Disk device attached successfully

step 5:

check the disk device is existing.

step 6:

Disk detached successfully

step 7:

check the disk device is not existing.
Notes:
Comments:

		176852 	[Disk hotplug] Hot-plug/unplug a SAN disk with NPIV 	weizhan 	None 	Manual 		Feature 	P1 	6000 	Edit
Setup:

do case " 124554 [NPIV] Discover SAN Storageï»¿" first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. start a guest
# virsh start demo

2. Attach the fc disk to guest
# virsh attach-disk demo /dev/disk/by-id/wwn-0x600a0b80005ad1d7000013bf4ecab373 vdb
Disk attached successfully

3. Check if the new disk file has been added into domain
....
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/disk/by-id/wwn-0x600a0b80005ad1d7000013bf4ecab373'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0b' function='0x0'/>
    </disk>
....

4. Run command 'fdisk -l' in guest and mount to check if it is readable/writible
there is vdb device exist

6. Detach the logical volume
# virsh detach-disk demo vdb
Disk detached successfully

7. Check the device on guest is removed

	
Expected Results:
Notes:
Comments:

		177572 	[snapshot]live snapshot 2 disks with --disk-only --bug 740375 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	6000 	Edit
Setup:

prepare a running  guest with 2 disks.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1. # virsh list
 Id Name                 State
----------------------------------
  3 foo                  running

2. # virsh snapshot-list foo
 Name                 Creation Time             State
------------------------------------------------------------


3. # qemu-img info /var/lib/libvirt/images/foo.img 
image: /var/lib/libvirt/images/foo.img
file format: qcow2
virtual size: 5.9G (6291456000 bytes)
disk size: 2.4G
cluster_size: 65536

4. # virsh snapshot-create-as foo s1 --disk-only
Domain snapshot s1 created

# virsh snapshot-list foo
 Name                 Creation Time             State
------------------------------------------------------------
 s1                   2010-03-15 20:42:02 -0400 disk-snapshot

5. # virsh snapshot-dumpxml foo s1
[...]
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/foo.img'/>
      <target dev='vda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2' cache='none'/>
      <source file='/var/lib/libvirt/images/foo1.img'/>
      <target dev='vdb' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='1'/>
    </disk>
[...]

6. # qemu-img info /var/lib/libvirt/images/foo.s1 
image: /var/lib/libvirt/images/foo.s1
file format: qcow2
virtual size: 5.9G (6291456000 bytes)
disk size: 460K
cluster_size: 65536
backing file: /var/lib/libvirt/images/foo.img (actual path:
/var/lib/libvirt/images/foo.img)

7. # qemu-img info /var/lib/libvirt/images/foo1.s1 
image: /var/lib/libvirt/images/foo1.s1
file format: qcow2
virtual size: 6.0G (6442450944 bytes)
disk size: 136K
cluster_size: 65536
backing file: /var/lib/libvirt/images/foo1.img (actual path:
/var/lib/libvirt/images/foo1.img)

 
	
Expected Results:

Step4:

For qemu-kvm-qemu, snapshot will succeed.

For qemu-kvm snapshot should fail like:

error: operation failed: Failed to take snapshot: unknown command: 'snapshot_blkdev'
Notes:
Comments:

		176853 	[Disk hotplug] Hot-unplug a disk device to guest with rebooting the guest 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	6010 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Define and start a guest with the following configure on xml
....
<disk type='file' device='disk'>
<driver name='qemu' type='raw'/>
<source file='/var/lib/libvirt/images/foo.img'/>
<target dev='vdb' bus='virtio'/>
</disk>
....

2. Run command 'fdisk -l' in guest
there is vdb device exist

3. Detach the disk
virsh detach-disk demo vdb
Disk detached successfully

4. Check if the new disk file has been removed from domain
# virsh dumpxml demo

5. Run command 'fdisk -l' in guest and mount to check if it is readable/writible
there is no vdb device exist

6. Shutdown and start the guest, the device should be back.
Run command 'fdisk -l' in guest
there is vdb device exist

	
Expected Results:
Notes:
Comments:

		177574 	[snapshot]revert a snapshot --bug 742615 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	6010 	Edit
Setup:

a running guest with a active snapshot
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

 


1 # virsh snapshot-revert dom snap



2 # virsh snapshot-revert dom snap --force



	
Expected Results:

 

Step2  will secceed
Notes:
Comments:

		177575 	[snapshot]revert a snapshot after add a disk --bug 742615 	zhpeng 	zhpeng 	Manual (Autoproposed) 		Regression 	P1 	6020 	Edit
Setup:

 create an inactive snapshot of guest.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1.prepare the second disk:

# qemu-img create -f qcow2 /var/lib/libvirt/images/bar.img 10M
Formatting '/var/lib/libvirt/images/bar.img', fmt=qcow2 size=10485760
encryption=off cluster_size=65536

2. # virsh edit dom (add the second disk)
Domain dom XML configuration edited.

# virsh dumpxml dom
<domain type='qemu'>
......
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/foo.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/bar.img'/>
      <target dev='vdb' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05'
function='0x0'/>
    </disk>
......
</domain>


Step3


virsh # snapshot-revert kvm1 snap

--force option apply to make an incompatible change and revert active snapshot, if revert to inactive snapshot, no need force option.
	
Expected Results:

Step 3 succeed
Notes:
Comments:

		176854 	[Disk hotplug] Hotplug a disk device to guest with rebooting the guest 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	6030 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Start a domain:
# virsh start demo

2. Create a image to attach
# qemu-img create -f raw -o foo.img 10M

3. Attach a virtio disk  
# virsh attach-disk demo /var/lib/libvirt/images/foo.img vdb
Disk attached successfully

4. Check if the new disk file has been added into domain
# virsh dumpxml demo
....
<disk type='file' device='disk'>
<driver name='qemu' type='raw'/>
<source file='/var/lib/libvirt/images/foo.img'/>
<target dev='vdb' bus='virtio'/>
</disk>
....

5. Run command 'fdisk -l' in guest and mount to check if it is readable/writible
there is vdb device exist

6. Shutdown and start the guest
The device should be removed.
	
Expected Results:
Notes:
Comments:

		176855 	[disk hotplug] hotunplug a device from guest during the process of start or shutdown guest (658713) 	jialiu 	None 	Manual (Autoproposed) 		--default-- 	P1 	6050 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1.# qemu-img create -f raw -o foo.img 10M

2. Start a domain with a second disk
# virsh dumpxml domain
...

<disk type="file" device="disk">
  <source file="/var/lib/libvirt/images/foo.img"/>
  <target dev="vdb" bus="virtio"/>
</disk>

...

Make sure the above disk file is attached to guest as a secondary disk

# virsh start domain

3. During the process of guest start, unhotplug the secondary disk

# virsh detach-device demo disk.xml

4. Run command 'fdisk -l' in guest

5. Restart the guest

6. Run command 'fdisk -l' in guest

7. # virsh shutdown demo

8. During the process of shutdown, dettach the disk

# virsh detach-disk demo vdb --persistent

9. # virsh start demo

log in guest:

# fdisk -l
	
Expected Results:

step 3:

Device dettached successfully

step 4:

check the disk device is not existing.

step 5:

Device will be back

step 6:

check the disk device is existing.

step 8:

Disk detached successfully

step 9:

check the disk device is not existing.
Notes:
Comments:

		176856 	[Disk hotplug] IDE storage devices unsoupport hotplug 	yimwang 	None 	Auto 		Negative test 	P1 	6060 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual disks
    virsh-rail

bug:

    No bug found

Actions:

 1. Start a guest
      # virsh start demo
 2. Create a disk image to /var/lib/libvirt/images/ directory using dd or
qemu-img, for example:
      # qemu-img create -f raw /var/lib/libvirt/images/foo.img 10M

  3. Attach the foo.img disk to guest demo
      # virsh attach-disk demo /var/lib/libvirt/images/foo.img hdb

      or using virsh attach-device demo disk.xml to replace step 3
      the content of disk.xml is:
        <disk type='file' device='disk'>
          <source file='/var/lib/libvirt/images/foo.img'/>
         <target dev='hdb' bus='ide'/>
        </disk>

	
Expected Results:

step 3:

 

# virsh attach-device demo disk.xml
error: Failed to attach device from disk.xml
error: this function is not supported by the connection driver: disk bus 'ide' cannot be hotplugged.

or

# virsh attach-disk demo /var/lib/libvirt/images/foo.img hdb
error: Failed to attach disk
error: this function is not supported by the connection driver: disk bus 'ide' cannot be hotplugged.

Notes:
Comments:

		176857 	[Disk hotplug] Inject/Eject a cdrom to/from guest 	weizhan 	None 	Manual (Autoproposed) 		Feature 	P1 	6070 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. start a guest with xml
....
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
....

2. Insert a CD/DVD to the CDROM

3. Connect a device to cdrom
# virsh attach-disk test /dev/sr0 hdc --type cdrom --mode readonly

4. Check the dump xml of guest
....
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/sr0'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
....

5. Check if it exist on guest
# mount /dev/sr0 /mnt
# ls /mnt
you can see the CD/DVD file on /mnt

then umount the /mnt dir.

6. Eject the CD/DVD from guest

# cat empty-cdrom.xml
 <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
 </disk>
# virsh attach-device rhel6-2 eject-cdrom.xml
Device attached successfully

7. Check the dump xml of guest
....
   <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
....

8. Check there is no CD/DVD on cdrom
	
Expected Results:
Notes:
Comments:

		176858 	[Disk hotplug] migration after attach disk device to guest 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	6080 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp

 

Becasue this case is relative to migration, so make sure both guest image file and hotplug disk file are in the same sharable nfs directory.

In this case, the image file in /var/lib/libvirt/migrate/, it is mounting the sharable nfs directory.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    virtual disks

bug:

    No bug found

Actions:

1. # virsh start demo

2.# qemu-img create -f raw -o foo.img 10M

3. Create a disk desription file as following XML:
# cat disk.xml
<disk type="file" device="disk">

   <driver name='qemu' type='raw' cache='none'/>
  <source file="/var/lib/libvirt/migrate/foo.img"/>
  <target dev="vdb" bus="virtio"/>
</disk>

4. # virsh attach-device demo disk.xml

5. Run command 'fdisk -l' in guest

6. Migration the guest.

# virsh migrate --live guestname qemu+ssh://<target_host>/system

7. on target host, log into guest, check the attached disk:

log in guest:

# fdisk -l

8. # virsh detach-disk demo vdb

10. Run command 'fdisk -l' in guest

11. on target host, migrate the guest back to source host.

# virsh migrate --live guestname qemu+ssh://<source_host>/system

12. on source host, 
log into guest, check the attached disk:

log in guest:

# fdisk -l
	
Expected Results:

step 4:

Device attached successfully

step 5:

check the disk device is existing.

step6:

migration is finished successfully

step 7:

check the disk device is existing.

step 8:

Disk detached successfully

step 10:

check the disk device is not existing.

step 11:

migration is finished successfully

step 12;

check the disk device is not existing.
Notes:
Comments:

		176866 	[Docs] The "virt-mem" is removed from manual page - bug 639603 	ccui 	ccui 	Manual 		Regression 	P3 	6090 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    docs
    Regression

bug:

    No bug found

Actions:

1.Check the manual page, the "virt-mem" has been removed.
  # man virsh

2.Unable to find the binary also.
# man 1 virt-mem
No entry for virt-mem in section 1 of the manual
# find / -name virt-mem
# locate virt-mem

	
Expected Results:

1. Output:

---------snip----from #man virsh-----------

LICENSE
       virsh is distributed under the terms of the GNU LGPL v2+.  This is free
software; see
       the source for copying conditions. There is NO warranty; not even for
MERCHANTABILITY
       or FITNESS FOR A PARTICULAR PURPOSE

SEE ALSO
       virt-install(1), virt-xml-validate(1), virt-top(1), virt-df(1),
       <http://www.libvirt.org/>

-----------/snip----------

Notes:
Comments:

		176869 	[Docs] virsh help: network can't be destroyed and dumped xml via id - bug 617439 	ccui 	ccui 	Manual 		Regression 	P3 	6100 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    docs
    Regression

bug:

    No bug found

Actions:

1. # virsh help net-destroy

2. # virsh help net-dumpxml
	
Expected Results:

1. Output:

------------

 NAME
    net-destroy - destroy a network

  SYNOPSIS
    net-destroy <network>

  DESCRIPTION
    Destroy a given network.

  OPTIONS
    [--network] <string>  network name or uuid

-----------

 

2. Output:

-----------

NAME
    net-dumpxml - network information in XML

  SYNOPSIS
    net-dumpxml <network>

  DESCRIPTION
    Output the network information as an XML dump to stdout.

  OPTIONS
    [--network] <string>  network name or uuid

----------

 
Notes:
Comments:

		176873 	[Log and debugging]libvirt log messages - 740943 890297 	yupzhang 	None 	Manual 		Regression 	P1 	6110 	Edit
Setup:

Bug 740943 - [RFE] Document libvirt log messages

The bug will be fixed in rhel6.3,so make the test case as NEED_UPDATE.Will update the case after bug fixed.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    log and debugging

bug:

    No bug found

Actions:

1.Change the log setting,make sure all libvirt log will print

#vim /etc/libvirt/libvirtd.conf

log_level = 1
log_filters="1:remote 1:event 1:qemu"
log_outputs="1:file:/var/log/libvirtd.log"

2.Run virsh command or do some operations will trigger libvirt error log

3.Check the  file libvirtd.log

 

Following steps are to trace the bug 890297 libvirtd.log will get an I/O error when close a virt-viewer guest terminal

Terminal 1:

# tailf /var/log/libvirt/libvirtd.log

Terminal 2:

# virt-viewer $guestname

close the window of virt-viewer, Terminal 1 will should not get error like:

2013-01-16 05:25:14.231+0000: 29152: error : virNetSocketReadWire:1184 : End of file while reading data: Input/output error
	
Expected Results:

3. #vim libvirtd.log

Check the error log like this :

[ERROR}
   [This error indicates a .....]

 
Notes:
Comments:

		176823 	[Disk hotplug] Attach iso from readonly filesystem to VM - bug 702044 	weizhan 	weizhan 	Manual (Autoproposed) 		Regression 	P1 	6120 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virtual disks

bug:

    No bug found

Actions:

1. setup read-only nfs

# cat /etc/exports
/var/lib/libvirt/images  *(ro, no_root_squash)
# service nfs restart

2. Mount nfs

  # mount -o vers=3 10.66.82.123:/var/lib/libvirt/images /mnt

3. Start a domain with empty cdrom device:
# virsh start demo:
# virsh dumpxml demo

...

    <disk type='file' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>

...
4. Download boot.iso from build tree

5. Attach the iso on nfs
# virsh attach-disk demo /mnt/boot.iso hdc --type cdrom

6. Log in guest to check if cdrom exists
# mount /dev/sr0 /mnt

 

	
Expected Results:

step 5: Disk attached successfully

step 6: Cdrom exists

Notes:
Comments:

		176824 	[Disk hotplug] Attach virtual disks to guest over the top limitation via attach-device 	ajia 	None 	Auto 		Stress 	P1 	6130 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Define and start a guest with only 1 IDE disk

2. prepare a disk template

# cat disk-template.xml

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='#SOURCEFILE#'/>
      <target dev='#TARGETDEV#' bus='virtio'/>
    </disk>

3. Attach 26 disks to guest with script

#cat attach.sh

#!/bin/sh

for i in {a..z}

do
qemu-img create /var/lib/libvirt/images/disk-$i.img 10M
sourcefile="/var/lib/libvirt/images/disk-$i.img"
cp disk-template.xml disk-$i.xml
sed -i -e "s,#SOURCEFILE#,$sourcefile,g"  \
           -e "s,#TARGETDEV#,vd$i,g" disk-$i.xml
virsh attach-device $1 disk-$i.xml
sleep 1
done

# sh attach.sh <guest>

4. login to the guest to check

# fdisk -l | grep /dev/vd
	
Expected Results:

step3. All the disks can be attached with no error, and can see the attached disk in domain xml

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/disk-a.xml'/>
      <target dev='vda' bus='virtio'/>
    </disk>

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/disk-z.xml'/>
      <target dev='vdz' bus='virtio'/>
    </disk>

...

step4. All the disks can be accessed in guest
Notes:
Comments:

		176825 	[Disk hotplug] Attach virtual disks to guest over the top limitation via attach-disk 	ajia 	None 	Auto 		Stress 	P1 	6140 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Define and start a guest with only 1 IDE disk

2. Attach 26 disks to guest with script

#cat attach.sh

#!/bin/sh

for i in {a..z}

do
qemu-img create /var/lib/libvirt/images/disk-$i.img 10M
virsh attach-disk $1 /var/lib/libvirt/images/disk-$i.img vd$i
sleep 1
done

# sh attach.sh <guest>

3.  login to the guest to check

# fdisk -l | grep /dev/vd

 
	
Expected Results:

step2. All the disks can be attached with no error, and can see the attached disk in domain xml

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/disk-a.xml'/>
      <target dev='vda' bus='virtio'/>
    </disk>

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/disk-z.xml'/>
      <target dev='vdz' bus='virtio'/>
    </disk>

...

step3. All the disks can be accessed in guest
Notes:
Comments:

		176828 	[Disk hotplug] Attach/Detach a block device to/from guest 	weizhan 	None 	Manual (Autoproposed) 		Feature 	P1 	6150 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. start a guest
# virsh start demo

2. Attach the block disk to guest
# virsh attach-disk demo /dev/sda10 vdb
Disk attached successfully

3. Check if the new disk file has been added into domain
# virsh dumpxml demo
....
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/sda10'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </disk>
....

4. Run command 'fdisk -l' in guest and mount to check if it is readable/writible
there is vdb device exist

5. Detach the block disk
# virsh detach-disk demo vdb
Disk detached successfully

6. Check the device on guest is removed
Expected Results:
	
Expected Results:
Notes:
Comments:

		176829 	[Disk hotplug] Attach/Detach a cow format disk device to guest 	ajia 	None 	Manual 		Feature 	P4 	6160 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

1. # virsh start demo

2. # qemu-img create -f cow -o foo.img 10M

3. # virsh attach-disk demo /var/lib/libvirt/images/foo.img vdb
Disk attached successfully    

4. Check if the new disk file has been added into domain
    # virsh dumpxml demo
       ....
       <disk type='file' device='disk'>
          <driver name='qemu' type='cow'/>
          <source file='/var/lib/libvirt/images/foo.img'/>
          <target dev='vdb' bus='virtio'/>
       </disk>
       ....

5. Run command 'fdisk -l' in guest

6. # virsh detach-disk demo vdb

7. Run command 'fdisk -l' in guest
	
Expected Results:

step 3:

Disk device attached successfully

step 5:

check the disk device is existing.

step 6:

Disk detached successfully

step 7:

check the disk device is NOT existing.
Notes:
Comments:

		176830 	[Disk hotplug] Attach/Detach a disk device to guest with --persistent option 	jialiu 	None 	Manual 		Feature 	P2 	6170 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp

 

NOTE:

Now there is bug for this case:

Bug 627143 - can't successfully start guest with a hot-plugging disk after stopping
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0

bug:

    No bug found

Actions:

1. Create a qcow2 image file.

# qemu-img create /var/lib/libvirt/images/hot-plug-disk.img 1M

2. Define and start a domain.

3. Attach the disk to guest with optional argument.

# virsh attach-disk <guestname> /var/lib/libvirt/images/hot-plug-disk.img vdb --persistent

4. Dumpxml to check disk is hotpluged.

# virsh dumpxml <guestname>

5. In guest, check the hotplug disk is seen in "fdisk -l"

6. shutdown the guest, and restart libvirtd service

7. check the disk is still in domain xml ouput

# virsh dumpxml <guestname>

8. Start domain again, and check the disk is existing in "fdisk -l"

9. Detach the disk from guest.

# virsh attach-disk <guestname> vdb --persistent

10. In guest, check the hotplug disk is not seen in "fdisk -l"

11. shutdown the guest, and restart libvirtd service

12. check the disk is NOT seen in domain xml ouput

# virsh dumpxml <guestname>

13. Start domain again, and check the disk is NOT existing in "fdisk -l"
	
Expected Results:

3, Command is run successfully

4. Make sure the following xml is seen in output

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/hot-plug-disk.img'/>
      <target dev='vdb' bus='virtio'/>
    </disk>

...

5.

6.

7. Make sure the following xml is seen in output

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/hot-plug-disk.img'/>
      <target dev='vdb' bus='virtio'/>
    </disk>

...

8.

9. virsh command is run successfully

10.

11.

12.

13.
Notes:
Comments:

		176831 	[Disk hotplug] Attach/Detach a disk device to guest with --persistent option 	weizhan 	weizhan 	Auto 		Feature 	P1 	6180 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1. Start a domain:
# virsh start demo

2. Create a image to attach
# qemu-img create -f raw -o foo.img 10M

3. Attach a virtio disk persistently
# virsh attach-disk demo /var/lib/libvirt/images/foo.img vdb --persistent

4. Check if the new disk file has been added into domain
# virsh dumpxml demo

5. Run command 'fdisk -l' in guest and mount to check if it is readable/writible

6. Restart the guest and check

7. Dettach a virtio disk persistently
# virsh detach-disk demo vdb --persistent

8. Check if the new disk file has been removed from domain
# virsh dumpxml demo

9. Run command 'fdisk -l' in guest and mount to check if it is removed

10. Restart the guest and check
	
Expected Results:

step3:Disk attached successfully

step4 output:

....
<disk type='file' device='disk'>
<driver name='qemu' type='raw'/>
<source file='/var/lib/libvirt/images/foo.img'/>
<target dev='vdb' bus='virtio'/>
</disk>
....

step5: there is vdb device exist

step6: The device should still exist

step7: Disk detached successfully

step8: no vdb device found on xml

step9: there is no vdb device exists

step10. there is no vdb device exists
Notes:
Comments:

		176827 	[Disk hotplug] attach-disk should detect disk source file type when sourcetype is not specified - bug 803577 	gsun 	gsun 	Manual 		Function 	P1 	6190 	Edit
Setup:

1. Prepare a running domain.

# virsh list
 Id    Name                           State
----------------------------------------------------
 6     rhel6u2                        running

 # virsh dumpxml rhel6u2

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/rhel6u2'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>

2. Prepare another disk in your host and have a partition

# fdisk -l

Disk /dev/sdb: 21.5 GB, 21485322240 bytes
64 heads, 32 sectors/track, 20490 cylinders
Units = cylinders of 2048 * 512 = 1048576 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x0dce4ff4

 Device Boot      Start         End      Blocks   Id  System
/dev/sdb1               1       20490    20981744   83  Linux

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks

bug:

    No bug found

Actions:

1.create a file img

#qemu-img create /var/lib/libvirt/images/hotplug.img 1G

2.hotplug file  disk to guest
# virsh attach-disk $domain_name  /var/lib/libvirt/images/hotplug.img vdb

3. check xml of domain disk type should be file.
#virsh dumpxml $domain_name

4.hotplug file  disk to guest
# virsh attach-disk $domain_name /dev/sdb1 vdc 

5.check xml of domain disk type should be block .
#virsh dumpxml $domain_name

 

 

	
Expected Results:

2.

Disk attached successfully 

 

3. 

...
<disk type='file' device='disk'>
<driver name='qemu' type='raw'/>
<source file='/var/lib/libvirt/images/hotplug.img'/>
<target dev='vdb' bus='virtio'/>
<alias name='virtio-disk1'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
</disk>
...

4. Disk attached successfully

5.

...
<disk type='block' device='disk'>
<driver name='qemu' type='raw'/>
<source dev='/dev/sdb1'/>
<target dev='vdc' bus='virtio'/>
<alias name='virtio-disk2'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
</disk>
...

 

 

Notes:
Comments:

		176832 	[Disk hotplug] Attach/Detach a disk device to guest with optional argument 	jialiu 	None 	Manual (Autoproposed) 		Feature 	P1 	6190 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp

 

NOTE:

Now there is bug for this case:

Bug 627143 - can't successfully start guest with a hot-plugging disk after stopping
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual disks

bug:

    872498 - From Run 49499
    872498 - From Run 50422
    872498 - From Run 54154

Actions:

1. Create a qcow2 image file.

# qemu-img create -f qcow2 /var/lib/libvirt/images/hot-plug-disk.img 1M

2. Define and start a domain.

3. Attach the disk to guest with optional argument.

# virsh attach-disk <guestname> /var/lib/libvirt/images/hot-plug-disk.img vdb --driver qemu --subdriver qcow2 --type disk --sourcetype file --mode shareable

4. Dumpxml to check disk is hotpluged.

# virsh dumpxml <guestname>

5. In guest, check the hotplug disk is seen in "fdisk -l" and mkfs in guest

6. Detach the disk from guest.

# virsh dettach-disk <guestname> vdb

7. In guest, check the hotplug disk is not seen in "fdisk -l"

8. repeat above steps with

#virsh attach-disk <guestname> /var/lib/libvirt/images/hot-plug-disk.img vdb --driver qemu  --subdriver  raw  --sourcetype block  --mode readonly

# virsh dumpxml <guestname>

9.in guest, mount disk and check if disk can be write

# mount /dev/vdb /mnt

# cd /mnt ; touch test
	
Expected Results:

3, Command is run successfully

4. Make sure the following xml is seen in output

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/hot-plug-disk.img'/>
      <target dev='vdb' bus='virtio'/>
      <shareable/>
      </disk>

...

5.

6. The command is run successfully

7.

8. Make sure the following xml is seen in output

...

   <disk type='block' device='disk'>
      <driver name='qemu' driver type='raw'/>
      <source dev='/var/lib/libvirt/images/tt-1.img'/>
      <target dev='vdd' bus='virtio'/>
      <readonly/>
    </disk>

...

9. touch: cannot touch `test': Read-only file system
Notes:
Comments:

		176849 	[Disk hotplug] detach-disk with wrong arguments - bug 803591 	gsun 	gsun 	Manual (Autoproposed) 		Negative test 	P1 	6190 	Edit
Setup:

Prepare a running domain.

# virsh list
 Id    Name                           State
----------------------------------------------------
 6     rhel6u2                        running

 # virsh dumpxml rhel6u2

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/rhel6u2'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' target='0' unit='0'/>
    </disk>

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    virtual disks

bug:

    No bug found

Actions:

1.  try to detach disk with wrong arguments

# virsh detach-disk vda rhel6u2

2. try with wrong target
# virsh detach-disk rhel6u2 vdc

	
Expected Results:

1.

error: failed to get domain 'vda'
error: Domain not found: no domain with matching name 'vda'

2.

error: No found disk whose source path or target is vdc

 

The error msg is expected and no mem leak.
Notes:
Comments:

		176833 	[Disk hotplug] Attach/Detach a disk to an inactive domain 	weizhan 	weizhan 	Auto 		Feature 	P1 	6200 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1. Prepare a guest which is in shutoff status:

2. Create a image to attach
# qemu-img create -f raw -o foo.img 10M

3. Attach a virtio disk  
# virsh attach-disk demo /var/lib/libvirt/images/foo.img vdb --persistent

4. Check if the new disk file has been added into domain
# virsh dumpxml demo

5. Start the guest
# virsh start demo

6. Run command 'fdisk -l' in guest and mount to check if it is readable/writible

7. Shutdown the guest

8. Detach the disk
# virsh detach-disk demo vdb --persistent

9. Check if the new disk file has been removed from domain
# virsh dumpxml demo

10. Start the guest
# virsh start demo

11. Run command 'fdisk -l' in guest and mount to check if it exist
	
Expected Results:

step3 : Disk attached successfully

step4 outputs:

....
<disk type='file' device='disk'>
<driver name='qemu' type='raw'/>
<source file='/var/lib/libvirt/images/foo.img'/>
<target dev='vdb' bus='virtio'/>
</disk>
....

step6: there is vdb device exist

step8: Disk detached successfully

step9: Disk is removed

step11: Disk vdb does no exist
Notes:
Comments:

		176834 	[Disk hotplug] Attach/Detach a disk to an inactive domain with xml 	weizhan 	None 	Manual (Autoproposed) 		Feature 	P1 	6210 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Prepare a guest which is in shutoff status

2.# qemu-img create -f raw -o foo.img 10M

3. Create a disk desription file as following XML:
# cat disk.xml
<disk type="file" device="disk">
  <source file="/var/lib/libvirt/images/foo.img"/>
  <target dev="vdb" bus="virtio"/>
</disk>

4. # virsh attach-device demo disk.xml --persistent

5. Check if the new disk file has been added into domain
# virsh dumpxml demo

6. Start the guest
# virsh start demo
7. Run command 'fdisk -l' in guest
8. Shutdown the guest

9. # virsh detach-device demo disk.xml --persistent

10. Check if the new disk file has been removed from domain
# virsh dumpxml demo

11. Start the guest
# virsh start demo

12. Run command 'fdisk -l' in guest
	
Expected Results:

step 4:

Device attached successfully

step5 output:

...

<disk type="file" device="disk">
  <source file="/var/lib/libvirt/images/foo.img"/>
  <target dev="vdb" bus="virtio"/>
</disk>

...

step 7:

check the disk device is existing.

step 9:

Device detached successfully

step10:

no foo.img info on domain xml

step 12:

check the disk device is not existing.
Notes:
Comments:

		176843 	[Disk hotplug] Attach/Detach an iscsi disk to/from guest 	weizhan 	None 	Manual (Autoproposed) 		Feature 	P1 	6220 	Edit
Setup:

do case " 124815 [Storage] iSCSI based storage pool" first

you can get the iscsi storage by setting /etc/iscsi/initiatorname.iscsi

InitiatorName=iqn.1994-05.com.redhat:libvirt

Then run

# iscsiadm --mode discovery --type sendtargets --portal 10.66.90.100

to discover the iscsi disk
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. start a guest
# virsh start demo

2. Attach the iscsi disk to guest
# virsh attach-disk demo /dev/disk/by-path/ip-10.66.90.100:3260-iscsi-iqn.2001-05.com.equallogic:0-8a0906-3de1f7d03-56b263eebce4c5a2-kyla-1-lun-0 vdb
Disk attached successfully

3. Check if the new disk file has been added into domain
....
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/disk/by-path/ip-10.66.90.100:3260-iscsi-iqn.2001-05.com.equallogic:0-8a0906-3de1f7d03-56b263eebce4c5a2-kyla-1-lun-0'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0b' function='0x0'/>
    </disk>
....

4. Run command 'fdisk -l' in guest and mount to check if it is readable/writible
there is vdb device exist

5. Detach the logical volume
# virsh detach-disk demo vdb
Disk detached successfully

6. Check the device on guest is removed
	
Expected Results:
Notes:
Comments:

		176837 	[Disk hotplug] Attach/Detach a logical volume to/from guest 	weizhan 	None 	Manual 		Feature 	P1 	6230 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. start a guest
# virsh start demo

2. create logical volume
# pvcreate /dev/sda10
  Physical volume "/dev/sda10" successfully created
# vgcreate VolGroup00 /dev/sda10
  Volume group "VolGroup00" successfully created
# lvcreate -L 10M VolGroup00 -n lvolhome
  Rounding up size to full physical extent 12.00 MiB
  Logical volume "lvolhome" created

3. Attach the block disk to guest
# virsh attach-disk demo /dev/VolGroup00/lvolhome vdb
Disk attached successfully

4. Check if the new disk file has been added into domain
# virsh dumpxml demo
....
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/VolGroup00/lvolhome'/>
      <target dev='vdd' bus='virtio'/>
      <alias name='virtio-disk3'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
....

5. Run command 'fdisk -l' in guest and mount to check if it is readable/writible
there is vdb device exist

6. Detach the logical volume
# virsh detach-disk demo vdb
Disk detached successfully

7. Check the device on guest is removed

	
Expected Results:
Notes:
Comments:

		176839 	[Disk hotplug] Attach/Detach a qcow format disk device from XML file to guest 	ajia 	None 	Manual 		Feature 	P4 	6240 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    No tag found

bug:

    No bug found

Actions:

1. # virsh start demo

2.# qemu-img create -f qcow -o foo.img 10M

3. Create a disk desription file as following XML:
# cat disk.xml
<disk type="file" device="disk">
  <source file="/var/lib/libvirt/images/foo.img"/>
  <target dev="vdb" bus="virtio"/>
</disk>

4. # virsh attach-device demo disk.xml

5. Run command 'fdisk -l' in guest

6. # virsh detach-device demo disk.xml

7. Run command 'fdisk -l' in guest
	
Expected Results:

step 4:

Device attached successfully

step 5:

check the disk device is existing.

step 6:

Device detached successfully

step 7:

check the disk device is NOT existing.
Notes:
Comments:

		176840 	[Disk hotplug] Attach/Detach a qcow2 format disk device to guest 	ajia 	None 	Auto 		Feature 	P1 	6250 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1. # virsh start demo

2.# qemu-img create -f qcow2 -o foo.img 10M

3. # virsh attach-disk demo /var/lib/libvirt/images/foo.img vdb
Disk attached successfully     

4. Check if the new disk file has been added into domain
    # virsh dumpxml demo
       ....
       <disk type='file' device='disk'>
          <driver name='qemu' type='raw'/>
          <source file='/var/lib/libvirt/images/foo.img'/>
          <target dev='vdb' bus='virtio'/>
       </disk>
       ....

5. Run command 'fdisk -l' in guest

6. # virsh detach-disk demo vdb

7. Run command 'fdisk -l' in guest
	
Expected Results:

step 3:

Disk device attached successfully

step 5:

check the disk device is existing.

step 6:

Disk detached successfully

step 7:

check the disk device is not existing.

 

 
Notes:
Comments:

		176841 	[Disk hotplug] Attach/Detach a raw format disk device from XML file to guest 	nzhang 	yoyzhang 	Auto 		Feature 	P1 	6260 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    auto
    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1. # virsh start demo

2.# qemu-img create -f raw -o foo.img 10M

3. Create a disk desription file as following XML:
# cat disk.xml
<disk type="file" device="disk">
  <source file="/var/lib/libvirt/images/foo.img"/>
  <target dev="vdb" bus="virtio"/>
</disk>

4. # virsh attach-device demo disk.xml

5. Run command 'fdisk -l' in guest

6. # virsh detach-disk demo vdb

7. Run command 'fdisk -l' in guest
	
Expected Results:

step 4:

Device attached successfully

step 5:

check the disk device is existing.

step 6:

Disk detached successfully

step 7:

check the disk device is not existing.

 
Notes:
Comments:

		176842 	[Disk hotplug] Attach/Detach a raw format disk device to guest 	nzhang 	None 	Auto 		Feature 	P1 	6270 	Edit
Setup:

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virsh-rail
    virtual disks

bug:

    No bug found

Actions:

1. Start a domain: # virsh start demo

2. # qemu-img create -f raw  foo.img 10M

3. # virsh attach-disk demo /var/lib/libvirt/images/foo.img vdb
Disk attached successfully     

4. Check if the new disk file has been added into domain

For Linux:

5. Run command 'fdisk -l' in guest

For Windows:

5.Right click "my computer" ->manager->storage->Disk management

5.1 Right click 'Disk 1'-> Initial disk

5.2.Right clink the "unallocate disk"->New Simple Volume->next until to the finished page.

6. # virsh detach-disk demo vdb

7. Run command 'fdisk -l' in Linux guest.
	
Expected Results:

step 3:

Disk device attached successfully

step 5:

check the disk device is existing.

step 6:

Disk detached successfully

step 7:

check the disk device is not existing.
Notes:
Comments:

		176845 	[Disk hotplug] Attach/Detach disk to guest for 256 times 	jialiu 	None 	Manual (Autoproposed) 		Stress 	P1 	6290 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    virtual disks

bug:

    No bug found

Actions:

1. # virsh start demo

2.# qemu-img create -f raw  foo.img 10M

3. Create a disk desription file as following XML:
# cat disk.xml
<disk type="file" device="disk">
  <source file="/var/lib/libvirt/images/foo.img"/>
  <target dev="vdb" bus="virtio"/>
</disk>

4. # virsh attach-device demo disk.xml

5. Run command 'fdisk -l' in guest

6. # virsh detach-disk demo vdb

7. Run command 'fdisk -l' in guest

8. Hotplug / hot-unplug the disk for 256 times.

# for i in `seq 256`; \
do echo "---${i}---"; virsh attach-device demo disk.xml; sleep 3; virsh detach-device demo disk.xml; sleep 3; \
done

	
Expected Results:

step 4:

Device attached successfully

step 5:

check the disk device is existing.

step 6:

Disk detached successfully

step 7:

check the disk device is not existing.

step 8:

All the commands work fine
Notes:
Comments:

		176846 	[Disk hotplug] Audit check when detach disk - bug 710150 	weizhan 	weizhan 	Manual 		Regression 	P1 	6300 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virtual disks

bug:

    No bug found

Actions:

1. Start a domain:
# virsh start rhel6

2. Create a image to attach
# qemu-img create -f raw -o tt1.img 10M

3. Attach a virtio disk
# virsh attach-disk rhel6 /var/lib/libvirt/images/tt1.img vda

4. Detach the disk
# virsh detach-disk rhel6 vda

5. Check the audit log
# grep detach /var/log/audit/audit.log
	
Expected Results:

step4: Disk detached successfully

step5:

...

type=VIRT_RESOURCE msg=audit(1310459461.154:31066): user pid=29784 uid=0 auid=0 ses=4 subj=unconfined_u:system_r:virtd_t:s0-s0:c0.c1023 msg='resrc=disk reason=detach vm="rhel6" uuid=09e67363-e1c9-7cf7-7d5c-372f5ed3045e old-disk="/var/lib/libvirt/images/tt1.img" new-disk="?": exe="/usr/sbin/libvirtd" hostname=? addr=? terminal=? res=success'
...
Notes:
Comments:

		176847 	[Disk hotplug] Detach virtual disks to guest over the top limitation via detach-device 	ajia 	None 	Auto 		Stress 	P1 	6310 	Edit
Setup:

do 95357 [Disk hotplug] Attach virtual disks to guest over the top limitation via attach-device first
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Detach 26 disks from guest

# for i in {a..z}; do virsh detach-device <guest> disk-$i.xml; done

2. Login guest to check

# fdisk -l | grep /dev/vd
	
Expected Results:

step1. All the guest can be detached successfully. No disk-a .. disk-z can be find in domain xml

step2. no virtio disk can be find
Notes:
Comments:

		176780 	[CPU Management] libvirt should not allow ambiguous XML config wrt <vcpus> and <topology> - bug 725271 	mzhan 	mzhan 	Manual 		Function 	P2 	6320 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    Regression

bug:

    No bug found

Actions:

1.  Select a health guest, make sure it is shutdown

# virsh edit <guest>

<domain>
  ...
  <vcpus>3</vcpus>
  <cpu>
    <topology sockets='2' cores='2' threads='2'/>
  </cpu>
  ...
</domain> 

Note: Here vcpus is not equal to sockets*cores*threads.

2. # virsh start <guest>

 

 
	
Expected Results:

2. libvirt should not accept this xml config. TBD because of bug 725271.
Notes:
Comments:

		176781 	[CPU Management] libvirt should support AMD Bulldozer cpu - bug 767364 	mzhan 	mzhan 	Manual 		Function 	P2 	6320 	Edit
Setup:

1. Tested with machine in beaker: amd-dinar-07.lab.bos.redhat.com, send email to eng-ops@ for taking this machine.

   Or you can use amd-6234-32-1 and amd-6234-32-2   in virtlab , more deails about machine can refer

   https://mirrorglass.englab.nay.redhat.com/XWiki/bin/view/Main/lab-CPUs

2. Make sure qemu-kvm >= 248 and libvirt >= 0.9.10-6
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Regression
    cpu

bug:

    No bug found

Actions:

1. Check definition for this cpu model:

# cat /usr/share/libvirt/cpu_map.xml
...
  <model name='Opteron_G4'>
      <vendor name='AMD'/>
      <feature name='fpu'/>
      <feature name='de'/>
      <feature name='pse'/>
      <feature name='tsc'/>
      <feature name='msr'/>
      <feature name='pae'/>
      <feature name='mce'/>
      <feature name='cx8'/>
      <feature name='apic'/>
      <feature name='sep'/>
      <feature name='mtrr'/>
      <feature name='pge'/>
      <feature name='mca'/>
      <feature name='cmov'/>
      <feature name='pat'/>
      <feature name='pse36'/>
      <feature name='clflush'/>
      <feature name='mmx'/>
      <feature name='fxsr'/>
      <feature name='sse'/>
      <feature name='sse2'/>
      <feature name='pni'/>
      <feature name='pclmuldq'/>
      <feature name='ssse3'/>
      <feature name='cx16'/>
      <feature name='sse4.1'/>
      <feature name='sse4.2'/>
      <feature name='popcnt'/>
      <feature name='aes'/>
      <feature name='xsave'/>
      <feature name='avx'/>
      <feature name='syscall'/>
      <feature name='nx'/>
      <feature name='pdpe1gb'/>
      <feature name='rdtscp'/>
      <feature name='lm'/>
      <feature name='lahf_lm'/>
      <feature name='svm'/>
      <feature name='abm'/>
      <feature name='sse4a'/>
      <feature name='sse4a'/>
      <feature name='misalignsse'/>
      <feature name='3dnowprefetch'/>
      <feature name='xop'/>
      <feature name='fma4'/>
    </model>

2. Tested with machine in beaker: amd-dinar-07.lab.bos.redhat.com, check the
cpu model should be Opteron_G4.

# virsh capabilities
<capabilities>

  <host>
    <uuid>00001111-0000-2222-0000-888800009999</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Opteron_G4</model> 
      <vendor>AMD</vendor>
      <topology sockets='2' cores='8' threads='2'/>
      <feature name='nodeid_msr'/>
      <feature name='wdt'/>
      <feature name='skinit'/>
      <feature name='ibs'/>
      <feature name='osvw'/>
      <feature name='cr8legacy'/>
      <feature name='extapic'/>
      <feature name='cmp_legacy'/>
      <feature name='fxsr_opt'/>
      <feature name='mmxext'/>
      <feature name='osxsave'/>
      <feature name='monitor'/>
      <feature name='ht'/>
      <feature name='vme'/>
    </cpu>
...

ï»¿

 

 

 
	
Expected Results:


Notes:
Comments:

		176804 	[CPU Management] libvirt should support cpu64-rhel* cpu models - bug 768450 	mzhan 	mzhan 	Manual 		Function 	P2 	6320 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    Regression
    cpu

bug:

    No bug found

Actions:

1. Check cpu_map.xml, found the cpu64-rhel5/6 are added.

# cat /usr/share/libvirt/cpu_map.xml |grep cpu64-rhel
    <model name='cpu64-rhel5'>
    <model name='cpu64-rhel6'>

2. Check guest could be started with cpu64-rhel6 model.

1) Tested with *Intel* machine

# virsh dumpxml guest
...
 <cpu mode='custom' match='exact'>
    <model fallback='allow'>cpu64-rhel6</model>
  </cpu>
...

# virsh start guest
Domain rhel62 started
# ps -ef|grep kvm
qemu      6006     1  6 23:09 ?        00:00:23 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -cpu cpu64-rhel6 -enable-kvm -m 1024 -smp
1,sockets=1,cores=1,threads=1 -name rhel62 ....

Also Check in guest, # cat /proc/cpuinfo found the cpu model is
model name: QEMU Virtual CPU version (cpu64-rhel6)


2) Tested with *AMD* machine:

# virsh dumpxml guest
...
 <cpu mode='custom' match='exact'>
    <model fallback='allow'>cpu64-rhel6</model>
  </cpu>
...

# virsh start guest
Domain rhel6.2 started

# ps -ef|grep kvm
qemu     27689     1 41 02:29 ?        00:00:44 /usr/libexec/qemu-kvm -S -M
rhel6.3.0 -cpu cpu64-rhel6 -enable-kvm -m 1024 -smp
1,sockets=1,cores=1,threads=1 -name rhel6.2 -uuid
4a3f08e4-77d3-94f2-acd4-009d254a35f8 -nodefconfig -nodefaults .....

Also Check in guest, # cat /proc/cpuinfo found the cpu model is
model name: QEMU Virtual CPU version (cpu64-rhel6)

3. The results are the same as cpu64-rhel5 type, just change cpu64-rhel6 to cpu64-rhel5 and test again.

	
Expected Results:


Notes:
Comments:

		176805 	[CPU Management] libvirt should support new sandy bridge cpu - bug 761005 	mzhan 	mzhan 	Manual 		Function 	P2 	6320 	Edit
Setup:

Using intel-e31225-8-1 which can be get from beaker/virtlab: 

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    Regression

bug:

    No bug found

Actions:


1. check the definition for this cpu model, it has the flags such as pclmuldq, avx, xsave.
# cat /usr/share/libvirt/cpu_map.xml
...
 <model name='SandyBridge'>
      <vendor name='Intel'/>
      <feature name='aes'/>
      <feature name='apic'/>
      <feature name='avx'/>
      <feature name='clflush'/>
      <feature name='cmov'/>
      <feature name='cx16'/>
      <feature name='cx8'/>
      <feature name='de'/>
      <feature name='fpu'/>
      <feature name='fxsr'/>
      <feature name='lahf_lm'/>
      <feature name='lm'/>
      <feature name='mca'/>
      <feature name='mce'/>
      <feature name='mmx'/>
      <feature name='msr'/>
      <feature name='mtrr'/>
      <feature name='nx'/>
      <feature name='pae'/>
      <feature name='pat'/>
      <feature name='pclmuldq'/>
      <feature name='pge'/>
      <feature name='pni'/>
      <feature name='popcnt'/>
      <feature name='pse'/>
      <feature name='pse36'/>
      <feature name='rdtscp'/>
      <feature name='sep'/>
      <feature name='sse'/>
      <feature name='sse2'/>
      <feature name='sse4.1'/>
      <feature name='sse4.2'/>
      <feature name='ssse3'/>
      <feature name='syscall'/>
      <feature name='tsc'/>
      <feature name='x2apic'/>
      <feature name='xsave'/>
    </model>


2. # virsh capabilities 

 
	
Expected Results:

2. 

# virsh capabilities
<capabilities>

  <host>
    <uuid>322008c7-3452-0943-3c7a-09732431099f</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>SandyBridge</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='osxsave'/>
      <feature name='tsc-deadline'/>
      <feature name='pdcm'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='smx'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='dtes64'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>

Notes:
Comments:

		177092 	[libvirtd] Kill qemu process using 'kill' behaves as user shutdown command and not as 'lost connection with qemu process'--Bug 656845 	kxiong 	kxiong 	Manual 		--default-- 	P2 	6320 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1.Define and start a domain.
# virsh start http_test
Domain http_test started

2.Edit "libvirtd.conf" file 
# vi /etc/libvirt/libvirtd.conf
log_level = 1
log_outputs="1:file:/var/lib/libvirt/images/libvirtd.log" 

3.# service libvirtd stop

4.# libvirtd

5.Open the second terminal.
# ps -ef|grep qemu
qemu      2038     1  1 04:31 ?        00:00:56 /usr/libexec/qemu-kvm -S -M
rhel6.0.0 -enable-kvm -m 512 -smp 1,sockets=1,cores=1,threads=1 -name http_test
-uuid 3d3a297b-8039-7143-033a-7ca7d9feb676 -nodefconfig -nodefaults -chardev
socket,id=monitor,path=/var/lib/libvirt/qemu/http_test.monitor,server,nowait
-mon chardev=monitor,mode=control -rtc base=utc -boot c -drive
file=/var/lib/libvirt/images/http_test.img,if=none,id=drive-ide0-0-0,format=raw,cache=none
-device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0 -drive
file=/var/lib/libvirt/images/test.sio,if=none,media=cdrom,id=drive-ide0-1-0,readonly=on,format=raw
-device ide-drive,bus=ide.1,unit=0,drive=drive-ide0-1-0,id=ide0-1-0 -netdev
tap,fd=26,id=hostnet0 -device
rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:7e:b1:46,bus=pci.0,addr=0x3
-chardev pty,id=serial0 -device isa-serial,chardev=serial0 -usb -vnc
127.0.0.1:0 -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x4 -device
virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5
root      6793  6774  0 05:36 pts/3    00:00:00 grep qemu

6.# kill -9 2038

	
Expected Results:

Get log from "/var/lib/libvirt/images/libvirtd.log" file
..............................
05:37:36.310: 6718: debug : qemuMonitorIO:580 : Triggering EOF callback error?
0
05:37:36.310: 6718: debug : qemuHandleMonitorEOF:1306 : Received EOF on
0x1bcaec0 'http_test'
05:37:36.310: 6718: debug : qemuHandleMonitorEOF:1313 : Monitor connection to
'http_test' closed without SHUTDOWN event; assuming the domain crashed
05:37:36.310: 6718: debug : qemudShutdownVMDaemon:4748 : Shutting down VM
'http_test' pid=2038 migrated=0
05:37:36.311: 6718: debug : qemuMonitorClose:694 : mon=0x1d43000
05:37:36.311: 6718: debug : virEventRemoveHandleImpl:163 : Remove handle w=5
05:37:36.311: 6718: debug : virEventRemoveHandleImpl:176 : mark delete 4 21
05:37:36.311: 6718: debug : virEventInterruptLocked:655 : Skip interrupt, 1
356493072
05:37:36.311: 6718: debug : qemuSecurityDACRestoreSecurityAllLabel:426 :
Restoring security label on http_test migrated=0
05:37:36.311: 6718: info : qemuSecurityDACRestoreSecurityFileLabel:80 :
Restoring DAC user and group on '/var/lib/libvirt/images/http_test.img'
05:37:36.311: 6718: info : qemuSecurityDACSetOwnership:40 : Setting DAC user
and group on '/var/lib/libvirt/images/http_test.img' to '0:0'
05:37:36.311: 6718: debug : SELinuxRestoreSecurityAllLabel:746 : Restoring
security label on http_test
05:37:36.311: 6718: info : SELinuxRestoreSecurityFileLabel:369 : Restoring
SELinux context on '/var/lib/libvirt/images/http_test.img'
05:37:36.385: 6718: info : SELinuxSetFilecon:323 : Setting SELinux context on
'/var/lib/libvirt/images/http_test.img' to 'system_u:object_r:virt_image_t:s0'
05:37:36.385: 6718: debug : virCgroupNew:555 : New group
/libvirt/qemu/http_test
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping 0:cpu
at /cgroup/cpu in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
1:cpuacct at /cgroup/cpuacct in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
2:cpuset at /cgroup/cpuset in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
3:memory at /cgroup/memory in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
4:devices at /cgroup/devices in
05:37:36.388: 6718: debug : virCgroupDetect:245 : Detected mount/mapping
5:freezer at /cgroup/freezer in
05:37:36.388: 6718: debug : virCgroupMakeGroup:497 : Make group
/libvirt/qemu/http_test
............................

Notes:
Comments:

		176807 	[CPU Management] Pin domain to non-exist cpu 	yoyzhang 	yoyzhang 	Auto 		Negative test 	P1 	6330 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

[Prerequisites]
1. Executed test case with summary 'Start a defined inactive domain', so get an active guest named rhel5u4_x86_64_kvm
2. The physical has 2 cpus
[Steps]
1. Issue below command in gnome-terminal
    # virsh vcpupin rhel5u4_x86_64_kvm 1 2


	
Expected Results:

1. Should report error info, sample:

    error: Physical CPU 2 doesn't exist.

    error: cpulist: Invalid format
Notes:
Comments:

		177112 	[libvirtd] Reference leak in libvirtd remote*() functions--Bug 603442 	kxiong 	kxiong 	Manual 		--default-- 	P2 	6330 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1.#mkdir /tmp/netfs
2.Create an XML file, save as netfspool.xml:
      <pool type="netfs">
        <name>netfs</name>
        <source>
          <host name="10.66.90.115"/>
          <dir path="/vol/libvirt1/auto"/>
        </source>
        <target>
          <path>/tmp/netfs</path>
        </target>
      </pool>
3.#virsh pool-destroy netfs
Pool netfs destroyed
4. # virsh pool-create netfspool.xml 
Pool netfs created from netfspool.xml

	
Expected Results:

Name:           netfs
UUID:           8b393483-5913-01aa-c717-dc2167b95877
State:          running
Persistent:     no
Autostart:      no
Capacity:       810.00 GB
Allocation:     745.93 GB
Available:      64.07 GB
For the second time,new UUID is used.

Notes:
Comments:

		176808 	[CPU Management] Start guest after offline host's cpu - bug 622515 	yoyzhang 	None 	Manual 		Regression 	P3 	6340 	Edit
Setup:

A host with 4 cpu
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu
    RHEL6.0

bug:

    No bug found

Actions:

1. The guest is assigned with 2 vcpus and initial pinning is 0-1. Then shutoff the guest.

# virsh dumpxml rhel5.5
<domain type='kvm'>
  <name>rhel5.5</name>
  <uuid>d5215f73-30da-00a6-8c06-91668ba75739</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu cpuset='0-1'>2</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/home/images/rhel5.5.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:8c:50:ba'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04'
function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ac97'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05'
function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02'
function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
function='0x0'/>
    </memballoon>
  </devices>
</domain>    

2. Offline host's the other 2 cpu (2-3)

echo 0 >/sys/devices/system/cpu/cpu3/online
echo 0 >/sys/devices/system/cpu/cpu2/online

3. Start the guest again
# virsh start rhel5.5

4.
#cat /var/log/libvirt/libvirtd.log|grep vcpus 2012-10-19 08:58:05.560+0000: 83902: warning : virDomainDefParseXML:8863 : Ignore vcpupin for not onlined vcpus
	
Expected Results:

3. The guest could be booted up successfully
Notes:
Comments:

		177126 	[libvirtd] Using a self recursive snapshot backing store *takes out* libvirtd--Bug 601067 	kxiong 	kxiong 	Manual 		--default-- 	P2 	6340 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1.create qcow2 snapshot volume 
#qemu-img create -f qcow2 -o backing_file=/home/images/sdbimagesnap2.qcow2,backing_fmt=host_device  /home/images/sdbimagesnap2.qcow2 20G



	
Expected Results:

output:

Error: Trying to create an image with the same filename as the backing file

Notes:
Comments:

		176801 	[CPU Management] Guest cpu topology should same with host CPU definition from capabilities XML when set cpu mode='host-model' - bug 806754 	mzhan 	mzhan 	Manual 		Function 	P2 	6350 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.4
    Regression
    cpu

bug:

    No bug found

Actions:

1. check host cpu definition from capabilities XML
#virsh capabilities
......
 <cpu>
      <arch>x86_64</arch>
      <model>Penryn</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='osxsave'/>
      <feature name='xsave'/>
......

#virsh nodeinfo
CPU model:           x86_64
CPU(s):              4
CPU frequency:       2000 MHz
CPU socket(s):       1
Core(s) per socket:  4
Thread(s) per core:  1
NUMA cell(s):        1
Memory size:         7575492 kB

2. edit guest xml and start guest 
#virsh edit $guest_name 
..... 
<cpu mode='host-model'/>
 .....

 
	
Expected Results:

bug 806754 is not fixed now.

1. After step 2, guest could be started successfully.

#ps -ef | grep $guest_name
qemu      3847     1 17 01:57 ?        00:01:31 /usr/libexec/qemu-kvm -S -M
rhel6.3.0 -cpu
Penryn,+osxsave,+xsave,+pdcm,+xtpr,+tm2,+est,+vmx,+ds_cpl,+monitor,+dtes64,+pbe,+tm,+ht,+ss,+acpi,+ds,+vme
-enable-kvm -m 1024 -smp 4,sockets=1,cores=4,threads=1 -name rhel6.3 -uuid
0cf5ebf9-2397-5dea-97b2-ea78da13b70c -nodefconfig -nodefaults -chardev
socket,id=charmonitor,path=/var/lib/libvirt/qemu/rhel6.3.monitor,server,nowait
-mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown
-device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -device
virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x6 -drive
file=/var/lib/libvirt/images/rhel6.3.img,if=none,id=drive-ide0-0-0,format=qcow2,cache=none
-device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1
-netdev tap,fd=28,id=hostnet0 -device
rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:8c:1a:53,bus=pci.0,addr=0x3
-chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0
-vnc 0.0.0.0:1 -vga cirrus -device intel-hda,id=sound0,bus=pci.0,addr=0x4
-device hda-duplex,id=sound0-codec0,bus=sound0.0,cad=0 -device
virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

2. Login the guest and check:

#virsh nodeinfo
CPU model:           x86_64
CPU(s):              4
CPU frequency:       2826 MHz
CPU socket(s):       1
Core(s) per socket: 4
Thread(s) per core:  1
NUMA cell(s):        1
Memory size:         1020412 kB

 

 
Notes:
Comments:

		176809 	[CPU Management] start the guest which CPU comprises flags that host doesn't support - bug 724893 	mzhan 	mzhan 	Manual 		Negative test 	P3 	6350 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    Regression
    RHEL6.4

bug:

    No bug found

Actions:

1. Host cpu flag:
# cat /proc/cpuinfo
...
model name : Intel(R) Core(TM)2 Quad CPU    Q9400  @ 2.66GHz
...
flags  : fpu vme de pse tsc msr pae mce cx8 apic mtrr pge mca cmov pat pse36
clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc
arch_perfmon pebs bts rep_good aperfmperf pni dtes64 monitor ds_cpl vmx smx est
tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority

2. Select a health guest, make sure it is shutdown

# virsh edit <guest>

....

 <cpu match='exact'>
    <model>Nehalem</model>   
    <vendor>Intel</vendor>
  </cpu>
...

Note: Nehalem can only be supported when host has flag sse4.2. And in host we have checked it is not existed.

3. # virsh start <guest>
	
Expected Results:

3. Guest should not start successfully. Also pop up message as following:

error: Failed to start domain kvm1
error: internal error guest CPU is not compatible with host CPU

 

TBD for bug 724893
Notes:
Comments:

		177131 	[libvirtd]After issue "event-test.py qemu:///system", opening virt-viewer will not kill service libvirtd --bug 741533 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	6350 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1.Start the libvirtd service, and then open two terminals.

2.In the first terminal run:
# python /usr/share/doc/libvirt-python-x.x.x/events-python/event-test.py 
qemu:///system

3.In another terminal run:
# virsh start $name_of_domin
# virt-viewer $name_of_domin

4. close the virt-viewer.

5. # service libvirtd status 
libvirtd (pid  22977) is running...


	
Expected Results:

Step5 libvirtd should not crashed like:

libvirtd dead but pid file exists.
Notes:
Comments:

		177134 	[libvirtd]libvirtd crashes after running virsh cpu-compare with unexpected input--Bug 630618 	kxiong 	kxiong 	Manual 		--default-- 	P2 	6360 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    libvirtd
    Regression

bug:

    No bug found

Actions:

1. # cat cpu.xml
<cpu>
<topology sockets='1' cores='1' threads='1'/>
</cpu>
2. # virsh cpu-compare cpu.xml

3. #service libvirtd status

	
Expected Results:

2.It will show error message:

error: Failed to compare host CPU with cpu.xml

error: Requested operation is not valid: no CPU model specified

3.

ï»¿libvirtd (pid  32763) is running...
Notes:
Comments:

		176811 	[CPU Management] Unplugging vCPUs - bug 562886 bug 816475 	yimwang 	yimwang 	Both 		Regression 	P3 	6370 	Edit
Setup:

Vcpu hot unplut will not be supported on rhel6 , disable the case

More details you can refer the discuss from Bug 816475 - RFE: libvirt should support cpu hot unplug
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    Regression

bug:

    No bug found

Actions:

1. Start a guest with more than 1 CPU.

2. Check with;
   # virsh vcpuinfo GUEST

3. Reduce the number of vCPUs using;
   # virsh setvcpus GUEST 1

4. Check using;
   # virsh vcpuinfo
   and inside the guest.

	
Expected Results:

All checkpoints are good.
Notes:
Comments:

		177196 	[Managed save] Cann't managed save a transient domain - bug 729714 	dyuan 	dyuan 	Auto 		Regression 	P1 	6370 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. # virsh create dom.xml 
Domain dom-create created from dom.xml

# virsh list --all
 Id Name                 State
----------------------------------
  8 dom-create           running

2. # virsh managedsave dom-create


3. # virsh list --all
 Id Name                 State
----------------------------------
  8 dom-create           running

	
Expected Results:

step 2:

# virsh managedsave dom-create

error: Failed to save domain dom-create state
error: Requested operation is not valid: cannot do managed save for transient
domain

Notes:
Comments:

		176812 	[CPU Management] vCPU balloon out - bug 562886 	yimwang 	None 	Manual 		Regression 	P1 	6380 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the guest with the following cpu configure:
snip...
<vcpu current='1'>2</vcpu>
snip...

   # virsh start guestA
   Domain guestA started

2. Check number of vCPUs.
   # virsh vcpuinfo guestA
   VCPU:           0
   CPU:            0
   State:          running
   CPU time:       2.8s
   CPU Affinity:   yyyy

3. Set the number of vCPUs.
   # virsh setvcpus guestA 2

4. Verify the vCPUs are added.
   # virsh vcpuinfo guestA

5. virsh dumpxml guestA

6. log in guest and check the actual cpu number
# cat /proc/cpuinfo

-----------negative testing----------------

7. Set the number of vCPUs.
   # virsh setvcpus guestA 3


	
Expected Results:

step 4:

   VCPU:           0
   CPU:            0
   State:          running
   CPU time:       0.5s
   CPU Affinity:   yyyy

   VCPU:           1
   CPU:            0
   State:          running
   CPU Affinity:   yyyy

step 5:
should include the following content:

<vcpu>2</vcpu>

step 6:
The actual cpu num should be 2 but not 1.

step 7:
# virsh setvcpus guestA 3
error: invalid argument in requested vcpus is greater than max allowable vcpus for the domain: 3 > 2

Notes:
Comments:

		177209 	[Managed save] invalid save file should not block the domain start - bug 730750 	dyuan 	dyuan 	Auto 		Regression 	P1 	6380 	Edit
Setup:



Bug 730750 - Error restoring domain: cannot send monitor command '{"execute":"qmp_capabilities"}': Connection reset by peer

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. start a domain

# virsh list --all

 Id Name                 State
----------------------------------
7  dom          running

2. save the domain, and before the save file is complete, make a copy of it.

# virsh managedsave dom


3. before the step 2 finished, make a copy of the save file

# cp /var/lib/libvirt/qemu/save/dom.save /home

4. after the save finished, cp the dom.save back to /var/lib/libvirt/qemu/save

# mv /var/lib/libvirt/qemu/save/dom.save /tmp

# cp /home/dom.save /var/lib/libvirt/qemu/save/

5. start the domain again

# virsh start dom
	
Expected Results:

step 5:

domain will boot normally and remove the incomplete save file.

# virsh start geust
Domain guest started

# ll /var/lib/libvirt/qemu/save/guest.save

 ls: cannot access /var/lib/libvirt/qemu/save/guest.save: No such file or directory

In /var/log/libvirt/libvirtd.log,there is meesage like this:

warning : qemuDomainObjStart:4708 : Ignoring incomplete managed state /var/lib/libvirt/qemu/save/dom.save

Notes:
Comments:

		176813 	[CPU Management] vCPU hotplug - bug562886,533138,857013 	yimwang 	None 	Both 		Function 	P3 	6390 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    Regression

bug:

    857013

Actions:

1. # virsh edit rhel63
Domain rhel63 XML configuration edited.

Make sure change as following: 
...
<vcpu current='1'>3</vcpu>
...

# virsh start rhel63
Domain rhel63 started

# virsh dumpxml rhel63
...
  <vcpu current='1'>3</vcpu>

# virsh vcpuinfo guest
VCPU:           0
CPU:            0
State:          running
CPU time:       44995.8s
CPU Affinity:   yyyy


2. Login the guest and check 
# cat /proc/cpuinfo|grep processor|wc -l 
1

3. Hotplug the vcpu
# virsh setvcpus rhel63 2

# virsh dumpxml rhel63
...
  <vcpu current='2'>3</vcpu>


# virsh cpu-stats rhel6.2

CPU0: 
cpu_time             7.947330261 seconds

vcpu_time            7.660429534 seconds

CPU1:

cpu_time             2.387255727 seconds
vcpu_time            2.203674969 seconds


# virsh edit rhel63
...
  <vcpu current='1'>3</vcpu>

# virsh vcpuinfo rhel63
VCPU:           0
CPU:            2
State:          running
CPU time:       3.3s
CPU Affinity:   yyyy

VCPU:           1
CPU:            2
State:          running
CPU Affinity:   yyyy



Login the guest and check 
# cat /proc/cpuinfo|grep processor|wc -l 
2

# virsh setvcpus rhel63 3 --live (or --current)

# virsh dumpxml rhel63
...
<vcpu>3</vcpu>

# virsh edit rhel63
...
  <vcpu current='1'>3</vcpu>

Login the guest and check 
# cat /proc/cpuinfo|grep processor|wc -l 
3

4. Some negatvie testing:

# virsh setvcpus rhel63 4 --live
error: invalid argument: requested vcpus is greater than max allowable vcpus for the domain: 4 > 3

# virsh destroy rhel63
Domain rhel63 destroyed

# virsh setvcpus rhel63 2
error: Requested operation is not valid: domain is not running

	
Expected Results:
Notes:
Comments:

    #1 yupzhang@redhat.com 2012-06-28 14:43:46
    Bug exist.
    https://bugzilla.redhat.com/show_bug.cgi?id=816652

		176814 	[CPU Management] vcpu pin testing -- offline 	weizhan 	None 	Auto 		Feature 	P2 	6400 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu
    virsh-rail

bug:

    855218 - From Run 45039
    855218 - From Run 46322

Actions:

1. Define a guest with the following <vcpu> element on domain xml and then start it;
         <vcpu cpuset="2">4</vcpu>

Note:If your host has more than 4 cpus you can use "<vcpu cpuset="2">4</vcpu>",

else you can change it according to your host environment.

For example, if your host has only 2 cpu cores, you can change it to "<vcpu cpuset="1">2</vcpu>" or "<vcpu cpuset="0">2</vcpu>",else you will get a error output in the stdout like:

error: Failed to start domain <domain>
error: internal error cannot set CPU affinity on process 0: Invalid argument

2.Check CPU affinity

# virsh vcpuinfo rhel6

# grep Cpus_allowed_list /proc/7598/task/*/status

here the 7598 is the qemu-kvm process id of domain rhel6, you can get it with # ps aux|grep qemu-kvm|grep rhel6

3. Define a guest with the following <vcpu> element on domain xml and then start it;
         <vcpu cpuset="1-3">4</vcpu>

Note:Just like step 1, you need to change it recording to your own environment.

And the results will get some change too.

4. Check CPU affinity

# virsh vcpuinfo rhel6

# grep Cpus_allowed_list /proc/7598/task/*/status

5. Define a guest with the following <vcpu> element and then start it;
         <vcpu cpuset="1-3,^2,0">4</vcpu>

Note:Just like step 1, you need to change it recording to your own environment.

And the results will get some change too.

6. Check CPU affinity

# virsh vcpuinfo rhel6

# grep Cpus_allowed_list /proc/7598/task/*/status
	
Expected Results:

Step 2 result:

# virsh vcpuinfo rhel6

VCPU:           0
CPU:            2
State:          running
CPU time:       1.7s
CPU Affinity:   --y-

VCPU:           1
CPU:            2
State:          running
CPU time:       0.1s
CPU Affinity:   --y-

VCPU:           2
CPU:            2
State:          running
CPU time:       0.1s
CPU Affinity:   --y-

VCPU:           3
CPU:            2
State:          running
CPU time:       0.1s
CPU Affinity:   --y-

# grep Cpus_allowed_list /proc/9150/task/*/status
/proc/9150/task/9150/status:Cpus_allowed_list:    2
/proc/9150/task/9166/status:Cpus_allowed_list:    2
/proc/9150/task/9173/status:Cpus_allowed_list:    2
/proc/9150/task/9174/status:Cpus_allowed_list:    2
/proc/9150/task/9175/status:Cpus_allowed_list:    2
/proc/9150/task/9184/status:Cpus_allowed_list:    2

Step 4 result:

# virsh vcpuinfo rhel6
VCPU:           0
CPU:            2
State:          running
CPU time:       0.3s
CPU Affinity:   -yyy

VCPU:           1
CPU:            3
State:          running
CPU Affinity:   -yyy

VCPU:           2
CPU:            2
State:          running
CPU Affinity:   -yyy

VCPU:           3
CPU:            2
State:          running
CPU Affinity:   -yyy

grep Cpus_allowed_list /proc/9222/task/*/status
/proc/9222/task/9222/status:Cpus_allowed_list:    1-3
/proc/9222/task/9242/status:Cpus_allowed_list:    1-3
/proc/9222/task/9243/status:Cpus_allowed_list:    1-3
/proc/9222/task/9244/status:Cpus_allowed_list:    1-3
/proc/9222/task/9245/status:Cpus_allowed_list:    1-3
/proc/9222/task/9254/status:Cpus_allowed_list:    1-3

Step 6 result:

# virsh vcpuinfo rhel6

VCPU:           0
CPU:            1
State:          running
CPU time:       18.3s
CPU Affinity:   yy-y

VCPU:           1
CPU:            1
State:          running
CPU time:       9.2s
CPU Affinity:   yy-y

VCPU:           2
CPU:            0
State:          running
CPU time:       7.5s
CPU Affinity:   yy-y

VCPU:           3
CPU:            3
State:          running
CPU time:       7.0s
CPU Affinity:   yy-y

# grep Cpus_allowed_list /proc/7598/task/*/status
/proc/7598/task/7598/status:Cpus_allowed_list:    0-1,3
/proc/7598/task/7618/status:Cpus_allowed_list:    0-1,3
/proc/7598/task/7619/status:Cpus_allowed_list:    0-1,3
/proc/7598/task/7620/status:Cpus_allowed_list:    0-1,3
/proc/7598/task/7621/status:Cpus_allowed_list:    0-1,3
/proc/7598/task/7644/status:Cpus_allowed_list:    0-1,3

Notes:
Comments:

		177215 	[Managed save] Skip the URI during host shutdown when the URI cannot to be connectted to via initscript 	dyuan 	dyuan 	Manual 		Regression 	P2 	6400 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    Regression

bug:

    No bug found

Actions:

1. install a os without libvirtd or remove libvirt manually

# rpm -e --nodeps libvirt

2. stop the servcie libvirt-guests, it shows:

#service libvirt-guests stop

3. start the service libvirt-guest

4. reboot host
# reboot

5.

install libvirt

#rpm -ivh libvirt-x.x.x.rpm

#service libvirtd status

if libvirtd is running , stop it

# service libvirtd stop
Stopping libvirtd daemon: [ OK ]

6. # service libvirt-guests stop

 

7.# service libvirt-guests status

stopped, with no saved guests

 

8. # service libvirtd start
Starting libvirtd daemon: [ OK ]

# service libvirt-guests stop

Running guests on default URI: vr-guest_managedsave

Suspending guests on default URI...
Suspending vr-guest_managedsave:
Suspending vr-guest_managedsave: done


9. # service libvirt-guests status

stopped, with saved guests


10. # service libvirtd stop
Stopping libvirtd daemon: [ OK ]

11. # service libvirt-guests start


 


 

	
Expected Results:

2. no error message and should show skip the URI

#service libvirt-guests stop
Can't connect to default. Skipping.


4. 

Can't connect to default. Skipping.
Stopping atd: [  OK  ]
Stopping abrt daemon: [  OK  ]

6.

Can't connect to default. Skipping.

 9.

Can't connect to default. Skipping.

 

Notes:
Comments:

		176815 	[CPU Management] vcpu pin testing -- runtime 	gren 	None 	Auto 		Feature 	P2 	6410 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    cpu
    virsh-rail

bug:

    No bug found

Actions:

1,  Suppose that you already have a running vm named rhel5u5 with 4 vcpu

2,  using "virsh vcpuinfo rhel5u5" list vcpu information of rhel5u5

# virsh vcpuinfo rhel5u5

3, using "virsh vcpupin rhel5u5 0 0" to pin the first vcpu onto first physical cpu

# virsh vcpupin rhel5u5 0 0

4, run "virsh vcpuinfo rhel5u5" again to check the result

# virsh vcpuinfo rhel5u5

5. using "virsh vcpupin rhel5u5 0 1,2" to pin the first vcpu onto 2 and 3 physical cpu

    using "virsh vcpupin rhel5u5 1 0,2,3" to pin the second vcpu onto 1,3 and 4 physical cpu

# virsh vcpupin rhel5u5 0 1,2

# virsh vcpupin rhel5u5 1 0,2,3

6. run "virsh vcpuinfo rhel5u5" again to check the result

# virsh vcpuinfo rhel5u5

7. run # grep Cpus_allowed_list /proc/11230/task/*/status to see the cpu pin info

here the 11230 is the qemu-kvm process id of domain rhel5u5, you can get it with # ps aux|grep qemu-kvm|grep rhel5u5
	
Expected Results:

Step 2 output:

VCPU:           0
CPU:            1
State:          running
CPU time:       337.3s
CPU Affinity:   yyyy

VCPU:           1
CPU:            2
State:          running
CPU time:       313.6s
CPU Affinity:   yyyy

VCPU:           2
CPU:            3
State:          running
CPU time:       311.8s
CPU Affinity:   yyyy

VCPU:           3
CPU:            2
State:          running
CPU time:       335.3s
CPU Affinity:   yyyy

 

Step 4 output:

VCPU:           0
CPU:            0
State:          running
CPU time:       337.3s
CPU Affinity:   y---

VCPU:           1
CPU:            3
State:          running
CPU time:       313.6s
CPU Affinity:   yyyy

VCPU:           2
CPU:            2
State:          running
CPU time:       311.9s
CPU Affinity:   yyyy

VCPU:           3
CPU:            2
State:          running
CPU time:       335.4s
CPU Affinity:   yyyy

 

Step 6 output:

VCPU:           0
CPU:            2
State:          running
CPU time:       337.5s
CPU Affinity:   -yy-

VCPU:           1
CPU:            3
State:          running
CPU time:       313.8s
CPU Affinity:   y-yy

VCPU:           2
CPU:            3
State:          running
CPU time:       312.1s
CPU Affinity:   yyyy

VCPU:           3
CPU:            2
State:          running
CPU time:       335.6s
CPU Affinity:   yyyy

 

Step 7 output:

# grep Cpus_allowed_list /proc/11230/task/*/status
/proc/11230/task/11230/status:Cpus_allowed_list:    0-3
/proc/11230/task/11231/status:Cpus_allowed_list:    1-2
/proc/11230/task/11232/status:Cpus_allowed_list:    0,2-3
/proc/11230/task/11233/status:Cpus_allowed_list:    0-3
/proc/11230/task/11234/status:Cpus_allowed_list:    0-3

Notes:
Comments:

		177218 	[Managed save] survive from 20 iterations without failure - bug 727249 	dyuan 	dyuan 	Auto 		Regression 	P1 	6410 	Edit
Setup:

Running a loop of 'virsh managedsave dom && virsh start dom' while virt-manager
is connected to libvirtd is able to crash libvirtd because both a sync job (the
query commands used by virt-manager) and async job (the managed save) can end
up trying to use the qemu monitor at the same time.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    managed save
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. # for i in `seq 20`; do virsh managedsave dom && virsh start dom || { echo
failed on $i; break; }; done

2. # service libvirtd status

3. # virsh list --all

	
Expected Results:

step 1:

no error and no crash


step 2:

# service libvirtd status
libvirtd (pid  13612) is running...


step 3:

# virsh list --all
 Id Name                 State
----------------------------------
  1 dom                  running
Notes:
Comments:

		176816 	[CPU Management] vCPUs balloon in - bug562886 	yimwang 	None 	Manual 		Regression 	P3 	6420 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0

bug:

    No bug found

Actions:

1. Start a guest with more than 1 vCPU.
snip...
<vcpu current='2'>3</vcpu>
snip...

2. Check with;
   # virsh vcpuinfo guestA

3. Reduce the number of vCPUs using;
   # virsh setvcpus guestA 1

4. Check using;
   # virsh vcpuinfo

5. virsh dumpxml guestA

6. log in guest and check the actual cpu number
# cat /proc/cpuinfo

---------------negative testing-------------------------

7. # virsh setvcpus guestA 0



	
Expected Results:

step 2:

VCPU:           0
CPU:            0
State:          running
CPU time:       4.9s
CPU Affinity:   yyyy

VCPU:           1
CPU:            3
State:          running
CPU Affinity:   yyyy

step 4:

VCPU:           0
CPU:            3
State:          running
CPU time:       11.9s
CPU Affinity:   yyyy


step 5: should include the following content:

<vcpu current='1'>3</vcpu>

step 6:

The actual cpu num should be 1 but not 2.

 step 7:

error: invalid argument in virDomainSetVcpus
Notes:
Comments:

		177232 	[Memory management] Memory usage works fine in libvirtd udev backend - bug 595490 	mzhan 	mzhan 	Manual 		Regression 	P3 	6420 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    memory
    Regression

bug:

    No bug found

Actions:

1. In host

# lspci |grep -i hba
04:00.0 Fibre Channel: QLogic Corp. ISP2432-based 4Gb Fibre Channel to PCI Express HBA (rev 03)
04:00.1 Fibre Channel: QLogic Corp. ISP2432-based 4Gb Fibre Channel to PCI Express HBA (rev 03)

2.

# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host2
scsi_host3
scsi_host4
scsi_host5

# virsh nodedev-dumpxml scsi_host5
<device>
  <name>scsi_host5</name>
  <parent>pci_0000_04_00_1</parent>
  <capability type='scsi_host'>
    <host>5</host>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9da4e</wwpn>
    </capability>
    <capability type='vport_ops' />
  </capability>
</device>

# ll /sys/class/fc_host/
total 0
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host4 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.0/host4/fc_host/host4
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host5 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.1/host5/fc_host/host5

# ll /sys/class/scsi_host/
total 0
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host0 -> ../../devices/pci0000:00/0000:00:07.0/host0/scsi_host/host0
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host1 -> ../../devices/pci0000:00/0000:00:07.0/host1/scsi_host/host1
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host2 -> ../../devices/pci0000:00/0000:00:08.0/host2/scsi_host/host2
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host3 -> ../../devices/pci0000:00/0000:00:08.0/host3/scsi_host/host3
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host4 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.0/host4/scsi_host/host4
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host5 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.1/host5/scsi_host/host5

# cat /sys/class/fc_host/host5/max_npiv_vports
127

# cat /sys/class/fc_host/host5/port_state
Online

3. Create new hba on the host

# cat virtualhba.xml
<device>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9f012</wwpn>
    </capability>
  </capability>
</device>

# virsh nodedev-create virtualhba.xml
Node device scsi_host143 created from virtualhba.xml

# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host143
scsi_host2
scsi_host3
scsi_host4
scsi_host5

# virsh nodedev-dumpxml scsi_host143
<device>
  <name>scsi_host143</name>
  <parent>scsi_host5</parent>
  <capability type='scsi_host'>
    <host>143</host>
    <capability type='fc_host'>
      <wwnn>2001001b32a9da4e</wwnn>
      <wwpn>2101001b32a9f012</wwpn>
    </capability>
  </capability>
</device>

4. Destroy this hba

# virsh nodedev-destroy scsi_host143
Destroyed node device 'scsi_host143'

# virsh nodedev-list --cap=scsi_host
scsi_host0
scsi_host1
scsi_host2
scsi_host3
scsi_host4
scsi_host5

# ll /sys/class/fc_host/
total 0

lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host4 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.0/host4/fc_host/host4
lrwxrwxrwx. 1 root root 0 Jan 11 05:57 host5 -> ../../devices/pci0000:00/0000:00:0d.0/0000:04:00.1/host5/fc_host/host5

5. Check all the steps are correct. Then do an extended run (about 8 hours) of creating and destroying device in a loop and check memory usage. If the numbers stabilize, there is no leak. A leak will case to increase without bound.

# cat test.sh

#!/bin/bash

hba_xml=$1
timeout=28800    # 8h
interval=2
log="/var/log/mem_monitor.log"

#Output memory usage of libvirt into log
output_log(){
  sleep $interval
  date "+%F %T" >> $log
  ps -C libvirtd -o rss,size,vsize >> $log
  echo >> $log
}

#Get new creation node device name
get_latest_node_name(){
  node_name=scsi_$(ls --time=ctime /sys/class/fc_host/|head -1)
  echo $node_name
}


if [ $# -ne 1 ]; then
  echo "Usage: <mem_monitor> <virtual_hba.xml>"
  exit 1
fi

lspci | grep -i hba
virsh nodedev-list --cap=scsi_host
ls -l /sys/class/fc_host/
ls -l /sys/class/scsi_host/


while [ $timeout -gt 0 ]
do
  output_log
  virsh nodedev-create $hba_xml
  output_log
  virsh nodedev-destroy $(get_latest_node_name)
  output_log
  let timeout=$timeout-3*$interval
done

echo "log info: $log"


# chmod +x test.sh

# sh test.sh virtualhba.xml

Final check the log in /var/log/mem_monitor.log if memory leaks or not.

	
Expected Results:

5. Script use this to monitor memory:

ps -C libvirtd -o rss,size,vsize

and the result can be checked in following file. RSS range from 11620 to 12920, and SZ=456836, VSZ=601868

# cat /var/log/mem_monitor.log

2011-01-11 07:22:31
  RSS    SZ    VSZ
12920 456836 601868

2011-01-11 07:22:33
  RSS    SZ    VSZ
12268 456836 601868

2011-01-11 07:22:35
  RSS    SZ    VSZ
12232 456836 601868

2011-01-11 07:22:37
  RSS    SZ    VSZ
12232 456836 601868

2011-01-11 07:22:39
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:22:41
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:22:43
  RSS    SZ    VSZ
12272 456836 601868

2011-01-11 07:22:45
  RSS    SZ    VSZ
12272 456836 601868

2011-01-11 07:22:47
  RSS    SZ    VSZ
12272 456836 601868

2011-01-11 07:22:49
  RSS    SZ    VSZ
12272 456836 601868

2011-01-11 07:22:52
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:22:54
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:22:56
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:22:58
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:23:00
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:23:02
  RSS    SZ    VSZ
11780 456836 601868

2011-01-11 07:23:04
  RSS    SZ    VSZ

11620 456836 601868


2011-01-11 07:23:06
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:23:08
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:23:10
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:23:12
  RSS    SZ    VSZ
11620 456836 601868

2011-01-11 07:23:14
  RSS    SZ    VSZ
11620 456836 601868

......
Notes:
Comments:

		176817 	[Disk hotplug] Attach 32 disks to guest 	jialiu 	None 	Both 		Boundary 	P1 	6430 	Edit
Setup:

make sure install qemu-img

#yum install qemu-img

if the guest is rhel5, execute the following command in guest before attach-disk:

# modprobe acpiphp
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    RHEL6.0
    virtual disks

bug:

    No bug found

Actions:

1. Create 32 disk image file.

# for i in `seq 32`; do qemu-img create /var/lib/libvirt/images/disk${i}.img 1M; done

2. Define and start a domain

3. Prepare the following file:

# cat disk_templ.xml
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/#IMG_FILE#'/>
      <target dev='#TARGET_DEV#' bus='virtio'/>
    </disk>

# cat hotplug_disk.sh
array=(`echo {a..z}`);
 
for i in `seq 26`; do
    echo "------${i}-------"
    disk_index=`expr $i - 1`
    sed -e "s/#IMG_FILE#/disk${i}.img/" \
        -e "s/#TARGET_DEV#/vd${array[$disk_index]}/" disk_templ.xml > hotplug_disk.xml
    virsh attach-device "$1" hotplug_disk.xml
done

# cat hotunplug_disk.sh
array=(`echo {a..z}`);
 
for i in `seq 26`; do
    echo "------${i}-------"
    disk_index=`expr $i - 1`
    sed -e "s/#IMG_FILE#/disk${i}.img/" \
        -e "s/#TARGET_DEV#/vd${array[$disk_index]}/" disk_templ.xml > hotplug_disk.xml
    virsh detach-device "$1" hotplug_disk.xml
done

3. Attach the first 26 disks to guest

# sh hotplug_disk.sh <guestname>

4. In guest, check the newly plug disk.

# fdisk -l

5. Repeat hotplug more disk to guest.

# cat more-disk.xml
    <disk type='file' device='disk'>
      <source file='/var/lib/libvirt/images/disk27.img'/>
      <target dev='vdaa' bus='virtio'/>
    </disk>

# virsh atttach-device <guestname> more-disk.xml

6. Dettach all the disk

# sh hotunplug_disk.sh <guestname>

for the attached disk in step5, please hotunplug them manually.

7. Shutdown the guest.

8. Start the guest again, and check if the hotplug disks are not existing via "fdisk -l"
	
Expected Results:

3. All the commands are working fine without any error.

4. Check from vda to  vdz is added.

5. Currently qemu-kvm support 32 pci slot, so repeat the step untill the following error is seen:

# virsh attach-disk <guestname> more-disk.xml
error: Failed to attach disk
error: internal error No more available PCI addresses

At the erroe is shown

# virsh dumpxml vr-migration-tls-tcp|grep "<address type='pci' "| wc -l
31

6. All the command is running fine without any error

7. shutdown guest successfully

8. Guest is started successfully, and the hotplug disks are not existing.
Notes:
Comments:

		176818 	[Disk hotplug] Attach disk with allow disk format probing 	weizhan 	None 	Manual (Autoproposed) 		Function 	P1 	6440 	Edit
Setup:

prepare 2 types of images with well file system,  raw and qcow2,

for example, 2 guests with raw and qcow2 boot disk.
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1.  enable allow_disk_format_probing

# cat /etc/libvirt/qemu.conf

...
    allow_disk_format_probing = 1
...

# service libvirtd restart

2. prepare 2 disk xml

# qemu-img create /var/lib/libvirt/images/tt.img 10M

# mkfs.ext3 /var/lib/libvirt/images/tt.img

# qemu-img create -f qcow2 /var/lib/libvirt/images/tt.qcow2 10M

# mkfs.ext3 /var/lib/libvirt/images/tt.qcow2

# cat disk.xml
    <disk type='file' device='disk'>
      <driver name='qemu'/>
      <source file='/var/lib/libvirt/images/tt.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>

# cat disk-qcow2.xml
    <disk type='file' device='disk'>
      <driver name='qemu'/>
      <source file='/var/lib/libvirt/images/tt.qcow2'/>
      <target dev='vdb' bus='virtio'/>
    </disk>

2. start a normal guest

3. hotplug disk device to guest

# virsh attach-device guest disk.xml

# virsh attach-device guest disk-qcow2.xml

4. check the guest xml

# virsh dumpxml guest

5. in guest, check if the disk works well

# fdisk -l

# mount /dev/vda /mnt

# mount /dev/vdb /mnt
	
Expected Results:

step 4:

...
  <disk type='file' device='disk'>
      <driver name='qemu'/>
      <source file='/var/lib/libvirt/images/tt.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu'/>
      <source file='/var/lib/libvirt/images/tt.qcow2'/>
      <target dev='vdb' bus='virtio'/>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </disk>
...

no disk type in guest xml about these 2 disks

step 5:

check that the disks show the right images size and can be mounted successfully
Notes:
Comments:

		176819 	[Disk hotplug] Attach disk with restart libvirtd - bug 728428 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	6450 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virtual disks

bug:

    No bug found

Actions:

1. Start a domain:
# virsh start guest

2. Create a image to attach
# qemu-img create -f raw -o /var/lib/libvirt/images/tt.img 10M

3. Attach disk with xml
# virsh attach-disk guest /var/lib/libvirt/images/tt.img vdb

4. Check the guest if the new disk can use

4. Service libvirtd restart
# service libvirtd restart

5. Destroy the guest and start it again
# virsh destroy guest
# virsh start guest
	
Expected Results:

the hot-plugged disk should be removed, there is no vdb exists in the guest
on domain xml, there is no xml like
...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/tt.img'/>
      <target dev='vdb' bus='virtio'/>
    </disk>

 ...

 

Notes:
Comments:

		176820 	[Disk hotplug] Attach disk with unsupported driver 	weizhan 	weizhan 	Manual (Autoproposed) 		Regression 	P1 	6470 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virtual disks

bug:

    No bug found

Actions:

1. Start a domain:
# virsh start demo

2. Create a image to attach
# qemu-img create -f raw -o foo.img 10M

3. Attach a disk with unsupported driver

# virsh attach-disk vr-rhel6u1-x86_64-kvm /var/lib/libvirt/images/foo.img vdb --driver phy



	
Expected Results:

will report error

error: Failed to attach disk
error: unsupported configuration: unsupported driver name 'phy' for disk '/var/lib/libvirt/images/foo.img'

make sure that the bold words here is not "internel error"
Notes:
Comments:

		176821 	[Disk hotplug] Attach disk with wrong xml 	weizhan 	None 	Manual (Autoproposed) 		Regression 	P1 	6480 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    virtual disks

bug:

    No bug found

Actions:

1. Start a domain:
# virsh start demo

2. Create a image to attach
# qemu-img create -f raw -o /var/lib/libvirt/images/tt.img 10M

3. Attach disk with wrong xml

cat disk.xml
    <disk type='ttt' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/tt.img'/>
      <target dev='vdc' bus='virtio'/>
    </disk>

# virsh attach-device demo disk.xml

 

 
	
Expected Results:

will report error
error: Failed to attach device from disk.xml
error: internal error unknown disk type 'ttt'

Notes:
Comments:

		177285 	[Migration] Migration with --persistent with changing the original xml of guest - bug 738148 	weizhan 	weizhan 	Manual 		Function 	P1 	6480 	Edit
Setup:

Prepare 2 hosts and prepare a nfs which is mounted on both hosts, and setting the virt_use_nfs boolean on both sides

  # setsebool virt_use_nfs 1

   and close the iptable on both sides

   # iptables -F
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    migration
    Regression

bug:

    No bug found

Actions:

1. Start a guest located on nfs

2. After the KVM guest runs, edit the XML definition with "# virsh edit rhel61_36x64 "

Change the number of vcpu 

# virsh dumpxml rhel61_36x64 ãã

<domain type='kvm' id='6'>

<name>rhel61_36x64</name>

<uuid>24a73864-5beb-395d-9c60-d7cd18995915</uuid>

<memory>1048576</memory>

<currentMemory>1048576</currentMemory>

<vcpu>2</vcpu>

 ... (snip) ...

 

# virsh dumpxml rhel61_36x64 --inactive

<domain type='kvm'>

<name>rhel61_36x64</name>

<uuid>24a73864-5beb-395d-9c60-d7cd18995915</uuid>

<memory>1048576</memory>

<currentMemory>1048576</currentMemory>

<vcpu current='1'>2</vcpu>

... (snip) ...

Change the number of vcpu from 2 to 1 in the XML definition file.

3. Execute migration

# virsh migrate rhel61_36x64 qemu+ssh://192.168.1.40/system --persistent

4. See the XML definition for the KVM guest.

# virsh dumpxml rhel61_36x64 ãã

<domain type='kvm' id='6'>

<name>rhel61_36x64</name>

<uuid>24a73864-5beb-395d-9c60-d7cd18995915</uuid>

<memory>1048576</memory>

<currentMemory>1048576</currentMemory>

<vcpu>2</vcpu>

 ... (snip) ...

 

# virsh dumpxml rhel61_36x64 --inactive

<domain type='kvm'>

<name>rhel61_36x64</name>

<uuid>24a73864-5beb-395d-9c60-d7cd18995915</uuid>

<memory>1048576</memory>

<currentMemory>1048576</currentMemory>

<vcpu current='1'>2</vcpu>

... (snip) ...

 

 

 
	
Expected Results:

The original xml has been changed after persistent migration
Notes:
Comments:

		176822 	[Disk hotplug] Attach disks with random target order in xml - bug 683005 	weizhan 	weizhan 	Manual 		Regression 	P1 	6500 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virtual disks

bug:

    No bug found

Actions:

1. Prepare a shutoff guest 
# for i in {1..4};do qemu-img create /opt/disk$i.img 10M;done

2. Add the following disk xml info into guest config file
# virsh edit rhel61
...
<disk type='block' device='disk'>
<driver name='qemu' cache='none'/>
<source dev='/opt/disk1.img'/>
<target dev='vdb' bus='virtio'/>
</disk>
<disk type='block' device='disk'>
<driver name='qemu' cache='none'/>
<source dev='/opt/disk2.img'/>
<target dev='vdc' bus='virtio'/>
</disk>
<disk type='block' device='disk'>
<driver name='qemu' cache='none'/>
<source dev='/opt/disk3.img'/>
<target dev='vde' bus='virtio'/>
</disk>
<disk type='block' device='disk'>
<driver name='qemu' cache='none'/>
<source dev='/opt/disk4.img'/>
<target dev='vdd' bus='virtio'/>
</disk> 

3. # virsh start rhel61 

4. # virsh dumpxml rhel61

5. # ps axu|grep rhel61 

 
	
Expected Results:

step 4 output:

...
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/opt/disk1.img'/>
      <target dev='vdb' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/opt/disk2.img'/>
      <target dev='vdc' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x08'
function='0x0'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/opt/disk4.img'/>
      <target dev='vdd' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x09'
function='0x0'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/opt/disk3.img'/>
      <target dev='vde' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x0a'
function='0x0'/>
    </disk>
...

step5 output:

qemu     28350  6.7  7.1 1322888 569044 ?      Sl   02:01  70:44
/usr/libexec/qemu-kvm -S -M rhel6.1.0 -enable-kvm -m 1024 -smp
1,sockets=1,cores=1,threads=1 -name rhel61 -uuid
e3d2704d-35c7-1f8b-c762-cc4ade35f12a -nodefconfig -nodefaults -chardev
socket,id=charmonitor,path=/var/lib/libvirt/qemu/rhel61.monitor,server,nowait
-mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -boot c -drive
file=/var/lib/libvirt/images/rhel61.img,if=none,id=drive-virtio-disk0,format=raw,cache=none,aio=threads
-device
virtio-blk-pci,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0
-drive file=/opt/disk1.img,if=none,id=drive-virtio-disk1,format=raw,cache=none
-device
virtio-blk-pci,bus=pci.0,addr=0x7,drive=drive-virtio-disk1,id=virtio-disk1
-drive file=/opt/disk2.img,if=none,id=drive-virtio-disk2,format=raw,cache=none
-device
virtio-blk-pci,bus=pci.0,addr=0x8,drive=drive-virtio-disk2,id=virtio-disk2
-drive file=/opt/disk4.img,if=none,id=drive-virtio-disk3,format=raw,cache=none
-device
virtio-blk-pci,bus=pci.0,addr=0x9,drive=drive-virtio-disk3,id=virtio-disk3
-drive file=/opt/disk3.img,if=none,id=drive-virtio-disk4,format=raw,cache=none
-device
...

Conclusion: after adding disks to guest, disks are reordered from vdb to vde,
0x7->id=drive-virtio-disk1
0x8->id=drive-virtio-disk2
0x9->id=drive-virtio-disk3
0xa->id=drive-virtio-disk4

 

 

 
Notes:
Comments:

		176776 	[console and serial devices] virtio channel connection and operation 	vbian 	None 	Manual 		Feature 	P2 	6510 	Edit
Setup:

virtio channel connection and operation

 

in the host install  minicom
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices

bug:

    No bug found

Actions:

1. Add the following XML section to guest domain
       <serial type='pty'>
      <source path='/dev/pts/1'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>

2. Boot the guest domain , and configure the guest grub.conf file , append the following parameters to kernel commandline
   
   console=tty0 console=ttyS0,115200n8

3. On the host , check the /var/log/libvirt/qemu/guest.log , and checkout the following line

   "char device redirected to /dev/pts/6"

4. On the host , perform the following commands to connect to the guest channel device

   # minicom -s
   
            +-----[configuration]------+
            | Filenames and paths      |
            | File transfer protocols  |
            | Serial port setup        |
            | Modem and dialing        |
            | Screen and keyboard      |
            | Save setup as dfl        |
            | Save setup as..          |
            | Exit                     |
            | Exit from Minicom        |
            +--------------------------+

    choose the Serial port setup to setup the serial console connection
    +-----------------------------------------------------------------------+
    | A -    Serial Device      : /dev/pts/6                                |
    | B - Lockfile Location     : /var/lock                                 |
    | C -   Callin Program      :                                           |
    | D -  Callout Program      :                                           |
    | E -    Bps/Par/Bits       : 115200 8N1                                |
    | F - Hardware Flow Control : Yes                                       |
    | G - Software Flow Control : No                                        |
    |                                                                       |
    |    Change which setting?                                              |
    +-----------------------------------------------------------------------+

    Type A to select the Serial Device , and modify the serial Device to /Dev/pts/6 -- which is checked out from guest.log char device line

    Press Enter to save the change and go back to the configuration menu
    
    Choose Save setup as dfl , and then select "Exit" to connect the guest console

CTRL+A then enter x  you can exit minicom

5. Reboot the guest , in the vncviewer or spice interface .
6. login to the guest from the host minicom console , and try to do following operations:
   # service network restart
   # ifconfig
   # ping $host_IP
   # hostname

  
	
Expected Results:

5. after rebooting , you could see the output from host minicom console
6. you could do all of the operations from host minicom console

Notes:
Comments:

    #1 whuang@redhat.com 2012-06-28 10:10:41
    virsh dumpxml guest

    ...










    ...

    pass

		176778 	[console and serial device] console is not properly released in interactive virsh -- bug 730101 	vbian 	vbian 	Manual 		Function 	P1 	6520 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices
    Regression

bug:

    No bug found

Actions:

1) start virsh in interactive mode
     # virsh 
2) start $VM
    virsh # start guest 
3) console $VM 
    virsh # console guest
   
    in the guest perform 
    #  shutdown -h now 

on host 
   virsh #  

	
Expected Results:

Make sure you can see the console connection get cut after guest shuts down
Notes:
Comments:

		176779 	[console and serial device] exiting the console during output causes subsequent commands to fail - bug731584 	vbian 	vbian 	Manual 		Function 	P1 	6530 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices
    Regression

bug:

    No bug found

Actions:

 

Firstly  #virsh console guest   

in the guest  run this scripts : 

1. write the following script in your guest

# cat test.sh
#/bin/sh
while true ; do echo -n "The quick brown fox jumped over the lazy
dogs.  The quick brown fox jumped over the lazy dogs.  "; done

# sh test.sh 

and exit  (CTRL+] )the console while the output is spraying, 

you can get the rolling output of :
 the lazy dogs.  The quick brown fox jumped over the lazy dogs.  The
 quick brown fox jumped over t

1. on the host perform  
virsh # console <your_guest>
Escape character is ^]

you can get the rolling output of :
 the lazy dogs.  The quick brown fox jumped over the lazy dogs.  The
 quick brown fox jumped over t


2. press Ctrl+] to quit the console output 

3. reconnect the virsh console 
# virsh  

virsh # console foo

Escape character is ^]

you can get the rolling output of :
 the lazy dogs.  The quick brown fox jumped over the lazy dogs.  The
 quick brown fox jumped over t



virsh #

	
Expected Results:

Make sure you won't meet following errors when you are trying to reconnect guest console 
virsh # console foo
error: failed to get domain 'foo'
error: An error occurred, but the cause is unknown

virsh # console foo
error: failed to get domain 'foo'
error: An error occurred, but the cause is unknown

virsh # console foo
error: failed to get domain 'foo'
error: no call waiting for reply with prog 536903814 vers 1 serial 6

 

Notes:
Comments:

		176782 	[CPU Management] Check baseline CPU for a set of given capabilities especially with NUMA 	mzhan 	mzhan 	Manual 		Feature 	P2 	6540 	Edit
Setup:

1. original capabilities.xml:

<capabilities>

  <host>
    <uuid>44454c4c-5a00-1056-8054-b2c04f573258</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Penryn</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>
    <migration_features>
      <live/>
      <uri_transports>
        <uri_transport>tcp</uri_transport>
      </uri_transports>
    </migration_features>
    <topology>
      <cells num='1'>
        <cell id='0'>
          <cpus num='4'>
            <cpu id='0'/>
            <cpu id='1'/>
            <cpu id='2'/>
            <cpu id='3'/>
          </cpus>
        </cell>
      </cells>
    </topology>
    <secmodel>
      <model>selinux</model>
      <doi>0</doi>
    </secmodel>
  </host>

  <guest>
    <os_type>hvm</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.2.0</machine>
      <machine canonical='rhel6.2.0'>pc</machine>
      <machine>rhel6.1.0</machine>
      <machine>rhel6.0.0</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <pae/>
      <nonpae/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.2.0</machine>
      <machine canonical='rhel6.2.0'>pc</machine>
      <machine>rhel6.1.0</machine>
      <machine>rhel6.0.0</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

</capabilities>


2. capabilities1.xml:

<capabilities>

  <host>
    <uuid>44454c4c-4400-104d-8036-b1c04f583258</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Penryn</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>
    <migration_features>
      <live/>
      <uri_transports>
        <uri_transport>tcp</uri_transport>
      </uri_transports>
    </migration_features>
    <topology>
      <cells num='1'>
        <cell id='0'>
          <cpus num='4'>
            <cpu id='0'/>
            <cpu id='1'/>
            <cpu id='2'/>
            <cpu id='3'/>
          </cpus>
        </cell>
      </cells>
    </topology>
    <secmodel>
      <model>selinux</model>
      <doi>0</doi>
    </secmodel>
  </host>

  <guest>
    <os_type>hvm</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.0.0</machine>
      <machine canonical='rhel6.0.0'>pc</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <pae/>
      <nonpae/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/libexec/qemu-kvm</emulator>
      <machine>rhel6.0.0</machine>
      <machine canonical='rhel6.0.0'>pc</machine>
      <machine>rhel5.5.0</machine>
      <machine>rhel5.4.4</machine>
      <machine>rhel5.4.0</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/libexec/qemu-kvm</emulator>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

</capabilities>

 

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu

bug:

    No bug found

Actions:

1, For the test we need at lease two hosts with same type of CPU

2, We could get each xml document describing capabilities of hosts by using "virsh capabilities"

Host 1: # virsh capabilities > capabilities.xml ï¼File can refer to the setup which has NUMAï¼

Host 2: # virsh capabilities > capabilities1.xml (File can refer to the setup which has NUMAï¼

# scp capabilities1.xml Host1:/root

Host 1: # cat capabilities1.xml >> capabilities.xml

3, # virsh cpu-baseline capabilities.xml

4. Recheck with cpu baseline with capabilities but without NUMA.

In the above 2 file, pls delete with this section then save.

        <cpus num='4'>
            <cpu id='0'/>
            <cpu id='1'/>
            <cpu id='2'/>
            <cpu id='3'/>
          </cpus>

# virsh cpu-baseline capabilities.xml

 
	
Expected Results:

Step 3 and Step 4 have the results as following:

# virsh cpu-baseline capabilities.xml
<cpu match='exact'>
  <model>Penryn</model>
  <vendor>Intel</vendor>
  <feature policy='require' name='xtpr'/>
  <feature policy='require' name='tm2'/>
  <feature policy='require' name='est'/>
  <feature policy='require' name='vmx'/>
  <feature policy='require' name='ds_cpl'/>
  <feature policy='require' name='monitor'/>
  <feature policy='require' name='pbe'/>
  <feature policy='require' name='tm'/>
  <feature policy='require' name='ht'/>
  <feature policy='require' name='ss'/>
  <feature policy='require' name='acpi'/>
  <feature policy='require' name='ds'/>
  <feature policy='require' name='vme'/>
</cpu>

Notes:
Comments:

		176785 	[CPU Management] Check guest CPU requirements are properly updated with host CPU features 	yimwang 	None 	Auto 		Feature 	P2 	6550 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0

bug:

    No bug found

Actions:

1.# cat test.xml
<domain type='kvm'>
  <name>test</name>
  <uuid>c7721ff9-9110-a632-dc09-57423f3cc1eb</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>1</vcpu>
  <cpu match='minimum'>
<model>Penryn</model>
<vendor>Intel</vendor>
<topology sockets='1' cores='2' threads='1'/>
<feature policy='optional' name='xtpr'/>
<feature policy='disable' name='tm2'/>
<feature policy='force' name='est'/>
<feature policy='forbid' name='vmx'/>
<feature policy='require' name='ds'/>
<feature  policy='optional' name='ia64'/>
</cpu>
<os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/http_test.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>
    <interface type='network'>
      <mac address='52:54:00:3d:6d:34'/>
      <source network='default'/>
      <model type='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <sound model='es1370'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </memballoon>
  </devices>
</domain>

Make sure that the match attribute should be "minimum"

2.# virsh define test.xml
Domain test defined from test.xml

3.# virsh dumpxml --update-cpu test

<domain type='kvm'>
.......
  <cpu match='exact'>
    <model>Penryn</model>
    <vendor>Intel</vendor>
    <topology sockets='1' cores='2' threads='1'/>
    <feature policy='require' name='xtpr'/>
    <feature policy='disable' name='tm2'/>
    <feature policy='force' name='est'/>
    <feature policy='forbid' name='vmx'/>
    <feature policy='require' name='ds'/>
    <feature policy='disable' name='ia64'/>
    <feature policy='require' name='ds_cpl'/>
    <feature policy='require' name='monitor'/>
    <feature policy='require' name='pbe'/>
    <feature policy='require' name='tm'/>
    <feature policy='require' name='ht'/>
    <feature policy='require' name='ss'/>
    <feature policy='require' name='acpi'/>
    <feature policy='require' name='vme'/>
  </cpu>
............

4. Repeat the upper steps with match attribute is "exact" , change <cpu match='minimum'> to <cpu match='exact'>

5. Repeat the upper steps with match attribute is "strict" , change <cpu match='minimum'> to <cpu match='strict'>
	
Expected Results:

3.1Match attribute should now be 'exact'.

3.2Features which were removed should be there with policy='require'.

    <feature policy='require' name='ds_cpl'/>
    <feature policy='require' name='monitor'/>
    <feature policy='require' name='pbe'/>
    <feature policy='require' name='tm'/>
    <feature policy='require' name='ht'/>
    <feature policy='require' name='ss'/>
    <feature policy='require' name='acpi'/>
    <feature policy='require' name='vme'/>

3.3 Otional features should have their policy changed to 'require'.

3.4 The 'ia64' feature should have policy='disable'.

3.5 Everything else should be the same as in the original guest XML.

4.1 Otional features should have their policy changed to 'require'.

4.2 The 'ia64' feature should have policy='disable'.

4.3 Everything else should be the same as in the original guest XML.

5.1 Otional features should have their policy changed to 'require'.

5.2 The 'ia64' feature should have policy='disable'.

5.3 Everything else should be the same as in the original guest XML.
Notes:
Comments:

		176789 	[CPU Management] check the compatibility of CPU between two host 	gren 	None 	Manual 		Feature 	P2 	6560 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    cpu

bug:

    No bug found

Actions:

1, Using "virsh capabilities" display the capabilities of the hypervisor.

2, Remove the <arch> element, and add match='minimum' to the <cpu> element and policy='require' to all <feature> elements and save the XML document into a file.

#cat compare.xml

<cpu match='minimum'>
      <model>Penryn</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature policy='require' name='xtpr'/>
      <feature policy='require' name='tm2'/>
      <feature policy='require' name='est'/>
      <feature policy='require' name='vmx'/>
      <feature policy='require' name='ds_cpl'/>
      <feature policy='require' name='monitor'/>
      <feature policy='require' name='pbe'/>
      <feature policy='require' name='tm'/>
      <feature policy='require' name='ht'/>
      <feature policy='require' name='ss'/>
      <feature policy='require' name='acpi'/>
      <feature policy='require' name='ds'/>
      <feature policy='require' name='vme'/>
</cpu>

5, Run "virsh cpu-compare compare.xml"

6. In original compare.xml change <feature policy='require' name='xtpr'/> to <feature policy='optional' name='xtpr'/> and then run "virsh cpu-compare compare.xml"

7. In original compare.xml remove <feature policy='require' name='xtpr'/> and then run "virsh cpu-compare compare.xml"

8. In original compare.xml change <feature policy='require' name='tm2'/> to <feature policy='forbid' name='tm2'/> and then run "virsh cpu-compare compare.xml

9. In original compare.xml change <feature policy='require' name='est'/> to <feature policy='disable' name='est'/> and then run "virsh cpu-compare compare.xml"

10. In original compare.xml add <feature policy='optional' name='ia64'/> and then run "virsh cpu-compare compare.xml"

11. in original compare.xml add <feature policy='disable' name='ia64'/>
# virsh cpu-compare compare.xml

12. in original compare.xml add <feature policy='require' name='ia64'/>
# virsh cpu-compare compare.xml

13. in original compare.xml add <feature policy='forbid' name='ia64'/>
# virsh cpu-compare compare.xml

14. in original compare.xml add <feature policy='force' name='ia64'/>
# virsh cpu-compare compare.xml

15. Repeat the upper steps with changing <cpu match='minimum'> to <cpu match='exact'> in original compare.xml, the results are same.

16. Repeat the upper steps with changing <cpu match='minimum'> to <cpu match='strict'> in original compare.xml, other results are same except step 7
	
Expected Results:

Results:

step 5: output

CPU described in compare.xml is identical to host CPU

step 6: output

CPU described in compare.xml is identical to host CPU

step 7: output

Host CPU is a superset of CPU described in compare.xml

step 8: output

CPU described in compare.xml is incompatible with host CPU

step 9: output

CPU described in compare.xml is identical to host CPU

step 10: output

CPU described in compare.xml is identical to host CPU

step 11: output

CPU described in compare.xml is identical to host CPU

step 12: output

CPU described in compare.xml is incompatible with host CPU

step 13: output

CPU described in compare.xml is identical to host CPU

step 14: output

CPU described in compare.xml is identical to host CPU

step 15: the results are as above

step 16: the results are as abouve except repeat step 7, output

CPU described in compare.xml is incompatible with host CPU
Notes:
Comments:

		176790 	[CPU Management] Check the information about host CPU model 	gren 	None 	Auto 		Feature 	P2 	6570 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    cpu
    virsh-rail

bug:

    No bug found

Actions:

1, using virsh command "virsh capabilities" to list the information about the host CPU model

# virsh capabilities

<...snippet>
  <host>
    <cpu>
      <arch>x86_64</arch>
      <model>core2duo</model>
      <topology sockets='1' cores='2' threads='1'/>
      <feature name='lahf_lm'/>
      <feature name='xtpr'/>
      <feature name='cx16'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
    </cpu>
<...snippet/>

2, check validation of the output information

 sockets indictates the number to pyhsical CPU

 cores means the number of cores in each pyhsical CPU

threads means the number of thread supported by per core
	
Expected Results:

ensure that the information about the host CPU model is correct

we can check the information through checking the /proc/cpuinfo and "virsh nodeinfo"

e.g.

 1. Check CPU topology in the <topology> element correctly describes

   host CPU topology (consult /proc/cpuinfo).

2. Look up the CPU model from the <model> element in cpu_map.xml and check

   that all features listed by the model and all additional features found 
   in the <cpu> element are also listed in /proc/cpuinfo.


  (Note that sse4.1 corresponds to sse4_1 and ds to dts in /proc/cpuinfo and
   not all features from /proc/cpuinfo have to be advertised in capabilities)

Notes:
Comments:

		176791 	[CPU Management] compare baseline CPU for a set of given capabilities - bug 731151 	mzhan 	mzhan 	Manual 		Feature 	P2 	6580 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    Regression

bug:

    No bug found

Actions:

1. # virsh capabilities > capabilities.xml
2. # virsh cpu-compare capabilities.xml

3. # virsh list --all
 Id Name                 State
----------------------------------
 17 rhel61               running

4. Check compare with guest xml which has cpu elements

# virsh dumpxml rhel62
....

<cpu match='exact'>

<model>core2duo</model>

<vendor>Intel</vendor>

<topology sockets='1' cores='2' threads='1'/>

<feature policy='disable' name='lahf_lm'/> </cpu>

...


# virsh dumpxml rhel62 > rhel62.xml 

5.# virsh cpu-compare rhel62.xml

6. Check compare with guest xml which has no cpu elements
# virsh cpu-compare rhel62.xml 

 

	
Expected Results:

 After step 4:

If compare with guest xml which has cpu elements

# cat rhel62.xml

...

<cpu match='exact'>

<model>core2duo</model>

<vendor>Intel</vendor>

<topology sockets='1' cores='2' threads='1'/>

<feature policy='disable' name='lahf_lm'/> </cpu>

...


5. # virsh cpu-compare rhel62.xml
Host CPU is a superset of CPU described in rhel62.xml

6. If compare with guest xml which has no cpu elements

# virsh cpu-compare rhel62.xml error:

File 'rhel62.xml' does not contain a <cpu> element or is not a valid domain or capabilities XML

 
Notes:
Comments:

		176792 	[CPU Management] compare baseline CPU for a set of given CPUs 	gren 	None 	Auto 		Feature 	P2 	6590 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    RHEL6.0
    cpu
    virsh-rail

bug:

    No bug found

Actions:

1, For the test we need at lease two hosts with same type of CPU

2, We could get each xml document describing capabilities of hosts by using "virsh capabilities"

3, we collect the <cpu> element from each of the xml documents and write them into a one file like the following:

    <cpu>
      <arch>x86_64</arch>
      <model>core2duo</model>
      <topology sockets='1' cores='2' threads='1'/>
      <feature name='lahf_lm'/>
      <feature name='sse4.1'/>
      <feature name='xtpr'/>
      <feature name='cx16'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
    </cpu>
    <cpu>
      <arch>x86_64</arch>
      <model>core2duo</model>
      <topology sockets='1' cores='2' threads='1'/>
      <feature name='lahf_lm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
    </cpu>

4 In this example, the file named 7052.xml,  then run command" virsh cpu-baseline 7052.xml"

Reference

   http://berrange.com/posts/2010/02/15/guest-cpu-model-configuration-in-libvirt-with-qemukvm/
	
Expected Results:

Result:

<cpu match='exact'>
  <model>core2duo</model>
  <feature policy='require' name='lahf_lm'/>
  <feature policy='require' name='ht'/>
  <feature policy='require' name='ss'/>
  <feature policy='require' name='acpi'/>
  <feature policy='require' name='ds'/>
</cpu>
Notes:
Comments:

		176783 	[CPU Management] check CPU models/flags added to cpu_map.xml - bug 797283 	mzhan 	mzhan 	Manual 		Feature 	P3 	6600 	Edit
Setup:

These cpu feature flags pcid,smep,smap

 
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu

bug:

    797283 - From Run 51203

Actions:

1. Check cpu_map.xml keeps consistent with cpu flags

# cat /usr/share/libvirt/cpu_map.xml

 
	
Expected Results:

 check features pcid,smep,smap are added
Notes:
Comments:

		176786 	[CPU Management] check guest when changing vcpu number - bug 799180 	mzhan 	mzhan 	Manual 		Feature 	P2 	6600 	Edit
Setup:

since bug 799180 duplicate of bug 788562  which is a kernel problem , disable the case
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu

bug:

    No bug found

Actions:

1. Make sure you have good rhel6 guest

#virsh dumpxml rhel6.2-new
...
<currentMemory>1048576</currentMemory>
<vcpu current='2'>4</vcpu>
...

2. Wait for the guest boot completely,then change the vcpu living

#virsh setvcpus rhel6.2-new 1 --live

3. 
#virsh setvcpus rhel6.2-new 3 --live

 

 
	
Expected Results:

 bug 799180 is not fixed now

 1. After step 2, guest works well

  2. After step 3, guest still works well
Notes:
Comments:

		176793 	[CPU Management] configuring the guest CPU model-bug822613,bug864097 	gren 	gren 	Both 		Feature 	P2 	6600 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    virsh-rail
    RHEL7

bug:

    822613
    864097

Actions:

1, Please firstly refer to the following test cases to know how to generate the baseline cpu description for guest to use

50256 [CPU Management] compare baseline CPU for a set of given CPUs

2, After we get the baseline description that can now be copied directly into the guest XML at the top level under the <domain> element as follows:

<domain type='kvm'>
  <name>rhel5u5</name>
  <memory>2097152</memory>
  <currentMemory>2097152</currentMemory>
  <vcpu>4</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <cpu match='exact'>
    <model>core2duo</model>
    <feature policy='require' name='lahf_lm'/>
    <feature policy='require' name='ht'/>
    <feature policy='require' name='ss'/>
    <feature policy='require' name='acpi'/>
    <feature policy='require' name='ds'/>
  </cpu>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/rhel5u5.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <interface type='network'>
      <mac address='52:54:00:2b:19:73'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <sound model='es1370'>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
    </video>
  </devices>
</domain>

3, define the new guest machine and start the vm, then let us compare the /proc/cpuinfo file in the guest machine with the one before adding the baseline cpu description.

--------------------------------------------------------------------------------------------------------------------------------------------------

1.
configure a domain with, e.g.,
    <cpu mode='custom' match='exact'>
      <model fallback='allow'>Penryn</model>
    </cpu>


2.# virsh start rhel6.3-new
Domain rhel6.3-new started


3.Check libvirtd debug log.

2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:141 : cpu=0x7f696403ad70, data=0x7f6964039150, nmodels=24, preferred=Penryn
2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:145 : models[0]=Opteron_G4
2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:145 : models[1]=Opteron_G3
2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:145 : models[2]=Opteron_G2
2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:145 : models[3]=Opteron_G1
2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:145 : models[4]=SandyBridge
2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:145 : models[5]=Westmere
2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:145 : models[6]=Nehalem
2012-10-16 07:40:20.697+0000: 13511: debug : cpuDecode:145 : models[7]=Penryn

	
Expected Results:

The result of comparison

the /proc/cpuinfo without <cpu> element

rocessor       : 0
vendor_id       : GenuineIntel
cpu family      : 6
model           : 2
model name      : QEMU Virtual CPU version 0.12.1
stepping        : 3
cpu MHz         : 3158.296
cache size      : 4096 KB
fpu             : yes
fpu_exception   : yes
cpuid level     : 4
wp              : yes
flags           : fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx lm pni cx16 lahf_lm
bogomips        : 6316.59
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:

the /proc/cpuinfo with <cpu> element

rocessor       : 0
vendor_id       : GenuineIntel
cpu family      : 6
model           : 15
model name      : Intel(R) Core(TM)2 Duo CPU     T7700  @ 2.40GHz
stepping        : 11
cpu MHz         : 3158.296
cache size      : 4096 KB
fpu             : yes
fpu_exception   : yes
cpuid level     : 10
wp              : yes
flags           : fpu de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx lm constant_tsc pni ssse3 lahf_lm
bogomips        : 6316.59
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:

step 4.  Check libvirt using qemu "enforce" -cpu flag.
Notes:
Comments:

    #1 yupzhang@redhat.com 2012-06-28 15:50:47
    The bug is not fixed.

		176798 	[CPU Management] enable/disable kvmclock - bug 783921 	gren 	None 	Manual (Autoproposed) 		Feature 	P2 	6600 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu

bug:

    No bug found

Actions:

1. For a shutdown guest,
# virsh edit guest
...
  <clock offset='utc'>
    <timer name='kvmclock' present='yes'/>
  </clock>
...

2. # virsh start guest
   # ps -ef |grep kvm

 3. # virsh destroy guest

# virsh edit guest

...

<clock offset='utc'>

<timer name='kvmclock' present='no'/>

</clock>

...

4. # virsh start guest

# ps -ef |grep kvm
	
Expected Results:

1. Step 2:

# ps -ef |grep kvm
   qemu     12826     1 43 04:56 ?        00:00:20 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -cpu qemu64,+kvmclock -enable-kvm -m 1024 -smp
1,sockets=1,cores=1,threads=1 -name rhel62......

You can find '-cpu qemu64,+kvmclock' from qemu cmd. Also login the guest and
check
# dmesg|grep kvm
kvm-clock: Using msrs 4b564d01 and 4b564d00
kvm-clock: cpu 0, msr 0:1c1df41, boot clock
kvm-clock: cpu 0, msr 0:2215f41, primary cpu clock
Switching to clocksource kvm-clock

2. Step 4:

   # ps -ef |grep kvm
qemu     13115     1 13 04:58 ?        00:00:00 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -cpu qemu64,-kvmclock -enable-kvm -m 1024 -smp
1,sockets=1,cores=1,threads=1 -name rhel62
......

You can find '-cpu qemu64,-kvmclock' from qemu cmd. Also login the guest and
check
# dmesg|grep kvm
nothing return.

 

 
Notes:
Comments:

		176806 	[CPU Management] new cpu's flags, to control hyper-v related features - bug 784836 	mzhan 	mzhan 	Manual 		Feature 	P2 	6600 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    rhel7
    cpu

bug:

    No bug found

Actions:

1. Reproduce this bug:

For a health guest,

# virsh edit guest

 <cpu match='exact'>
    <model>core2duo</model>
    <feature policy='require' name='hv_vapic'/>
    <feature policy='require' name='ss'/>
    <feature policy='require' name='acpi'/>
    <feature policy='require' name='ds'/>
  </cpu>

# virsh start guest

Error starting domain: internal error unkown CPU feature hv_vapic/hv_relaxed
hv_spinlocks=xxx
	
Expected Results:
Notes:
Comments:

		176810 	[CPU Management] Support host-model and host-passthrough modes - bug 700272,bug858147 	mzhan 	mzhan 	Manual 		Feature 	P2 	6600 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu

bug:

    700272
    858147

Actions:

1. Check host-model mode

For a shutdown guest, add the following
# virsh edit guest
...
<cpu mode='host-model'/>
...

# virsh start guest
# ps -ef|grep kvm 
qemu      8338     1 28 04:14 ?        00:00:20 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -cpu Penryn,+osxsave,+xsave,+pdcm,+xtpr,+tm2,+est,+smx,+vmx,+ds_cpl,+monitor,+dtes64,+pbe,+tm,+ht,+ss,+acpi,+ds,+vme
-enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1 -name rhel62......

# virsh capabilities
...
  <host>
...
      <model>Penryn</model>
...

Found the guest is using the same model as host. Here is Penryn. 

------------------------------------------------------------------------------------------------------------------------------------
Note : Waiting for the following bug to be fixed.
Bug 858147 - when cpu mode is 'host-model' ,the vendor_id attribute isn't effective but still can be set in domain's xml.  
Configure the following domain. 

 # virsh dumpxml rhel6.2
<domain type='kvm' id='73'>
 ......
  <cpu mode='host-model'>
    <model fallback='forbid' vendor_id='AuthenticAMD'/>
  </cpu>
......
</domain>


# virsh start rhel6.2
Domain rhel6.2 started


Login guest .check cpu vendor id
# cat /proc/cpuinfo
processor    : 0
vendor_id    : GenuineIntel
cpu family    : 6
model        : 42
model name    : Intel(R) Xeon(R) CPU E31225 @ 3.10GHz
stepping    : 7
...snipped...

 
-----------------------------------------------------------------------------------------------------------------------------------


 2. Check host-passthrough model

For a shutdown guest, add the following
# virsh edit guest
...
<cpu mode='host-passthrough'/>
...

# virsh start guest

# ps -ef |grep kvm
qemu     10137     1 10 04:28 ?        00:00:00 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -cpu host -enable-kvm -m 1024 -smp 1,sockets=1,cores=1,threads=1
-name rhel62 ....

We can find '-cpu host' from qemu cmd.

 
	
Expected Results:

 

 

 
Notes:
Comments:

		176794 	[CPU Management] Control multi domain vcpu affinity 	yoyzhang 	None 	Auto 		Feature 	P2 	6610 	Edit
Setup:

Host with 4 cpu
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Guest rhel55 is configed 2 vcpu, now pin host cpu 0-1 to it.

# virsh vcpupin rhel55 0 0

# virsh vcpupin rhel55 1 1

2. Guest rhel6 is configed the other 2 vcpu, now ping host cpu 2-3 to it

# virsh vcpupin rhel6 0 2

# virsh vcpupin rhel6 1 3
	
Expected Results:

1. Check pin successfully

# virsh vcpuinfo rhel55
VCPU:           0
CPU:            0
State:          running
CPU time:       10.7s
CPU Affinity: y---

VCPU:           1
CPU:            1
State:          running
CPU time:       0.5s
CPU Affinity:   -y--

2. Check pin successfully

# virsh vcpuinfo rhel6
VCPU:           0
CPU:           2
State:          running
CPU time:       10.7s
CPU Affinity:    --y-

VCPU:           1
CPU:            3
State:          running
CPU time:       0.5s
CPU Affinity:    ---y
Notes:
Comments:

		176795 	[CPU Management] CPU checks during migration - feature check 	yimwang 	None 	Manual 		Feature 	P2 	6620 	Edit
Setup:

1. Find two hosts with a different set of CPU features (ideally, one of them being a superset of the other, not completely incompatible).

For example:

on one hosts  #virsh capabilities

...

<cpu>
      <arch>x86_64</arch>
      <model>Penryn</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>

...

on the other host # virsh capabilities

...

<cpu>
      <arch>x86_64</arch>
      <model>Nehalem</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='rdtscp'/>
      <feature name='dca'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>

...

CPU1 is a subset of CPU2

CPU2 is a superset of CPU1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0

bug:

    No bug found

Actions:

1. Compute baseline CPU of the CPUs of the two hosts on one host.
#cat cpus-info.xml
<cpu>
      <arch>x86_64</arch>
      <model>Penryn</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>
    <cpu>
      <arch>x86_64</arch>
      <model>Nehalem</model>
      <vendor>Intel</vendor>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='rdtscp'/>
      <feature name='dca'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>

cpu2 is a superset of cpu1

# virsh cpu-baseline cpus-info.xml
<cpu match='exact'>
  <model>Penryn</model>
  <vendor>Intel</vendor>
  <feature policy='require' name='xtpr'/>
  <feature policy='require' name='tm2'/>
  <feature policy='require' name='est'/>
  <feature policy='require' name='vmx'/>
  <feature policy='require' name='ds_cpl'/>
  <feature policy='require' name='monitor'/>
  <feature policy='require' name='pbe'/>
  <feature policy='require' name='tm'/>
  <feature policy='require' name='ht'/>
  <feature policy='require' name='ss'/>
  <feature policy='require' name='acpi'/>
  <feature policy='require' name='ds'/>
  <feature policy='require' name='vme'/>
</cpu>


2. Create a guest XML using the baseline CPU on host1.


add above <cpu>...</cpu> between <domain> and </domain> on guest xml



  3. Start the guest on host1, migrate to host2.
      For migration, we need to use a shared storage device, such as nfs, and mount on both host with the same point. Then be sure that the iptable and selinux may permit the nfs access and ssh transport, for example, we can simply do
      #iptabls -F
      #setenforce 0
      on both sides, then do migration with
      #virsh migrate --live <guestname> qemu+ssh://{dest ip}/system

   4.Migrate back on dest host
     #virsh migrate --live <guestname> qemu+ssh://{source ip}/system

   5.Change the match attribute in the <cpu> element from <cpu match='exact'> to <cpu match='minimum'> on host1

   6.Repeat steps 3.   

7.Do the steps 2-4 but start with the second host.


   8.Change the match attribute in the <cpu> element from <cpu match='exact'> to <cpu match='minimum'> on host2

   9. Repeat steps 3-4.
 

	
Expected Results:

1-4. All operations should complete successfully

6.If the CPU of host2 was a superset of the CPU of host1, the migration from host1 should succeed.

7. All operations should complete successfully

9.If the CPU of host2 was a superset of the CPU of host1, the migration from host2 will will report an error:
error: internal error guest CPU is not compatible with host CPU

If the two hosts had incompatible CPUs, both step 6 and step 9 will fail with error.
Notes:
Comments:

		176796 	[CPU Management] CPU checks during migration - verdor check 	weizhan 	None 	Manual 		Feature 	P2 	6630 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu

bug:

    No bug found

Actions:

1. use different vendor host for migration, e.g. Inter and AMD
2. get the baseline of these two cpu

# cat baseline.xml
    <cpu>
      <arch>x86_64</arch>
      <model>Penryn</model>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>

    <cpu>
      <arch>x86_64</arch>
      <model>Opteron_G3</model>
      <topology sockets='1' cores='4' threads='1'/>
      <feature name='osvw'/>
      <feature name='3dnowprefetch'/>
      <feature name='cr8legacy'/>
      <feature name='extapic'/>
      <feature name='cmp_legacy'/>
      <feature name='3dnow'/>
      <feature name='3dnowext'/>
      <feature name='pdpe1gb'/>
      <feature name='fxsr_opt'/>
      <feature name='mmxext'/>
      <feature name='ht'/>
      <feature name='vme'/>
    </cpu>

# virsh cpu-baseline baseline.xml
<cpu match='exact'>
  <model>core2duo</model>
  <feature policy='require' name='lahf_lm'/>
  <feature policy='require' name='cx16'/>
  <feature policy='require' name='ht'/>
  <feature policy='disable' name='ssse3'/>
</cpu>
3 add vendor on Inter host with <vendor>Intel</vendor> after <mode></mode> between <cpu> </cpu>
4 start the guest

For migration, we need to use a shared storage device, such as nfs, and mount on both host with the same point. Then be sure that the iptable and selinux may permit the nfs access and ssh transport, for example, we can simply do

      #iptabls -F
      #setenforce 0
on both sides,

5 do migration
# virsh migrate guest qemu+ssh://10.66.83.204/system


	
Expected Results:

step 5: migration should be failed

# virsh migrate rhel6 qemu+ssh://10.66.93.194/system
root@10.66.93.194's password:
error: unsupported configuration: guest and host CPU are not compatible: host CPU vendor does not match required CPU vendor Intel

Notes:
Comments:

		176797 	[CPU Management] CPU counting on a host with 2 cells 	weizhan 	None 	Manual (Autoproposed) 		Feature 	P2 	6640 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu

bug:

    No bug found

Actions:

   1. Find ahost with 2 NUMA cells and see if the NUMA cell(s) is right

       # virsh nodeinfo  
       CPU model:           x86_64
       CPU(s):              8
       CPU frequency:       1050 MHz
       CPU socket(s):       1
       Core(s) per socket:  4
       Thread(s) per core:  1
       NUMA cell(s):        2
       Memory size:         8059856 kB

	
Expected Results:

The NUMA cells is as it really has.
Notes:
Comments:

		176799 	[CPU Management] Get the per-cpu physical cpu usage per VM 	mzhan 	mzhan 	Manual 		Feature 	P2 	6650 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0

bug:

    No bug found

Actions:

# virt-top -1

virt-top 12:59:42 - x86_64 4/4CPU 3092MHz 7944MB
3 domains, 1 active, 1 running, 0 sleeping, 0 paused, 2 inactive D:0 O:0 X:0
CPU: 1.1%  Mem: 1024 MB (1024 MB by guests)

PHYCPU %CPU kvm1                                                                                                        
   0           0.1      0.1=
   1           0.1      0.1=
   2           0.1      0.1=#
   3           0.1      0.1=



check per-cpu usage per vm

 

# cat /cgroup/cpuacct/cpuacct.usage_percpu

3809829307257 2253514551724 413688079617 279932689663
 
	
Expected Results:

Need update

https://bugzilla.redhat.com/show_bug.cgi?id=737726 this bug is not fixed now.

 

the expect result should be:

virt-top get physical cpu utilization per cpu
Notes:
Comments:

		176800 	[CPU Management] Guest cpu topology check 	weizhan 	None 	Manual (Autoproposed) 		Feature 	P2 	6660 	Edit
Setup:

Note:

vcpu num = sockets * cores * threads
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    cpu
    QE consumption

bug:

    889427 - From Run 53625
    889427 - From Run 53626

Actions:

1. create a new guest with following xml

<domain type='kvm'>
  <name>rhel6</name>
  <uuid>d5215f73-30da-00a6-8c06-91668ba75739</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>4</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <cpu>
    <topology sockets='2' cores='1' threads='2'/>
  </cpu>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/home/images/rhel6.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:8c:50:ba'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04'
function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ac97'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05'
function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02'
function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
function='0x0'/>
    </memballoon>
  </devices>
</domain>    

2. start the guest and check the qemu-kvm command

3. on guest, you can find the corresponding information: (sometime it is not correct because on qemu-kvm it has bug 689665)
two methods:
a. lscpu 
b. # cat /proc/cpuinfo |grep "physical id"  (when socket num = vcpu num, there is no "physical id" in /proc/cpuinfo)
# for i in {0..3}; do cat /sys/devices/system/cpu/cpu$i/topology/thread_siblings_list; done
# cat /proc/cpuinfo |grep cores

4. Define another guest with wrong cpu topology - Bug 880017-libvirt should check if vcpu topology is right
 # virsh dumpxml rhel63 
<domain type='kvm' id='104'> 
....... 
<vcpu placement='static'>4</vcpu> 
...... 
<cpu> <topology sockets='1' cores='4' threads='2'/> </cpu> 


5. # ps -ef|grep qemu-kvm 
qemu 21296 1 14 16:41 ? 00:00:17 /usr/libexec/qemu-kvm -name rhel63 -S -M rhel6.4.0 -enable-kvm -m 1024 -smp 4,sockets=1,cores=4,threads=2 ...... 

 

	
Expected Results:

step 2:

qemu     16154 10.1  8.2 1005580 317060 ?      Sl   16:11   1:30 /usr/libexec/qemu-kvm -S -M rhel6.0.0 -enable-kvm -m 512 -smp 4,sockets=2,cores=1,threads=2 -name rhel6 -uuid d5215f73-30da-00a6-8c06-91668ba75739 -nodefconfig -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/mig.monitor,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -drive file=/var/lib/libvirt/migrate/mig,if=none,id=drive-ide0-0-0,format=qcow2 -device ide-drive,bus=ide.0,unit=0,drive=drive-ide0-0-0,id=ide0-0-0,bootindex=1 -netdev tap,fd=22,id=hostnet0 -device rtl8139,netdev=hostnet0,id=net0,mac=52:54:00:82:77:52,bus=pci.0,addr=0x3 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -usb -vnc 0.0.0.0:1 -k en-us -vga cirrus -device AC97,id=sound0,bus=pci.0,addr=0x4 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5

step 3:

with 2 methods:

a. lscpu to check

CPU(s):                4
Thread(s) per core:    2
Core(s) per socket:    1
CPU socket(s):         2

b. # cat /proc/cpuinfo |grep "physical id"

physical id    : 0
physical id    : 0
physical id    : 1
physical id    : 1
which means it has 2 physical cpus.

# for i in {0..3}; do cat /sys/devices/system/cpu/cpu$i/topology/thread_siblings_list; done
0-1
0-1
2-3
2-3
which represent the thread siblings to cpu X in the same core

# cat /proc/cpuinfo |grep cores
cpu cores    : 1
cpu cores    : 1
cpu cores    : 1
cpu cores    : 1

Notes:
Bug 880017 - libvirt should check if vcpu topology is right
Comments:

		176802 	[CPU Management] Illegal cpu_shares value of schedinfo 	mzhan 	mzhan 	Manual (Autoproposed) 		Negative test 	P2 	6670 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0

bug:

    No bug found

Actions:

1. Negative value for cpu_shares 

# virsh schedinfo rhel6 --set cpu_shares=-1 --live
Scheduler      : posix
cpu_shares     : 262144

# virsh schedinfo rhel6 --current
Scheduler      : posix
cpu_shares     : 262144


2. Larger than the max value of cpu_shares (262144)
# virsh schedinfo rhel6 --set cpu_shares=1024 --live
Scheduler      : posix
cpu_shares     : 1024

# virsh schedinfo rhel6 --current
Scheduler      : posix
cpu_shares     : 1024

# virsh schedinfo rhel6 --set cpu_shares=262146 --live
Scheduler      : posix
cpu_shares     : 262144

# virsh schedinfo rhel6 --current
Scheduler      : posix
cpu_shares     : 262144


ï»¿
	
Expected Results:

All checkpoints are good.
Notes:
Comments:

		176803 	[CPU Management] Illegal number of vCPUs 	yimwang 	None 	Auto 		Negative test 	P2 	6680 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    RHEL6.0

bug:

    No bug found

Actions:

1. Start the guest.
   # virsh start guestA
   Domain guestA started

2. Check number of vCPUs.
   # virsh vcpuinfo guestA
   VCPU:           0
   CPU:            1
   State:          running
   CPU time:       2.8s
   CPU Affinity:   yy

3. Set the number of vCPUs.
   # virsh setvcpus guestA  'AA'
  error: Invalid number of virtual CPUs.

ï»¿
	
Expected Results:

All checkpoints are good.
Notes:
Comments:

		176784 	[CPU Management] Check for maximum number of vcpus exceeding topology limit - bug 725269 	mzhan 	mzhan 	Manual 		Negative test 	P3 	6690 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    cpu
    Regression

bug:

    No bug found

Actions:

1. Check max number of vcpus NOT exceed topology limit
# virsh edit guest
...
  <vcpus>3</vcpus>
  <cpu>
    <topology sockets='2' cores='2' threads='2'/>
  </cpu>
... 
In which 3 < 2*2*2, guest can run successfully.

2. Check vcpu exceed the max allowed by topology, such as
# virsh edit guest
...
  <vcpus>9</vcpus>
  <cpu>
    <topology sockets='2' cores='2' threads='2'/>
  </cpu>

In which 9 > 2*2*2, when want to save the changes for the guest and quit, an
error display for this cmd: 

 
	
Expected Results:

1. # virsh start guest

Guest can start successfully.

# ps -ef|grep kvm
qemu     18014     1 10 05:47 ?        00:00:00 /usr/libexec/qemu-kvm -S -M
rhel6.2.0 -enable-kvm -m 1024 -smp 3,sockets=2,cores=2,threads=2 -name rhel62
...

 2. # virsh edit rhel62

error: Maximum CPUs greater than topology limit
Notes:
Comments:

		176755 	[configuration] domain resume when nfs with hard option is not available temporarily 	gren 	None 	Manual 		Regression 	P1 	6700 	Edit
Setup:

 

1, setup a NFS server and a shared folder.

2, install a guest wih its disk image placed into the NFS shared folder.

3, mount the shared folder with option "hard"

   <NFS server>:/var/lib/libvirt/images/ on /mnt/nfsdir type nfs (rw,hard,vers=4,addr=<IP>,clientaddr=<IP>)

   mount -o hard nfs-server:/shared/folder /mnt

 4 set dynamic_ownership = 1 in the /etc/libvirt/qemu.conf
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    configuration

bug:

    No bug found

Actions:

1, # virsh list --all

 Id Name                 State
----------------------------------
  5 rhel6                running

# virsh dumpxml rhel6   // Make sure guest img file is in NFS

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='threads'/>
      <source file='/mnt/rhel6.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>

...

2, At the same time, on host use iptables to drop the packet into the NFS server

     iptables -A OUTPUT -d 10.66.5.5 -p tcp -j DROP

3, After the step 2, guest operation has some hang, execute the following

# iptables -D OUTPUT -d 10.66.5.5 -p tcp -j DROP
	
Expected Results:

1. After step 2, the operation in guest will pause, such as scp guest big file to other host

2. After step 3, the guest will still hang

 
Notes:
Comments:

		176757 	[configuration] domain resume when nfs with soft option is not available temporarily 	gren 	None 	Manual 		Regression 	P1 	6710 	Edit
Setup:

1, setup a NFS server and a shared folder.

2, install a guest wih its disk image placed into the NFS shared folder.

3, mount the shared folder with option "soft"

   <NFS server>:/var/lib/libvirt/images/ on /mnt/nfsdir type nfs (rw,soft,vers=4,addr=<IP>,clientaddr=<IP>)

   mount -o soft nfs-server:/shared/folder /mnt

4, set dynamic_ownership = 0 in /etc/libvirt/qemu.conf, then restart libvirtd
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    configuration

bug:

    No bug found

Actions:

1, # virsh list --all

 Id Name                 State
----------------------------------
  5 rhel6                running

# virsh dumpxml rhel6   // Make sure guest img file is in NFS

...

    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none' io='threads'/>
      <source file='/mnt/rhel6.img'/>
      <target dev='vda' bus='virtio'/>
      <alias name='virtio-disk0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </disk>

...

2, At the same time, on host use iptables to drop the packet into the NFS server

     iptables -A OUTPUT -d 10.66.5.5 -p tcp -j DROP

3, After the step 2, guest operation has some hang, execute the following

# iptables -D OUTPUT -d 10.66.5.5 -p tcp -j DROP
	
Expected Results:

1. After step 2, the operation in guest will pause, such as scp guest big file to other host

2. After step 3, the guest will be unlocked for the operation above. The file can be sent successfully.
Notes:
Comments:

		176766 	[console and serial devices] exiting console in interactive virsh causes subsequent commands to fail - Bug 731579 	vbian 	vbian 	Manual 		Function 	P1 	6740 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices
    Regression

bug:

    No bug found

Actions:

1. configure guest grub with "console=tty0 console=ttyS0,115200n8" appended in
the kernel command line 
2. configure guest with serial device added 
    <serial type='pty'>
      <source path='/dev/pts/5'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
3. start guest 
4. enter interactive mode of virsh 
   # virsh  
   Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # console test1
Connected to domain test1
Escape character is ^]
ï¿½  Welcome to Red Hat Enterprise Linux Client
Starting udev: 
virsh # console test1
Connected to domain test1
Escape character is ^]

[/sbin/fsck
untin quotas:  
virsh # console test1
Connected to domain test1
Escape character is ^]
6_64 on an x86_646_64 on an x86_64x86_64 on an x86_64

After times of Enter-Exit loop , we get the garbled console output in
interactive virsh session . 

virsh # dominfo foo
error: failed to get domain 'foo'
error: An error occurred, but the cause is unknown

virsh # dominfo foo
error: failed to get domain 'foo'
error: no call waiting for reply with prog 536903814 vers 1 serial 300

virsh # define bar.xml
error: Failed to define domain from bar.xml
error: no call waiting for reply with prog 536903814 vers 1 serial 301

	
Expected Results:

Make sure you won't meet any garbled output from guest console and also , in the active mode of virsh shell, and all of the other commands could be performed correctly after performing console command.
Notes:
Comments:

		176767 	[Console and serial devices] Connect to a guest console 	nzhang 	None 	Auto 		Feature 	P2 	6750 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    console and serial devices
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:
   1. Add console=ttyS0,115200 to guest kernel line.
   2. Add the following XML section to a domain.

    <serial type='pty'>
    <target port='0'/>
    </serial>
    <console type='pty'>
    <target port='0'/>
    </console>

   3. Start the domain.
   4. Run #virsh console [domain id].
   5. Log on to the guest.

	
Expected Results:

1. after step 4, there will show sth, for example:


Connected to domain rhel55-x86-64
Escape character is ^]

Kernel 2.6.18-183.el5 on an x86_64

localhost.localdomain login: root
Password:

2. Successful connection to a guest via virtual console and serial.
Notes:
Comments:

		176768 	[console and serial devices] garbled console in interactive virsh session -- bug 731583 	vbian 	vbian 	Manual 		Function 	P1 	6760 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices
    Regression

bug:

    No bug found

Actions:

1. configure guest grub with "console=tty0 console=ttyS0,115200n8" appended in
the kernel command line 
2. configure guest with serial device added 
    <serial type='pty'>
      <source path='/dev/pts/5'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
3. start guest 
4. enter interactive mode of virsh 
   # virsh  
   Welcome to virsh, the virtualization interactive terminal.

Type:  'help' for help with commands
       'quit' to quit

virsh # console test1
Connected to domain test1
Escape character is ^]
ï¿½  Welcome to Red Hat Enterprise Linux Client
Starting udev: 
virsh # console test1
Connected to domain test1
Escape character is ^]

[/sbin/fsck
untin quotas:  
virsh # console test1
Connected to domain test1
Escape character is ^]
6_64 on an x86_646_64 on an x86_64x86_64 on an x86_64

After times of Enter-Exit loop , we get the garbled console output in
interactive virsh session . 

	
Expected Results:

Make sure after your  TIMES of reconnecting to guest console , you can always get the correct output for current guest booting process, and finally , you can get the login prompt , and you can login to guest successfully .

There is NO garbled output .
Notes:
Comments:

		177569 	[snapshot]libvirt should give error when qemu-kvm disabled live snapshot support --bug 747115 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	6760 	Edit
Setup:


Remove live snapshot support from RHEL 6.2 qemu-kvm package.

Live snapshot support should be removed from qemu-kvm and deferred until RHEL 6.3.

	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1. # rpm -q libvirt qemu-kvm
   libvirt-0.9.4-19.el6.x86_64
   qemu-kvm-0.12.1.2-2.199.el6.x86_64

   # virsh snapshot-list foo
    Name                 Creation Time             State
   ------------------------------------------------------------
    s1                   2010-02-16 19:42:28 -0500 running

2. # virsh snapshot-create --disk-only foo
   error: operation failed: Failed to take snapshot: unknown command:
   'snapshot_blkdev'

3. # virsh snapshot-list foo
    Name                 Creation Time             State
   ------------------------------------------------------------
    s1                   2010-02-16 19:42:28 -0500 running

4. # qemu-img info /var/lib/libvirt/images/foo.img 
   image: /var/lib/libvirt/images/foo.img
   file format: qcow2
   virtual size: 5.9G (6291456000 bytes)
   disk size: 2.4G
   cluster_size: 65536
   Snapshot list:
   ID        TAG                 VM SIZE                DATE       VM CLOCK
   1         s1                     3.1M 2010-02-16 19:42:28   00:00:04.605

	
Expected Results:

Step 2 should show error information like:


   error: operation failed: Failed to take snapshot: unknown command:
   'snapshot_blkdev'

 
Notes:
Comments:

		176769 	[console and serial devices] no output when set character device type to pipe - bug 740478 	ydu 	ydu 	Manual 		Regression 	P2 	6770 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices
    Regression

bug:

    No bug found

Actions:

1.installed a RHEL guest , and add the following arguments to
/boot/grub/grub.conf kernel line

"console=ttyS0,115200"

2. create pipe files with mkfifo OR mknod

# mkfifo /tmp/testpipe   or    # mknod /tmp/testpipe.in p

# mkfifo /tmp/testpipe.in   or   # mknod /tmp/testpipe.in  p

# mkfifo /tmp/testpipe.out   or   # mknod /tmp/testpipe.out  p

3. Run virt-manager, Open guest virtual machine details, make sure a guest is
installed and in shutdown status

4. Edit guest xml file
    #virsh edit $guest_name
    Add 
<serial type='pipe'>
   <source path='/tmp/testpipe'/>
   <target port='1'/>
   <alias name='serial1'/>
</serial>

5. Start the guest
   #virsh start $guest_name

6. Check the output of testpipe.out
   #cat /tmp/testpipe.out

	
Expected Results:

6.the boot process of guest can print out by #cat testpipe.out
Notes:
05-Features_not_to_be_tested
Comments:

		177576 	[snapshot]revert should be forbidden without --force 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	6770 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1 # virsh list
 Id Name                 State
----------------------------------
  6 vr-rhel5u4-x86_64-kvm running
 11 dom                  running

2 # virsh snapshot-revert dom snap

# virsh list
 Id Name                 State
----------------------------------
  6 vr-rhel5u4-x86_64-kvm running

3 # virsh snapshot-revert dom snap --running

# virsh list
 Id Name                 State
----------------------------------
  6 vr-rhel5u4-x86_64-kvm running
 12 dom                  running

4 # virsh edit dom (add the second disk)

# virsh start dom
Domain dom started

5 # virsh snapshot-revert dom snap --running
error: revert requires force: must respawn qemu to start inactive snapshot

6 # virsh snapshot-revert dom snap --running --force

# virsh list
 Id Name                 State
----------------------------------
  6 vr-rhel5u4-x86_64-kvm running
 14 dom                  running

7 # virsh dumpxml dom
<domain type='qemu'>
......
    <disk type='file' device='disk'>
      <driver name='qemu' type='qcow2'/>
      <source file='/var/lib/libvirt/images/foo.img'/>
      <target dev='vda' bus='virtio'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
function='0x0'/>
    </disk>
......
</domain>

	
Expected Results:

All steps succeed
Notes:
Comments:

		176770 	[console and serial devices] Prevent multiple connections to same VM console BZ#729940 	ydu 	None 	Manual 		Function 	P2 	6780 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1. append 'console=ttyS0,115200' to guest kernel cmdline in grub.conf

2. reboot guest

3. open a terminal and run #virsh console $guest

4. open another terminal and run #virsh console $guest

5 . if add virsh console $guest --force   

	
Expected Results:

4. libvirt should prevent subsequent connections until the current connection ends.

 

5. old connection is disconnect ,it will make a new connection

Notes:
Comments:

		176771 	[Console and serial devices] Receive data from character device to a file 	nzhang 	None 	Auto 		Feature 	P2 	6790 	Edit
Setup:

Bug 863992 - The domain can't be started while the source path of serial redirect to "/root" path [CLOSED AS NOTABUG]
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    console and serial devices
    RHEL6.0
    virsh-rail

bug:

    No bug found

Actions:

1. Add console=ttyS0,115200 to guest kernel line.

[on host]

1. # mkdir /test

3. Add the following XML to a domain, and delete other serial nodes.
<serial type="file">
   <source path="/test/vm-serial.log"/>
   <target port="1"/>
</serial>

3. Start the domain.

4. # cat //vm-serial.log

5. # virsh destroy guest

6. # virsh edit guest chand the serial part to

<serial type="file">
   <source path="/test/unexisting_dir/vm-serial.log"/>
   <target port="1"/>
</serial>

7. # virsh start guest

error: Failed to start domain guest
error: Unable to pre-create chardev file '/test/unexisting_dir/vm-serial.log': No such file or directory
	
Expected Results:

Make sure you can get the exactly the same output with action steps.
Notes:
Comments:

		176772 	[Console and serial devices] Serial console disconnection after guest os gets shut down 	vbian 	None 	Manual (Autoproposed) 		Function 	P2 	6800 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices

bug:

    No bug found

Actions:

1. Add console=ttyS0,115200 to a guest kernel line.
   2. Add the following XML section to the guest.

    <serial type='pty'>
    <target port='0'/>
    </serial>
    <console type='pty'>
    <target port='0'/>
    </console>

   3. Start the domain.
   4. Run #virsh console [domain id].
   5. Log on to the guest.

   6. in the guest , perform "# shutdown -h now"
	
Expected Results:

6 , after step 6, the serial console connection get disonccected
Notes:
Comments:

		177587 	[snapshot]virsh snapshot-edit should edit disk snapshots --bug 744071 	zhpeng 	zhpeng 	Manual 		Regression 	P1 	6800 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    snapshot
    Regression

bug:

    No bug found

Actions:

1. virsh start dom
2. virsh snapshot-create-as dom snap
3. virsh snapshot-edit dom snap # try adding <description>
like:
<domainsnapshot>
<description>Snapshot of dom</description>
  <name>snap</name>
  <state>running</state>
  <parent>
    <name>1319531063</name>
  </parent>
  <creationTime>1319532663</creationTime>
.......



	
Expected Results:

Step3 should not error like:

error: Failed to update snap
error: argument unsupported: unable to handle disk requests in snapshot

should be:
Snapshot snap edited.

 
Notes:
Comments:

		176775 	[console and serial devices] vc console connection and operations 	vbian 	None 	Manual 		Feature 	P2 	6810 	Edit
Setup:

<domain type='kvm' id='5'>
  <name>test</name>
  <uuid>1490cddf-cd72-c1a0-ac25-9a8ad0d81d56</uuid>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <alias name='ide0-1-0'/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/cdrom_test.img'/>
      <target dev='hda' bus='ide'/>
      <alias name='ide0-0-0'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <alias name='ide0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <controller type='virtio-serial' index='0'>
      <alias name='virtio-serial0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:b5:d6:4c'/>
      <source network='default'/>
      <target dev='vnet0'/>
      <alias name='net0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <serial type='pty'>
      <source path='/dev/pts/0'/>
      <target port='0'/>
      <alias name='serial0'/>
    </serial>
    <serial type='vc'>
      <target port='1'/>
      <alias name='serial1'/>
    </serial>
    <console type='pty' tty='/dev/pts/0'>
      <source path='/dev/pts/0'/>
      <target type='serial' port='0'/>
      <alias name='serial0'/>
    </console>
    <channel type='pty'>
      <source path='/dev/pts/3'/>
      <target type='virtio' name='test.virtio.pty'/>
      <alias name='channel0'/>
      <address type='virtio-serial' controller='0' bus='0' port='0'/>
    </channel>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='5904' autoport='no' listen='0.0.0.0'/>
    <sound model='ac97'>
      <alias name='sound0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <alias name='video0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <alias name='balloon0'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
  <seclabel type='dynamic' model='selinux'>
    <label>system_u:system_r:svirt_t:s0:c134,c485</label>
    <imagelabel>system_u:object_r:svirt_image_t:s0:c134,c485</imagelabel>
  </seclabel>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    console and serial devices

bug:

    No bug found

Actions:

1.Define the guest with the setup xml file, start the guest , and edit the guest grub.conf file like this
    root (hd0,0)
    kernel /vmlinuz-2.6.32-71.el6.x86_64 ro root=/dev/mapper/VolGroup-lv_root rd_LVM_LV=VolGroup/lv_root rd_LVM_LV=VolGroup/lv_swap rd_NO_LUKS rd_NO_MD rd_NO_DM LANG=en_US.UTF-8 SYSFONT=latarcyrheb-sun16 KEYBOARDTYPE=pc KEYTABLE=us crashkernel=auto rhgb quiet console=tty0 console=ttyS1,115200n8
    initrd /initramfs-2.6.32-71.el6.x86_64.img

2. reboot the guest , in the vncviewer , and then send key "Ctrl+Alt+F2" to access the vc serial console
3. login to the guest from the vc console , and try to do following operations:
   # service network restart
   # ifconfig
   # ping $host_IP
   # hostname
	
Expected Results:

2. after rebooting , you could see the output from vc console
3. you could do all of the operations from vc console
Notes:
Comments:

		177641 	[storage] Clone a new vol from the source vol - Bug 671104 	nzhang 	nzhang 	Manual (Autoproposed) 		Regression 	P2 	6840 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    storage
    Regression

bug:

    No bug found

Actions:

# virsh vol-create-as --pool default --name foo --capacity 6G --allocation 0
--format qcow2
Vol foo created

# virsh vol-info --pool default foo
Name:           foo
Type:           file
Capacity:       6.00 GB
Allocation:     140.00 KB

# virsh vol-clone --pool default --vol foo --newname foo1
Vol foo1 cloned from foo

	
Expected Results:

The volume should be cloned successfully, verify no following error occurs.

error: Failed to clone vol from foo
error: Cannot run /usr/bin/qemu-img to create /var/lib/libvirt/images/foo1:
Invalid argument

ï»¿
Notes:
Comments:

		176744 	Create domains with unsupported charactors -- Bug 638962 	vbian 	None 	Manual 		Regression 	P1 	6850 	Edit
Setup:

1. Create a template image
   # virsh create /var/lib/libvirt/images/pure.img
2. Create a template xml file
   # cat template.xml
<domain type='kvm'>
  <name>XXXX</name>
  <memory>524288</memory>
  <currentMemory>524288</currentMemory>
  <vcpu>1</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.1.0'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/libvirt/images/pure.img'/>
      <target dev='hda' bus='ide'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
    </controller>
    <interface type='network'>
      <mac address='52:54:00:6a:7d:f5'/>
      <source network='default'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
    </interface>
    <interface type='network'>
      <mac address='52:54:00:a4:fd:07'/>
      <source network='default'/>
      <model type='rtl8139'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x06' function='0x0'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <sound model='ac97'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x05' function='0x0'/>
    </memballoon>
  </devices>
</domain>
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    No tag found

bug:

    No bug found

Actions:

1.Perform the following command to verify the special charactor supporting:
  # for i in {\<,\>,\~,\`,\!,\@,\#,\$,\%,\^,\&,\(,\),\+,\=,\?,\|,\\\,\\/,\',\",\:,\;,\,,\*}; do cp template.xml test.xml; sed -i -e "s/<name>XXXX<\/name>/<name>$i<\/name>/g" test.xml ; virsh create test.xml;rm -f test.xml; done

	
Expected Results:

You get all of the charactors FAILED to create the domain
Notes:
Comments:

		177645 	[Storage] Delete a storage pool - Bug 496579 	nzhang 	nzhang 	Auto 		Feature 	P2 	6850 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    storage
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. Define a storage pool with following XML.

<pool type="dir">
  <name>test-dir</name>
  <target>
    <path>/var/lib/libvirt/images/test-dir</path>
  </target>
</pool>


# virsh pool-define test-dir.xml
Pool test-dir defined from test-dir.xml
# ll /var/lib/libvirt/images/test-dir
ls: cannot access /var/lib/libvirt/images/test-dir: No such file or directory

2. Check if the pool was defined in the domain list.
# virsh pool-list --all
Name                 State      Autostart 
-----------------------------------------
default              active     yes       
pool-mig             inactive   no        
pool-migration       inactive   no        
test-dir             inactive   no        


3. Build the pool
# virsh pool-build test-dir
Pool test-dir built
# ll /var/lib/libvirt/images/test-dir
total 0



4. Delete the storage pool.
# virsh pool-delete test-dir
Pool test-dir deleted
# ll /var/lib/libvirt/images/test-dir
ls: cannot access /var/lib/libvirt/images/test-dir: No such file or directory

5. # virsh pool-list --all
Name                 State      Autostart 
-----------------------------------------
default              active     no        
test-dir             inactive   no      



	
Expected Results:

The storage pool can be deleted successfully, and no errors.

Acctually virsh delete means virsh unbuild.
Notes:
Comments:

		177695 	[sVirt] Failed to reload iptables rules with root permission when starting libvirtd in the way of executing /usr/sbin/libvirtd - bug 716612 	gsun 	gsun 	Manual 		Regression 	P2 	6880 	Edit
Setup:

Modify /etc/libvirt/libvirtd.conf logging controls part

log_level = 1
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt
    Regression

bug:

    No bug found

Actions:

1. service libvirtd stop
2. /usr/sbin/libvirtd
3. checkout the libvirtd log

	
Expected Results:

1. # service libvirtd stop
Stopping libvirtd daemon:                                  [  OK  ]

2. The log output with error

3.

 In the libvirtd.log it showed error as the follows.

21:51:55.634: 11972: debug : virCommandRunAsync:1310 : About to run
/sbin/iptables --table nat --delete POSTROUTING --source 192.168.122.0/24 -p
tcp ! --destination 192.168.122.0/24 --jump MASQUERADE --to-ports 1024-65535
21:51:55.634: 11972: debug : virExecWithHook:488 : /sbin/iptables --table nat
--delete POSTROUTING --source 192.168.122.0/24 -p tcp ! --destination
192.168.122.0/24 --jump MASQUERADE --to-ports 1024-65535
21:51:55.634: 11972: debug : virCommandRunAsync:1326 : Command result 0, with
PID 11979
21:51:55.638: 11972: error : virCommandWait:1393 : internal error Child process
(/sbin/iptables --table nat --delete POSTROUTING --source 192.168.122.0/24 -p
tcp ! --destination 192.168.122.0/24 --jump MASQUERADE --to-ports 1024-65535)
status unexpected: exit status 1
21:51:55.638: 11972: debug : virCommandRun:1147 : Result status 0, stdout: ''
stderr: '21:51:55.635: 11979: info : libvirt version: 0.9.2, package: 1.el6
(Red Hat, Inc. <http://bugzilla.redhat.com/bugzilla>, 2011-06-20-09:08:08,
hs20-bc2-5.build.redhat.com)
21:51:55.635: 11979: debug : virCommandHook:1244 : Hook is done 0
libvir: error : cannot execute binary /sbin/iptables: Permission denied

This is expected, libvirtd can not be started for not with right selinux domain type (virt_t). 

 
Notes:
Comments:

		177723 	[sVirt] sVirt error message handling for NFS permission denial 	jialiu 	jialiu 	Manual 		Regression 	P1 	6890 	Edit
Setup:

Make sure your os is in enforcing mode, and allow virt nfs access.

# setenforce 1

# getenforce
Enforcing

 set log_outputs="1:file:/tmp/libvirtd.log" in /etc/libvirt/libvirtd.conf and restart libvirt

 

Reference bug:

Bug 589922 - permission denied error for NFS image, should libvirt error message mention virt_use_nfs?
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    sVirt
    Regression

bug:

    No bug found

Actions:

1. Setup a nfs shrare dir with root_squash

# cat /etc/exports
/var/lib/libvirt/migrate/     *(rw,no_root_squash)

2. On test machine, mount the nfs share.

# mount <nfs_server_ip>:/var/lib/libvirt/migrate/ /var/lib/libvirt/migrate/
3. Make sure virt_use_nfs is off

# getsebool -a|grep virt_use_nfs
virt_use_nfs --> off

4. Start a domain.

5. Permission deny error is seen.

6. After virt_use_nfs is on, domain is started successfully.

# setsebool virt_use_nfs on
	
Expected Results:

5. Out put:

libvirt should check for this and throw an error if the boolean
isn't set. If we can't reliably detect that it is required, we can at least try
and tack on a 'enable virt_use_nfs' to a QEMU error if we know the image was
NFS and security is dynamic selinux.

Notes:
Comments:

		177763 	[Virtio] Create domain with virtio block device driver (only for Linux) 	yimwang 	yimwang 	Auto 		Regression 	P1 	6920 	Edit
Setup:
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    Regression
    virsh-rail
    installation

bug:

    No bug found

Actions:

1.Create  disk image.

# qemu-img create /var/lib/libvirt/images/virtio.img 8G

2. Defined  virtio guest.

For Windows:

#cat virtio.xml

<domain type='kvm'>
  <name>virtio</name>+
  <uuid>6785f9df-12e9-3f5a-32b3-39de72020e12</uuid>
  <memory>1048576</memory>
  <currentMemory>1048576</currentMemory>
  <vcpu>2</vcpu>
  <os>
    <type arch='x86_64' machine='rhel6.0.0'>hvm</type>
    <boot dev='cdrom'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='utc'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>destroy</on_reboot>
  <on_crash>destroy</on_crash>
  <devices>
    <emulator>/usr/libexec/qemu-kvm</emulator>
    <disk type='block' device='cdrom'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/sr0'/>
      <target dev='hdc' bus='ide'/>
      <readonly/>
      <address type='drive' controller='0' bus='1' unit='0'/>
    </disk>
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw'/>
      <source file='/var/lib/libvirt/images/virtio.img'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <controller type='ide' index='0'>
    </controller>
    <controller type='fdc' index='0'/>
    <interface type='network'>
      <mac address='52:54:00:e7:7e:04'/>
      <source network='default'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/>
    <sound model='ac97'>
    </sound>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
    </video>
    <memballoon model='virtio'>
    </memballoon>
  </devices>
</domain>

2. Start guest.

# virsh start virtio

3.Install the Linux domain guest

4.After Install finished,edit domain XML.

  #virsh edit  virtio

 Delete that xml " <boot dev='cdrom'/>" in domain  XML

 5.Start domain guest.

#virsh start virtio

 
	
Expected Results:

3. The guest could be installed successfully

no error output

5.1 Could create folder/file or edit folder/file successfully,

could create file using dd successfully

5.2 firefox or IE application should be launched

5.3 The output value should be nearly equal to the following value given in xml confile file

  <memory>1048576</memory>

5.4  vcpu number should be equal to the following value given in xml config file

  <vcpu>2</vcpu>

5.5. Should ping to host successfully
Notes:
Comments:

		177847 	[Watch dog device] Watchdog device "ib700" - shutdown (bug667090) 	yimwang 	yimwang 	Auto 		Regression 	P3 	6960 	Edit
Setup:

Bug 667090 - kernel needs ib700 driver
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='ib700' action='shutdown'/>to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, load the mode and check the watchdog dirvier is installed:

#modprobe ib700wdt
# dmesg | grep ib700
# /sbin/lsmod | grep ib700

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

NOTE:(RHEL-3U9-i386 guest does not have ACPI mode)

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm

.........................

-watchdog ib700

-watchdog-action shutdown

5.watchdong software is installed successfully.

8.After 130 seconds, the guest is shutdown automatically.
Notes:
Comments:

		177848 	[Watch dog device]Watchdog device "ib700" - none(bug667090) 	yimwang 	yimwang 	Auto 		Regression 	P3 	6970 	Edit
Setup:

Bug 667090 - kernel needs ib700 driver
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='ib700' action='none'/>t to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, Load the ib700 mode and check the watchdog dirvier is installed:


#modprobe ib700wdt

# dmesg | grep ib700

# /sbin/lsmod | grep ib700

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm

.........................

-watchdog ib700

-watchdog-action none

5.watchdong software is installed successfully.

8.After 130 seconds, guest status have not changed.
Notes:
Comments:

		177849 	[Watch dog device]Watchdog device "ib700" - pause(bug667090) 	yimwang 	yimwang 	Auto 		Regression 	P3 	6980 	Edit
Setup:

Bug 667090 - kernel needs ib700 driver
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='ib700' action='pause'/>to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest,load the ib700 mode and check the watchdog dirvier is installed:

#modprobe ib700wdt

# dmesg | grep ib700

# /sbin/lsmod | grep ib700

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm

.........................

-device ib700,id=watchdog0 -watchdog-action pause

5.watchdong software is installed successfully.

8. After about 2 minutes, the guest is paused automatically.
Notes:
Comments:

		177850 	[Watch dog device]Watchdog device "ib700" - reset(bug667090) 	yimwang 	yimwang 	Auto 		Regression 	P3 	6990 	Edit
Setup:

Bug 667090 - kernel needs ib700 driver
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    watchdog
    Regression
    virsh-rail

bug:

    No bug found

Actions:

1. Prepare a vm.

2. Add <watchdog model='ib700' action='reset'/> to the <devices> section of the XML, and start the guest.

3.Observe the command line parameters to qemu-kvm:

#ps ax | grep qemu-kvm

4. In guest, load the ib700 mode and check the watchdog dirvier is installed:

#modprobe ib700wdt

# dmesg | grep ib700

# /sbin/lsmod | grep ib700

5. In guest, install watchdog software.

# yum install watchdog

6. In guest, adjust settings in /etc/watchdog.conf:
interval        = 130
watchdog-device = /dev/watchdog

** should be larger than 60.

7. In guest, start watchdog process.
# watchdog -f

8. Wait for some minutes, check the status of guest.
	
Expected Results:

 

2. watchdog device is added successfully to domain xml config file.

3. Verify that the following parameters have been added
to the qemu-kvm command line:

#ps ax | grep qemu-kvm

.........................

-device ib700,id=watchdog0 -watchdog-action reset

8. After about 2 minutes, the guest is restart automatically.
Notes:
Comments:

		176945 	[Guest kernel debugging]Check the dumped images can be analyzed with crash tool sucessfully 	yupzhang 	yupzhang 	Manual 		Function 	P1 	7000 	Edit
Setup:

Please update crash to lastest version to aviod bug Bug 843093

You can refer the changlog of crash on brewweb for more details
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    guest kernel debugging

bug:

    No bug found

Actions:

1.Start a guest
 # virsh start rhel5u5

2. Dump the core of a domain 
#virsh dump rhel5u5 /tmp/dump

3. # ll -h /tmp

4. Check guest kernel version
#uname -r
2.6.18-183.el5

5. Install the same version kernel-debug package on host
# rpm -ivh kernel-debuginfo-2.6.18-183.el5.x86_64.rpm kernel-debuginfo-common-2.6.18-183.el5.x86_64.rpm
Preparing...                ########################################### [100%]
   1:kernel-debuginfo-common########################################### [ 50%]
   2:kernel-debuginfo       ########################################### [100%]

6.# crash /usr/lib/debug/lib/modules/2.6.18-183.el5/vmlinux /tmp/dump 

	
Expected Results:

1. # virsh start rhel5u5
Domain rhel5u5 started

2.#virsh dump rhel5u5 /tmp/dump

Domain rhel5u5 dumped to /tmp/dump

3.# ll -h /tmp
total 445M
-rw-r--r-- 1 root root 445M Dec 30 15:24 dump

6. Check the dumped images can be analyzed with crash tool sucessfully

# crash /usr/lib/debug/lib/modules/2.6.18-183.el5/vmlinux /tmp/dump 
crash 4.1.2-1.el5
Copyright (C) 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009  Red Hat, Inc.
Copyright (C) 2004, 2005, 2006  IBM Corporation
Copyright (C) 1999-2006  Hewlett-Packard Co
Copyright (C) 2005, 2006  Fujitsu Limited
Copyright (C) 2006, 2007  VA Linux Systems Japan K.K.
Copyright (C) 2005  NEC Corporation
Copyright (C) 1999, 2002, 2007  Silicon Graphics, Inc.
Copyright (C) 1999, 2000, 2001, 2002  Mission Critical Linux, Inc.
This program is free software, covered by the GNU General Public License,
and you are welcome to change it and/or distribute copies of it under
certain conditions.  Enter "help copying" to see the conditions.
This program has absolutely no warranty.  Enter "help warranty" for details.

GNU gdb 6.1                                     
Copyright 2004 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB.  Type "show warranty" for details.
This GDB was configured as "x86_64-unknown-linux-gnu"...

      KERNEL: /usr/lib/debug/lib/modules/2.6.18-183.el5/vmlinux
    DUMPFILE: /tmp/dump
        CPUS: 1
        DATE: Wed Dec 30 15:24:09 2009
      UPTIME: 00:22:14
LOAD AVERAGE: 0.02, 0.05, 0.23
       TASKS: 91
    NODENAME: localhost.localdomain
     RELEASE: 2.6.18-183.el5
     VERSION: #1 SMP Mon Dec 21 18:37:42 EST 2009
     MACHINE: x86_64  (2992 Mhz)
      MEMORY: 1 GB
       PANIC: ""
         PID: 0
     COMMAND: "swapper"
        TASK: ffffffff80308b60  [THREAD_INFO: ffffffff803fa000]
         CPU: 0
       STATE: TASK_RUNNING (ACTIVE)
     WARNING: panic task not found

crash> bt
PID: 0      TASK: ffffffff80308b60  CPU: 0   COMMAND: "swapper"
 #0 [ffffffff803fbeb8] schedule at ffffffff80063f96
 #1 [ffffffff803fbec0] thread_return at ffffffff80063ff8
 #2 [ffffffff803fbf68] default_idle at ffffffff8006c3a5
 #3 [ffffffff803fbf90] cpu_idle at ffffffff800497b7
crash> 

 
Notes:
Comments:

		177478 	[Scalability] 256 autostarted storage pools reloading 	jiachen 	None 	Auto 		Stress 	P2 	7010 	Edit
Setup:

1.  cat pool.xml
<pool type="dir">
    <name>#pool-name#</name>
    <target>
       <path>#pool-path#</path>
    </target>
</pool>

2. Also generate other types of pools refering

124818 [Storage] Logical based storage pool

124819 [Storage] mpath based storage pool

124820 [Storage] netfs based storage pool

124805[Storage] FileSystem based storage pool

124815 [Storage] iSCSI based storage pool

Define and start above pools manually
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    libvirt

Tag:

    scalability

bug:

    No bug found

Actions:

 

1.Save the following script into pool-testing.sh , then

# sh pool-testing.sh pool.xml

#!/bin/sh

#make sure there is no more than 256 pools exist
existing_pool_num=$(virsh pool-list --all|awk 'NR>2'|wc -l)
if [[ $existing_pool_num -gt 1 ]];then
    for i in $(virsh pool-list --all|awk 'NR>2'|awk -F' ' '{ if($1 != "default") print $1}')
    do
        virsh pool-destroy $i
        virsh pool-undefine $i
    done
fi

name="pool"
parent_path="/var/lib/libvirt/images"
for i in {1..255}
do
    new_path=$parent_path/$name$i
    sed -i -e "s,#pool-name#,$name$i,g" pool.xml
    sed -i -e "s,#pool-path#,$new_path,g" pool.xml
    virsh pool-define pool.xml
    mkdir -p $new_path
    sed -i -e "s,$new_path,#pool-path#,g" pool.xml
    sed -i -e "s,$name$i,#pool-name#,g" pool.xml
done

for k in {1..255}
do
    virsh pool-start pool$k
done

virsh pool-start default

for j in {1..10} ;do
#reload 256 pool

echo "------${j}-------"

echo
service libvirtd restart
wait
running_pool_num=$(virsh pool-list --all |grep active |wc -l)
if [[ $running_pool_num -lt 256 ]];then
    echo "pools reloading failed"
else
    echo "succeed"
fi

done
	
Expected Results:


Make sure you get Succeed return value after performing the script

Note: for dir pool after restart libvirtd inactive pool will became active even if without autostart mark
Notes:
Comments:

		177594 	[SR-IOV] Assign seven VFs to the same guest. 	yoyzhang 	yoyzhang 	Manual 		Feature 	P2 	7020 	Edit
Setup:

VT-D is enabled
	
Breakdown:
	
Attachment:

    No attachment found

Component:

    No component found

Tag:

    SR-IOV

bug:

    No bug found

Actions:

1. Assign the first VF to guest following test steps in  test cases 'Assign one VF to one guest'

2. Assign the second VF to guest following the same test steps in test cases 'Assign one VF to one guest', except VF info should be updated to second VF such as below example:

step 7:

# virsh nodedev-dumpxml pci_8086_10ca_0

According to <bus> <slot> <function> value, transfer to 42:11.5 , find the match VF with command

# lspci | grep 42:11.5
42:11.5 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

step8:

Add this VF xml info to VM config xml (kvm.xml)

...
          <hostdev mode='subsystem' type='pci'>
            <source>
              <address bus='66' slot='17' function='5'/>
            </source>
          </hostdev>
          ...

step 9:

# virsh nodedev-dettach pci_8086_10ca_0

step 12:

On the guest

#lscpi | grep 82576

# service network restart

# ifconfig

# ping {host}

3. Assign the 3rd or 4th, .... VF to guest following the same test steps in test cases 'Assign one VF to one guest', except VF info should be updated to 3rd, 4th, ....VF
	
Expected Results:

1. The first VF is assigned to guest successfully

2. Output of step7 :

<device>
  <name>pci_8086_10ca_0</name>
  <parent>pci_8086_340a</parent>
  <capability type='pci'>
    <domain>0</domain>
    <bus>66</bus>
    <slot>17</slot>
    <function>5</function>
    <product id='0x10ca'>82576 Virtual Function</product>
    <vendor id='0x8086'>Intel Corporation</vendor>
  </capability>
</device>

Output of step 9:

Device pci_8086_10ca dettached

Output of step 12:

00:04.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

00:05.0 Ethernet controller: Intel Corporation 82576 Virtual Function (rev 01)

And could get ip and ping host successfully
Notes:
Comments:

